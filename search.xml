<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Nginx反向代理/配置]]></title>
    <url>%2F2022%2F03%2F29%2FNginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[原始表格]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx配置示例1]]></title>
    <url>%2F2022%2F03%2F29%2Fnginx%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B1%2F</url>
    <content type="text"><![CDATA[Nginx:1.20.1版本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130# For more information on configuration, see:# * Official English Documentation: http://nginx.org/en/docs/# * Official Russian Documentation: http://nginx.org/ru/docs/# 使用用户和组# user nginx nginx;# 使用使用用户user nginx;# 指定工作进程数（一般等于CPU的总核数或者总核数的2倍）# 默认值为 auto（根据核心数自动生成对应数量的work进程）。worker_processes 8;# 指定错误日志存放位置error_log /var/log/nginx/error.log info;# 指定pid文件位置pid /run/nginx.pid; # 指定文件描述符数量（默认不配置）# 将此值增加到大于worker_processes * worker_connections的值worker_rlimit_nofile 51200; # Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.# 加载本地动态模块include /usr/share/nginx/modules/*.conf; events &#123;# 使用的网络I/O模型，Linux系统推荐epoll模型，FreeBSD系统推荐kqueue模型（默认不配置） use epoll;# 允许的最大连接数（默认500） worker_connections 51200;&#125; http &#123; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"';# 指定访问日志位置 access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 4096; include /etc/nginx/mime.types; default_type application/octet-stream; # 设置使用的字符集，如果网站有多个字符集，不要随便设置，应让程序员在HTML代码中通过Meta标签设置# 默认不配置# charset gb2312 # 设置客户端能够上传的文件大小（默认不配置） client_max_body_size 8m; # 开启gzip压缩(默认不配置) gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.1; gzip_comp_level 3; gzip_types text/plain application/javascript application/json application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png; gzip_vary on; # 静态缓存 proxy_connect_timeout 10; proxy_read_timeout 180; proxy_send_timeout 5; proxy_buffer_size 16k; proxy_buffers 4 32k; proxy_busy_buffers_size 96k; proxy_temp_file_write_size 96k; proxy_temp_path /tmp/temp_dir; proxy_cache_path /tmp/cache levels=1:2 keys_zone=cache_one:100m inactive=1d max_size=10g; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; server &#123; listen 80; listen [::]:80; server_name _; root /usr/share/nginx/html; # 开启目录文件列表（此位置开启全局，放到location下，只开启对应路劲的列表） autoindex on; autoindex_exact_size on; autoindex_localtime on; charset utf-8,gbk; # 设置列表字符集 # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; error_page 404 /404.html; location = /404.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; &#125; # Settings for a TLS enabled server.## server &#123;# listen 443 ssl http2;# listen [::]:443 ssl http2;# server_name _;# root /usr/share/nginx/html;## ssl_certificate "/etc/pki/nginx/server.crt";# ssl_certificate_key "/etc/pki/nginx/private/server.key";# ssl_session_cache shared:SSL:1m;# ssl_session_timeout 10m;# ssl_ciphers HIGH:!aNULL:!MD5;# ssl_prefer_server_ciphers on;## # Load configuration files for the default server block.# include /etc/nginx/default.d/*.conf;## error_page 404 /404.html;# location = /40x.html &#123;# &#125;## error_page 500 502 503 504 /50x.html;# location = /50x.html &#123;# &#125;# &#125; &#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes学习五-kubernetes集群二进制安装]]></title>
    <url>%2F2022%2F02%2F23%2Fkubernetes%E5%AD%A6%E4%B9%A0%E4%BA%94-kubernetes%E9%9B%86%E7%BE%A4%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、简介今天这篇文章教给大家如何快速部署一套Kubernetes集群。K8S集群部署有几种方式：kubeadm、minikube和二进制包。前两者属于自动部署，简化部署操作，我们这里强烈推荐初学者使用二进制包部署，因为自动部署屏蔽了很多细节，使得对各个模块感知很少，非常不利用学习。 所以，这篇文章也是使用二进制包部署Kubernetes集群。 二、环境规划环境说明： 软件 版本 Centos 7 etcd v3.5.2 Docker v20.10.12 kubelet v1.23.3 kubectl v1.23.3 关闭selinux。 角色 IP 组件 master 192.168.164.128 etcd kube-apiserver kube-controller-manager kube-scheduler docker node01 192.168.164.129 kubelet kube-proxy docker node02 192.168.164.130 kubelet kube-proxy docker 三、部署集群3.1下载二进制包打开下面网址，下载下面两个红色框框的包。 kubernetes/CHANGELOG-1.23.md at master · kubernetes/kubernetes · GitHub 下载完成后，上传到服务器： kubernetes-server-linux-amd64.tar.gz 上传到master节点。 kubernetes-node-linux-amd64.tar.gz 上传到node节点。 卸载旧程序： 12345678910$ sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 3.2 master节点操作3.2.1 安装etcdetcd服务作为Kubernetes集群的主数据库，在安装Kubernetes各服务之前需要首先安装和启动。 1234567# yum install etcd –y# vi /etc/etcd/etcd.conf 修改：ETCD_NAME="default"ETCD_DATA_DIR="/var/lib/etcd/default"ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"ETCD_ADVERTISE_CLIENT_URLS="http://0.0.0.0:2379" 1234567启动：$ systemctl daemon-reload$ systemctl enable etcd$ systemctl start etcd$ etcdctl endpoint health0.0.0.0:2379 is healthy: successfully committed proposal: took = 4.295226ms 注意：Ubuntu系统etcd配置文件在/etc/default/etcd。 3.2.2 安装Master节点组件12345$ tar zxvf kubernetes-server-linux-amd64.tar.gz$ mkdir -p /opt/kubernetes/&#123;bin,cfg&#125;$ mv kubernetes/server/bin/&#123;kube-apiserver,kube-scheduler,kube-controller-manager,kubectl&#125; /opt/kubernetes/bin 3.2.2.1 Apiserver新建配置文件： #vi /opt/kubernetes/cfg/kube-apiserver 添加： 12345678910111213141516# 启用日志标准错误KUBE_LOGTOSTDERR="--logtostderr=true"# 日志级别KUBE_LOG_LEVEL="--v=4"# Etcd服务地址KUBE_ETCD_SERVERS="--etcd-servers=http://192.168.164.128:2379"# API服务监听地址KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"# API服务监听端口KUBE_API_PORT="--insecure-port=8080"# 对集群中成员提供API服务地址KUBE_ADVERTISE_ADDR="--advertise-address=192.168.164.128"# 允许容器请求特权模式，默认falseKUBE_ALLOW_PRIV="--allow-privileged=false"# 集群分配的IP范围KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.10.10.0/24" 新建systemd服务文件：/lib/systemd/system/kube-apiserver.service 添加： 1234567891011121314151617181920212223242526272829303132333435[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-apiserver\#ExecStart=/opt/kubernetes/bin/kube-apiserver $&#123;KUBE_APISERVER_OPTS&#125;ExecStart=/opt/kubernetes/bin/kube-apiserver \$&#123;KUBE_LOGTOSTDERR&#125; \$&#123;KUBE_LOG_LEVEL&#125; \$&#123;KUBE_ETCD_SERVERS&#125; \$&#123;KUBE_API_ADDRESS&#125; \$&#123;KUBE_API_PORT&#125; \$&#123;KUBE_ADVERTISE_ADDR&#125; \$&#123;KUBE_ALLOW_PRIV&#125; \$&#123;KUBE_SERVICE_ADDRESSES&#125;Restart=on-failure[Install]WantedBy=multi-user.target 启动服务，并设置开机启动： 123$ systemctl daemon-reload$ systemctl enable kube-apiserver$ systemctl restart kube-apiserver 注意：apiserver默认支持etcd3，如果是etcd2，需启动时指定版本选项–storage-backend=etcd2 参考文献：docker-compose教程：https://blog.csdn.net/pushiqiang/article/details/78682323 //用于把多个服务串联起来统一管理 Kubernetes常用命令：https://www.cnblogs.com/FRESHMANS/p/8444098.html Kubernetes中文社区：https://www.kubernetes.org.cn/k8s 群集搭建：https://blog.csdn.net/zhenliang8/article/details/78611004 web管理平台portainer：https://www.cnblogs.com/gao88/p/7011175.html]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes学习四-kubernetes配置NFS存储类]]></title>
    <url>%2F2022%2F02%2F15%2Fkubernetes%E5%AD%A6%E4%B9%A0%E5%9B%9B-kubernetes%E9%85%8D%E7%BD%AENFS%E5%AD%98%E5%82%A8%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[1 环境k8s集群基于kubeadm工具部署，过程参考《kubernetes学习二-kubeadm安装kubernetes集群-单master》 kubernetes版本：1.23.3 主机 IP 角色 node1 192.168.1.201 master node2 192.168.1.202 node node3 192.168.1.203 nfs server 2 准备NFS服务器123456789101112yum -y install nfs-utils rpcbindsystemctl start rpcbind.service systemctl start nfs#配置, * 代表所有IPmkdir /mnt/nfs -pchown nfsnobody.nfsnobody /data/nfscat&gt;&gt;/etc/exports&lt;&lt;EOF/mnt/nfs *(rw,sync,no_root_squash,no_all_squash)EOFexportfs -arv 2 kubernetes配置参考：https://www.cnblogs.com/zhangb8042/p/14252294.html 2.1 所有kubernetes集群节点安装nfs-utils1yum install -y nfs-utils 验证nfs服务器挂载点 123[root@node1 nfs]# showmount -e 192.168.1.203Export list for 192.168.1.203:/mnt/nfs * 2.2 master导入nfs的yaml配置github地址: https://github.com/kubernetes-retired/external-storage 2.2.1 下载yaml配置123wget https://raw.githubusercontent.com/kubernetes-retired/external-storage/master/nfs-client/deploy/rbac.yamlwget https://raw.githubusercontent.com/kubernetes-retired/external-storage/master/nfs-client/deploy/class.yamlwget https://raw.githubusercontent.com/kubernetes-retired/external-storage/master/nfs-client/deploy/deployment.yaml 2.2.2 导入rbac与class12kubectl apply -f class.yamlkubectl apply -f rbac.yaml 2.2.3 修改deploymet文件12345678910111213141516171819202122232425262728293031323334353637383940[root@master ~]# cat deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: defaultspec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.1.203 #nfs服务器ip - name: NFS_PATH value: /mnt/nfs #nfs服务器的挂载目录 volumes: - name: nfs-client-root nfs: server: 192.168.1.2036 #nfs服务器ip path: /mnt/nfs #nfs服务器的挂载目录 2.2.4 导入deployment123456[root@master ~]# kubectl get pod NAME READY STATUS RESTARTS AGEnfs-client-provisioner-dfb75c8bb-42p8l 1/1 Running 0 80s[root@master ~]# kubectl get scNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEmanaged-nfs-storage fuseim.pri/ifs Delete Immediate false 5m56s 2.2.5 设置managed-nfs-storage为kubernetes的默认存储后端 我们可以用kubectl patch命令来更新 1234# 设置managed-nfs-storage为默认存储kubectl patch storageclass managed-nfs-storage -p '&#123;"metadata": &#123;"annotations":&#123;"storageclass.kubernetes.io/is-default-class":"true"&#125;&#125;&#125;'# 取消设置managed-nfs-storage为默认存储kubectl patch storageclass managed-nfs-storage -p '&#123;"metadata": &#123;"annotations":&#123;"storageclass.kubernetes.io/is-default-class":"false"&#125;&#125;&#125;' 也可以修改class.yaml文件 增加如下2行，设置为默认存储类 123456789apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: managed-nfs-storage annotations: # 配置默认存储类 storageclass.kubernetes.io/is-default-class: "true" # 配置默认存储类provisioner: nfsserver-test # or choose another name, must match deployment's env PROVISIONER_NAME'parameters: archiveOnDelete: "false" 验证 123[root@node1 nfs]# kubectl get scNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEmanaged-nfs-storage (default) nfsserver-test Delete Immediate false 160m 2.2.6 修改api-server的配置修改/etc/kubernetes/manifests/kube-apiserver.yaml 文件 添加添加- –feature-gates=RemoveSelfLink=false 1234567[root@master ~]# grep -B 5 'feature-gates' /etc/kubernetes/manifests/kube-apiserver.yaml - --service-account-key-file=/etc/kubernetes/pki/sa.pub - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key - --service-cluster-ip-range=10.96.0.0/12 - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --feature-gates=RemoveSelfLink=false #添加内容 修改后 apiserver会自动重启。 2.2.7 测试12345678910111213141516171819[root@master ~]# cat test-pvc.yamlkind: PersistentVolumeClaimapiVersion: v1metadata: name: test-claim annotations: volume.beta.kubernetes.io/storage-class: "managed-nfs-storage"spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi[root@master ~]# kubectl apply -f test-pvc.yamlpersistentvolumeclaim/test-claim created [root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-claim Bound pvc-eeb6dd0b-feb8-4884-999a-a15bcd27d7ea 1Mi RWX managed-nfs-storage 9m18s 3 问题汇总3.1 pvc一直Pengding现象： 1234567891011121314151617181920212223242526# pvc状态[root@node1 nfs]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-claim Pending managed-nfs-storage 3s# pvc信息[root@node1 nfs]# kubectl describe pvc test-claim Name: test-claimNamespace: defaultStorageClass: managed-nfs-storageStatus: PendingVolume: Labels: &lt;none&gt;Annotations: volume.beta.kubernetes.io/storage-class: managed-nfs-storage volume.beta.kubernetes.io/storage-provisioner: nfsserver-test volume.kubernetes.io/storage-provisioner: nfsserver-testFinalizers: [kubernetes.io/pvc-protection]Capacity: Access Modes: VolumeMode: FilesystemUsed By: &lt;none&gt;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ExternalProvisioning 7s (x3 over 26s) persistentvolume-controller waiting for a volume to be created, either by external provisioner "nfsserver-test" or manually created by system administrator[root@node1 nfs]# 原因： 需修改api-server.yaml配置，添加- –feature-gates=RemoveSelfLink=false nfs挂载目录需要有权限 nfs存储类没有配置正确。（我遇到的问题是nfs的depoyment无法正常连接api-server，原因是集群初始化时候flannel的网络配置和默认网络不一致。） 解决办法： 参考 3.2 nfs的deployment异常 3.2 nfs的deployment异常参考： Kubeadm 部署 使用flannel无法连接service/kubernetes 现象：deployment创建的pod总是异常挂掉或无法正常启动。 123456789101112131415# nfs deploy状态[root@node1 nfs]# kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-b8b468858-m9c79 0/1 ContainerCreating 0 4m24s# 查看pod状态[root@node1 nfs]# kubectl describe pod nfs-client-provisioner-b8b468858-m9c79 ...... Normal SandboxChanged 20s (x12 over 31s) kubelet Pod sandbox changed, it will be killed and re-created. Warning FailedCreatePodSandBox 19s (x4 over 22s) kubelet (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "01a249d53627976e4d872e44004b436600bef233a5dee8814760e9e0cdabc015" network for pod "nfs-client-provisioner-b8b468858-m9c79": networkPlugin cni failed to set up pod "nfs-client-provisioner-b8b468858-m9c79_default" network: failed to delegate add: failed to set bridge addr: "cni0" already has an IP address different from 101.100.1.1/24# 查看pod日志[root@node1 nfs]# kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-b8b468858-m9c79 0/1 ContainerCreating 0 4m24s 1234567# 查看各节点flannel网络配置[root@node1 nfs]# cat /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=101.100.0.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=true[root@node1 nfs]# 原因：flanenl网络地址端配置不正确，FLANNEL_NETWORK和FLANNEL_SUBNET需要在一个网段。 解决办法： flannel网络的FLANNEL_NETWORK和FLANNEL_SUBNET需要在一个网段，使flannel的默认网段和集群初始化的cni网络在一个网段。重新安装kubernetes 使用默认–pod-network-cidr=10.244.0.0/16 （下面示例均使用101.100.0.0/16） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 查看flannel.yaml,发现flannel默认网络是10.244.0.0/16[root@node1 home]# cat flannel.yml..... net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;---.....# 配置集群初始化yaml文件，设置podSubnet:10.244.0.0/16(也可以配置为其他段，只需要保证和flannel.yaml中的网段即`FLANNEL_NETWORK`和`FLANNEL_SUBNET`需要在一个网段。)# 这里我直接修改flannel.yaml和初始化yaml文件内的网段均为101.100.0.0/16# 重新初始化安装kubernentes[root@node1 nfs]# cat /run/flannel/subnet.env FLANNEL_NETWORK=101.100.0.0/16FLANNEL_SUBNET=101.100.0.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=true[root@node1 nfs]# # 查看nfs pod和pvc均正常了[root@node1 nfs]# kubectl describe pod nfs-client-provisioner-b8b468858-gn2b6 ......Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 15s default-scheduler Successfully assigned default/nfs-client-provisioner-b8b468858-gn2b6 to node2 Normal Pulling 14s kubelet Pulling image "quay.io/external_storage/nfs-client-provisioner:latest" Normal Pulled 10s kubelet Successfully pulled image "quay.io/external_storage/nfs-client-provisioner:latest" in 4.239848452s Normal Created 10s kubelet Created container nfs-client-provisioner Normal Started 10s kubelet Started container nfs-client-provisioner[root@node1 nfs]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-claim Bound pvc-811d031b-e842-4103-bdaa-cc89f51e1fc8 1Mi RWX managed-nfs-storage 2m18s# pvc状态[root@node1 nfs]# kubectl describe pvc test-claim ......Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ExternalProvisioning 87s (x6 over 2m13s) persistentvolume-controller waiting for a volume to be created, either by external provisioner "nfsserver-test" or manually created by system administrator Normal ExternalProvisioning 7s persistentvolume-controller waiting for a volume to be created, either by external provisioner "nfsserver-test" or manually created by system administrator Normal Provisioning 5s nfsserver-test_nfs-client-provisioner-b8b468858-gn2b6_bb6b9b33-8e34-11ec-ab4a-ca0aed401744 External provisioner is provisioning volume for claim "default/test-claim" Normal ProvisioningSucceeded 5s nfsserver-test_nfs-client-provisioner-b8b468858-gn2b6_bb6b9b33-8e34-11ec-ab4a-ca0aed401744 Successfully provisioned volume pvc-811d031b-e842-4103-bdaa-cc89f51e1fc8]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes学习三-kubeadm安装k8s高可用集群]]></title>
    <url>%2F2022%2F02%2F09%2Fkubernetes%E5%AD%A6%E4%B9%A0%E4%B8%89-kubeadm%E5%88%9B%E5%BB%BAk8s%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[1 集群逻辑架构 这里主要基于k8s集群高可用负载均衡方案进行介绍，过程包含基于keepalived的高可用方案。 2 环境2.1 物理环境 主机 IP 配置 OS 角色 node1 192.168.1.7 2core 2GB Centos7.9 master1 node2 192.168.1.8 2core 2GB Centos7.9 master2 node3 192.168.1.9 2core 2GB Centos7.9 master3 node4 192.168.1.10 2core 2GB Centos7.9 工作节点 2.2 软件环境 软件 版本 docker v20.10.12 kubeadmin v1.23.3 kubelet v1.23.3 kubectl v1.23.3 keeplalived 1.3.5 haproxy 2.5 2.3 镜像 镜像 版本 conformance v1.23.3 kube-apiserver v1.23.3 kube-controller-manager v1.23.3 kube-proxy v1.23.3 kube-scheduler v1.23.3 flannel v1.0.1 3 步骤3.1 准备工作3.1.1 确保各节点MAC和product_uuid的唯一性 你可以使用命令 ip link 或 ifconfig -a 来获取网络接口的 MAC 地址 可以使用 sudo cat /sys/class/dmi/id/product_uuid 命令对 product_uuid 校验 一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。 3.1.2 运行iptables检查桥接流量确保 br_netfilter 模块被加载。这一操作可以通过运行 lsmod | grep br_netfilter 来完成。若要显式加载该模块，可执行 sudo modprobe br_netfilter。 为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的 sysctl 配置中将 net.bridge.bridge-nf-call-iptables 设置为 1。例如： 123456789cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.confbr_netfilterEOFcat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsudo sysctl --system 3.1.3 关闭防火墙12systemctl stop firewalld systemctl disable firewalld 如果各个主机启用了防火墙，需要开放Kubernetes各个组件所需要的端口。如下图所示，详细信息请看官网。 3.1.4 永久关闭swapvim /etc/fstab 如果不修改，kubelet启动报错： 3.1.5 关闭selinux12345# 将 SELinux 设置为 permissive 或 disabled模式（相当于将其禁用）sudo setenforce 0sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config或sudo sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config 3.2 安装docker卸载旧版本 12345678910yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 源安装 执行以下命令安装依赖包： 1$ sudo yum install -y yum-utils 鉴于国内网络问题，强烈建议使用国内源，官方源请在注释中查看。 执行下面的命令添加 yum 软件源： 12345678910$ sudo yum-config-manager \ --add-repo \ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo$ sudo sed -i 's/download.docker.com/mirrors.aliyun.com\/docker-ce/g' /etc/yum.repos.d/docker-ce.repo# 官方源# $ sudo yum-config-manager \# --add-repo \# https://download.docker.com/linux/centos/docker-ce.repo 安装docker-ce 1$ sudo yum install docker-ce docker-ce-cli containerd.io 开机自启 123启动 Docker$ sudo systemctl enable docker$ sudo systemctl start docker 3.3 安装 kubeadm、kubelet 和 kubectl你需要在每台机器上安装以下的软件包： kubeadm：用来初始化集群的指令。 kubelet：在集群中的每个节点上用来启动 Pod 和容器等。 kubectl：用来与集群通信的命令行工具。 12345678910111213141516cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearchenabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgexclude=kubelet kubeadm kubectlEOF# 安装服务sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes# 设置kubelet开机自启sudo systemctl enable kubelet 3.4 配置cgroup驱动程序需保证容器服务和kubelet的cgroup驱动一致，否则kubelet启动报错： 由于 kubeadm 把 kubelet 视为一个系统服务来管理，所以对基于 kubeadm 的安装， 我们推荐使用 systemd 驱动，不推荐 cgroupfs 驱动。 3.4.1 Cgroup 驱动程序说明 警告： 你需要确保容器（docker）和 kubelet 所使用的是相同的 cgroup 驱动，否则 kubelet 进程会失败。 相关细节可参见kubelet配置 cgroup 驱动。容器配置cgroup驱动 3.4.2 查看kubelet和docker的驱动程序 kubelet 1234[root@node1 ~]# cat /var/lib/kubelet/kubeadm-flags.envKUBELET_KUBEADM_ARGS="--cgroup-driver=cgroup --hostname-override=node --network-plugin=cni --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/k8s-images-kx/pause:3.6"# --cgroup-driver=cgroup 该节点驱动程序为cgroup docker 12345[root@node1 ~]# docker info |grep Cgroup Cgroup Driver: cgroupfs Cgroup Version: 1# docker驱动程序为 cgroupfs 3.4.4 Docker配置cgroup驱动1234567891011121314151617sudo mkdir /etc/dockercat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json&#123; "exec-opts": ["native.cgroupdriver=systemd"], "log-driver": "json-file", "log-opts": &#123; "max-size": "100m" &#125;, "storage-driver": "overlay2"&#125;EOF[root@node3 ~]# systemctl daemon-reload [root@node3 ~]# systemctl restart docker[root@node3 ~]# docker info | grep cgroup Cgroup Driver: systemd Cgroup Version: 1 3.4.5 kubelet配置cgroup驱动（可选）kubelet不配置默认即使用systemd 1234567# 将kubelet和docker 的驱动程序改成一致。vim /var/lib/kubelet/kubeadm-flags.envKUBELET_KUBEADM_ARGS="--cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.1"# 将cgroup改成systemd[root@node3 ~]# systemctl daemon-reload [root@node3 ~]# systemctl restart kubelet 以上步骤和《kubernetes学习二:kubeadm安装kubernetes集群（单master）》完全一致，主要区别在集群负载均衡及集群初始化方式上。 3.5 keepalived 和 haproxykeepalived : 高可用软件，通过vrrp协议，vip地址的漂移，实现服务的高可用。 haproxy: haproxy是一个开源的，高性能的，负载均衡软件。 haproxy主要用来实现kube-apiserver的负载，实际操作中非必选。 官网说明 3.5.1 请求架构 3.5.2 keepalived1yum install -y keepalived 主mastet 12345678910111213141516171819202122232425262728293031323334353637383940! Configuration File for keepalivedglobal_defs &#123;# 关于邮箱的配置全部注释# notification_email &#123;# acassen@firewall.loc# failover@firewall.loc# sysadmin@firewall.loc# &#125;# notification_email_from Alexandre.Cassen@firewall.loc# smtp_server 192.168.200.1# smtp_connect_timeout 30# 节点起名/标识，不能一致，局域网内唯一。 router_id LVS_01 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER# 绑定网卡 interface ens33# 工作组，保持各节点一致，自定义 virtual_router_id 51# 权重，抢占模式下高权限节点获得VIP priority 100 advert_int 1 authentication &#123;# 认证通信认证方式和密码 auth_type PASS auth_pass k8stest &#125;# VIP virtual_ipaddress &#123; 192.168.1.233 &#125;&#125; 备master 12345678910111213141516171819202122232425262728293031323334353637383940! Configuration File for keepalivedglobal_defs &#123;# 关于邮箱的配置全部注释# notification_email &#123;# acassen@firewall.loc# failover@firewall.loc# sysadmin@firewall.loc# &#125;# notification_email_from Alexandre.Cassen@firewall.loc# smtp_server 192.168.200.1# smtp_connect_timeout 30# 节点起名/标识，不能一致，局域网内唯一。 router_id LVS_02 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER# 绑定网卡 interface ens33# 工作组，保持各节点一致，自定义 virtual_router_id 51# 权重，抢占模式下高权限节点获得VIP priority 90 advert_int 1 authentication &#123;# 认证通信认证方式和密码 auth_type PASS auth_pass k8stest &#125;# VIP virtual_ipaddress &#123; 192.168.1.233 &#125;&#125; 123# 重启systemctl restart keeplivedsystemctl enable keeplived 3.5.3 haproxy官方源码下载地址 示例：https://www.haproxy.org/download/2.5/src/haproxy-2.5.1.tar.gz 想偷懒也可以直接 yum install haproxy -y 使用默认仓库的版本，相对版本较低。 安装依赖 1yum install -y make gcc gcc-c++ pcre-devel zlib-devel openssl-devel 创建安装目录 1mkdri /usr/local/haproxy/ 编译安装 1234567tar -zxvf haproxy-2.5.1.tar.gzcd haproxy-2.5.1# 编译# 参考目录下INSTALL安装文档，相关参数说明make TARGET=linux-glibc ARCH=x86_64 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 PREFIX=/usr/local/haproxy# 安装make install PREFIX=/usr/local/haproxy 创建配置文件目录 12mkdir -p /etc/haproxy/mkdir -p /usr/local/haproxy/conf/ 从配置文件名模板复制配置文件 12cp examples/option-http_proxy.cfg /usr/local/haproxy/conf/haproxy.cfgln -s /usr/local/haproxy/conf/haproxy.cfg /etc/haproxy/haproxy.cfg 拷贝开机启动文件 12cp haproxy-2.5.1/examples/haproxy.init /etc/init.d/haproxychmod +x /etc/init.d/haproxy 添加haproxy命令脚本软连接 1ln -s /usr/local/haproxy/sbin/haproxy /usr/sbin 设置开机自启 12chkconfig --add haproxychkconfig haproxy on 创建用户 123groupadd haproxyuseradd -g haproxy haproxymkdir /usr/share/haproxy 修改配置/etc/haproxy/haproxy.cfg 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#---------------------------------------------------------------------# Global settings#---------------------------------------------------------------------global # to have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the '-r' option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local2 chroot /var/lib/haproxy# pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket# stats socket /var/lib/haproxy/stats#---------------------------------------------------------------------# common defaults that all the 'listen' and 'backend' sections will# use if not designated in their block#---------------------------------------------------------------------defaults mode http log global option httplog option dontlognull option http-server-close# option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000#---------------------------------------------------------------------# kubernetes apiserver frontend which proxys to the backends#---------------------------------------------------------------------frontend kubernetes-apiserver mode tcp bind *:16443 option tcplog default_backend kubernetes-apiserver#---------------------------------------------------------------------# round robin balancing between the various backends#---------------------------------------------------------------------backend kubernetes-apiserver mode tcp balance roundrobin server node1 192.168.1.7:6443 check server node2 192.168.1.8:6443 check server node3 192.168.1.9:6443 check#---------------------------------------------------------------------# collection haproxy statistics message#---------------------------------------------------------------------listen stats bind *:1080 stats auth admin:awesomePassword stats refresh 5s stats realm HAProxy\ Statistics stats uri /admin?stats 各节点配置文件一致。 3.6 kubeadm创建集群官网文档 3.5.1 kubeadm configkubeadm已经进入GA阶段，其控制面初始化和加入节点步骤都支持大量的可定制内容，因此kubeadm还提供了配置文件功能用于复杂定制。同时，kubeadm将配置文件以ConfigMap的形式保存到集群之中，便于后续的查询和升级工作。kubeadm config子命令提供了对这一组功能的支持： ◎ kubeadm config upload from-file：由配置文件上传到集群中生成ConfigMap。 ◎ kubeadm config upload from-flags：由配置参数生成ConfigMap。 ◎ kubeadm config view：查看当前集群中的配置值。 ◎ kubeadm config print init-defaults：输出kubeadm init默认参数文件的内容。 ◎ kubeadm config print join-defaults：输出kubeadm join默认参数文件的内容。 ◎ kubeadm config migrate：在新旧版本之间进行配置转换。 ◎ kubeadm config images list：列出所需的镜像列表。 ◎ kubeadm config images pull：拉取镜像到本地。例如，执行kubeadm config print init-defaults，可以取得默认的初始化参数文件： 1kubeadm config print init-defaults &gt; init.default.yaml 3.5.2 导出kubeadm集群默认配置文件1kubeadm config print init-defaults &gt; init.default.yaml 3.5.3 修改默认配置文件修改如下内容： 主节点IP——advertiseAddress 国内阿里镜像地址imageRepository——registry.cn-hangzhou.aliyuncs.com/k8s-images-kx（通过阿里镜像仓库拉取的国外镜像） pod网段配置——不同网络插件网段不一样详细见官网（配置网络插件subnet地址段） 高可用集群api请求地址——controlPlaneEndpoint（这里设置为keepalived的VIP地址+haproxy负载均衡监听端口） ![(https://gitee.com/kxinter/images/raw/master/2022/02/image-20220210153000232.png) 纠正： podSubent的值设置为flannel的默认网段，FLANNEL_NETWORK的默认值（可通过flannel.yaml文件查看）；podSubent=10.244.0.0/16 原因： 后面配置NFS存储类的时候，pod无法正常访问kube-apiserver。 参考： k8s 1.20.x版本NFS动态存储配置 - 巽逸 - 博客园 (cnblogs.com) (71条消息) Kubeadm 部署 使用flannel无法连接service/kubernetes_weixin_40455124的博客-CSDN博客 3.5.4 下载kubernetes相关镜像使用config images list 子命令查询所需的镜像，例如 12345678910[root@node1 home]# kubeadm config images list --config=init.default.yamlk8s.gcr.io/kube-apiserver:v1.23.0k8s.gcr.io/kube-controller-manager:v1.23.0k8s.gcr.io/kube-scheduler:v1.23.0k8s.gcr.io/kube-proxy:v1.23.0k8s.gcr.io/pause:3.6k8s.gcr.io/etcd:3.5.1-0k8s.gcr.io/coredns/coredns:v1.8.6# 默认初始化配置文件下载的是1.23.0版本，如上图显示，可以修改初始化配置文件，改为1.23.3版本 使用config images pull 子命令下载所需的镜像，例如（国内无法下载，只做演示） 预拉取镜像 通过阿里云镜像仓库构建下载镜像，具体方法不做细说。 镜像使用方式有2种： 修改镜像名为国外镜像名，例如 k8s.gcr.io/kube-apiserver:v1.23.0k8s.gcr.io/kube-controller-manager:v1.23.0k8s.gcr.io/kube-scheduler:v1.23.0k8s.gcr.io/kube-proxy:v1.23.0k8s.gcr.io/pause:3.6k8s.gcr.io/etcd:3.5.1-0k8s.gcr.io/coredns/coredns:v1.8.6 修改初始化配置文件，修改国外仓库为阿里云仓库 imageRepository: registry.cn-hangzhou.aliyuncs.com/k8s-images-kxkind: ClusterConfigurationkubernetesVersion: 1.23.3networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 1[root@node1 home]# kubeadm config images pull --config=init.default.yaml 3.5.5 初始化集群12# --upload-certs根据情况可选kubeadm init --config /home/init.default.yaml --upload-certs ![(https://gitee.com/kxinter/images/raw/master/2022/02/image-20220210154221965.png) 3.5.6 加入master节点123kubeadm join 192.168.1.133:6443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:d264137849995438cfda460507958165ad026fc804206c9000e7f2c6c4b8b66c \ --control-plane 1234# 执行以下步骤，才能使用kubectl命令mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 3.5.7 加入工作节点12kubeadm join 192.168.1.133:6443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:d264137849995438cfda460507958165ad026fc804206c9000e7f2c6c4b8b66c 3.5.8 检查节点1kubectl get node 3.5.9 验证关闭master1节点，通过master2或master3节点尝试集群命令。 vip转移到node2即master2节点，node2和node4执行管理权限不受影像 到此简单验证成功。其他问题参考《kubenetes学习二》 参考Kubeadm + HAProxy + Keepalived部署高可用Kubernetes集群]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes学习二:kubeadm安装kubernetes集群（单master）]]></title>
    <url>%2F2022%2F02%2F07%2Fkubernetes%E5%AD%A6%E4%B9%A0%E4%BA%8C-kubeadm%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4-%E5%8D%95master%2F</url>
    <content type="text"><![CDATA[1 参考文档官方文档 kubeadm安装k8s v1.18.0 cgroup造成的kubelet启动失败 2 环境准备2.1 物理环境 主机 IP 配置 OS master/node1 192.168.67.139 2core 2GB Centos7.9 node2 192.168.67.140 2core 2GB Centos7.9 node3 192.168.67.141 2core 2GB Centos7.9 node4 192.168.67.142 2core 2GB Centos7.9 2.2 软件环境 软件 版本 docker v20.10.12 kubeadmin v1.23.3 kubelet v1.23.3 kubectl v1.23.3 2.3 镜像 镜像 版本 conformance v1.23.3 kube-apiserver v1.23.3 kube-controller-manager v1.23.3 kube-proxy v1.23.3 kube-scheduler v1.23.3 flannel v1.0.1 3 步骤3.1 准备工作3.1.1 确保各节点MAC和product_uuid的唯一性 你可以使用命令 ip link 或 ifconfig -a 来获取网络接口的 MAC 地址 可以使用 sudo cat /sys/class/dmi/id/product_uuid 命令对 product_uuid 校验 一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。 3.1.2 运行iptables检查桥接流量确保 br_netfilter 模块被加载。这一操作可以通过运行 lsmod | grep br_netfilter 来完成。若要显式加载该模块，可执行 sudo modprobe br_netfilter。 为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的 sysctl 配置中将 net.bridge.bridge-nf-call-iptables 设置为 1。例如： 123456789cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.confbr_netfilterEOFcat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsudo sysctl --system 3.1.3 关闭防火墙12systemctl stop firewalld systemctl disable firewalld 如果各个主机启用了防火墙，需要开放Kubernetes各个组件所需要的端口。如下图所示，详细信息请看官网。 3.1.4 永久关闭swapvim /etc/fstab 如果不修改，kubelet启动报错： 3.1.5 关闭selinux12345# 将 SELinux 设置为 permissive 或 disabled模式（相当于将其禁用）sudo setenforce 0sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config或sudo sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config 3.2 安装docker卸载旧版本 12345678910yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 源安装 执行以下命令安装依赖包： 1$ sudo yum install -y yum-utils 鉴于国内网络问题，强烈建议使用国内源，官方源请在注释中查看。 执行下面的命令添加 yum 软件源： 12345678910$ sudo yum-config-manager \ --add-repo \ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo$ sudo sed -i 's/download.docker.com/mirrors.aliyun.com\/docker-ce/g' /etc/yum.repos.d/docker-ce.repo# 官方源# $ sudo yum-config-manager \# --add-repo \# https://download.docker.com/linux/centos/docker-ce.repo 安装docker-ce 1$ sudo yum install docker-ce docker-ce-cli containerd.io 开机自启 123启动 Docker$ sudo systemctl enable docker$ sudo systemctl start docker 3.3 安装 kubeadm、kubelet 和 kubectl你需要在每台机器上安装以下的软件包： kubeadm：用来初始化集群的指令。 kubelet：在集群中的每个节点上用来启动 Pod 和容器等。 kubectl：用来与集群通信的命令行工具。 12345678910111213141516cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装服务sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes# 设置kubelet开机自启sudo systemctl enable kubelet 3.4 配置cgroup驱动程序需保证容器服务和kubelet的cgroup驱动一致，否则kubelet启动报错： 由于 kubeadm 把 kubelet 视为一个系统服务来管理，所以对基于 kubeadm 的安装， 我们推荐使用 systemd 驱动，不推荐 cgroupfs 驱动。 3.4.1 Cgroup 驱动程序说明 警告： 你需要确保容器（docker）和 kubelet 所使用的是相同的 cgroup 驱动，否则 kubelet 进程会失败。 相关细节可参见kubelet配置 cgroup 驱动。容器配置cgroup驱动 3.4.2 查看kubelet和docker的驱动程序 kubelet 1234[root@node1 ~]# cat /var/lib/kubelet/kubeadm-flags.envKUBELET_KUBEADM_ARGS="--cgroup-driver=cgroup --hostname-override=node --network-plugin=cni --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/k8s-images-kx/pause:3.6"# --cgroup-driver=cgroup 该节点驱动程序为cgroup docker 12345[root@node1 ~]# docker info |grep Cgroup Cgroup Driver: cgroupfs Cgroup Version: 1# docker驱动程序为 cgroupfs 3.4.4 Docker配置cgroup驱动1234567891011121314151617sudo mkdir /etc/dockercat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json&#123; "exec-opts": ["native.cgroupdriver=systemd"], "log-driver": "json-file", "log-opts": &#123; "max-size": "100m" &#125;, "storage-driver": "overlay2"&#125;EOF[root@node3 ~]# systemctl daemon-reload [root@node3 ~]# systemctl restart docker[root@node3 ~]# docker info | grep cgroup Cgroup Driver: systemd Cgroup Version: 1 3.4.5 kubelet配置cgroup驱动（可选）kubelet不配置默认即使用systemd 1234567# 将kubelet和docker 的驱动程序改成一致。vim /var/lib/kubelet/kubeadm-flags.envKUBELET_KUBEADM_ARGS="--cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.1"# 将cgroup改成systemd[root@node3 ~]# systemctl daemon-reload [root@node3 ~]# systemctl restart kubelet 3.5 kubeadm创建集群3.5.1 kubeadm configkubeadm已经进入GA阶段，其控制面初始化和加入节点步骤都支持大量的可定制内容，因此kubeadm还提供了配置文件功能用于复杂定制。同时，kubeadm将配置文件以ConfigMap的形式保存到集群之中，便于后续的查询和升级工作。kubeadm config子命令提供了对这一组功能的支持： ◎ kubeadm config upload from-file：由配置文件上传到集群中生成ConfigMap。 ◎ kubeadm config upload from-flags：由配置参数生成ConfigMap。 ◎ kubeadm config view：查看当前集群中的配置值。 ◎ kubeadm config print init-defaults：输出kubeadm init默认参数文件的内容。 ◎ kubeadm config print join-defaults：输出kubeadm join默认参数文件的内容。 ◎ kubeadm config migrate：在新旧版本之间进行配置转换。 ◎ kubeadm config images list：列出所需的镜像列表。 ◎ kubeadm config images pull：拉取镜像到本地。例如，执行kubeadm config print init-defaults，可以取得默认的初始化参数文件： 1kubeadm config print init-defaults &gt; init.default.yaml 3.5.2 导出kubeadm集群默认配置文件1kubeadm config print init-defaults &gt; init.default.yaml 3.5.3 修改默认配置文件修改如下内容： 主节点IP——advertiseAddress 国内阿里镜像地址imageRepository——registry.cn-hangzhou.aliyuncs.com/k8s-images-kx（通过阿里镜像仓库拉取的国外镜像） pod网段配置——不同网络插件网段不一样详细见官网（配置网络插件subnet地址段） 将上面的内容保存为init-config.yaml备用。 纠正： podSubent的值设置为flannel的默认网段，FLANNEL_NETWORK的默认值（可通过flannel.yaml文件查看）；podSubent=10.244.0.0/16 原因： 后面配置NFS存储类的时候，pod无法正常访问kube-apiserver。 参考： k8s 1.20.x版本NFS动态存储配置 - 巽逸 - 博客园 (cnblogs.com) (71条消息) Kubeadm 部署 使用flannel无法连接service/kubernetes_weixin_40455124的博客-CSDN博客 3.5.4 下载kubernetes相关镜像使用config images list 子命令查询所需的镜像，例如 12345678910[root@node1 home]# kubeadm config images list --config=init.default.yamlk8s.gcr.io/kube-apiserver:v1.23.0k8s.gcr.io/kube-controller-manager:v1.23.0k8s.gcr.io/kube-scheduler:v1.23.0k8s.gcr.io/kube-proxy:v1.23.0k8s.gcr.io/pause:3.6k8s.gcr.io/etcd:3.5.1-0k8s.gcr.io/coredns/coredns:v1.8.6# 默认初始化配置文件下载的是1.23.0版本，如上图显示，可以修改初始化配置文件，改为1.23.3版本 使用config images pull 子命令下载所需的镜像，例如（国内无法下载，只做演示） 预拉取镜像 通过阿里云镜像仓库构建下载镜像，具体方法不做细说。 镜像使用方式有2种： 修改镜像名为国外镜像名，例如 k8s.gcr.io/kube-apiserver:v1.23.0k8s.gcr.io/kube-controller-manager:v1.23.0k8s.gcr.io/kube-scheduler:v1.23.0k8s.gcr.io/kube-proxy:v1.23.0k8s.gcr.io/pause:3.6k8s.gcr.io/etcd:3.5.1-0k8s.gcr.io/coredns/coredns:v1.8.6 修改初始化配置文件，修改国外仓库为阿里云仓库 imageRepository: registry.cn-hangzhou.aliyuncs.com/k8s-images-kxkind: ClusterConfigurationkubernetesVersion: 1.23.3networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 1[root@node1 home]# kubeadm config images pull --config=init.default.yaml 3.5.5 初始化集群1[root@node1 home]# kubeadm init --config=init.default.yaml 初始化失败，可以如下命令重置 1[root@node1 home]# kubeadm reset 显示如下安装成功 普通用户使用集群需如下配置 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 3.5.6 查询集群状态1kubectl get node notready原因是没有安装pod网络。 3.5.7 安装pod网络（二选一）各pod组件的链接均可以通过如下地址找到，及相关使用教程。 安装扩展（Addons） | Kubernetes 集群网络系统 | Kubernetes 网络说明 Kubernetes的网络通信问题: 1. 容器间通信： 即同一个Pod内多个容器间通信，通常使用loopback来实现。 2. Pod间通信： K8s要求,Pod和Pod之间通信必须使用Pod-IP 直接访问另一个Pod-IP 3. Pod与Service通信： 即PodIP去访问ClusterIP，当然，clusterIP实际上是IPVS 或 iptables规则的虚拟IP，是没有TCP/IP协议栈支持的。但不影响Pod访问它. 4. Service与集群外部Client的通信，即K8s中Pod提供的服务必须能被互联网上的用户所访问到。 需要注意的是，k8s集群初始化时的service网段，pod网段，网络插件的网段，以及真实服务器的网段，都不能相同，如果相同就会出各种各样奇怪的问题，而且这些问题在集群做好之后是不方便改的，改会导致更多的问题，所以，就在搭建前将其规划好。 CNI(容器网络接口)： 这是K8s中提供的一种通用网络标准规范，因为k8s本身不提供网络解决方案。 目前比较知名的网络解决方案有: flannel calico canel kube-router …….等等，目前比较常用的时flannel和calico，flannel的功能比较简单，不具备复杂网络的配置能力，calico是比较出色的网络管理插件，单具备复杂网络配置能力的同时，往往意味着本身的配置比较复杂，所以相对而言，比较小而简单的集群使用flannel，考虑到日后扩容，未来网络可能需要加入更多设备，配置更多策略，则使用calico更好 所有的网络解决方案，它们的共通性： 1. 虚拟网桥 2. 多路复用：MacVLAN 3. 硬件交换：SR-IOV（单根-I/O虚拟网络）：它是一种物理网卡的硬件虚拟化技术，它通过输出VF(虚拟功能)来将网卡虚拟为多个虚拟子接口，每个VF绑定给一个VM后，该VM就可以直接操纵该物理网卡。 kubelet来调CNI插件时，会到 /etc/cni/net.d/目录下去找插件的配置文件，并读取它，来加载该插件,并让该网络插件来为Pod提供网络服务。 flannel网络插件要怎么部署？ 1. flannel部署到那个节点上？ 因为kubelet是用来管理Pod的，而Pod运行需要网络，因此凡是部署kubelet的节点，都需要部署flannel来提供网络，因为kubelet正是通过调用flannel来实现为Pod配置网络的(如:添加网络，配置网络，激活网络等)。 3.5.7.1 flannelflannel GIT仓库 在所有节点都pull下载flannel镜像，节点加入集群时自动创建flannel容器。 1kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml 3.5.7.2 calicocalico官网 在所有节点都pull下载calico镜像 1kubectl apply -f https://projectcalico.docs.tigera.io/manifests/canal.yaml 3.5.7.3 查看集群状态coredns正常运行，节点状态ready. 3.5.8 添加Node节点官方文档 1234# token和hash 下面会介绍如何获取# control-plane-host和control-plane-port分别是控制面对应的ip和端口kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 输出如下内容 1234567891011121314[root@node3 ~]# kubeadm join --token abcdef.01234ad56789abcdef 192.168.76.139:6443 --discovery-token-ca-cert-hash sha256:500813d6c8feca1d82468821993be623d3d7ccdfa33d202f3c7fe3c6b39da086c7d[preflight] Running pre-flight checks[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Starting the kubelet[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 查询token 1kubeadm token list 输出类似于以下内容： 15didvk.d09sbcov8ph2amjw 查询 –discovery-token-ca-cert-hash 12openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | \ openssl dgst -sha256 -hex | sed 's/^.* //' 输出类似于以下内容： 18cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78 3.5.8 测试首先验证kube-apiserver, kube-controller-manager, kube-scheduler, pod network 是否正常： 123456789101112# 部署一个 Nginx Deployment，包含两个Pod# https://kubernetes.io/docs/concepts/workloads/controllers/deployment/kubectl create deployment nginx --image=nginx:alpinekubectl scale deployment nginx --replicas=2# 验证Nginx Pod是否正确运行，并且会分配192.168.开头的集群IPkubectl get pods -l app=nginx -o wide# 输出如下：NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-54458cd494-p8jzs 1/1 Running 0 31s 192.168.1.2 node1 &lt;none&gt; &lt;none&gt;nginx-54458cd494-v2m4b 1/1 Running 0 24s 192.168.1.3 node1 &lt;none&gt; &lt;none&gt; 再验证一下kube-proxy是否正常： 12345678910111213# 以 NodePort 方式对外提供服务 https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/kubectl expose deployment nginx --port=80 --type=NodePort# 查看集群外可访问的Portkubectl get services nginx# 输出NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx NodePort 10.110.49.49 &lt;none&gt; 80:31899/TCP 4s# 可以通过任意 NodeIP:Port 在集群外部访问这个服务，本示例中部署的集群IP分别是192.168.76.139-142curl http://192.168.76.139:31899curl http://192.168.76.140:31899 最后验证一下dns, pod network是否正常： 12345678910111213141516171819202122# 运行Busybox并进入交互模式kubectl run -it curl --image=radial/busyboxplus:curl# 输入`nslookup nginx`查看是否可以正确解析出集群内的IP，已验证DNS是否正常[ root@curl-66959f6557-6sfqh:/ ]$ nslookup nginx# 输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: nginxAddress 1: 10.110.49.49 nginx.default.svc.cluster.local# 通过服务名进行访问，验证kube-proxy是否正常[ root@curl-66959f6557-6sfqh:/ ]$ curl http://nginx/# 输出如下：# &lt;!DOCTYPE html&gt; ---省略# 分别访问一下2个Pod的内网IP，验证跨Node的网络通信是否正常[ root@curl-66959f6557-6sfqh:/ ]$ curl http://192.168.1.2/[ root@curl-66959f6557-6sfqh:/ ]$ curl http://192.168.1.3/ 4 补充4.1 允许master节点部署pod这是因为k8s集群默认不让在master节点创建pod，也就是说Master Node不参与工作负载。 当前的master节点被打上了node-role.kubernetes.io/master:NoSchedule的污点： 1[root@k8s-master k8s]# kubectl describe nodes node1 |grep -E '(Roles|Taints)' 允许master部署pod 123[root@k8s-master nginx]# kubectl taint nodes node1 node-role.kubernetes.io/master-# 输出 node/k8s-master untainted 禁止master部署pod 1[root@k8s-master]# kubectl taint nodes node1 node-role.kubernetes.io/master=true:NoSchedule 4.2 卸载集群想要撤销kubeadm执行的操作，首先要排除节点，并确保该节点为空, 然后再将其关闭。 在Master节点上运行： 12kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;node name&gt; 然后在需要移除的节点上，重置kubeadm的安装状态： 1sudo kubeadm reset 12# 卸载集群后，建议手动删除CNI网络的地址段配置文件，避免重装集群造成影像。rm -rf /run/flannel 如果你想重新配置集群，使用新的参数重新运行kubeadm init或者kubeadm join即可。 5 问题总结5.1 创建deployment后，配置NodePort，无法通过任意节点访问 问题描述： 创建nginx的depolyment，并配置nodeport端口配置，只能通过pod所在的节点IP和暴漏端口访问，无法通过其他节点“IP:端口”访问;集群内pod之间不能通过pod IP互相访问；kube-dns解析异常等问题。 查看节点kube-proxy服务日志显示如下错误 问题原因：节点没有开启路由转发功能。 解决办法：添加net.ipv4.ip_forward = 1到/etc/sysctl.conf或/etc/sysctl.d/k8s.conf，执行sysctl -p命令生效。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx安全加固]]></title>
    <url>%2F2022%2F01%2F28%2Fnginx%E5%AE%89%E5%85%A8%E5%8A%A0%E5%9B%BA%2F</url>
    <content type="text"><![CDATA[1 禁用目录浏览禁用nginx.conf中的autoindex模块。 12autoindex off# 或者删除autoindex配置 2 隐藏版本信息如果开启的话（默认情况下）所有的错误页面都会显示服务器的版本和信息。nginx.conf配置如下： 12345678910111213http&#123; include naxsi_core.rules; include mime.types; default_type application/octet-stream; sendfile on;# 增加下面配置 server_tokens off; ... ... 加固检查： 1234567891011121314151617181920212223root@9340a450ae64:/workspace# curl -I insight.jx-lab.comHTTP/1.1 200 OKServer: nginx/1.16.1Date: Fri, 28 Jan 2022 03:26:35 GMTContent-Type: text/html; charset=utf-8,gbkContent-Length: 14052Last-Modified: Tue, 14 Apr 2020 06:26:50 GMTConnection: keep-aliveETag: "5e9557aa-36e4"Accept-Ranges: bytes# 修改后root@9340a450ae64:/workspace# curl -I insight.jx-lab.comHTTP/1.1 200 OKServer: nginxDate: Fri, 28 Jan 2022 03:30:29 GMTContent-Type: text/html; charset=utf-8,gbkContent-Length: 14052Last-Modified: Tue, 14 Apr 2020 06:26:50 GMTConnection: keep-aliveETag: "5e9557aa-36e4"Accept-Ranges: bytes 3 限制HTTP请求方法备份nginx.conf配置文件。 编辑配置文件，添加如下内容： 123if ($request_method !~ ^(GET|HEAD|POST)$ ) &#123;return 444;&#125; 保存，然后后重启nginx服务。 备注：只允许常用的GET和POST方法，顶多再加一个HEAD方法 4 限制IP访问备份nginx.conf配置文件。 编辑配置文件，添加如下内容： 123if ($request_method !~ ^(GET|HEAD|POST)$ ) &#123;return 444;&#125; 保存，然后后重启nginx服务。 备注：只允许常用的GET和POST方法，顶多再加一个HEAD方法 5 限制并发和速度编辑配置文件，在server标签内添加如下内容： 1234567891011121314limit_zone one $binary_remote_addr 10m;server&#123; listen 80; server_name down.test.com; index index.html index.htm index.php; root /usr/local/www; #Zone limit; location / &#123; limit_conn one 1; limit_rate 20k; &#125;………&#125; 保存，然后后重启nginx服务。 6 控制超时时间编辑配置文件，具体设置如下： 1234client_body_timeout 10; #设置客户端请求主体读取超时时间client_header_timeout 10; #设置客户端请求头读取超时时间keepalive_timeout 5 5; #第一个参数指定客户端连接保持活动的超时时间，第二个参数是可选的，它指定了消息头保持活动的有效时间send_timeout10; #指定响应客户端的超时时间 保存，然后后重启nginx服务。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WSL2下的GPU调用]]></title>
    <url>%2F2022%2F01%2F28%2FWSL2%E4%B8%8B%E7%9A%84GPU%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1 环境wsl wsl2 分支 ubuntu20.4 组件 nvidia-docker2 2 安装wsl21234# windown 10 2004及以上（保证windown环境已正确安装驱动）wsl --install # 默认安装ubuntuwsl --update # 更新到最新版本wsl -d ubuntu # 启动并进入ubuntu 3 安装docker参考ubuntu安装docker官方教程 4 安装nvidia-docker4.1 安装nvidia库123$curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -$curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu18.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list$sudo apt-get update 4.2 安装nvidia-docker2并重新加载docker守护进程123$sudo apt-get install nvidia-docker2$sudo pkill -SIGHUP dockerd$docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi 备注：运行上一步若报错 123qyh@qyh-mas:~$ sudo docker run --runtime=nvidia --rm nvidia/cuda nvidia-smidocker: Error response from daemon: Unknown runtime specified nvidia.See 'docker run --help'. 当显卡驱动安装好的前提下（用指令nvidia-smi来显示显卡驱动），则换成运行下步，指定安装的cuda版本： 123456789101112131415161718192021222324252627282930313233343536373839404142root@YW-HUAN-PC:/etc/apt/sources.list.d# nvidia-smiFri Jan 28 10:34:52 2022+-----------------------------------------------------------------------------+| NVIDIA-SMI 470.57.01 Driver Version: 471.41 CUDA Version: 11.4 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA GeForce ... Off | 00000000:01:00.0 On | N/A || 38% 29C P8 1W / 38W | 520MiB / 2048MiB | ERR! Default || | | N/A |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=============================================================================|+-----------------------------------------------------------------------------+root@YW-HUAN-PC:/etc/apt/sources.list.d# nvidia-docker run --rm nvidia/cuda:10.1-devel | nvidia-smiFri Jan 28 10:35:24 2022+-----------------------------------------------------------------------------+| NVIDIA-SMI 470.57.01 Driver Version: 471.41 CUDA Version: 11.4 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA GeForce ... Off | 00000000:01:00.0 On | N/A || 38% 29C P8 1W / 38W | 519MiB / 2048MiB | ERR! Default || | | N/A |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=============================================================================|+-----------------------------------------------------------------------------+root@YW-HUAN-PC:/etc/apt/sources.list.d# 显示如上表示成功了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>gpu</tag>
        <tag>wsl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat安全加固]]></title>
    <url>%2F2022%2F01%2F25%2FTomcat%E5%AE%89%E5%85%A8%E5%8A%A0%E5%9B%BA%2F</url>
    <content type="text"><![CDATA[1. 删除文档和实例程序检查方法 ：打开tomcat_home/webapps文件夹，默认存在docs和examples文件夹 加固方法：建议删除docs和examples文件夹 2 设置shutdown字符串安全基线项说明：防止恶意用户telnet到8005端口后，发送SHUTDOWN命令停止tomcat服务 检查方法：打开tomcat_home/conf/server.xml，查看是否设置了复杂的字符串&lt;Server port=&quot;8005&quot;shutdown=“复杂的字符串”&gt; 加固方法：设置复杂的字符串，防止恶意用户猜测 3 检查控制台口令安全基线项说明：加固tomcat控制台，设置复杂的口令 检查方法： 如果不需要使用控制台Tomcat 6.x/7.x：默认通过http://ip:8080/manager/html可以访问tomcatmanager，如果不需要使用，建议删除tomcat_home/webapps/manager和host-manager文件夹； 如果需要使用tomcatmanagerTomcat 5.x/6.x:打开tomcat_home/conf/tomcat-users.xml，查看用户密码复杂度 例如： &lt;user username=&quot;tomcat&quot;password=&quot;复杂的口令&quot;roles=“manager”/&gt; 4 禁止列目录安全基线项说明：防止直接访问目录时由于找不到默认主页而列出目录下所有文件 检查方法：打开应用程序的web.xml，查看listings是否设置为falselistingsfalse 5 日志审核安全基线项说明：检查tomcat是否记录了访问日志 检查方法：tomcat的日志信息默认存放在tomcat_home/logs中，访问日志默认未开启 加固方法：如果tomcat前端有Apache，Apache可以记录访问日志。如果tomcat独立运行，可以开启tomcat访问日志，修改tomcat_home/conf/server.xml，取消注释：&lt;ValveclassName=&quot;org.apache.catalina.valves.AccessLogValve&quot;directory=“logs” prefix=“localhost_access_log.“suffix=”.txt” pattern=&quot;common&quot;resolveHosts=“false”/&gt;启用access_log后，重启tomcat，在tomcat_home/logs中可以看到访问日志注：这里记录的时间转换为北京时间需要+8小时 6 禁止非法HTTP方法安全基线项目名称：禁用PUT、DELETE等危险的HTTP方法 检查方法：编辑web.xml文件中配置，查看readonly的param-value值是否为false 加固方法：编辑web.xml文件中配置，将readonly的param-value值设为falseorg.apache.catalina.servlets.DefaultServlet的 1234&lt;init-param&gt;&lt;param-name&gt;readonly&lt;/param-name&gt;&lt;param-value&gt;true&lt;/param-value&gt;&lt;/init-param&gt; 7 系统Banner信息安全基线项说明：修改系统Banner信息 检查方法：telnet判断信息： 12telnet ip 8080HEAD /HTTP1.1然后两次回车，可以看到server的信息 参考：安全服务之安全基线及加固（四）Tomcat篇 (qq.com)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx代理示例]]></title>
    <url>%2F2022%2F01%2F20%2Fnginx%E4%BB%A3%E7%90%86%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[1 示例11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556server &#123; listen 80; listen [::]:80; server_name localhost; #access_log /var/log/nginx/host.access.log main; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125;# 多本地路径代理 location /test &#123; alias /test; &#125; location /admin &#123; alias /admin; &#125;# 反向代理 location /landBlockOpt/ &#123; proxy_pass http://192.168.0.91:85/landBlockOpt/; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\.ht &#123; # deny all; #&#125;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7系统安全审计]]></title>
    <url>%2F2022%2F01%2F14%2FCentos7%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E5%AE%A1%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[1 Centos7审计配置通过auditd，系统审核允许系统管理员监视其系统这样他们就可以检测到未经授权的数据访问或修改。 默认情况下，auditd将审核SELinux AVC拒绝、系统登录、帐户修改和身份验证事件。事件将记录到 /var/log/audit/audit.log 2 audit使用2.1 配置文件auditctl : 即时控制审计守护进程的行为的工具，比如如添加规则等等 aureport : 查看和生成审计报告的工具 ausearch : 查找审计事件的工具 auditspd : 转发事件通知给其他应用程序，而不是写入到审计日志文件中 autrace : 一个用于跟踪进程的命令 /etc/audit/auditd.conf /etc/audit/rules.d/audit.rules /etc/audit/audit.rules # 根据/etc/audit/rules.d/audit.rules自动生成 2.1.1 auditd.conf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 是否记录本地事件，如果设为no，只记录来自网络的事件local_events = yeswrite_logs = yes# 日志文件log_file = /var/log/audit/audit.loglog_group = rootlog_format = RAW# 日志文件刷新方式，可选的选项有：# NONE：不做特别处理# INCREMENTAL：用freq选项的值确定多长时间发生一次向磁盘的刷新# DATA：审计数据和日志文件是同步的# SYNC：写日志文件时，数据和元数据是同步的flush = INCREMENTAL_ASYNCfreq = 50# 日志文件最大8MBmax_log_file = 8# 日志文件数量num_logs = 5# 进程优先级（-4），ps axl | grep auditd 可查priority_boost = 4disp_qos = lossydispatcher = /sbin/audispdname_format = NONE##name = mydomain# 当log文件达到max_log_file设定大小后的动作。可选的动作有：# IGNORE：忽略max_log_file设置的限制，继续写log文件# SYSLOG：向syslog中写入一条warning# SUSPEND：auditd不再写log文件，但是auditd继续运行# ROTATE：分多个log文件，一个log文件达到上限后在创建一个新的不同名字的log文件max_log_file_action = ROTATE# log_file文件所在的分区空闲空间少于这个设定的值时，触发相应的动作，单位是MBspace_left = 75# space_left设定被触发时的动作。可选动作有：# IGNORE, SYSLOG, SUSPEND：与前面max_log_file_action相似# SINGLE：audit进程会将系统模式变为单用户模式# HALT：audit进程将会触发系统关机space_left_action = SYSLOGverify_email = yesaction_mail_acct = rootadmin_space_left = 50admin_space_left_action = SUSPEND# 磁盘满后触发的动作disk_full_action = SUSPEND# 磁盘错误触发的动作disk_error_action = SUSPENDuse_libwrap = yes##tcp_listen_port = 60tcp_listen_queue = 5tcp_max_per_addr = 1##tcp_client_ports = 1024-65535tcp_client_max_idle = 0enable_krb5 = nokrb5_principal = auditd##krb5_key_file = /etc/audit/audit.keydistribute_network = no 2.2 常用命令12345/*查看规则*/auditctl -l/*查看命令帮助*/auditctl -h 12345auditctl -w /data -p rwxa/*监控/data目录-w path : 指定要监控的路径-p : 指定触发审计的文件/目录的访问权限rwxa ： 指定的触发条件，r 读取权限，w 写入权限，x 执行权限，a 属性（attr）*/ 永久保存审计规则 1234vi /etc/audit/rules.d/audit.rules例如将-w /data/ -p rwxa加入到最后一行service auditd restartauditctl -l 3 配置开机启动1systemctl enable auditd 4 配置规则规则类型可分为： 控制规则：控制audit系统的规则； 文件系统规则：也可以认为是文件监控，可以监控一个特定文件或者一个路径。 系统调用规则：可以记录特定程序的系统调用。 4.1 控制规则-b 设置在内核中audit缓冲空间的最大值。 -f 这个选项来决定内核如何处理critical erros：0=silent 1=printk 2=panic.默认值为1。 -e 设置使能标志，设置为0，为关闭了audit，设置为1，则开启audit；当设置为2时，表示锁定，一般在设置完其他规则后最后设置，防止其他人修改规则；任何修改规则的行为都会被拒绝，并且记录审计日志，只有当重启系统后，这个使能标志才可以被修改。 4.2 文件系统规则1auditctl -w path -p permissions -k key_name -w : 目录或者文件路径 -p： 描述文件系统监视将触发的权限访问类型，r=读取，w=写入，x=执行，a=属性更改。 -k: 设置审核规则的筛选关键字 4.3 系统调用规则1auditctl -a [list,action|action,list] -S [Syscall name or number|all] -F field=value -k key_name -a: action和list 明确一个事件被记录。action可以为always或者never，list明确出对应的匹配过滤，list可以为：task,exit,user,exclude,filesystem。 -S: system_call 明确出系统调用的名字，几个系统调用可以写在一个规则里，如-S xxx -S xxx。系统调用的名字可以在/usr/include/asm/unistd_64.h文件中找到。 -F: field=value 作为附加选项，修改规则以匹配特定架构、GroupID，ProcessID等的事件。具体有哪些字段，可以参考man linux https://linux.die.net/man/8/auditctl 5 配置实例5.1 记录系统的日期和时间的修改vim /etc/audit/rules.d/audit.rules 12345-a always,exit -F arch=b64 -S adjtimex -S settimeofday -k time-change-a always,exit -F arch=b32 -S adjtimex -S settimeofday -S stime -k timechange-a always,exit -F arch=b64 -S clock_settime -k time-change-a always,exit -F arch=b32 -S clock_settime -k time-change-w /etc/localtime -p wa -k time-change 5.2 记录用户和组的修改事件12345-w /etc/group -p wa -k identity-w /etc/passwd -p wa -k identity-w /etc/gshadow -p wa -k identity-w /etc/shadow -p wa -k identity-w /etc/security/opasswd -p wa -k identity 5.3 记录网络环境修改时间1234567-a always,exit -F arch=b64 -S sethostname -S setdomainname -k system-locale-a always,exit -F arch=b32 -S sethostname -S setdomainname -k system-locale-w /etc/issue -p wa -k system-locale-w /etc/issue.net -p wa -k system-locale-w /etc/hosts -p wa -k system-locale-w /etc/sysconfig/network -p wa -k system-locale-w /etc/sysconfig/network-scripts/ -p wa -k system-locale 5.4 记录登录和登出事件12-w /var/log/lastlog -p wa -k logins-w /var/run/faillock/ -p wa -k logins 5.5 记录会话启动事件123-w /var/run/utmp -p wa -k session-w /var/log/wtmp -p wa -k logins-w /var/log/btmp -p wa -k logins 文件/var/run/utmp跟踪当前登录的所有用户。所有审计记录都将用标识符“session”标记， 可以用who命令读取 /var/log/wtmp文件跟踪登录、注销、关机和重新启动事件。 文件/var/log/btmp跟踪失败的登录尝试，可以通过输入命令 ‘/usr/bin/last-f /var/log/btmp’ 读取。所有审核记录都将被标记为标识符“logins” 5.6 监视对文件权限、属性、所有权和组的更改123456-a always,exit -F arch=b64 -S chmod -S fchmod -S fchmodat -F auid&gt;=1000 -F auid!=4294967295 -k perm_mod-a always,exit -F arch=b32 -S chmod -S fchmod -S fchmodat -F auid&gt;=1000 -F auid!=4294967295 -k perm_mod-a always,exit -F arch=b64 -S chown -S fchown -S fchownat -S lchown -F auid&gt;=1000 -F auid!=4294967295 -k perm_mod-a always,exit -F arch=b32 -S chown -S fchown -S fchownat -S lchown -F auid&gt;=1000 -F auid!=4294967295 -k perm_mod-a always,exit -F arch=b64 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid&gt;=1000 -F auid!=4294967295 -k perm_mod-a always,exit -F arch=b32 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid&gt;=1000 -F auid!=4294967295 -k perm_mod 在所有情况下，审核记录将只为非系统用户id（auid&gt;=1000）并将忽略守护进程事件（auid=4294967295）。 所有审计记录用标识符“perm_mod”标记 5.7 记录未授权文件访问尝试1234-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid&gt;=1000 -F auid!=4294967295 -k access-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid&gt;=1000 -F auid!=4294967295 -k access-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid&gt;=1000 -F auid!=4294967295 -k access-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid&gt;=1000 -F auid!=4294967295 -k access 5.8 确保收集使用特权命令监视特权程序（那些在执行时设置了setuid和/或setgid位的程序）以确定没有权限的用户是否正在运行这些命令。 通过下面命令 1234 find / -xdev \( -perm -4000 -o -perm -2000 \) -type f | awk'&#123;print \"-a always,exit -F path=" $1 " -F perm=x -F auid&gt;=1000 -F auid!=4294967295 \-k privileged" &#125;' 得到记录 12345678910111213141516171819202122232425-a always,exit -F path=/usr/bin/wall -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/chfn -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/chsh -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/chage -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/gpasswd -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/newgrp -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/mount -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/su -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/umount -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/sudo -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/write -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/crontab -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/ssh-agent -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/pkexec -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/bin/passwd -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/sbin/unix_chkpwd -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/sbin/pam_timestamp_check -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/sbin/netreport -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/sbin/usernetctl -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/sbin/postdrop -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/sbin/postqueue -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/lib/polkit-1/polkit-agent-helper-1 -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/libexec/utempter/utempter -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/libexec/dbus-1/dbus-daemon-launch-helper -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged-a always,exit -F path=/usr/libexec/openssh/ssh-keysign -F perm=x -F auid&gt;=1000 -F auid!=4294967295 -k privileged 5.9 收集成功挂载磁盘事件12-a always,exit -F arch=b64 -S mount -F auid&gt;=1000 -F auid!=4294967295 -k mounts-a always,exit -F arch=b32 -S mount -F auid&gt;=1000 -F auid!=4294967295 -k mount 在所有情况下，审核记录将只为非系统用户id（auid&gt;=1000）并将忽略守护进程事件（auid=4294967295）。 所有审计记录用标识符“mount”和“mounts”标记 5.10 确保收集用户的文件删除事件12-a always,exit -F arch=b64 -S unlink -S unlinkat -S rename -S renameat -F auid&gt;=1000 -F auid!=4294967295 -k delete-a always,exit -F arch=b32 -S unlink -S unlinkat -S rename -S renameat -F auid&gt;=1000 -F auid!=4294967295 -k delete 在所有情况下，审核记录将只为非系统用户id（auid&gt;=1000）并将忽略守护进程事件（auid=4294967295）。 所有审计记录用标识符“delete”标记 5.11 确保收集对系统管理范围（sudoers）的更改12-w /etc/sudoers -p wa -k scope-w /etc/sudoers.d/ -p wa -k scope 5.12 监视sudo日志文件如果系统已正确配置为禁用su命令并强制所有管理员必须先登录，然后使用sudo执行特权命令， 然后所有管理员命令将被记录到/var/log/sudo.log文件. 每当执行命令时，审核事件将被触发为/var/log/sudo.log文件将打开文件进行写入，并执行管理命令将写入日志。 1-w /var/log/sudo.log -p wa -k actions 5.13 确保收集内核模块加载和卸载1234-w /sbin/insmod -p x -k modules-w /sbin/rmmod -p x -k modules-w /sbin/modprobe -p x -k modules-a always,exit -F arch=b64 -S init_module -S delete_module -k modules 5.14 确保审核配置是不可变的审核规则不能使用auditctl修改。设置标志“-e2“强制将审核置于不可变模式。进行审核更改只能对系统重新启动。 1-e 2 6 参考https://cloud.tencent.com/developer/article/1653503 https://www.cnblogs.com/zydev/p/13086818.html auditctl(8) - Linux man page (die.net)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>审计</tag>
        <tag>linux</tag>
        <tag>安全</tag>
        <tag>auditd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql开启密码复杂度配置]]></title>
    <url>%2F2022%2F01%2F13%2Fmysql%E5%BC%80%E5%90%AF%E5%AF%86%E7%A0%81%E5%A4%8D%E6%9D%82%E5%BA%A6%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[mysql 5.6对密码的强度进行了加强，推出了 validate_password 插件。支持密码的强度要求。 mysql默认带validate_password 插件，直接装载插件即可。 1 装载plugin1mysql&gt;&gt;INSTALL PLUGIN validate_password SONAME 'validate_password.so'; 2 修改配置文件在配置文件中打开 123plugin-load=validate_password.sovalidate-password=FORCE_PLUS_PERMANENTvalidate_password_policy=2 3 验证当建立用户密码时，如果不符合预设的规则，那么就不会通过： 12mysql&gt; grant all on *.* totester@'localhost' identified by 'tasssss';ERROR 1819 (HY000): Your password does not satisfy the current policy requirements； 修改密码时，一样不通过 12mysql&gt; set password for test@‘localhost’ = password('123qwe!');ERROR 1819 (HY000): Your password does not satisfy the current policy requirements 4 密码默认规则 必须包含数字 必须包含大、小写字母 必须包含特殊字符 必须大于等于8位 5 插件说明5.1 相关选项12345678910111213validate-password=ON/OFF/FORCE/FORCE_PLUS_PERMANENT # 决定是否使用该插件(及强制/永久强制使用)。validate_password_dictionary_file # 插件用于验证密码强度的字典文件路径。validate_password_length # 密码最小长度。validate_password_mixed_case_count # 密码至少要包含的小写字母个数和大写字母个数。validate_password_number_count # 密码至少要包含的数字个数。validate_password_policy # 密码强度检查等级，0/LOW、1/MEDIUM、2/STRONG。validate_password_special_char_count # 密码至少要包含的特殊字符数。 其中，关于validate_password_policy-密码强度检查等级： 123450/LOW # 只检查长度。1/MEDIUM # 检查长度、数字、大小写、特殊字符。2/STRONG # 检查长度、数字、大小写、特殊字符字典文件。 5.2 插件的安装启用插件对应的库对象文件需在配置选项plugin_dir指定的目录中。 可使用--plugin-load=validate_password.so，在server启动时载入插件，或者将plugin-load=validate_password.so写入配置文件。 也可以通过如下语句在server运行时载入插件（会注册进mysql.plugins表） 1mysql&gt; INSTALL PLUGIN validate_password SONAME 'validate_password.so'; 5.3 为阻止该插件在运行时被删除可在配置文件中添加12plugin-load=validate_password.sovalidate-password=FORCE_PLUS_PERMANENT 4 参考MySql5.6使用validate password 插件加强密码强度的安装及使用方法Mysql脚本之家 (jb51.net)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>审计</tag>
        <tag>密码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql开启日志审计]]></title>
    <url>%2F2022%2F01%2F12%2Fmysql%E5%BC%80%E5%90%AF%E6%97%A5%E5%BF%97%E5%AE%A1%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[1安装1.1 下载插件Releases · mcafee/mysql-audit · GitHub 备注：下载插件一定要注意插件支持数据库的版本，否则安装失败。 1.2 解压插件12unzip audit-plugin-mysql-5.7-1.1.7-805-linux-x86_64.zipcd audit-plugin-mysql-5.7-1.1.7-805/lib 1.3 查询mysql的插件目录1SHOW GLOBAL VARIABLES LIKE 'plugin_dir' 1.4 拷贝libaudit_plugin.so到mysql插件目录123cp libaudit_plugin.so /usr/local/mysql/lib/plugin/chmod +x libaudit_plugin.sochown mysql:mysql libaudit_plugin.so 1.5 安装插件1mysql&gt; install plugin audit soname 'libaudit_plugin.so'; 1.6 开启设计功能1mysql&gt; set global audit_json_file=1; 1.7 修改mysql配置文件1234audit_record_cmds='insert,delete,update,create,drop,alter,grant,truncate' # 记录操作内容audit_json_file=on # 保证mysql重启后自动启动插件plugin-load=AUDIT=libaudit_plugin.so # 防止删除了插件，重启后又会加载audit_json_log_file=/data/logs/mysql/mysql_audit.json # 日志路径（可选，默认在数据库data目录下生成mysql_audit.json日志文件） 2 其他相关命令 查询插件相关配置 1mysql &gt; SHOW GLOBAL VARIABLES LIKE '%audit%'; 查询插件版本 1mysql &gt; show global status like 'AUDIT_version'; 显示当前插件 1mysql &gt; show plugins; 3 延申内容3.1 docker版mysql5.6安装说明采用docker版本mysql5.6安装插件，发现只有mysql5.6.36版本可以正常加载插件安装，该版本以上没有成功。 参考 https://www.cnblogs.com/syy1757528181/p/14480232.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>日志</tag>
        <tag>审计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgresql设置密码复杂度校验]]></title>
    <url>%2F2022%2F01%2F12%2FPostgresql%E8%AE%BE%E7%BD%AE%E5%AF%86%E7%A0%81%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%A0%A1%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[1 postgresql设置密码复杂度校验1.1 默认密码校验策略在PG中可以使用passwordcheck.so模块实现密码复杂度校验的功能，不过这个工具的默认要求很低，只可用于简单的密码复杂度校验，默认检查规则如下： 密码长度大于8位 密码不能与用户名相同 密码必须包括字母和数字 1.1.1 passwordcheck密码校验安装完PostgreSQL之后，默认是没有开启密码复杂度，为了数据库安全以及应对等保测评等要求，有时我们需要设置密码复杂度。 PostgreSQL支持通过动态库的方式扩展PG的功能，pg在使用这些功能时需要预加载相关的共享库。而密码复杂度可以通过预加载passwordcheck.so模块实现。 有几种设置可用于将共享库预加载到服务器中，如下： local_preload_libraries (string) session_preload_libraries (string) shared_preload_libraries (string) 下面介绍shared_preload_libraries (string)方式加载passwordcheck.so模块，此模块可以检查密码，如果密码太弱，他会拒绝连接；创建用户或修改用户密码时，强制限制密码的复杂度，限制密码不能重复使用例如密码长度，包含数字，字母，大小写，特殊字符等，同时排除暴力破解字典中的字符串。 1.1.2 shared_preload_libraries 方式启用passwordcheck.so模块在PG库的数据目录下（centos默认路径为：/var/lib/pgsql/11/data，windows默认路径为：D:\PostgreSQL\11\data）找到postgresql.conf文件，修改 修改内容行为：shared_preload_libraries = &#39;passwordcheck&#39; # (change requires restart)。 修改完成后重启服务服务生效（systemctl restart postgresql-11）。 1.1.3 验证1234567891011121314# 提示密码太短postgres=# create role ttt with password '123123';ERROR: password is too short# 提示密码必须包含字母和非字母postgres=# create role ttt with password '12345678';ERROR: password must contain both letters and nonletterspostgres=# create role ttt with password 'qweqweqwe';ERROR: password must contain both letters and nonletters# 提示密码不能包含用户名postgres=# create role tttt with password 'tttt123456';ERROR: password must not contain user name 1.2 增强密码校验策略通过修改源代码实现。本文记录如何通过修改源码passwordcheck.c达到增强复杂度检验的目的，修改后验证规则如下： 密码长度大于8位 密码不能与用户名相同 密码必须包括字母 密码必须包括数字 密码必须包括特殊字符 1.2.1 步骤1.2.1.1 实验环境实验环境：CentOS7.6 + PG11.8 source code 源码下载地址: https://www.postgresql.org/ftp/source/v11.8/postgresql-11.8.tar.gz 源码安装文档：https://www.postgresql.org/docs/11/install-short.htm 1.2.1.2 使用方式 替换目录 ../postgresql-11.4/contrib/passwordcheck 下的 passwordcheck.c 编译安装 make &amp;&amp; make install postgresql配置文件内修改 (postgresql.conf) shared_preload_libraries = ‘passwordcheck’ passwordcheck.level = ‘true’ 1.2.1.3 效果当密码长度足够，不符合规则的时候，无法新建用户 1.2.1.4 源码修改将下载后的源码解压缩, 找到postgresql-11.8/contrib/passwordcheck/passwordcheck.c源文件，修改后保存退出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192/*------------------------------------------------------------------------- * Luckyness * 20191202 * 在源代码上修改自用，配置pg密码必须包含特殊字符 * pg版本11.4 * 使用方式： * 替换目录 ../postgresql-11.4/contrib/passwordcheck 下的 passwordcheck.c * 编译安装 make &amp;&amp; make install * postgresql配置文件内修改 (postgresql.conf) * shared_preload_libraries = 'passwordcheck' * passwordcheck.level = 'true' *------------------------------------------------------------------------- *//*------------------------------------------------------------------------- * * passwordcheck.c * * * Copyright (c) 2009-2018, PostgreSQL Global Development Group * * Author: Laurenz Albe &lt;laurenz.albe@wien.gv.at&gt; * * IDENTIFICATION * contrib/passwordcheck/passwordcheck.c * *------------------------------------------------------------------------- */#include "postgres.h"#include &lt;ctype.h&gt;#ifdef USE_CRACKLIB#include &lt;crack.h&gt;#endif#include "commands/user.h"#include "libpq/crypt.h"#include "fmgr.h"/* 引入扩展 */#include "utils/guc.h"PG_MODULE_MAGIC;/* * 配置文件内passwordcheck.level='true' 为需要特殊字符 * passwordcheck.level='false' 为只需要英文和数字 */static bool passwordcheck_level = false;/* passwords shorter than this will be rejected */#define MIN_PWD_LENGTH 8extern void _PG_init(void);/* * check_password * * performs checks on an encrypted or unencrypted password * ereport's if not acceptable * * username: name of role being created or changed * password: new password (possibly already encrypted) * password_type: PASSWORD_TYPE_* code, to indicate if the password is * in plaintext or encrypted form. * validuntil_time: password expiration time, as a timestamptz Datum * validuntil_null: true if password expiration time is NULL * * This sample implementation doesn't pay any attention to the password * expiration time, but you might wish to insist that it be non-null and * not too far in the future. */static voidcheck_password(const char *username, const char *shadow_pass, PasswordType password_type, Datum validuntil_time, bool validuntil_null)&#123; if (password_type != PASSWORD_TYPE_PLAINTEXT) &#123; /* * Unfortunately we cannot perform exhaustive checks on encrypted * passwords - we are restricted to guessing. (Alternatively, we could * insist on the password being presented non-encrypted, but that has * its own security disadvantages.) * * We only check for username = password. */ char *logdetail; if (plain_crypt_verify(username, shadow_pass, username, &amp;logdetail) == STATUS_OK) ereport(ERROR, (errcode(ERRCODE_INVALID_PARAMETER_VALUE), errmsg("password must not equal user name"))); &#125; else &#123; /* * For unencrypted passwords we can perform better checks */ const char *password = shadow_pass; int pwdlen = strlen(password); int i; /* bool pwd_has_letter,*/ bool pwd_has_number,pwd_has_special,pwd_has_letter; /* enforce minimum length */ if (pwdlen &lt; MIN_PWD_LENGTH) ereport(ERROR, (errcode(ERRCODE_INVALID_PARAMETER_VALUE), errmsg("password is too short"))); /* check if the password contains the username */ if (strstr(password, username)) ereport(ERROR, (errcode(ERRCODE_INVALID_PARAMETER_VALUE), errmsg("password must not contain user name"))); if(passwordcheck_level) &#123; /* check if the password contains both letters and number and specialchar */ pwd_has_number = false; pwd_has_special = false; pwd_has_letter = false; for (i = 0; i &lt; pwdlen; i++) &#123; if (isalpha((unsigned char) password[i])) pwd_has_letter = true; else if (isdigit((unsigned char) password[i])) pwd_has_number = true; else pwd_has_special = true; &#125; if (!pwd_has_number || !pwd_has_letter || !pwd_has_special) ereport(ERROR, (errcode(ERRCODE_INVALID_PARAMETER_VALUE), errmsg("password must contain both letters and number and specialchar"))); &#125; else &#123; /* check if the password contains both letters and non-letters */ pwd_has_letter = false; pwd_has_number = false; for (i = 0; i &lt; pwdlen; i++) &#123; if (isalpha((unsigned char) password[i])) pwd_has_letter = true; else pwd_has_number = true; &#125; if (!pwd_has_letter || !pwd_has_number) ereport(ERROR, (errcode(ERRCODE_INVALID_PARAMETER_VALUE), errmsg("password must contain both letters and nonletters"))); &#125;#ifdef USE_CRACKLIB /* call cracklib to check password */ if (FascistCheck(password, CRACKLIB_DICTPATH)) ereport(ERROR, (errcode(ERRCODE_INVALID_PARAMETER_VALUE), errmsg("password is easily cracked")));#endif &#125; /* all checks passed, password is ok */&#125;/* * Module initialization function */void_PG_init(void)&#123; /* 密码级别参数 */ DefineCustomBoolVariable( "passwordcheck.level", gettext_noop("passwordcheck_level true: Password must contain leter, number, special characters;false : Password must contain leter, special characters"), NULL, &amp;passwordcheck_level, false, PGC_POSTMASTER, GUC_SUPERUSER_ONLY, NULL, NULL, NULL); /* activate password checks when the module is loaded */ check_password_hook = check_password;&#125; 1.2.1.5 编译123456789101112131415161718192021222324##编译安装pg server##进入源码解压目录, 执行cd postgresql-11.8./configure--prefix=/u01/pgsql11.8 --without-zlib --without-readline --&gt;&gt;&gt;&gt;指定安装目录为/u01/pgsql11.8, 处于演示目的,所以没有安装zlib和readline包。makemake install##编译安装contrib,是一些第三方组织贡献出来的工具代码。##进入源码解压目录执行cd postgresql-11.8/contribmakemake install##编译完成后，在postgresql-11.8/contrib/passwordcheck/目录下增加passwordcheck.so文件##编译后的文件，可以迁移到同版本数据库内使用 1.2.2 验证首先开启passwordcheck验证, 修改参数文件postgresql.conf 123# 修改如下shared_preload_libraries= 'passwordcheck'passwordcheck.level='true‘ 重启实例生效 1pg_ctl restart 测试 12345678910111213141516171819202122232425262728293031# 提示密码太短postgres=# create role btest with login password '123';ERROR: password is too short# 提示必须同时包括字母、数字、特殊字符postgres=# create role btest with login password '12345678';ERROR: password must contain both letters and number and specialcharpostgres=# create role btest with login password 'qqqqqqqq';ERROR: password must contain both letters and nonletterspostgres=# create role btest with login password '!!!!!!!!!!!!';ERROR: password must contain both letters and nonletterspostgres=# create role btest with login password '12345678a';ERROR: password must contain both letters and number and specialcharpostgres=# create role btest with login password '1234567!';ERROR: password must contain both letters and nonletterspostgres=# create role btest with login password '12345678a!';CREATE ROLE# 修改密码提示必须同时包含字母、数字和特殊字符postgres=# alter role btest password '12345678!';ERROR: password must contain both letters and number and specialchar# 提示密码不能包含用户名postgres=# alter role btest password '123456btest!';ERROR: password must not contain user name 1.2.3 问题警告这里还存在个问题，就是通过\password命令修改的话，可以输入不满足长度的密码，原因是使用\password时passwordcheck检查的是加密后的口令，官方文档提到过，检查md5加密后的口令是很困难的，所以当passwordcheck检查加密的口令时，只检查密码是否与用户名相同这一项，实际上是将用户名通过md5加密后与数据库中的md5密码做比较，如果相同，则报错口令不能与用户名相同。 123456789101112131415161718192021postgres=#\password bertEnter new password: ----&gt;&gt;&gt;&gt;&gt;可以输入小于8位的口令, 而不被阻. 但是输入的是与用户名相同的话可以被检测出来。Enter itagain:postgres=# postgres=#\set ECHO_HIDDEN ONpostgres=#\password bertEnter newpassword:Enter itagain:*********QUERY **********ALTERUSER bert PASSWORD 'md5efabd7549b98ddce9b14ba5e2e83eae1' 1.4 参考https://www.modb.pro/db/171257 https://www.cnblogs.com/Luckyness/p/11996834.html https://github.com/Luckyness/passwordcheck/blob/master/passwordcheck.c]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
        <tag>密码校验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu20.04安装NVIDIA显卡驱动+CUDA]]></title>
    <url>%2F2022%2F01%2F12%2FPostgresql%E5%BC%80%E5%90%AF%E6%97%A5%E5%BF%97%E5%AE%A1%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[postgresql开启日志审计1、审计清单说明 123456789101112131415161718192021222324252627282930313233logging_collector # 是否开启日志收集开关，默认off，推荐onlog_destination -- 日志记录类型，默认是stderr，只记录错误输出，推荐csvlog，总共包含：`stderr, csvlog, syslog, and eventlog,`log_directory # 日志路径，默认是$PGDATA/pg_log, log_filename # 日志名称，默认是postgresql-%Y-%m-%d_%H%M%S.loglog_file_mode # 日志文件类型，默认为0600log_truncate_on_rotation # 默认为off，设置为on的话，文件内容覆盖方式：off后面附加，on：清空再加log_rotation_age # 保留单个文件的最大时长,默认是1d,也有1h,1min,1slog_rotation_size # 保留单个文件的最大尺寸，默认是10MBlog_error_verbosity # 默认为default，verbose表示冗长的log_connections # 用户session登陆时是否写入日志，默认off，推荐为onlog_disconnections # 用户session退出时是否写入日志，默认off，推荐为onlog_statement --`记录用户登陆数据库后的各种操作` 1. none，即不记录 2. ddl(记录create,drop和alter) 3. mod(记录ddl+insert,delete,update和truncate) 4. all(mod+select)log_min_duration_statement = 2s -- `记录超过2秒的SQL`log_checkpoints = on # 检查点和重启点被记录再服务器日志中。一些统计信息也被包括再日志消息中，包括写入缓冲区的数据和写他们呢所花的时间。这个参数智能在postgresql.conf文件中或在服务器命令行上设置。默认是关闭（off）log_lock_waits ＝ on # 如果一个会话等待某个类型的锁的时间超过deadlock_timeout的值，该参数决定是否在数据库日志中记录这个信息。deadlock_timeout ＝ 1s # 数据库的锁通常可以在pg_locks这个系统表里找，但这只是当前的锁表/行信息，如果你想看一天内有多少个超过死锁时间的锁发生，可以在日志里设置并查看，log_lock_waits 默认是off，可以设置开启。这个可以区分SQL慢是资源紧张还是锁等待的问题。 2、推荐的设置参数 12345678910111213logging_collector = onlog_destination = 'csvlog'log_truncate_on_rotation = onlog_connections = onlog_disconnections = onlog_error_verbosity = verboselog_statement = ddl# 这部分是postgresql.conf文件中没有的log_min_duration_statement = 60s # 记录超过60s的SQLlog_checkpoints = on log_lock_waits ＝ ondeadlock_timeout ＝ 1s 123# 查看日志目录和日志文件名show log_directory;show log_filename; 3、参数修改方法 直接修改配置文件 postgresql.conf默认位于$PGDATA目录下。 vim /usr/data/pgsql/data/postgresql.conf 用超级用户运行：postgres=# SELECT pg_reload_conf(); show命令可以查询当前状态 4、 参考文档 http://t.zoukankan.com/tiandi-p-13568675.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
        <tag>日志</tag>
        <tag>审计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch导入导出数据-Elasticdump]]></title>
    <url>%2F2022%2F01%2F12%2FElasticsearch%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE-Elasticdump%2F</url>
    <content type="text"><![CDATA[1 导入/导出导出： 1234#因为导出的是mapping，所以设置type为mapping$ elasticdump --input http://192.168.56.105:9200/ --output ./test_index_mapping.json --type=mapping#因为导出的是data（真实数据）所以设置type为data$ elasticdump --input http://192.168.56.105:9200/ --output ./test_index.json --type=data 导入： 123456# 创建索引$ curl -XPUT http://192.168.56.104:9200/test_index#因为导入的是mapping，所以设置type为mapping$ elasticdump --input ./test_index_mapping.json --output http://192.168.56.105:9200/ --type=mapping#因为导入的是data（真实数据）所以设置type为data$ elasticdump --input ./test_index.json --output http://192.168.56.105:9200/ --type=data 2 使用Elasticdump镜像进行数据导入/导出12345# 镜像下载$ docker pull taskrabbit/elasticsearch-dump# 下面还是例子：通过镜像导出数据到本地# 创建一个文件夹用于保存导出数据$ mkdir -p /root/data 导出 12345678910# 下面需要对路径进行映射并执行命令（导出mapping）$ docker run --rm -ti -v /data:/tmp taskrabbit/elasticsearch-dump \ --input=http://production.es.com:9200/my_index \ --output=/tmp/my_index_mapping.json \ --type=mapping# 导出（data）$ docker run --rm -ti -v /root/data:/tmp taskrabbit/elasticsearch-dump \ --input=http://192.168.56.104:9200/test_index \ --output=/tmp/elasticdump_export.json \ --type=data 导入 123456789101112# 创建索引$ curl -XPUT http://192.168.56.104:9200/test_index# 导入mapping$ docker run --rm -ti -v /data:/tmp taskrabbit/elasticsearch-dump \ --input=/tmp/my_index_mapping.json \ --output=http://192.168.56.104:9200/my_index \ --type=mapping# 导入（data）$ docker run --rm -ti -v /root/data:/tmp taskrabbit/elasticsearch-dump \ --input=/tmp/elasticdump_export.json \ --output=http://192.168.56.104:9200/test_index \ --type=data 3 以下内容为ES -&gt; ES的数据迁移例子123456789$ docker run --rm -ti taskrabbit/elasticsearch-dump \ --input=http://production.es.com:9200/my_index \ --output=http://staging.es.com:9200/my_index \ --type=mapping $ docker run --rm -ti taskrabbit/elasticsearch-dump \ --input=http://production.es.com:9200/my_index \ --output=http://staging.es.com:9200/my_index \ --type=data]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>elasticdump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu20.04安装NVIDIA显卡驱动+CUDA]]></title>
    <url>%2F2022%2F01%2F12%2Fubuntu20-04%E5%AE%89%E8%A3%85NVIDIA%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8-CUDA%2F</url>
    <content type="text"><![CDATA[1 安装驱动1.1 查看显卡的型号打开终端，输入指令以查看电脑的显卡型号：· 1lspci | grep -i vga ubuntu-drivers devices 可以看到如下界面： model即为显卡的型号信息，此处为GeForce RTX 2070 SUPER；推荐的显卡驱动版本号为nvidia-driver-450 - distro non-free。 1.2 官网下载显卡驱动到官网下载对应的显卡驱动： 下载最新版官方 GeForce 驱动程序www.nvidia.cn ubuntu系统的话，选择对应显卡型号的Display Driver进行下载。 1.3 安装相关依赖1234sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compilersudo apt-get install --no-install-recommends libboost-all-devsudo apt-get install libopenblas-dev liblapack-dev libatlas-base-devsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev 1.4 禁用系统默认显卡驱动sudo gedit /etc/modprobe.d/blacklist-nouveau.conf 打开文件，在文件末尾写入： 12blacklist nouveauoptions nouveau modeset=0 保存后手动更新； 1sudo update-initramfs -u 电脑重启，输入下列指令进行确认，若无输出，则禁用成功： 1lsmod | `grep` nouveau 1.5 停止lightdm桌面服务1sudo service lightdm stop 如果提示没有lightdm，就安装一下，再执行上面的命令 1sudo apt install -y lightdm （因为后面使用的命令行界面tty1~7 需要lightdm） 接下来按住键盘的Ctrl+Alt+F1，如果黑屏就切换到tty2，按Ctrl+Alt+F2 如果Ctrl+Alt+F1后还显示图形化桌面就重启一下电脑，之后不要登录，在登录界面直接按Ctrl+Alt+F1或F2，这时应该进了命令行界面了，然后登录root用户或者你的普通用户，如果登录root用户时所有密码都登录失败，一般是因为你装系统时没设置root密码的原因，这时就需要设置一下root账户密码，执行以下命令： 1sudo passwd root 然后会让你输入你的普通用户密码，再输入两次root密码，然后就可以在命令行界面登录root账户了。 1.6 安装驱动 （1）首先查看你有没有安装gcc 1gcc --version 如果没有安装gcc一般make也没安装，这时需要安装一下gcc和make 1234sudo apt install gcc sudo apt install make# 或者直接执行sudo apt install gcc &amp; make # 同时安装gcc和make，不用一条一条执行了，效果和上面两条命令相同 （2）执行安装 cd到你下载的显卡驱动的目录 执行： 12345sudo chmod a+x NVIDIA-Linux-x86_64-450.80.02.runsudo ./NVIDIA-Linux-x86_64-450.80.02.run -no-x-check -no-nouveau-check -no-opengl-files# -no-x-check:安装时关闭X服务# -no-nouveau-check: 安装时禁用nouveau# -no-opengl-files:只安装驱动文件，不安装OpenGL文件 下面者两个按图中选择，其他默认就好 安装完成以后执行 1nvidia-smi 如果显示如下内容则表示安装成功 2 安装CUDA 从上面的图中可以看到我的显卡最高可以支持cuda11.0(绿色框内) 下载cuda https://developer.nvidia.com/cuda-toolkit-archive 首先需要知道不同版本的cuda需要的gcc版本是不同的，cuda11.0与gcc的对应关系如下： 其他cuda版本与gcc的版本关系可以在cuda对应版本后面的document—&gt;Installation Guide Linux下找到，不再赘述。 找到自己需要的cuda版本如下选择，最下面虚线框内会给出下载和命令 在终端中执行： 1wget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda_11.0.2_450.51.05_linux.run 下载完成后进行cuda安装 cd到下载的cuda所在目录 12# 安装cudasudo sh cuda_11.0.2_450.51.05_linux.run 正常安装的情况下，会出现下面显示： 这是软件协议，跟普通软件安装时一样，这时输入： accept 在Driver处敲回车，选择不安装驱动，因为之前已经安装过驱动程序，这里是因为每个cuda都会自带一套符合当前版本cuda最低要求的驱动程序，如果这里选择安装的驱动的话，在Windows上安装时会将之前安装的驱动覆盖，但是在linux上覆盖安装的话，可能会出现错误，具体没有试过，如果想试试可以自行测试。我们使用之前自己下载的驱动。 然后按方向键将光标选中最后的Install再回车，开始安装，过一会儿会显示如下： 如果显示如上图，则表示安装完成，这时还不算安装成功，根据上面提示需要配置环境变量，进行如下操作： 12345# 打开.bashrc文件gedit ~/.bashrc# 将下面的11.0替换为你的cuda版本，其他不变，如果不知道自己安装的是哪个版本，就去/usr/local/文件夹下找一下export PATH=/usr/local/cuda-11.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125; 完事儿以后，执行如下命令更新变量，使其生效 1source ~/.bashrc 在终端输入 1nvcc -V 如果显示 至此，cuda的安装就算完成了。 3 参考Ubuntu20.04安装NVIDIA显卡驱动+cuda+cudnn配置深度学习环境 (mlzhilu.com)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>显卡</tag>
        <tag>ubuntu</tag>
        <tag>驱动</tag>
        <tag>cuda</tag>
        <tag>nvidia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Tower 安装]]></title>
    <url>%2F2021%2F11%2F02%2FAnsible-Tower-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、ansible-tower简介1）公司中实现运维自动化的架构中主要用到ansible，ansible脚本在部署服务器指令行中显得不太直观。Ansible-Tower（之前叫做awx）是将ansible的指令界面化，简明直观，简单易用。 2）Ansibke-tower其实就是一个图形化的任务调度，复杂服务部署，IT自动化的一个管理平台，属于发布配置管理系统，支持Api及界面操作，Django编写。 3）Ansible-tower可以通过界面从github拉取最新playbook实施服务部署，提高生产效率。当然它也提供一个RESET API和命令行的CLI以供python脚本调用 官方网站：https://www.ansible.com/products/tower Ansible权威指南：http://www.ansible.com.cn/docs/tower.html 官方中文文档：https://docs.ansible.com/ansible-tower/3.8.4/html_zh/ 官方安装文档：http://docs.ansible.com/ansible-tower/latest/html/quickinstall/index.html 官方源下载地址：http://releases.ansible.com/ansible-tower/setup-bundle/ 二、ansible-tower安装及配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@tower ~]# cd /opt/[root@tower opt]# wget https://releases.ansible.com/ansible-tower/setup-bundle/ansible-tower-setup-bundle-3.6.2-1.el7.tar.gz[root@tower opt]# tar xf ansible-tower-setup-bundle-3.6.2-1.el7.tar.gz[root@tower opt]# cd ansible-tower-setup-bundle-3.6.2-1/[root@tower ansible-tower-setup-bundle-3.6.2-1]# lsbackup.yml bundle group_vars install.yml inventory licenses README.md rekey.yml restore.yml roles setup.sh[root@tower ansible-tower-setup-bundle-3.6.2-1]# vim inventory[tower]localhost ansible_connection=local [database] [all:vars]admin_password='tower' #tower登录密码 pg_host=''pg_port='' pg_database='awx'pg_username='awx'pg_password='tower'pg_sslmode='prefer' # set to 'verify-full' for client-side enforced SSL rabbitmq_username=towerrabbitmq_password='tower'rabbitmq_cookie=cookiemonster # Isolated Tower nodes automatically generate an RSA key for authentication;# To disable this behavior, set this value to false# isolated_key_generation=true # SSL-related variables # If set, this will install a custom CA certificate to the system trust store.# custom_ca_cert=/path/to/ca.crt # Certificate and key to install in nginx for the web UI and API# web_server_ssl_cert=/path/to/tower.cert# web_server_ssl_key=/path/to/tower.key # Use SSL for RabbitMQ inter-node communication. Because RabbitMQ never# communicates outside the cluster, a private CA and certificates will be# created, and do not need to be supplied.# rabbitmq_use_ssl=False # Server-side SSL settings for PostgreSQL (when we are installing it).# postgres_use_ssl=False# postgres_ssl_cert=/path/to/pgsql.crt# postgres_ssl_key=/path/to/pgsql.key #开始安装[root@tower ansible-tower-setup-bundle-3.6.2-1]# ./setup.sh 访问https://10.0.0.203/#/login 三、ansible-tower激活步骤简述： 安装Ansible Tower，以setup.sh执行成功且完毕为开始 修改licensing.py文件 运行”ansible-tower-service restart”重启服务 Enjoy it btw：不需要去官网申请Trial License 1. 安装没什么好说的，setup.sh执行完了且不报错即可。说人话就是页面可以正常打开，没有任何报错。 2. 修改licensing.py文件该文件位于： 1/var/lib/awx/venv/awx/lib/python3.6/site-packages/awx/main/utils/licensing.py 该文件内的方法是负责License验证的核心，将其用你熟悉的编辑器打开 找到validate方法，该方法就负责License的验证，在我这其位于该文件的409行。行数可能随着版本的升级或修改不一定准确，以方法名为主。 该方法原文如下： 12345678910111213141516171819202122232425262728293031def validate(self): # Return license attributes with additional validation info. attrs = copy.deepcopy(self._attrs) type = attrs.get('license_type', 'none') if (type == 'UNLICENSED' or False): attrs.update(dict(valid_key=False, compliant=False)) return attrs attrs['valid_key'] = True if Host: current_instances = Host.objects.active_count() else: current_instances = 0 available_instances = int(attrs.get('instance_count', None) or 0) attrs['current_instances'] = current_instances attrs['available_instances'] = available_instances free_instances = (available_instances - current_instances) attrs['free_instances'] = max(0, free_instances) license_date = int(attrs.get('license_date', 0) or 0) current_date = int(time.time()) time_remaining = license_date - current_date attrs['time_remaining'] = time_remaining if attrs.setdefault('trial', False): attrs['grace_period_remaining'] = time_remaining else: attrs['grace_period_remaining'] = (license_date + 2592000) - current_date attrs['compliant'] = bool(time_remaining &gt; 0 and free_instances &gt;= 0) attrs['date_warning'] = bool(time_remaining &lt; self.SUBSCRIPTION_TIMEOUT) attrs['date_expired'] = bool(time_remaining &lt;= 0) return attrs 将其改成这个样子即可： 1234567891011121314151617181920212223242526272829303132333435363738def validate(self): # Return license attributes with additional validation info. attrs = copy.deepcopy(self._attrs) attrs['license_type'] = 'enterprise' # 设置License类型为企业版 attrs['instance_count'] = MAX_INSTANCES # 设置Host数量为MAX_INSTANCES，即9999999。扛不住就改成自己需要的数。 attrs['license_date'] = '2567433600' # 设置License过期日期为”2051-05-12 00:00:00“，Unix时间戳，有需要自己改 attrs['subscription_name'] = 'mxd' # 你猜 type = attrs.get('license_type', 'none') # 注释掉下面的判断 #if (type == 'UNLICENSED' or False): #attrs.update(dict(valid_key=False, compliant=False)) #return attrs attrs['valid_key'] = True # 直接将 valid_key 设为 true if Host: current_instances = Host.objects.active_count() else: current_instances = 0 available_instances = int(attrs.get('instance_count', None) or 0) attrs['current_instances'] = current_instances attrs['available_instances'] = available_instances free_instances = (available_instances - current_instances) attrs['free_instances'] = max(0, free_instances) license_date = int(attrs.get('license_date', 0) or 0) current_date = int(time.time()) time_remaining = license_date - current_date attrs['time_remaining'] = time_remaining if attrs.setdefault('trial', False): attrs['grace_period_remaining'] = time_remaining else: attrs['grace_period_remaining'] = (license_date + 2592000) - current_date attrs['compliant'] = bool(time_remaining &gt; 0 and free_instances &gt;= 0) attrs['date_warning'] = bool(time_remaining &lt; self.SUBSCRIPTION_TIMEOUT) attrs['date_expired'] = bool(time_remaining &lt;= 0) return attrs 3. 运行”ansible-tower-service restart”重启服务1ansible-tower-service restart 参考文档： https://www.milkfish.site/2021/05/13/1038.loli https://blog.csdn.net/m0_46393435/article/details/108171655]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgresql+repmgr高可用]]></title>
    <url>%2F2021%2F08%2F17%2Fpostgresql-repmgr%E9%AB%98%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1 参考文档repmgr 官方文档：repmgr 5.3dev Documentation 其他文档：Configure PostgreSQL Replication With Repmgr_qq_38461429的博客-CSDN博客 2 环境 主机名 IP OS 角色 软件包 pg1 192.168.44.131 Centos 7 primary postgresql 11 and repmgr pg2 192.168.44.131 Centos 7 standby postgresql 11 and repmgr pg3 192.168.44.135 Centos 7 standby postgresql 11 and repmgr 备注以下安装方式均采用在线安装方式。 3 修改hosts123192.168.44.131 pg1192.168.44.132 pg2192.168.44.135 pg3 4 配置SSH免密登陆12345678910111213# 生产密钥ssh-keygen -t rsa# 写入文件cat id_rsa.pub &gt; authorized_keys# 赋权 600chmod +600 authorized_keys# 拷贝 pg2\pg3 密钥到一个authorized_keys文件ssh pg2 cat /root/.ssh/id_rsa.pub &gt;&gt; authorized_keysssh pg3 cat /root/.ssh/id_rsa.pub &gt;&gt; authorized_keys# 拷贝authorized_keys 到pg2\pg3scp authorized_keys pg2:/root/.ssh/scp authorized_keys pg3:/root/.ssh/# ssh pg1\pg2\pg3 测试是否生效 注意后面postgresql安装后，也需要配置postgres用户的SSH免密登陆。 4 所有节点安装PG软件和postgis插件1234567891011# Install the repository RPM:sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm# Install PostgreSQL:sudo yum install -y postgresql11-server# Install Postgis:sudo yum install -y postgis25_11# Install rsyncsudo yum install -y rsync ‎如果您希望‎‎repmgr‎‎复制位于 PostgreSQL 数据目录之外的配置文件和/或测试功能，您还需要在两个服务器之间建立无密码的 SSH 连接，并且应安装‎‎rsync。 5 所有节点安装repmgr官方教程 1234567891011# 安装源curl https://dl.2ndquadrant.com/default/release/get/11/rpm | sudo bash# 检查源sudo yum repolist# 安装repmgr11sudo yum install repmgr11# 添加repmgr命令到环境变量vim /etc/profilePATH=/usr/pgsql-11/bin:$PATH 6 primary节点配置初始化数据库 12345# Optionally initialize the database and enable automatic start:# 初始化数据库sudo /usr/pgsql-11/bin/postgresql-11-setup initdbsudo systemctl enable postgresql-11sudo systemctl start postgresql-11 修改配置postgresql.conf 123456789101112# 在文件末尾添加如下配置# repmgr 配置# walsender 进程的最大数量max_wal_senders = 10# 复制槽的最大数量max_replication_slots = 10wal_level = 'hot_standby'hot_standby = on# Log配置archive_mode = onarchive_command = '/bin/true'wal_log_hints = on 一般只需要在primary节点修改配置即可，我为了方便主从切换，其他从节点可能升级为主节点，就把从节点的配置同样进行了修改。 修改配置pg_hba.conf 1234567local replication repmgr trusthost replication repmgr 127.0.0.1/32 trusthost replication repmgr 192.168.44.0/24 trustlocal repmgr repmgr trusthost repmgr repmgr 127.0.0.1/32 trusthost repmgr repmgr 192.168.44.0/24 trust 创建repmgr用户和repmgr数据库 1234# 切换到postgres用户执行，root用户执行报错bash-4.2$ createuser --superuser repmgrbash-4.2$ createdb --owner=repmgr repmgrbash-4.2$ psql -c ”ALTER USER repmgr SET search_path TO repmgr, "$user", public;“ 或者 1234567891011# 登陆数据库后执行相关创建# psqlpostgres# create user repmgr superuser login;postgres# alter user repmgr with password 'repmgr';postgres# create database repmgr;postgres# alter database repmgr owner to repmgr;postgres# ALTER USER repmgr SET search_path TO repmgr, "$user", public; 重启数据库 1systemctl restart postgresql-11 7 standby测试连接primary节点数据库12345[root@pg2 ~]# psql 'host=pg1 user=repmgr dbname=repmgr connect_timeout=2'psql (11.12)输入 "help" 来获取帮助信息.repmgr=# 注意standby节点不要初始化数据库，不用启动数据库，保证/var/lib/pgsql/11/data/目录为空 8 注册Primary节点修改配置repmgr.conf 1234567891011[root@pg1 ~]# vim /etc/rempgr/11/rempgr.conf# 添加如下配置node_id=1node_name='pg1'conninfo='host=pg1 user=repmgr dbname=repmgr connect_timeout=2'data_directory='/var/lib/pgsql/11/data'log_level=onlog_facility=STDERRlog_file='/var/log/repmgr/repmgr.log'pg_bindir='/usr/pgsql-11/bin' 注册Primary节点 1234567[root@pg1 ~]# su postgresbash-4.2$bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf primary registerINFO: connecting to primary database...NOTICE: attempting to install extension "repmgr"NOTICE: "repmgr" extension successfully installedNOTICE: primary node record (ID: 1) registered 验证集群状态 1234bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+-----------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | primary | * running | | default | 100 | 1 | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 查看repmgr数据表中的信息 123456789bash-4.2$ psql -U repmgr -d repmgr -h pg1psql (11.12)Type "help" for help.repmgr=# select * from repmgr.nodes; node_id | upstream_node_id | active | node_name | type | location | priority | conninfo | repluser | slot_name | config_file---------+------------------+--------+-----------+---------+----------+----------+------------------------------------------------------+----------+-----------+---------------------------- 1 | | t | pg1 | primary | default | 100 | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 | repmgr | | /etc/repmgr/11/repmgr.conf(1 row) 9 standby节点同步主节点数据配置repmbr.conf 1234node_id=2node_name='pg2'conninfo='host=pg2 user=repmgr dbname=repmgr connect_timeout=2'data_directory='/var/lib/pgsql/11/data' ‎使用--dry-run选项检查可克隆待机状态‎ 12345678910111213141516171819bash-4.2$ repmgr -h pg1 -U repmgr -d repmgr -f /etc/repmgr/11/repmgr.conf standby clone --dry-runNOTICE: destination directory "/var/lib/pgsql/11/data" providedINFO: connecting to source nodeDETAIL: connection string is: host=pg1 user=repmgr dbname=repmgrDETAIL: current installation size is 31 MBINFO: "repmgr" extension is installed in database "repmgr"INFO: replication slot usage not requested; no replication slot will be set up for this standbyINFO: parameter "max_wal_senders" set to 10NOTICE: checking for available walsenders on the source node (2 required)INFO: sufficient walsenders available on the source nodeDETAIL: 2 required, 10 availableNOTICE: checking replication connections can be made to the source server (2 required)INFO: required number of replication connections could be made to the source serverDETAIL: 2 replication connections requiredWARNING: data checksums are not enabled and "wal_log_hints" is "off"DETAIL: pg_rewind requires "wal_log_hints" to be enabledNOTICE: standby will attach to upstream node 1HINT: consider using the -c/--fast-checkpoint optionINFO: all prerequisites for "standby clone" are met ‎如果没有报告任何问题，则可以同步到备用节点‎ 12345678910111213141516171819bash-4.2$ repmgr -h pg1 -U repmgr -d repmgr -f /etc/repmgr/11/repmgr.conf standby cloneNOTICE: destination directory "/var/lib/pgsql/11/data" providedINFO: connecting to source nodeDETAIL: connection string is: host=pg1 user=repmgr dbname=repmgrDETAIL: current installation size is 31 MBINFO: replication slot usage not requested; no replication slot will be set up for this standbyNOTICE: checking for available walsenders on the source node (2 required)NOTICE: checking replication connections can be made to the source server (2 required)WARNING: data checksums are not enabled and "wal_log_hints" is "off"DETAIL: pg_rewind requires "wal_log_hints" to be enabledINFO: checking and correcting permissions on existing directory "/var/lib/pgsql/11/data"NOTICE: starting backup (using pg_basebackup)...HINT: this may take some time; consider using the -c/--fast-checkpoint optionINFO: executing: pg_basebackup -l "repmgr base backup" -D /var/lib/pgsql/11/data -h pg1 -p 5432 -U repmgr -X streamNOTICE: standby clone (using pg_basebackup) completeNOTICE: you can now start your PostgreSQL serverHINT: for example: pg_ctl -D /var/lib/pgsql/11/data startHINT: after starting the server, you need to register this standby with "repmgr standby register" 连接到主节点数据库查询 这表明先前克隆备用( pg2显示的字段 application_name)连接到主要的IP地址 192.168.44.132 123456789101112[root@pg1 ~]# su - postgres -c "psql -U repmgr -d repmgr -h pg1"psql (11.12)输入 "help" 来获取帮助信息.repmgr=# SELECT * FROM pg_stat_replication; pid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | backend_xmin | state | sent_lsn | write_lsn | flush_lsn | replay_lsn | write_lag | flush_lag | replay_lag | sync_priority | sync_state------+----------+---------+------------------+----------------+-----------------+-------------+-------------------------------+--------------+-----------+------------+------------+------------+------------+-----------+-----------+------------+---------------+------------ 1122 | 16384 | repmgr | pg2 | 192.168.44.132 | | 43672 | 2021-08-11 14:20:09.038097+08 | | streaming | 0/180001B0 | 0/180001B0 | 0/180001B0 | 0/180001B0 | | | | 0 | async(1 行记录) 连接从节点数据库查询 1234567891011repmgr=# SELECT * FROM pg_stat_wal_receiver; pid | status | receive_start_lsn | receive_start_tli | received_lsn | received_tli | last_msg_send_time | last_msg_receipt_time | latest_end_lsn | latest_end_time | slot_name | sender_host | sender_port | conninfo-------+-----------+-------------------+-------------------+--------------+--------------+-------------------------------+-------------------------------+----------------+------------------------------+-----------+-------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 65103 | streaming | 0/18000000 | 1 | 0/180001B0 | 1 | 2021-08-11 16:58:06.901487+08 | 2021-08-11 16:58:06.900314+08 | 0/180001B0 | 2021-08-11 14:25:17.92164+08 | | pg1 | 5432 | user=repmgr passfile=/var/lib/pgsql/.pgpass connect_timeout=2 dbname=replication host=pg1 port=5432 application_name=pg2 fallback_application_name=walreceiver sslmode=prefer sslcompression=0 krbsrvname=postgres target_session_attrs=any(1 行记录) 10 注册standby注册standby节点到集群 123456bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf standby registerINFO: connecting to local node "pg2" (ID: 2)INFO: connecting to primary databaseWARNING: --upstream-node-id not supplied, assuming upstream node is primary (node ID 1)INFO: standby registration completeNOTICE: standby node "pg2" (ID: 2) successfully registered 检查集群状态 12345bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+-----------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | primary | * running | | default | 100 | 1 | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | standby | running | pg1 | default | 100 | 1 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2 备注：standby节点默认为只读模式，切换为主节点后才能够写入数据。 主从集群配置完成，暂时只能手动切换 修改主节点的数据库数据，即可实时同步到从节点了。 11 主从切换(只适用于主从双节点)备注主从节点切换，只适用于主从双节点集群；3个以上节点集群执行切换命令后原主节点会自动宕掉，需要手动对其进行故障恢复。参考第 12章节故障转移。 11.1 场景各节点正常运行情况下，手动切换主从节点。 手动switchover，切换主从节点，提升从节点为主，主节点自动变为从节点 11.2 操作查看集群状态 12345bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+-----------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | standby | running | pg2 | default | 100 | 4 | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | primary | * running | | default | 100 | 4 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2 在standby节点执行命令，升级为primary节点 123456789101112131415161718192021222324252627282930# repmgr standby switchoverbash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf standby switchover# 下为执行过程NOTICE: executing switchover on node "pg1" (ID: 1)NOTICE: local node "pg1" (ID: 1) will be promoted to primary; current primary "pg2" (ID: 2) will be demoted to standbyNOTICE: stopping current primary node "pg2" (ID: 2)NOTICE: issuing CHECKPOINT on node "pg2" (ID: 2)DETAIL: executing server command "/usr/pgsql-11/bin/pg_ctl -D '/var/lib/pgsql/11/data' -W -m fast stop"INFO: checking for primary shutdown; 1 of 60 attempts ("shutdown_check_timeout")INFO: checking for primary shutdown; 2 of 60 attempts ("shutdown_check_timeout")NOTICE: current primary has been cleanly shut down at location 0/23000028NOTICE: promoting standby to primaryDETAIL: promoting server "pg1" (ID: 1) using "/usr/pgsql-11/bin/pg_ctl -w -D '/var/lib/pgsql/11/data' promote"等待服务器进程加载 .... 完成服务器加载完毕NOTICE: waiting up to 60 seconds (parameter "promote_check_timeout") for promotion to completeNOTICE: STANDBY PROMOTE successfulDETAIL: server "pg1" (ID: 1) was successfully promoted to primaryINFO: local node 2 can attach to rejoin target node 1DETAIL: local node's recovery point: 0/23000028; rejoin target node's fork point: 0/23000098NOTICE: setting node 2's upstream to node 1WARNING: unable to ping "host=pg2 user=repmgr dbname=repmgr connect_timeout=2"DETAIL: PQping() returned "PQPING_NO_RESPONSE"NOTICE: starting server using "/usr/pgsql-11/bin/pg_ctl -w -D '/var/lib/pgsql/11/data' start"NOTICE: NODE REJOIN successfulDETAIL: node 2 is now attached to node 1NOTICE: node "pg1" (ID: 1) promoted to primary, node "pg2" (ID: 2) demoted to standbyNOTICE: switchover was successfulDETAIL: node "pg1" is now primary and node "pg2" is attached as standbyNOTICE: STANDBY SWITCHOVER has completed successfully 查看集群状态 可以看到standby节点已升级为primary节点，原primary节点自动降级为standby节点。 123456bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+-----------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | primary | * running | | default | 100 | 5 | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | standby | running | pg1 | default | 100 | 4 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2bash-4.2$ 其他standby节点指向新的primary节点 1bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf standby follow 12 故障转移，手动切换12.1 场景主节点宕机，升级从节点为primary，恢复原primary节点，并降级为standby. 12.2 操作查看集群状态 123456bash-4.2$ repmgr cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+-----------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | primary | * running | | default | 100 | 5 | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | standby | running | pg1 | default | 100 | 5 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2bash-4.2$ 关闭主节点，模拟宕机 1systemctl stop postgresql-11.service 查看集群状态 pg1 已经掉线 1234567891011121314bash-4.2$ repmgr cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+---------------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | primary | ? unreachable | ? | default | 100 | | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | standby | running | ? pg1 | default | 100 | 5 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2WARNING: following issues were detected - unable to connect to node "pg1" (ID: 1) - node "pg1" (ID: 1) is registered as an active primary but is unreachable - unable to connect to node "pg2" (ID: 2)'s upstream node "pg1" (ID: 1) - unable to determine if node "pg2" (ID: 2) is attached to its upstream node "pg1" (ID: 1)HINT: execute with --verbose option to see connection error messagesbash-4.2$ 手动提升 pg2 节点为 primary节点 12345678910bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf standby promoteNOTICE: promoting standby to primaryDETAIL: promoting server "pg2" (ID: 2) using "/usr/pgsql-11/bin/pg_ctl -w -D '/var/lib/pgsql/11/data' promote"could not change directory to "/root": 权限不够waiting for server to promote.... doneserver promotedNOTICE: waiting up to 60 seconds (parameter "promote_check_timeout") for promotion to completeNOTICE: STANDBY PROMOTE successfulDETAIL: server "pg2" (ID: 2) was successfully promoted to primarybash-4.2$ 查看集群状态 pg2 已经为primary节点 1234567891011bash-4.2$ repmgr cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+-----------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | primary | - failed | ? | default | 100 | | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | primary | * running | | default | 100 | 6 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2WARNING: following issues were detected - unable to connect to node "pg1" (ID: 1)HINT: execute with --verbose option to see connection error messagesbash-4.2$ 集群其他standby节点重新指向新primary 1bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf standby follow 将宕机primary节点，降级为standby节点 123456789101112131415161718192021222324252627282930313233# 清空宕机节点数据目录bash-4.2$ rm -rf /var/lib/pgsql/11/data/*# 重新从新primary节点克隆备份服务器bash-4.2$ repmgr -h pg2 -U repmgr -d repmgr -f /etc/repmgr/11/repmgr.conf standby cloneNOTICE: destination directory "/var/lib/pgsql/11/data" providedINFO: connecting to source nodeDETAIL: connection string is: host=pg2 user=repmgr dbname=repmgrDETAIL: current installation size is 75 MBINFO: replication slot usage not requested; no replication slot will be set up for this standbyNOTICE: checking for available walsenders on the source node (2 required)NOTICE: checking replication connections can be made to the source server (2 required)INFO: checking and correcting permissions on existing directory "/var/lib/pgsql/11/data"NOTICE: starting backup (using pg_basebackup)...HINT: this may take some time; consider using the -c/--fast-checkpoint optionINFO: executing: /usr/pgsql-11/bin/pg_basebackup -l "repmgr base backup" -D /var/lib/pgsql/11/data -h pg2 -p 5432 -U repmgr -X streamNOTICE: standby clone (using pg_basebackup) completeNOTICE: you can now start your PostgreSQL serverHINT: for example: pg_ctl -D /var/lib/pgsql/11/data startHINT: after starting the server, you need to re-register this standby with "repmgr standby register --force" to update the existing node recordbash-4.2$ # 启动数据库systemctl start postgresql-11.service# 注册standby 节点bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf standby register --forceINFO: connecting to local node "pg1" (ID: 1)INFO: connecting to primary databaseINFO: standby registration completeNOTICE: standby node "pg1" (ID: 1) successfully registeredbash-4.2$ 查看集群状态 原宕机节点已经降级为standby节点。 123456bash-4.2$ repmgr cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+-----------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | standby | running | pg2 | default | 100 | 6 | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | primary | * running | | default | 100 | 6 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2bash-4.2$ 12.3 补充官方文档 主节点注销 1repmgr primary unregister -f /etc/repmgr/11/repmgr.conf --node-id=3 从节点注销 1repmgr standby unregister -f /etc/repmgr/11/repmgr.conf --node-id=3 13 故障自动转移13.1 编辑sudoers文件12345678910vi /etc/sudoers# postgres用户，所有sudo命令都不用密码postgres ALL=(ALL) NOPASSWD: ALL或者# postgres用户，特定sudo命令不需要密码Defaults:postgres !requirettypostgres ALL = NOPASSWD: /usr/bin/systemctl stop postgresql-11.service, /usr/bin/systemctl start postgresql-11.service, /usr/bin/systemctl restart postgresql-11.service, /usr/bin/systemctl reload postgresql-11.service, /usr/bin/systemctl start repmgr11.service, /usr/bin/systemctl stop repmgr11.service 备注: 修改sudoers文件是必须的，否则standby节点在自动切换接入新primary节点时候，会报错重启postgres命令失败。 原因：postgres用户采用sudo 命令执行postgresql数据重启操作的时候需要密码验证，没有权限。 13.2 配置postgresql.conf（所有节点）在文件内添加如下内容， 1shared_preload_libraries = 'repmgr' 13.3 配置repmgr.conf文件（所有节点）123456789101112131415161718192021# 必须项failover='automatic'promote_command='/usr/pgsql-11/bin/repmgr standby promote -f /etc/repmgr/11/repmgr.conf --log-to-file'follow_command='/usr/pgsql-11/bin/repmgr standby follow -f /etc/repmgr/11/repmgr.conf --log-to-file --upstream-node-id=%n'# 可选项# 注意：standby的priority值需要更改，因为默认是100，而primary使用的是默认值。这里设置standby的priority为60，其他standby的priority为40。另外，priority的值越大，成为primary的优先级就越高。priority=70connection_check_type=pingreconnect_attempts=6reconnect_interval=10monitoring_history=yesmonitor_interval_secs=2standby_disconnect_on_failover=trueprimary_visibility_consensus=truelog_status_interval=60service_start_command = 'sudo /usr/bin/systemctl start postgresql-11.service'service_stop_command = 'sudo /usr/bin/systemctl stop postgresql-11.service'service_restart_command = 'sudo /usr/bin/systemctl restart postgresql-11.service'service_reload_command = 'sudo /usr/bin/systemctl reload postgresql-11.service'repmgrd_service_start_command = 'sudo /usr/bin/systemctl start repmgr11.service'repmgrd_service_stop_command = 'sudo /usr/bin/systemctl stop repmgr11.service' 13.3 启动repmgr服务（所有节点）12systemctl start repmgrd11.servicesystemctl enable repmgrd11.service 启动完成后，可以在primary或者standby节点查询集群的events，如下： 123456[root@pg3 .ssh]# su - postgres -c "repmgr cluster event --event=repmgrd_start" Node ID | Name | Event | OK | Timestamp | Details---------+------+---------------+----+---------------------+------------------------------------------------------ 3 | pg3 | repmgrd_start | t | 2021-08-17 12:10:43 | monitoring connection to upstream node "pg1" (ID: 1) 2 | pg2 | repmgrd_start | t | 2021-08-17 12:08:54 | monitoring connection to upstream node "pg1" (ID: 1) 1 | pg1 | repmgrd_start | t | 2021-08-17 12:08:32 | monitoring cluster primary "pg1" (ID: 1) 13.4 故障模拟测试13.4.1停止pg1节点（primary）的postgresql服务1systemctl stop postgresql-11.service 查看集群状态，gp1节点状态变为unreachable 12345678910111213141516bash-4.2$ repmgr cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+---------------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | primary | ? unreachable | ? | default | 100 | | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | standby | running | ? pg1 | default | 70 | 10 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2 3 | pg3 | standby | running | ? pg1 | default | 60 | 10 | host=pg3 user=repmgr dbname=repmgr connect_timeout=2WARNING: following issues were detected - unable to connect to node "pg1" (ID: 1) - node "pg1" (ID: 1) is registered as an active primary but is unreachable - unable to connect to node "pg2" (ID: 2)'s upstream node "pg1" (ID: 1) - unable to determine if node "pg2" (ID: 2) is attached to its upstream node "pg1" (ID: 1) - unable to connect to node "pg3" (ID: 3)'s upstream node "pg1" (ID: 1) - unable to determine if node "pg3" (ID: 3) is attached to its upstream node "pg1" (ID: 1)HINT: execute with --verbose option to see connection error messages 1分钟后，再次查看集群状态，pg2升级为primary,gp3自动连接到pg2,作为主节点。 1234567891011bash-4.2$ repmgr cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+-----------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | primary | - failed | ? | default | 100 | | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | primary | * running | | default | 70 | 11 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2 3 | pg3 | standby | running | pg2 | default | 60 | 10 | host=pg3 user=repmgr dbname=repmgr connect_timeout=2WARNING: following issues were detected - unable to connect to node "pg1" (ID: 1)HINT: execute with --verbose option to see connection error messages 13.4.2 旧primary节点恢复当旧primary故障恢复后，并不会自动转换为standby，而是以primary角色独自运行，这时就需要将其重新加入到集群中。如下： 关闭postgresql服务 1bash-4.2$ repmgr node service --action=stop --checkpoint 重新加入集群 123456789101112bash-4.2$ repmgr -f /etc/repmgr/11/repmgr.conf -d "host=pg2 user=repmgr dbname=repmgr" node rejoin --force-rewindNOTICE: pg_rewind execution required for this node to attach to rejoin target node 2DETAIL: rejoin target server's timeline 11 forked off current database system timeline 10 before current recovery point 0/39000028NOTICE: executing pg_rewindDETAIL: pg_rewind command is "/usr/pgsql-11/bin/pg_rewind -D '/var/lib/pgsql/11/data' --source-server='host=pg2 user=repmgr dbname=repmgr connect_timeout=2'"NOTICE: 0 files copied to /var/lib/pgsql/11/dataNOTICE: setting node 1's upstream to node 2WARNING: unable to ping "host=pg1 user=repmgr dbname=repmgr connect_timeout=2"DETAIL: PQping() returned "PQPING_NO_RESPONSE"NOTICE: starting server using "sudo /usr/bin/systemctl start postgresql-11.service"NOTICE: NODE REJOIN successfulDETAIL: node 1 is now attached to node 2 查看集群状态 123456bash-4.2$ repmgr cluster show ID | Name | Role | Status | Upstream | Location | Priority | Timeline | Connection string----+------+---------+-----------+----------+----------+----------+----------+------------------------------------------------------ 1 | pg1 | standby | running | pg2 | default | 100 | 10 | host=pg1 user=repmgr dbname=repmgr connect_timeout=2 2 | pg2 | primary | * running | | default | 70 | 11 | host=pg2 user=repmgr dbname=repmgr connect_timeout=2 3 | pg3 | standby | running | pg2 | default | 60 | 11 | host=pg3 user=repmgr dbname=repmgr connect_timeout=2 到此，postgres高可用自动故障转移方案成功，配合VIP服务，可实现自动故障转移及服务高可用。 备注如果不能重新加入，可以将旧primary强制(-F)转换为standby。或参考 13章节，手动清除，克隆，加入standby。 repmgr -h pg2 -U repmgr -d repmgr -f /etc/repmgr/11/repmgr.conf standby clone -F systemctl start postgresql-12.service repmgr -f /etc/repmgr/11/repmgr.conf standby register -F 问题汇总错误1从节点同步主节点时，报错连接失败 bash-4.2$ repmgr -h pg1 -U repmgr -d repmgr -f /etc/repmgr/11/repmgr.conf standby clone –dry-runNOTICE: destination directory “/var/lib/pgsql/11/data” providedINFO: connecting to source nodeDETAIL: connection string is: host=pg1 user=repmgr dbname=repmgrDETAIL: current installation size is 31 MBINFO: “repmgr” extension is installed in database “repmgr”INFO: replication slot usage not requested; no replication slot will be set up for this standbyINFO: parameter “max_wal_senders” set to 10NOTICE: checking for available walsenders on the source node (2 required)INFO: sufficient walsenders available on the source nodeDETAIL: 2 required, 10 availableNOTICE: checking replication connections can be made to the source server (2 required)ERROR: connection to database failedDETAIL:致命错误: 没有来自主机 “192.168.44.132”, 用户”repmgr”, SSL 关闭的复制连接的pg_hba.conf记录 ERROR: connection to database failedDETAIL:致命错误: 没有来自主机 “192.168.44.132”, 用户”repmgr”, SSL 关闭的复制连接的pg_hba.conf记录 ERROR: unable to establish necessary replication connectionsHINT: check replication permissions on the source server 解决办法： 少加了 前三条授权，添加上后问题解决。 1234567local replication repmgr trusthost replication repmgr 127.0.0.1/32 trusthost replication repmgr 192.168.44.0/24 trustlocal repmgr repmgr trusthost repmgr repmgr 127.0.0.1/32 trusthost repmgr repmgr 192.168.44.0/24 trust 错误2同样同步时错误 bash-4.2$ repmgr -h pg1 -U repmgr -d repmgr -f /etc/repmgr/11/repmgr.conf standby cloneNOTICE: destination directory “/var/lib/pgsql/11/data” providedINFO: connecting to source nodeDETAIL: connection string is: host=pg1 user=repmgr dbname=repmgrDETAIL: current installation size is 31 MBINFO: replication slot usage not requested; no replication slot will be set up for this standbyNOTICE: checking for available walsenders on the source node (2 required)NOTICE: checking replication connections can be made to the source server (2 required)WARNING: data checksums are not enabled and “wal_log_hints” is “off”DETAIL: pg_rewind requires “wal_log_hints” to be enabledINFO: checking and correcting permissions on existing directory “/var/lib/pgsql/11/data”NOTICE: starting backup (using pg_basebackup)…HINT: this may take some time; consider using the -c/–fast-checkpoint optionINFO: executing:pg_basebackup -l “repmgr base backup” -D /var/lib/pgsql/11/data -h pg1 -p 5432 -U repmgr -X streampg_basebackup: 无法得到来自服务器的预写日志终止位置: 错误: 无法打开文件 “./postgresql.conf.bak”: 权限不够pg_basebackup: 删除数据目录 “/var/lib/pgsql/11/data” 的内容ERROR: unable to take a base backup of the source serverHINT: data directory (“/var/lib/pgsql/11/data”) may need to be cleaned up manually 原因： 1 数据库数据目录下存在非postgres用户权限的文件，postgresql.conf.bak权限是root权限，授权为postgres用户权限即可 2 从节点的数据目录要保证为空。 错误312345# repmgr standby switchover 命令bash-4.2$ repmgr standby switchover -f /etc/repmgr/11/repmgr.confNOTICE: checking switchover on node "pg2" (ID: 2) in --dry-run modeWARNING: unable to connect to remote host "pg1" via SSHERROR: unable to connect via SSH to host "pg1", user "" 解决办法： 创建postgres 用户免密SSH登陆。 错误41234bash-4.2$ repmgr standby switchoverNOTICE: executing switchover on node "pg1" (ID: 1)ERROR: unable to execute "repmgr" on "pg2"HINT: check "pg_bindir" is set to the correct path in "repmgr.conf"; current value: (not set) 解决办法： 修改repmgr.conf配置文件，增加 pg_bindir参数，设置postgresql的bin路径。 1pg_bindir='/usr/pgsql-11/bin' 错误5standby升级为primary节点后，其他低权重的standby连接新primary使出现postgresql重启错误。 1234567[2021-08-17 14:24:08] [INFO] local node 3 can attach to follow target node 2[2021-08-17 14:24:08] [DETAIL] local node's recovery point: 0/3802CD38; follow target node's fork point: 0/3802CD38[2021-08-17 14:24:08] [NOTICE] setting node 3's upstream to node 2[2021-08-17 14:24:08] [NOTICE] restarting server using "sudo /usr/bin/systemctl restart postgresql-11.service"[2021-08-17 14:24:10] [ERROR] unable to restart server[2021-08-17 14:24:10] [NOTICE] STANDBY FOLLOW failed[2021-08-17 14:24:10] [ERROR] connection to database failed 解决办法：配置postgres用户sudo命令免输入密码 12345678910vi /etc/sudoers# postgres用户，所有sudo命令都不用密码postgres ALL=(ALL) NOPASSWD: ALL或者# postgres用户，特定sudo命令不需要密码Defaults:postgres !requirettypostgres ALL = NOPASSWD: /usr/bin/systemctl stop postgresql-11.service, /usr/bin/systemctl start postgresql-11.service, /usr/bin/systemctl restart postgresql-11.service, /usr/bin/systemctl reload postgresql-11.service, /usr/bin/systemctl start repmgr11.service, /usr/bin/systemctl stop repmgr11.service]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http1对比http2实测]]></title>
    <url>%2F2021%2F07%2F29%2Fhttp1%E5%AF%B9%E6%AF%94http2%E5%AE%9E%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[1 测试硬件条件 设备 CPU 内存 带宽 服务器 8核心 8G 千兆 客户端 4核心 16G 千兆 2 测试2.1 测试场景1无限制 2.1.1 测试结果http/1.1 ​ http/2 根据以上结果，因为每个请求花费的时间很小，加载速度几乎没有变化。 2.2 测试场景2限制请求带宽10MB/s 2.2.1 测试结果http/1.1 http/2 根据以上结果，限制传输带宽为10M/s，2700次请求，http/1.1下，450秒加载完成；http/2下260秒加载完成；效率提升在40%左右。 综上结果得出判断，http2更适合在长请求情况下提高请求量达到并发请求数提高加载速度，对于每个请求文件较小，返回花费时间较短的请求，性能和http1差不多。 同时，在高并发请求的情况下，对客户端的cpu性能和内存要求较高，提高客户端的性能瓶颈，可以提高并发请求数，达到更快加载的效果。 客户端资源占用]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>http2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机体验]]></title>
    <url>%2F2021%2F07%2F29%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[虚拟机体验之 QEMU 篇 - sunylat - 博客园 (cnblogs.com) 虚拟机体验之 KVM 篇 - sunylat - 博客园 (cnblogs.com) 虚拟机体验之 VirtualBox 篇 —— 性能强大的经典架构 - sunylat - 博客园 (cnblogs.com) 虚拟机体验之 Xen 篇 —— 令人脑洞大开的奇异架构 - sunylat - 博客园 (cnblogs.com)]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop优化及问题汇总]]></title>
    <url>%2F2021%2F07%2F16%2FHadoop%E4%BC%98%E5%8C%96%E5%8F%8A%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[[TOC] 1 HDFS副本设置 说明：HDFS文件上传默认创建3个副本，在3个节点上面分别写入一个副本； 这里修改副本数为默认2个。 修改client端的hdfs-site.xml文件，哪里上传，哪里修改。 修改hdfs-site.xml中的dfs.replication参数，默认为3，这里修改为1 也可以通过命令更改已经上传的文件的副本数。 1hadoop fs -setrep -R 2 / 通过命令指定上传文件的副本数 1hadoop dfs -D dfs.replication=1 -put testfile /testdir/ 服务端的修改方式，可以登录各个datanode节点修改hdfs-size.xml文件，也可以通过修改Ambari平台进行修改 HDFS&gt;&gt;configs&gt;&gt;Adbanced 保存，重启指定服务。 2 删除HDFS文件空间不释放-回收站 core-size.xml 中的fs.trash.interval 参数 删除检查点后的分钟数。如果为零，则禁用垃圾功能。可以在服务器和客户机上配置此选项。如果禁用了垃圾桶服务器端，则检查客户端配置。如果在服务器端启用了垃圾桶，那么将使用在服务器上配置的值，并忽略客户机配置值。 core-size.xml 中的fs.trash.checkpoint.interval 参数 垃圾检查点之间的分钟数。应小于或等于fs.trash.interval。如果为零，则该值设置为fs.trash.interval的值。每次检查点运行时，它都会在当前之外创建一个新的检查点，并删除几分钟前创建的超过fs.trash.interval的检查点。 fs.trash.interval是在指在这个回收周期之内，文件实际上是被移动到trash的这个目录下面，而不是马上把数据删除掉。等到回收周期真正到了以后，hdfs才会将数据真正删除。默认的单位是分钟。fs.trash.checkpoint.interval则是指垃圾回收的检查间隔，应该是小于或者等于fs.trash.interval。 2.1 举例操作2.1.1 一个副本备注：下面操作上传文件，默认只创建一个副本 通过hdfs dfs -rm 命令删除HDFS集群文件后，集群空间不释放（即DFS Used ） 12345# 上传文件hadoop fs -put /hw1/* /hw1/# 删除文件hadoop fs -rm /hw1/*# 因为默认开启了垃圾回收功能，文件被放到了回收站，所以容量没有变化 12# 清空回收站（跟windows不一样，它是打包，文件名为时间戳，时间到了依然删除）hadoop fs -expunge 发现清空回收站后，DFS Used依然没有变化。 手动删除回收站文件 12# 手动删除回收站的文件，查看空间回收hadoop fs -rm -f /user/hdfs/.Trash/210716162922 通过命令删除文件，跳过回收站 12# 跳过回收站，可达到上面操作删除文件释放空间同样的效果hadoop fs -rm -r -skipTrash /hw1/* 2.1.2 多个副本 12345# 上传600M文件创建2份副本，占用空间是双倍的hadoop fs -D dfs.replication=2 -put /hw1/* /hw1/# 删除文件hadoop fs -rm /hw1/*# 因为默认开启了垃圾回收功能，文件被放到了回收站，所以容量没有变化 12# 跳过回收站删除文件,空间回收了hadoop fs -rm -r -skipTrash /hw1/* 3 Ambari启动成功后File View无法访问参考链接 基本类似页面无法访问均可以通过重新创建可以得到解决 启动成功后，打开文件视图报错Issues detectedService ‘hdfs’ check failed: Server Error 查看日志：Caused by: org.apache.ambari.server.ClusterNotFoundException: Cluster not found, clusterId=2at org.apache.ambari.server.state.cluster.ClustersImpl.getCluster(ClustersImpl.java:277)at org.apache.ambari.server.view.ViewRegistry.getCluster(ViewRegistry.java:928)… 101 more 出现原因：在新建文件视图之后，对俩个文件视图进行了对比，怀疑可能是集群名字的问题 解决方案：新建一个文件视图 发现可以成功查看 4 Ambari Hive View 页面提示错误Service ‘userhome’ check failed: File does not exist: /user/admin 我访问Ambari页面的电脑默认的用户名是admin 解决办法：自己创建一个 1hadoop fs -mkdir /user/admin 5 Ambari Server Alert 的故障排查问题现象本篇文章适用于 HDI(3.6) ，内置 Ambari-2.5 以上版本中常见的异常报警： 1There are xx stale Alerts from 1 host（s）: 问题分析如果在 Ambari Server 中突然发现了大量的 stale alerts(&gt;20)，通常来说是因为 Ambari-agent 出现了线程异常，导致无法正常将 metrics 的结果返回给 Server。 该问题通常表现为所有的 HDI 服务都可以正常访问，但是 Ambar 监控界面中断断续续的出现 Ambari Server Alert 并且会断断续续的出现。 问题截图如下：1There are 36 stale Alerts from 1 host(s): 123456789101112131415161718192021222324252627282930 There are 27 stale alerts from 1 host(s):NameNode High Availability Health,hdp4.test [Ambari Agent Distro/Conf Select Versions (1h 30m), DataNode Health Summary (1h 26m), DataNode Heap Usage (1h 28m), DataNode Process (1h 26m), DataNode Storage (1h 28m), DataNode Unmounted Data Dir (1h 28m), DataNode Web UI (1h 26m), Flume Agent Status (1h 26m), HBase RegionServer Process (1h 26m), HDFS Capacity Utilization (1h 28m), HDFS Pending Deletion Blocks (1h 28m), HDFS Upgrade Finalized State (1h 26m), Host Disk Usage (1h 26m), JournalNode Web UI (1h 26m), Metrics Monitor Status (1h 26m), NameNode Blocks Health (1h 28m), NameNode Client RPC Processing Latency (Hourly) (1h 30m), NameNode Client RPC Queue Latency (Hourly) (1h 30m), NameNode Directory Status (1h 26m), NameNode Host CPU Utilization (1h 30m), NameNode Last Checkpoint (1h 26m), NameNode RPC Latency (1h 28m), NameNode Web UI (1h 26m), NodeManager Health (1h 26m), NodeManager Web UI (1h 26m), ZooKeeper Failover Controller Process (1h 26m)] 解决办法 使用 putty 登入头节点。 使用命令确认 Ambari-agent 的 PID: Ambari-agent status 。 使用命令: top |grep 5130 确认 Ambari-agent 进程的 CPU 使用率. 5130 需要替换成上条命令中的结果。 如果 CPU 使用率为 100% 则使用命令: service Ambari-agent restart 来重启 Ambari-agent service. 稍等几分钟，再次刷新 Ambari 界面，Alert 消失。]]></content>
      <categories>
        <category>Centos</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多环境批处理脚本]]></title>
    <url>%2F2021%2F07%2F12%2F%E5%A4%9A%E7%8E%AF%E5%A2%83%E6%89%B9%E5%A4%84%E7%90%86%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[version 3 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323@echo off &amp; SetLocal EnableDelayedExpansioncd /d %~dp0:: 检测mysql端口:: 检查端口，如果冲突则随机端口，重新执行检查set myport=16603rem set myport=137:myportDetectnetstat -aon|findstr "%myport%" &gt; NUL 2&gt;NUL &amp;&amp; set myport=%random% &amp;&amp; goto myportDetect || echo %myport%:: 因随机端口后myport的值后，替换端口后面会又空格，执行如下步骤，转换以下，使空格消除。for /F %%i in ('echo %myport%') do ( set myport=%%i)echo myport=%myport%set inipath="%cd%\mysql\mybak.ini"rem del "%cd%\mysql\my.ini" &gt;nul 2&gt;nulfor /f "delims=" %%a in (.\mysql\mybak.ini) do ( set a=%%a set a=!a:16603=%myport%! echo !a!&gt;&gt;"%cd%\mysql\my.ini"):: 检查redis端口set redisport=17963rem set redisport=137:redisportDetectnetstat -aon|findstr "%redisport%" &gt; NUL 2&gt;NUL &amp;&amp; set redisport=%random% &amp;&amp; goto redisportDetect || echo %redisport%for /F %%i in ('echo %redisport%') do ( set redisport=%%i)echo redisport=%redisport%rem set reconfpath="%cd%\redis\redis.windows-service-bak.conf"del "%cd%\redis\redis.windows-service.conf" &gt;nul 2&gt;nulfor /f "delims=" %%b in (.\redis\redis.windows-service-bak.conf) do ( set b=%%b set b=!b:17963=%redisport%! echo !b!&gt;&gt;"%cd%\redis\redis.windows-service.conf"):: 检查tomcat端口::: zxzb1 端口set zxzb1=18785rem set zxzb1=137:javaportDetect1netstat -aon|findstr "%zxzb1%" &gt; NUL 2&gt;NUL &amp;&amp; set zxzb1=%random% &amp;&amp; goto javaportDetect1 || echo %zxzb1%for /F %%i in ('echo %zxzb1%') do ( set zxzb1=%%i)echo zxzb1=%zxzb1%::: zxzb2 端口set zxzb2=18388rem set zxzb2=137:javaportDetect2netstat -aon|findstr "%zxzb2%" &gt; NUL 2&gt;NUL &amp;&amp; set zxzb2=%random% &amp;&amp; goto javaportDetect2 || echo %zxzb2%for /F %%i in ('echo %zxzb2%') do ( set zxzb2=%%i)echo zxzb2=%zxzb2%::: zxzb3 端口set zxzb3=18489rem set zxzb3=137:javaportDetect3netstat -aon|findstr "%zxzb3%" &gt; NUL 2&gt;NUL &amp;&amp; set zxzb3=%random% &amp;&amp; goto javaportDetect3 || echo %zxzb3%for /F %%i in ('echo %zxzb3%') do ( set zxzb3=%%i)echo zxzb3=%zxzb3%:: dos命令替换文本内容，存在弊端，对一些特殊服务转义存在问题，只适合简单文档的替换:: 原理读取文档每一行，检查每一行，存在匹配，替换到新文档，然后新文档覆盖原来的文档rem set javaconfpath="%cd%\service\conf\server_bak.xml"del "%cd%\service\conf\server.xml" &gt;nul 2&gt;nulfor /f "delims=" %%c in (.\service\conf\server_bak.xml) do ( set c=%%c set c=!c:18785=%zxzb1%! set c=!c:18388=%zxzb2%! set c=!c:18489=%zxzb3%! echo !c!&gt;&gt;"%cd%\service\conf\server.xml"):: 检查地图服务端口set mapport=10000rem set mapport=137:mapportDetectnetstat -aon|findstr "%mapport%" &gt; NUL 2&gt;NUL &amp;&amp; set mapport=%random% &amp;&amp; goto mapportDetect || echo %mapport%for /F %%i in ('echo %mapport%') do ( set mapport=%%i)echo mapport=%mapport%:: 替换env.bat内容:: 本方法是采用vbs命令方式，对特殊符号转义稍好，原理和dos命令差不多，复杂符号还是需要其他深入配置del ".\mapser\env.bat" &gt;nul 2&gt;nulecho set fso=createobject("scripting.filesystemobject") &gt;3.vbsecho set file=fso.opentextfile("env.bat") &gt;&gt;3.vbsecho s=file.readall &gt;&gt;3.vbsecho file.close &gt;&gt;3.vbsecho s=replace(s,"10000","%mapport%") &gt;&gt;3.vbsecho set file=fso.createtextfile(".\mapser\env.bat") &gt;&gt;3.vbsecho file.write s &gt;&gt;3.vbsecho file.close &gt;&gt;3.vbsstart 3.vbs:: 替换gdw.confdel ".\mapser\serv\conf\gdw.conf" &gt;nul 2&gt;nulecho set fso=createobject("scripting.filesystemobject") &gt;4.vbsecho set file=fso.opentextfile("gdw.conf") &gt;&gt;4.vbsecho s=file.readall &gt;&gt;4.vbsecho file.close &gt;&gt;4.vbsecho s=replace(s,"10000","%mapport%") &gt;&gt;4.vbsecho set file=fso.createtextfile(".\mapser\serv\conf\gdw.conf") &gt;&gt;4.vbsecho file.write s &gt;&gt;4.vbsecho file.close &gt;&gt;4.vbsstart 4.vbs:: 修改war config:zxzbconfigrem set zxzbconfpath=%cd%\service\webapps\zxzb\WEB-INF\classes\application-dev-bak.propertiesrem set vbsfile1=%cd%\service\webapps\zxzb\WEB-INF\classes\vbs1.txtrem set vbsfile2=%cd%\service\webapps\zxzb\WEB-INF\classes\vbs2.txtrem set zxzbconf=%cd%\service\webapps\zxzb\WEB-INF\classes\application-dev.propertiesdel "%cd%\service\webapps\zxzb\WEB-INF\classes\application-dev.properties" &gt;nul 2&gt;nulecho set fso=createobject("scripting.filesystemobject") &gt;1.vbsecho set file=fso.opentextfile(".\service\webapps\zxzb\WEB-INF\classes\application-dev-bak.properties") &gt;&gt;1.vbsecho s=file.readall &gt;&gt;1.vbsecho file.close &gt;&gt;1.vbsecho s=replace(s,"16603","%myport%") &gt;&gt;1.vbsecho set file=fso.createtextfile(".\service\webapps\zxzb\WEB-INF\classes\vbs1.txt") &gt;&gt;1.vbsecho file.write s &gt;&gt;1.vbsecho file.close &gt;&gt;1.vbsecho set fso=createobject("scripting.filesystemobject") &gt;&gt;1.vbsecho set file=fso.opentextfile(".\service\webapps\zxzb\WEB-INF\classes\vbs1.txt") &gt;&gt;1.vbsecho s=file.readall &gt;&gt;1.vbsecho file.close &gt;&gt;1.vbsecho s=replace(s,"17963","%redisport%") &gt;&gt;1.vbsecho set file=fso.createtextfile(".\service\webapps\zxzb\WEB-INF\classes\vbs2.txt") &gt;&gt;1.vbsecho file.write s &gt;&gt;1.vbsecho file.close &gt;&gt;1.vbsecho set fso=createobject("scripting.filesystemobject") &gt;&gt;1.vbsecho set file=fso.opentextfile(".\service\webapps\zxzb\WEB-INF\classes\vbs2.txt") &gt;&gt;1.vbsecho s=file.readall &gt;&gt;1.vbsecho file.close &gt;&gt;1.vbsecho s=replace(s,"10000","%mapport%") &gt;&gt;1.vbsecho set file=fso.createtextfile(".\service\webapps\zxzb\WEB-INF\classes\application-dev.properties") &gt;&gt;1.vbsecho file.write s &gt;&gt;1.vbsecho file.close &gt;&gt;1.vbsstart 1.vbsping 127.1 -n 2 &gt;nuldel .\service\webapps\zxzb\WEB-INF\classes\vbs1.txtdel .\service\webapps\zxzb\WEB-INF\classes\vbs2.txtrem del 1.vbsrem for /f "tokens=*" %%f in (%zxzbconfpath%) do (rem endlocalrem set f=%%frem setlocal enabledelayedexpansionrem set f=!f:16603=%myport%!rem set f=!f:17963=%redisport%!rem set f=!f:18785=%zxzb1%!rem set f=!f:18388=%zxzb2%!rem set f=!f:18489=%zxzb3%!rem echo !f! &gt;&gt;"%cd%\service\webapps\zxzb\WEB-INF\classes\application-dev.properties"rem )::: 前端urlport修改（只用在web封装版本）:webconfigrem set javaconfpath=requestUrl.jsdel "..\app\static\js\requestUrl.js" &gt;nul 2&gt;nulecho set fso=createobject("scripting.filesystemobject") &gt;2.vbsecho set file=fso.opentextfile("requestUrl.js") &gt;&gt;2.vbsecho s=file.readall &gt;&gt;2.vbsecho file.close &gt;&gt;2.vbsecho s=replace(s,"24486","%zxzb2%") &gt;&gt;2.vbsecho set file=fso.createtextfile("..\app\static\js\requestUrl.js") &gt;&gt;2.vbsecho file.write s &gt;&gt;2.vbsecho file.close &gt;&gt;2.vbs:: app\static\js\configdel "..\app\static\js\config.js" &gt;nul 2&gt;nulecho set fso=createobject("scripting.filesystemobject") &gt;&gt;2.vbsecho set file=fso.opentextfile("config.js") &gt;&gt;2.vbsecho s=file.readall &gt;&gt;2.vbsecho file.close &gt;&gt;2.vbsecho s=replace(s,"10000","%mapport%") &gt;&gt;2.vbsecho set file=fso.createtextfile("..\app\static\js\config.js") &gt;&gt;2.vbsecho file.write s &gt;&gt;2.vbsecho file.close &gt;&gt;2.vbs::这个作为测试页面webdel ".\service\webapps\zxzb\WEB-INF\classes\templates\static\js\config.js" &gt;nul 2&gt;nulecho set fso=createobject("scripting.filesystemobject") &gt;&gt;2.vbsecho set file=fso.opentextfile("config.js") &gt;&gt;2.vbsecho s=file.readall &gt;&gt;2.vbsecho file.close &gt;&gt;2.vbsecho s=replace(s,"10000","%mapport%") &gt;&gt;2.vbsecho set file=fso.createtextfile(".\service\webapps\zxzb\WEB-INF\classes\templates\static\js\config.js") &gt;&gt;2.vbsecho file.write s &gt;&gt;2.vbsecho file.close &gt;&gt;2.vbsstart 2.vbsrem for /f "delims=" %%d in (requestUrl.js) do (rem set d=%%drem set d=!d:24486=%zxzb2%!rem echo !c!&gt;&gt;"..\app\static\js\requestUrl.js"rem )::::: 安装服务::: 安装vc服务::: 原理 查询注册表，根据返回值判断执行不同结果命令，采用判断语句:2010Detect cd /d %~dp0rem set "HKLMU=HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall"::::: 判断系统是否已经安装了 Visual C++ 2010（包括所有版本）::::: 有就跳过安装Visual C++ 2010，没有则先安装。:: 此处采用 if defined 命令返回变量的值，执行不同结果命令，下面&#123;&#125;是vc工具ID，进行匹配，但是版本太多，没有手机完，总成匹配不全rem reg query %HKLMU%\&#123;196BB40D-1578-3D01-B289-BEFC77A11A1E&#125;&gt;nul 2&gt;nul&amp;&amp;set VC2010=Microsoft Visual C++ 2010 Redistributable X86rem reg query %HKLMU%\&#123;DA5E371C-6333-3D8A-93A4-6FD5B20BCC6E&#125;&gt;nul 2&gt;nul&amp;&amp;set VC2010=Microsoft Visual C++ 2010 Redistributable X64rem if defined VC2010 (echo %VC2010% 已安装！&amp;goto mysqlserviceDetect &gt;nul) else (goto 2010Install)::::: 判断系统是否已经安装了 Visual C++ 2010（包括所有版本）::::: 有就跳过安装Visual C++ 2010，没有则先安装。:: 此处采用检查注册表路径下有没有特定版本的目录，返回结果，真和假。匹配准确性待测试，个人想到的方式set "HKLMU=HKLM\SOFTWARE\WOW6432Node\Microsoft\VisualStudio\"if %errorlevel% ==0 (echo %vc10% 已安装！&amp; goto 2015Detect) else (echo noexsit &amp; goto 2010Install):2010Install:::::各版本下载地址 http://www.microsoft.com/downloads/details.aspx?FamilyID=766a6af7-ec73-40ff-b072-9112bab119c2&amp;DisplayLang=zh-cn"%cd%\mysql\vcredist_x64_2010.exe":2015Detectreg query %HKLMU%14.0 &gt;nul &amp;&amp; set vc15=Microsoft Visual C++ 2015if %errorlevel% ==0 (echo %vc15% 已安装！ &amp; goto mysqlserviceDetect) else (echo noexsit &amp; goto 2015Install):2015Installvcredist_x64_2015.exe::: 安装mysql:mysqlserviceDetectcd /d %~dp0set "mysqlservice=jx_mysql5.6"sc query "%mysqlservice%"&gt;nulIF ERRORLEVEL 1060 GOTO mysqlInstall GOTO mysqlexist:mysqlInstall"%cd%\mysql\bin\mysqld.exe" -install %mysqlservice% --defaults-file="%cd%\mysql\my.ini"sc config %mysqlservice% start=autonet start %mysqlservice%::: 安装redis:redisserviceDetectcd /d %~dp0set "redisservice=jx_redis3.2.100"sc query "%redisservice%"&gt;nulIF ERRORLEVEL 1060 GOTO redisInstall GOTO redisexist:redisInstall"%cd%\redis\redis-server" --service-install --service-name %redisservice% "%cd%\redis\redis.windows-service.conf" --loglevel verbosesc config %redisservice% start=autonet start %redisservice%echo ------------------------------------------------------------------------------::: 安装tomcat:javaserviceDetectcd /d %~dp0set "javaservice=jx_tomcat8.5_jdk8"sc query "%javaservice%"&gt;nulIF ERRORLEVEL 1060 GOTO javaInstall GOTO javaexist:javaInstallcd servicecall "%cd%\bin\service.bat" install %javaservice%ping 127.0.0.1 -n 5 &gt; nulsc config %javaservice% start=autonet start %javaservice%::: 安装mapserver:mapserviceDetectcd /d %~dp0set "mapservice=jx_mapserv0.20.1"sc query "%mapservice%"&gt;nulIF ERRORLEVEL 1060 GOTO mapInstallGOTO mapexist:mapInstallinstsrv.exe jx_mapserv0.20.1 "%cd%\mapser\start.exe"ping 127.0.0.1 -n 5 &gt; nulsc config jx_mapserv0.20.1 start=autonet start jx_mapserv0.20.1echo mysqlport=%myport% &gt; READ.txtecho redisport=%redisport% &gt;&gt; READ.txtecho javaport1=%zxzb1% &gt;&gt; READ.txtecho javaport2-http=%zxzb2% &gt;&gt; READ.txtecho javaport3=%zxzb3% &gt;&gt; READ.txtecho mapport=%mapport% &gt;&gt; READ.txtping 127.0.0.1 -n 60 &gt; nulgoto END:mysqlexistmshta vbscript:msgbox("ERROR：%mysqlservice% 服务已存在！！！",64,"异常提醒")(window.close)GOTO END:redisexistmshta vbscript:msgbox("ERROR：%redisservice% 服务已存在！！！",64,"异常提醒")(window.close)GOTO END:javaexistmshta vbscript:msgbox("ERROR：%javaservice% 服务已存在！！！",64,"异常提醒")(window.close)GOTO END:END version 2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149::[Bat To Exe Converter]::::YAwzoRdxOk+EWAnk::fBw5plQjdG8=::YAwzuBVtJxjWCl3EqQJgSA==::ZR4luwNxJguZRRnk::Yhs/ulQjdF+5::cxAkpRVqdFKZSzk=::cBs/ulQjdF+5::ZR41oxFsdFKZSDk=::eBoioBt6dFKZSDk=::cRo6pxp7LAbNWATEpCI=::egkzugNsPRvcWATEpCI=::dAsiuh18IRvcCxnZtBJQ::cRYluBh/LU+EWAnk::YxY4rhs+aU+JeA==::cxY6rQJ7JhzQF1fEqQJQ::ZQ05rAF9IBncCkqN+0xwdVs0::ZQ05rAF9IAHYFVzEqQJQ::eg0/rx1wNQPfEVWB+kM9LVsJDGQ=::fBEirQZwNQPfEVWB+kM9LVsJDGQ=::cRolqwZ3JBvQF1fEqQJQ::dhA7uBVwLU+EWDk=::YQ03rBFzNR3SWATElA==::dhAmsQZ3MwfNWATElA==::ZQ0/vhVqMQ3MEVWAtB9wSA==::Zg8zqx1/OA3MEVWAtB9wSA==::dhA7pRFwIByZRRnk::Zh4grVQjdCyDJGyX8VAjFBpQQQ2MAE+/Fb4I5/jH9fKdoHEUWvEreYPXlLGWJYA=::YB416Ek+ZW8=::::::978f952a14a936cc963da21a135fa983@echo offcd /d %~dp0call envport.bat:myportDetectnetstat -aon|findstr LISTENING|findstr "%myport%" &gt; NUL 2&gt;NUL &amp;&amp; mshta vbscript:msgbox("ERROR：%myport% 端口已占用！！！",64,"异常提醒")(window.close) || echo %myport%:redisportDetectnetstat -aon|findstr LISTENING|findstr "%redisport%" &gt; NUL 2&gt;NUL &amp;&amp; mshta vbscript:msgbox("ERROR：%redisport% 端口已占用！！！",64,"异常提醒")(window.close) || echo %redisport%:javaportDetect1netstat -aon|findstr LISTENING|findstr "%zxzb1%" &gt; NUL 2&gt;NUL &amp;&amp; mshta vbscript:msgbox("ERROR：%zxzb1% 端口已占用！！！",64,"异常提醒")(window.close) || echo %zxzb1%:javaportDetect2netstat -aon|findstr LISTENING|findstr "%zxzb2%" &gt; NUL 2&gt;NUL &amp;&amp; mshta vbscript:msgbox("ERROR：%zxzb2% 端口已占用！！！",64,"异常提醒")(window.close) || echo %zxzb2%:javaportDetect3netstat -aon|findstr LISTENING|findstr "%zxzb3%" &gt; NUL 2&gt;NUL &amp;&amp; mshta vbscript:msgbox("ERROR：%zxzb3% 端口已占用！！！",64,"异常提醒")(window.close) || echo %zxzb3%:mapportDetectnetstat -aon|findstr "%mapport%" &gt; NUL 2&gt;NUL &amp;&amp; mshta vbscript:msgbox("ERROR：%mapport% 端口已占用！！！",64,"异常提醒")(window.close) || echo %mapport%::::: 安装服务::: 安装vc2010:2010Detect cd /d %~dp0set "HKLMU=HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall"::::: 判断系统是否已经安装了 Visual C++ 2010（包括所有版本）::::: 有就跳过安装Visual C++ 2010，没有则先安装。reg query %HKLMU%\&#123;196BB40D-1578-3D01-B289-BEFC77A11A1E&#125;&gt;nul 2&gt;nul&amp;&amp;set VC2010=Microsoft Visual C++ 2010 Redistributable X86reg query %HKLMU%\&#123;DA5E371C-6333-3D8A-93A4-6FD5B20BCC6E&#125;&gt;nul 2&gt;nul&amp;&amp;set VC2010=Microsoft Visual C++ 2010 Redistributable X64if defined VC2010 (echo %VC2005% 已安装！&amp;goto mysqlserviceDetect &gt;nul) else (goto 2010Install):2010Install:::::各版本下载地址 http://www.microsoft.com/downloads/details.aspx?FamilyID=766a6af7-ec73-40ff-b072-9112bab119c2&amp;DisplayLang=zh-cn"%cd%\mysql\vcredist_x64_2010.exe"::: 安装mysql:mysqlserviceDetectcd /d %~dp0set "mysqlservice=jx_mysql5.6"sc query "%mysqlservice%"&gt;nulIF ERRORLEVEL 1060 GOTO mysqlInstall GOTO mysqlexist:mysqlInstall"%cd%\mysql\bin\mysqld.exe" -install %mysqlservice% --defaults-file="%cd%\mysql\my.ini"sc config %mysqlservice% start=autonet start %mysqlservice%::: 安装redis:redisserviceDetectcd /d %~dp0set "redisservice=jx_redis3.2.100"sc query "%redisservice%"&gt;nulIF ERRORLEVEL 1060 GOTO redisInstall GOTO redisexist:redisInstall"%cd%\redis\redis-server" --service-install --service-name %redisservice% "%cd%\redis\redis.windows-service.conf" --loglevel verbosesc config %redisservice% start=autonet start %redisservice%echo ------------------------------------------------------------------------------::: 安装tomcat:javaserviceDetectcd /d %~dp0set "javaservice=jx_tomcat8.5_jdk8"sc query "%javaservice%"&gt;nulIF ERRORLEVEL 1060 GOTO javaInstall GOTO javaexist:javaInstallcd servicecall "%cd%\bin\service.bat" install %javaservice%ping 192.0.2.2 -n 1 -w 10000 &gt; nulsc config %javaservice% start=autonet start %javaservice%ping 192.0.2.2 -n 1 -w 10000 &gt; nul:mapserviceDetectcd /d %~dp0set "mapservice=jx_mapserv0.20.1"sc query "%mapservice%"&gt;nulIF ERRORLEVEL 1060 GOTO mapInstallGOTO mapexist:mapInstallinstsrv.exe jx_mapserv0.20.1 "%cd%\mapser\start.exe"ping 192.0.2.2 -n 1 -w 3000 &gt; nulsc config jx_mapserv0.20.1 start=autonet start jx_mapserv0.20.1goto END:mysqlexistmshta vbscript:msgbox("ERROR：%mysqlservice% 服务已存在！！！",64,"异常提醒")(window.close)GOTO END:redisexistmshta vbscript:msgbox("ERROR：%redisservice% 服务已存在！！！",64,"异常提醒")(window.close)GOTO END:javaexistmshta vbscript:msgbox("ERROR：%javaservice% 服务已存在！！！",64,"异常提醒")(window.close)GOTO END:mapexistmshta vbscript:msgbox("ERROR：%mapservice% 服务已存在！！！",64,"异常提醒")(window.close)GOTO END:END]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>bat</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitlab_gitbook搭建WIKI]]></title>
    <url>%2F2021%2F06%2F18%2Fgitlab-gitbook%E6%90%AD%E5%BB%BAWIKI%2F</url>
    <content type="text"><![CDATA[1 环境OS： centos7.9gitlab: gitlab-ce-10.0.0-ceGitBook CLI version: 2.3.2GitBook version: 3.2.3Gitlab-runner 13.12.0 2 环境准备2.1 安装nodejs12yum install epel-releaseyum install nodejs 2.2 安装git1yum install git 3 安装GitLab 下载gitlab的rpm包地址：https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ 执行安装 1yum install gitlab-ce-10.0.0-ce.0.el7.x86_64.rpm 修改访问URL 1vim /etc/gitlab/gitlab.rb ## 修改以下内容并保存 1external_url http://服务器IP地址 info备注：这里可以写服务器IP地址，如果映射域名了，可以写域名，不用加端口号，端口号另外配置 载入配置 12gitlab-ctl reconfiguregitlab-ctl restart 4 GitLab汉化（可选）GitLab10版本支持配置中文，但是翻译率较低，通过其他方式增加汉化info备注：此方式暂时只适用于10版本参考 Gitlab10.0.X社区版本，汉化https://kinggoo.com/gitlab-chinesize.htm 下载汉化文件汉化文件：下载 https://kinggoo.com/wp-content/upload/2017/10/10.0.x.zip 停止gitlab 12gitlab-ctl stoppatch -d /opt/gitlab/embedded/service/gitlab-rails -p1 &lt; 10.0.diff 启动gitlab 1gitlab-ctl start 访问页面验证。 5 安装GitBooknpm install -g gitbook-cli -g 6 安装GitLab-Runner 下载GitLab Runner通过 uname –m 命令查看 Linux 系统的位数，然后下载对应的安装包 123456# x86-64sudo wget -O /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64 # x86sudo wget -O /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-386 # armsudo wget -O /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-arm 文件放置文件已经放置于 /usr/local/bin/gitlab-runner，需要配置相应权限chmod +x /usr/local/bin/gitlab-runner 安装GitLab Runner 123456**## 安装**gitlab-runner install --user=root --working-directory=/software/gitlab-runner备注：/software/gitlab-runner 是安装路径，可以自定义，目录需要提前创建，否则启动报错。**## 启动**gitlab-runner start 7 打通GitLab与GitBook 创建wiki的工程，如下所示： 注册GitLab Runner 12345678**## 注册**gitlab-runner register然后依次输入以下参数：## 1. GitLab的url，参考下图## 2. Token，参考下图## 3. 描述，随便写一个## 4. Runner描述，随便写一个## 5. shell，意思是执行方式，这里用shell即可 配置完成后，可以在GitLab看到Runner已经注册上来了，如下所示： 项目配置CI在项目中新增.gitlab-ci.yml文件，内容如下： 12345678910stages: - buildxx-wiki: stage: build script: - p=`pwd` - echo $p - gitbook install - gitbook build - setsid nohup sh startup.sh &gt; nohup.out 2&gt;&amp;1 &amp; 在项目中新增startup.sh文件，内容如下： 123#!/bin/bashfor i in `ps -ef | grep gitbook | grep serve`; do kill -9 $i ; done;gitbook serve 提交到GitLab后，通过Pipelines菜单查看任务执行情况： 8 验证修改Wiki内容，然后推送到GitLab，可以看到GitLab上的任务被执行： 访问http://IP:4000，可以看到对应的Wiki被更新，证明WiKi搭建完成，并能实时更新 9 问题9.1 runner一直处于Pending状态这是由于在注册gitlab runner完成后，有一个配置项需要修改，否则不会执行，详见以下链接：https://www.daxiblog.com/gitlab-pages无法运行，一直处于pending状态的原因/https://www.daxiblog.com/gitlab-pages%E6%97%A0%E6%B3%95%E8%BF%90%E8%A1%8C%EF%BC%8C%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Epending%E7%8A%B6%E6%80%81%E7%9A%84%E5%8E%9F%E5%9B%A0/在gitlab中启动pages服务并配置好gitlab runner之后，push代码后，虽然触发了自动CI，但是一直处于Pending状态， danger错误信息是：This job is stuck, because you don’t have any active runners that can run this job 这是由于在注册gitlab runner完成后，有一个配置项需要修改，就是是否在没有标记tag的job上运行，如果选择默认值false，那没有标记tag的代码提交是不会触发gitlab runner的，如果做测试，最好填true。这个配置可以在runner的编辑页面进行修改： 打开settings→CI/CD→Runners settings，然后会看到已经启用的runner，点击编辑按钮，勾选以下选项即可： 9.2 runner一直处于running这是由于直接通过gitbook serve启动时，没有以后台方式启动。最后通过setsid解决 10 优化10.1 优化GitBook展示页面创建book.js，可配置页面超链接，书籍标题等信息，安装及禁用了部分插件 123456789101112131415161718192021222324252627&#123; "title": "test标题", "author": "XXX作者", "authorHomepage": "http://zhjxdns.imwork.net:332/", "description": "site description", "language": "zh-hans", "copyright": "All Rights Reserved", "variables": &#123;&#125;, "plugins": [ "theme-code", "splitter", "prism", "-font-settings", "folding-chapters", "-sharing", "search-pro", "-smart-nav-collapse", "include-codeblock", "-livereload", "toggle-chapters" ], "pluginsConfig": &#123; "theme-default": &#123; "showLevel": true &#125; &#125;&#125; 10.2 优化默认Introduction 修改SUMMARY.md文件，添加* [前言]（README.md） 123456* [前言](README.md)# 目录* [第一章](1.md) * [1.1](1.1.md)* [第二章](2.md)* [第三章](3.md)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>gitlib</tag>
        <tag>gitbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop集群部署2-离线版]]></title>
    <url>%2F2021%2F04%2F19%2Fhadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B22%2F</url>
    <content type="text"><![CDATA[1 环境1.1 软件Centos7.9 //最低7.2版本 兼容 中标麒麟（服务器操作系统V7.0U6） Ambari 2.7.4 //支持HDP-3.1.5 和 HDF-3.2.0Mysql 5.7 //只支持mysql5.7或者postgresql10.7/ 10.5/ 10. 2/ 9.6JDK 1.80_77及以上版本Chrome 57.0.2 / 56.0.2https://supportmatrix.hortonworks.com/ (版本兼容查询) 1.2 硬件推荐最低配置： 1.3 参考文档https://docs.cloudera.com/HDPDocuments/Ambari-2.7.5.0/bk_ambari-installation/content/ch_Getting_Ready.html （网上历史方式cloudera.com,已经收费）备选：https://ambari.apache.org (ambari开源版本) 2 前期准备（各节点均配置）2.1 关闭防火墙/selinux1systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.service 12setenforce 0/etc/selinux/config //永久关闭修改config文件 2.2 修改网络设置主机IP地址为静态地址，同时开启网卡自启动。 2.3 系统优化设置文件最大打开数 123456vim /etc/security/limits.conf* soft nofile 65535 // 当前系统生效的最大文件打开数* hard nofile 65535 // 当前系统所能设定的最大值设置最大进程数* soft nproc 11000* hard nproc 11000 重启或重新进入shell窗口 2.4 设置hosts修改主机名，并将主机名加入各节点hosts文件 2.5 ssh免密登录SSH免密码登录，因为Hadoop需要通过SSH登录到各个节点进行操作，我用的是root用户，每台服务器都生成公钥，再合并到authorized_keys(1)CentOS默认没有启动ssh无密登录，去掉/etc/ssh/sshd_config其中2行的注释，每台服务器都要设置， 123$ vi /etc/ssh/sshd_configRSAAuthentication yes PubkeyAuthentication yes (2)输入命令，ssh-keygen -t rsa，生成key，都不输入密码，一直回车，/root就会生成.ssh文件夹，每台服务器都要设置。 1$ ssh-keygen -t rsa (3)合并公钥到authorized_keys文件，在Master服务器，进入/root/.ssh目录，通过SSH命令合并 12345$ cat id_rsa.pub&gt;&gt; authorized_keys$ Chmod 600 authorized_kdys$ ssh root@192.168.0.165 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keys$ ssh root@192.168.0.166 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keys$ ssh root@192.168.0.167 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keys (4)把Master服务器的authorized_keys、known_hosts复制到Slave服务器的/root/.ssh目录 123$ scp authorized_keys root@192.168.0.165:~/.ssh$ scp authorized_keys root@192.168.0.166:~/.ssh$ scp authorized_keys root@192.168.0.167:~/.ssh (5)完成，ssh Slave1、ssh Slave2、ssh Slave3就不需要输入密码了 2.6 时间同步(提前下载)1) master节点作为时间同步服务器，所有节点安装ntp ntpdate 1yum install ntp ntpdate 2) master节点配置 1vim /etc/ntp.config 启动时间同步服务器 12systemctl start ntpd #启动时间同步程序systemctl enable ntpd #允许时间同步程序开机启动 3) 在其它节点上运行如下命令同步时间 1ntpdate -u 192.168.214.128 设置定时任务 13 * * * * root ntpdate -u 192.168.214.128 # 每三分钟执行一次同步 2.7 jdk安装jdk-8u144-linux-x64.tar.gz 或 jdk-8u152-linux-x64.tar.gz版本需要高于8u77配置jdk环境 12export JAVA_HOME=/usr/local/java_versionexport PATH=$JAVA_HOME/bin/:$PATH 2.8 mysql安装（ambari节点）下载mysql-5.7.32-linux-glibc2.12-x86_64.tar.gzmysql支持5.7版本1) 下载安装包http://dev.mysql.com/downloads/mysql/#downloads推荐下载通用安装方法的tar.gz包2) 安装过程 1234567891011121314shell&gt; groupadd mysqlshell&gt; useradd -r -g mysql -s /bin/false mysqlshell&gt; cd /usr/localshell&gt; tar zxvf /path/to/mysql-VERSION-OS.tar.gzshell&gt; ln -s full-path-to-mysql-VERSION-OS mysqlshell&gt; cd mysqlshell&gt; mkdir mysql-filesshell&gt; chown mysql:mysql mysql-filesshell&gt; chmod 750 mysql-filesshell&gt; bin/mysqld --initialize --user=mysqlshell&gt; bin/mysql_ssl_rsa_setupshell&gt; bin/mysqld_safe --user=mysql &amp;# Next command is optionalshell&gt; cp support-files/mysql.server /etc/init.d/mysql.server 默认密码如下 3) 修改my.cnf的内容如下 12345678910111213[mysql] default-character-set=utf8[mysqld]skip-name-resolve port=3306basedir=/usr/local/mysql # 设置mysql的安装目录datadir=/usr/local/mysql/data # 设置mysql数据库的数据的存放目录max_connections=200 # 允许最大连接数character-set-server=utf8default-storage-engine=INNODB # 创建新表时将使用的默认存储引擎lower_case_table_names=1max_allowed_packet=16M 4) 初始化密码 12345mysql&gt; SET PASSWORD = PASSWORD('123456');Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 5) 添加远程访问权限 123456789101112131415161718192021mysql&gt; use mysql; Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; update user set host = '%' where user = 'root';Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; grant all privileges on *.* to root@'%' identified by '123456' with grant option;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; select host, user from user;+-----------+-----------+| host | user |+-----------+-----------+| % | root || localhost | mysql.sys |+-----------+-----------+mysql&gt; quit$ /etc/init.d/mysqld restart //一定要重启才会生效。 6) 设置开机自启 1chkconfig mysql on 3 设置本地库3.1 apache安装（ambari节点）12yum install apache //搭建本地仓库systemctl start httpd &amp;&amp; systemctl enable httpd 3.2 下载并上传软件包Ambari https://archive.cloudera.com/p/ambari/2.x/2.7.4.14/centos7/ambari-2.7.4.14-1-centos7.tar.gzHDP https://archive.cloudera.com/p/HDP/3.x/3.1.4.0/centos7/HDP-3.1.4.0-centos7-rpm.tar.gzHDP-UTILS https://archive.cloudera.com/p/HDP-UTILS/1.1.0.22/repos/centos7/HDP-UTILS-1.1.0.22-centos7.tar.gz HDP-GPL https://archive.cloudera.com/p/HDP-GPL/3.x/3.1.4.0/centos7/HDP-GPL-3.1.4.0-centos7-gpl.tar.gz 解压缩文件，拷贝到/var/www/html下 3.3 配置源4 Ambari安装4.1 创建源文件ambari.repo 1234567[ambari]name=ambari-2.7.4.0baseurl=http://192.168.1.3/ambari/centos7/2.7.4.0-118/gpgcheck=1gpgkey=http://192.168.1.3/ambari/centos7/2.7.4.0-118/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1 hdp.repo 123456[HDP]name=HDP-3.1.4.0baseurl=http://192.168.1.3/HDP/centos7/3.1.4.0-315/gpgcheck=0enabled=1 hdp-gpl.repo 123456[HDP-GPL]name=HDP-GPL-3.1.4.0baseurl=http://192.168.1.3/HDP-GPL/centos7/3.1.4.0-315/gpgcheck=0enabled=1 hdp-utils.repo 123456[HDP-UTILS]name=HDP-UTILS-3.1.4.0baseurl=http://192.168.1.3/HDP-UTILS/centos7/1.1.0.22/gpgcheck=0enabled=1 4.2 生成缓存12yum clean allyum makecache 4.3 安装Ambari Server1yum install ambari-server 离线版没有postgresql，需要提前使用–downloadonly下载好. 4.3.1 初始化Ambar 备注：mysql连接器提前下载好放到/usr/share/java目录https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-5.1.49.tar.gz修改jar文件名为mysql-connector-java.jar 放到/usr/share/java目录。 4.3.2 初始化mysql4.3.2.1 创建ambari数据库1CREATE DATABASE `ambari` CHARACTER SET utf8 COLLATE utf8_general_ci; 4.3.2.2 初始化数据1mysql -uroot -p123456 ambari &lt; /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql 4.3.3 启动Ambari serverambari-server start启动服务。 此时，登录10.10.10.31:8080，即可看到ambari的页面，如下：默认账号密码：admin admin 5 安装HDP集群集群名称 zhjx centos选择rathat7,输入本地私库的url，其他类型删除。 输入节点主机名，下载管理节点的ssh密码，id_rsa 文件，在页面上传密钥备注，必须安装如下格式，不能使用hadoop2 没有按照上图设置主机名，会出现主机名不合规的提示。 这里经常安装失败，可以查看日志排查。 检查无误，NEXT→通过即可如果这个步骤失败了错误，记得多看日志，多找问题，如果还不行的话，回档咯 1234[root@master ~]# # ambari-server stop #停止命令[root@master ~]# # ambari-server reset #重置命令[root@master ~]# # ambari-server setup #重新设置 [root@master ~]# # ambari-server start #启动命令 问题1：centos7.3/4安装ambari2.6以上版本报EOF occurred in violation of protocol (_ssl.c:579) 解决办法： 开始可以少选些服务，减少错误，后面还可以添加。HDFS、Hive、HBase、Sqoop、ZooKeeper、Flume 节点分配，我这里默认了 设置密码，我这里直接都设置为admin1234 数据库连接都采用root用户， 配置数据库的时候执行下黄色部分的命令 设置各组件的路径，我这里默认 默认了 默认了默认了 这一步也是经常报错，看日志解决吧。 5.1.1 设置组件自启动admin&gt;&gt;Service Auto Start 所有组件全部开启 6 问题6.1 问题1：缺少软件（提前downloadonly）unzipncpsmiscredhat-lsblibtirpc-develgccpython-develpython-kerberos 6.2 问题2：没有flume组件高版本没有集成组件，需要手动集成，建议采用低版本hdp安装（ambari 2.6.2.2+hdp2.6.5）。 6.3 问题3：重启主机，ambari-server无法启动日志vim /var/log/ambari-server/ambari-server.out问题原因：数据库连接失败 网上查找原因：是需要在mysql连接url后面添加 ?useSSL=false,其实不然。 以上问题都是一个原因经尝试，都和ambari-server的配置文件有关系。配置如下： 无论是使用IP地址，还是配置的主机名，都不可以链接到数据库。配置为localhost恢复正常。如下： 6.4 问题4：hive组件配置数据库时，需要执行一下mysql连接驱动的路径的命令在master节点否则数据库连接测试总是失败。 1ambari-server setup --jdbc-db=mysql –jdbc-driver=/usr/share/java/mysql-connector-java.jar 6.5 6.5 问题5：SmartSense Gateway is not active原因：服务器离线，无法访问域名地址，忽略即可。 7 扩展功能7.1 配置namenode HA参考资料：https://blog.csdn.net/qq_21153619/article/details/81974140 1) 在Ambari UI中，选择Services&gt;HDFS&gt;Summary 2) 点击Service Actions,点击Enable NameNode HA 3) 在Get Started页面中，输入一个Nameservice ID然后点Next 备注：设置HA前，需要先关闭HBase服务 调整服务节点 如上图操作： 在nameNode上创建检查点 1、 登录Namenode Host Hadoop3.htsb。 2、 将NameNode放在安全模式（只读模式） 1sudo su hdfs -l -c 'hdfs dfsadmin -safemode enter' 3、 一旦处于安全模式，创建一个检查点 1sudo su hdfs -l -c 'hdfs dfsadmin -saveNamespace' 如上图所示： 初始化journalnode 1、 登录Namenode Host Hadoop3.htsb。 2、 运行初始化journalnode 1sudo su hdfs -l -c 'hdfs namenode -initializeSharedEdits' 3、 一旦Ambari检测到已成功初始化，您将能够继续。 如上图所示： 初始化预期的HA元数据 1、 登录Namenode Host Hadoop3.htsb 2、 通过运行初始化NameNode自动故障转移的元数据 1sudo su hdfs -l -c 'hdfs zkfc -formatZK' 3、 登录新Namenode节点namenode1.htsb 4、 通过运行初始化其他nameNode的元数据 1sudo su hdfs -l -c 'hdfs namenode -bootstrapStandby' 下一步]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop集群部署1]]></title>
    <url>%2F2021%2F04%2F19%2Fhadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B21%2F</url>
    <content type="text"><![CDATA[1. 环境平台使用 Centos 7 64位作为系统环境。采用1个Master节点与3个Slave节点搭建成为分布式集群，详情见表 1。 2. 基础安装2.1. 安装CentOS 7拷制U盘启动后，选择开机使用U盘启动，进入主引导装机页面如下图。 选择Test this medis &amp; install CentOS Linux 7，进入下面界面： 稍等片刻后，进入CenOS7可视化安装界面，选择中文。 点击Continue后，进入安装信息摘要界面，选择日期和时间为亚洲/上海 时区，主要保持每个服务器的时区一致，软件选择为最小安装，点击安装位置后如图 12所示，根据需要选择自动分区或者手动分区，这里为自动分区。点击完成后，回到图 11，点击开始安装。 安装进行过程中提示用户设置如下图，点击ROOT密码，为ROOT用户设置密码，如需要创建其他用户，可点击创建用户，新增其他用户。 之上内容设置完成后，点击重启，完成系统安装。 2.2. 配置网络设置各节点静态IP修改hostname：$ vi /etc/hostname #编辑配置文件Master 2.3. 关闭防火墙和SeLinuxCentOS 7.0默认使用的是firewall作为防火墙，默认为最小化安装时，系统没有安装防火墙，安装完可视化界面后，可能防火墙也会被安装上，因此需要关闭防火墙。 12$ systemctl stop firewalld.service #停止firewall$ systemctl disable firewalld.service #禁止firewall开机启动 使用sestatus -v 命令，查看Selinux状态。如果不是disable状态，编辑/etc/sysconfig/selinux 文件。 12$ vi /etc/sysconfig/selinuxSELINUX=disabled 2.4. SSH免密码登录SSH免密码登录，因为Hadoop需要通过SSH登录到各个节点进行操作，我用的是root用户，每台服务器都生成公钥，再合并到authorized_keys(1) CentOS默认没有启动ssh无密登录，去掉/etc/ssh/sshd_config其中2行的注释，每台服务器都要设置， 123$ vi /etc/ssh/sshd_configRSAAuthentication yes PubkeyAuthentication yes (2) 输入命令，ssh-keygen -t rsa，生成key，都不输入密码，一直回车，/root就会生成.ssh文件夹，每台服务器都要设置。 1$ ssh-keygen -t rsa (3) 合并公钥到authorized_keys文件，在Master服务器，进入/root/.ssh目录，通过SSH命令合并 12345$ cat id_rsa.pub&gt;&gt; authorized_keys$ Chmod 600 authorized_kdys$ ssh root@192.168.0.165 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keys$ ssh root@192.168.0.166 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keys$ ssh root@192.168.0.167 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keys (4) 把Master服务器的authorized_keys、known_hosts复制到Slave服务器的/root/.ssh目录 123$ scp authorized_keys root@192.168.0.165:~/.ssh$ scp authorized_keys root@192.168.0.166:~/.ssh$ scp authorized_keys root@192.168.0.167:~/.ssh (5) 完成，ssh Slave1、ssh Slave2、ssh Slave3就不需要输入密码了 2.5. 安装JDK安装JDK，Hadoop2.7需要JDK7，由于我的CentOS是最小化安装，所以没有OpenJDK，直接解压下载的JDK并配置变量即可(1)下载“jdk-7u79-linux-x64.gz”，放到/home/java目录下(2)解压，输入命令，tar -zxvf jdk-7u79-linux-x64.gz(3)编辑/etc/profile 123export JAVA_HOME=/home/java/jdk1.7.0_79export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin (4)使配置生效，输入命令，source /etc/profile(5)输入命令，java -version，完成 3. 数据库安装1) 下载安装包http://dev.mysql.com/downloads/mysql/#downloads推荐下载通用安装方法的tar.gz包2) 检查库文件是否存在，如有删除。$ rpm -qa | grep mysqlmysql-libs-5.1.52-1.el6_0.1.x86_64$ rpm -e mysql-libs-5.1.52.x86_64 –nodeps3) 检查mysql组和用户是否存在，如无创建。 1234$ cat /etc/group | grep mysqlmysql:x:490:$ cat /etc/passwd | grep mysqlmysql:x:496:490::/home/mysql:/bin/bash 以上为默认存在的情况，如无，执行添加命令： 12$groupadd mysql$useradd -r -g mysql mysql //useradd -r参数表示mysql用户是系统用户，不可用于登录系统。4) 解压TAR包，更改所属的组和用户 1234$ cd /usr/local/$ tar -xvfz mysql-5.6.37-linux-glibc2.5-x86_64.tar.gz$ chown -R mysql mysql-5.6.37-linux-glibc2.5-x86_64/$ chgrp -R mysql mysql-5.6.37-linux-glibc2.5-x86_64/ 5) 安装和初始化数据库 1$ bin/mysql_install_db --user=mysql --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/ 6) 改写配置文件 1234$ cp -a ./support-files/my-default.cnf /etc/my.cnf$ cp -a ./support-files/mysql.server /etc/init.d/mysqld$ cd etc/$ vi my.cnf 修改my.cnf的内容如下[mysql]default-character-set=utf8socket=/var/lib/mysql/mysql.sock[mysqld]skip-name-resolveport=3306socket=/var/lib/mysql/mysql.sockbasedir=/usr/local/mysql # 设置mysql的安装目录datadir=/usr/local/mysql/data # 设置mysql数据库的数据的存放目录max_connections=200 # 允许最大连接数character-set-server=utf8default-storage-engine=INNODB # 创建新表时将使用的默认存储引擎lower_case_table_names=1max_allowed_packet=16M 7) 启动数据库服务并设置服务开机自动启动 123$ ./mysqld_safe --user=mysql &amp;$ /etc/init.d/mysqld restart $ chkconfig --level 35 mysqld on //设置开机启动 8) 初始化密码 1$ cat /root/.mysql_secret # Password set for user &#39;root@localhost&#39; at 2016-06-01 15:23:25,xxxxxR5H9$ mysql -uroot –pEnter password:Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.7.12 Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners. Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. mysql&gt; SET PASSWORD = PASSWORD(‘123456’);Query OK, 0 rows affected, 1 warning (0.00 sec) mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)9) 添加远程访问权限mysql&gt; use mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -A Database changedmysql&gt; update user set host = ‘%’ where user = ‘root’;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0 mysql&gt; select host, user from user;+———–+———–+| host | user |+———–+———–+| % | root || localhost | mysql.sys |+———–+———–+mysql&gt; quit$ /etc/init.d/mysqld restart //一定要重启才会生效。 4. Ambari安装由于在线安装容易受到网络不稳定等方面影响，因此采用平台选用离线安装的方式。 4.1. 搭建Yum源服务器1) 安装httpd服务检查是否已经安装apache http服务。 1$ which httpd 如果没有出现目录信息，则说明没有安装。通过下面的语句进行安装。 1$ sudo yum install httpd 安装成功之后，apache工作目录默认在/var/www/html。检查如没有此目录，需要手动创建相同目录结构。检查端口是否占用，http服务使用80端口 1$ netstat -nltp | grep 80 如果有占用情况，安装完毕之后需要修改apache http服务的端口号： 1$ sudo vi /etc/httpd/conf/httpd.conf 修改监听端口，Listen 80为其他端口。为了加快安装速度和防止超时错误，建议为HDP配置本地源，请在事先在网上下载HDP、HDP-UTILS和Ambari，本例中几个组件版本为HDP-2.6.0，HDP-UTILS-1.1.0.21和Ambari-2.5.1.0。此处把slave3服务器做为源服务器。以下操作在slave3机器运行。2) 下载压缩包首先下载包含必要软件的压缩包（tarball）到本地，在https://docs.hortonworks.com/HDPDocuments/Ambari-2.5.1.0/bk_ambari-installation/content/ambari_repositories.html中，查看需要下载的tarball。下表为本平台所用的内容：服务内容 Tarball源地址Ambari http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.1.0/ambari-2.5.1.0-centos7.tar.gzHDP http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.1.0/HDP-2.6.1.0-centos7-rpm.tar.gzHDP-UTILS http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7/HDP-UTILS-1.1.0.21-centos7.tar.gz建立本地仓库，这里使用 httpd 来建立，安装启动httpd省略。解压三个包到 /var/www/html 目录下。 123$ tar -zxvf ambari-2.5.1-centos7.tar.gz -C /var/www/html/$ tar -zxvf HDP-2.6.1.0-centos7-rpm.tar.gz -C /var/www/html/$ tar -zxvf HDP-UTILS-1.1.0.21-centos7.tar.gz -C /var/www/html/ 3) 修改Repo修改 ambari.repo，使之与 yum源机器匹配。 1234567891011$ cd /etc/yum.repos.d$vi ambary.repo#VERSION_NUMBER=2.5.1.0-159[Updates-ambari-2.5.1.0]name=ambari-2.5.1.0- Updatesbaseurl=http://192.168.1.3/data/ambari/centos7/gpgcheck=1gpgkey=http://192.168.1.3/data/ambari/centos7/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1修改 hdp.repo，使之与 yum源机器匹配。 1234567$vi ambary.repo[HDP-2.6]name=HDP-2.6baseurl=http://192.168.1.3/data/HDP/centos7/path=/enabled=1gpgcheck=0 修改 HDP-UTILS.repo，使之与 yum源机器匹配。 1234567$vi ambary.repo[HDP-UTILS-1.1.0.21]name=HDP-UTILS-1.1.0.21baseurl=http://192.168.1.3/data/HDP-UTILS-1.1.0.21-centos7/path=/enabled=1gpgcheck=0 4.2. 安装Ambari Server在上述内容配置完成后，安装ambari-service。出现Complete! 则可 1$ yum install ambari-server 启动Ambari-server服务，出现提示后，根据提示填写相应的回答。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647$ ambari-server setupUsing python /usr/bin/python2.6Setup ambari-serverChecking SELinux...SELinux status is 'disabled'Customize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):Adjusting ambari-server permissions and ownership...Checking firewall status...Checking JDK...[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7[3] Custom JDK==============================================================================Enter choice (1): 3WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JAVA_HOME: /usr/jdk64/jdk1.7.0_67Validating JDK on Ambari Server...done.Completing setup...Configuring database...Enter advanced database configuration [y/n] (n)? yConfiguring database...==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle[3] - MySQL[4] - PostgreSQL[5] - Microsoft SQL Server (Tech Preview)==============================================================================Enter choice (1): 3Hostname (localhost): Port (3306): Database name (ambari): Username (ambari): Enter Database Password (bigdata): Configuring ambari database...Copying JDBC drivers to server resources...Configuring remote database connection properties...WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sqlProceed with configuring remote database connection properties [y/n] (y)? Extracting system views......ambari-admin-2.1.0.1470.jar...Adjusting ambari-server permissions and ownership...Ambari Server 'setup' completed successfully. 看到以上结果后，安装成功。上面出现如下错误 下载mysql_jdbc驱动，放到/usr/share/java/目录，并重命名为mysql-connector-java.jar,否则后面安装会报错。下载：https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gz 4.3. 初始化数据库12cd /var/lib/ambari-server/resources/mysql -uroot -p123456 ambari &lt; Ambari-DDL-MySQL-CREATE.sql 没初始化数据库，启动服务会报如下错误 4.4. 启动服务ambari-server start启动服务。 此时，登录192.168.1.168:8080，即可看到ambari的页面，如下： 4.5. 安装 HDP 集群在ambari的登录页面中输入用户名：admin，密码：admin，即可开始准备创建集群。1) 点击 launch install wizard ,开始创建一个集群，输入集群名称：BasicPlatform。 2) 修改下面红框的内容，选择HDP的版本为2.6，修改HDP-2.6的数据源Base URL为http://192.168.1.3/HDP/centos7/，修改HDP-UTILS的数据源Base URL为http://192.168.1.3/HDP-UTILS-1.1.0.21-centos7/。 3) 设置集群机器：输入Target Hosts如下：备注，必须安装如下格式，不能使用hadoop2hadoop2.zhjxhadoop3.zhjxhadoop4.zhjx从master中拷贝出来id_rsa文件，通过选择文件，上传到ambari中。点击确认后，开始向每个机器中安装ambari-client，稍等到下面页面，即可显示安装完成。 问题1：centos7.3/4安装ambari2.6以上版本报EOF occurred in violation of protocol (_ssl.c:579) 解决办法： 4) 选择要安装的服务，与下图中的一致。可先按照如下组件HDFS、Hive、HBase、Sqoop、ZooKeeper、Flume这里选择默认了，也可以自己调整 配置标红的组件，进行修改配置 5) 各个服务Master配置详见附件26) 服务的Slaves 和 Clients节配置:详见附件37) 服务详情配置看到如下界面后，如有红色警告提示，根据警告修改服务详情内容，注意把Ooize等服务中使用的数据库改为MySQL数据库。 待确定完修改后的配置时，开始安装： 全部安装成功界面如下： 点击下一步，确定安装内容后，即可看到集群情况。 、Hadooop开启高可用（现在安装好Hadoop是不支高可用的，下面准备开启Hadoop高可用）1、 开启高可用开关 3、 设置高可用名称（如果hbase是启动的话请关闭在开启HA高可用） 5. 安装ELK5.1. 安装Elasticsearch 安装elasticsearch下载地址： https://www.elastic.co/downloads/elasticsearch下载对应的版本，下载后解压到想安装的文件夹中，因为es是绿色版本所以解压后就可以使用./bin/elasticsearch其实是一个shell脚本，最终还是启动在java的虚拟机环境中，并加入了一定参数。 12345# rpm -ivh elasticsearch-5.6.4.rpm# chkconfig --add elasticsearch# chkconfig elasticsearch on # vim etc/elasticsearch/elasticsearch.yml# /etc/init.d/elasticsearch.rpmnew start 可执行文件目录：/usr/share/elasticsearch/配置文件目录： /etc/elasticsearch/日志文件目录：/var/log/elasticsearch/data文件目录：/var/lib/elasticsearch/pid文件路径：/var/run/elasticsearch/日志文件：/var/log/elasticsearch/ 启动elasticsearch守护进程 ./bin/elasticsearch -d前台运行 ./bin/elasticsearch配合elasticsearch-servicewrapper 插件将脚本服务化更易管理 （2.x加不再支持，弃用）。运行日志在../log/下,每一个索引一个文件，每日一个文件，包括运行的慢日志和服务日志。 测试elasticsearchcurl -XGET http://xxx:9200/?pretty5.2. 安装Kibana下载页面：https://www.elastic.co/cn/downloads/kibana安装rpm包，我的电脑是x64的所以下载64位的安装包，rpm包安装完毕后会自动在/etc/init.d/下生成执行脚本，提供给service/chkconfig，更方便我们使用。 123# wget https://artifacts.elastic.co/downloads/kibana/kibana-5.6.4-x86_64.rpm# rpm -ivh kibana-5.6.4-x86_64.rpm# vim /etc/kibana/kibana.yml 5.3. 安装x-pack插件官方下载地址：https://www.elastic.co/downloads/x-pack官方文档地址：https://www.elastic.co/guide/en/x-pack/current/xpack-introduction.htmlhttps://www.elastic.co/gu ide/en/x-pack/6.0/setting-up-authentication.html#set-built-in-user-passwords elasticsearch安装x-pack插件 如果是集群架构，则每一台新机器都需要安装插件。 kibana安装了x-pack，elasticsearch也必须要安装。 123456# /usr/share/elasticsea rch/bin/elasticsearch-plugin install x-pack# /usr/share/kibana/bin/kibana-plugin install x-pack2. 编辑配置文件# vim /etc/elasticsearch/x-pack/3. 用户管理# /usr/share/elasticsearch/bin/x-pack/users useradd test -p 123456 -r superuser 其他安装6.1. 安装VNC Service由于165、166、167当做服务器的同时，也作为外网开发机使用，为了建立远程可编程环境，加入了可视化桌面也远程桌面，需要在每个机器上安装VNC Service。1) 安装 X-Window由于我们安装操作系统为最小化版本，因此首先需要安装 X-Window，在终端中运行下面的命令，安装会花费一点时间。 123$ yum check-update$ yum groupinstall "X Window System"$ yum install gnome-classic-session gnome-terminal nautilus-open-terminal control-center liberation-mono-fonts 待安装完成后，修改系统启动界面为图形界面。 12$ unlink /etc/systemd/system/default.target$ ln -sf /lib/systemd/system/graphical.target /etc/systemd/system/ default.target 重启电脑，重启完成后即可进入图形化界面。2) 安装 VNC 服务器 1$ yum install tigervnc-server -y 安装完成后后，我们需要在 /etc/systemd/system/ 目录里创建一个配置文件。我们可以将 /lib/systemd/sytem/vncserver@.service 拷贝一份配置文件范例过来。 1$cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver @:1.service 打开 /etc/systemd/system/vncserver@:1.service ，找到下面这几行，用自己的用户名替换掉 。 12ExecStart=/sbin/runuser -l hadoop -c "/usr/bin/vncserver %i"PIDFile=/home/hadoop/.vnc/%H%i.pid 如果是 root 用户则替换内容如下： 12ExecStart=/sbin/runuser -l root -c "/usr/bin/vncserver %i"PIDFile=/root/.vnc/%H%i.pid 修改完成后，按Esc退出编辑模式，!wq保存，下面重启 systemd。 1$ systemctl daemon-reload 最后还要设置一下用户的 VNC 密码。要设置某个用户的密码，必须要有能通过 sudo 切换到用户的权限。这里我们用的root用户，直接输入一下代码即可。 1$ sudo vncpasswd 3) 开启服务用下面的命令（永久地）开启服务： 1$ sudo systemctl enable vncserver@:1.service 启动服务。 1$ sudo systemctl start vncserver@:1.service 4) 用 VNC 客户端连接服务器要使用 VNC 连接服务器，我们还需要一个在本地计算机上安装的仅供连接远程计算机使用的 VNC 客户端。可以用像 Tightvnc viewer 和 Realvnc viewer 的客户端来连接到服务器。此处用RealVNC Viewer连接，配置如下： 要用更多的用户连接，需要创建配置文件和端口，请回到第2步，添加一个新的用户和端口。你需要创建 vncserver@:2.service 并替换配置文件里的用户名和之后步骤里相应的文件名、端口号。请确保你登录 VNC 服务器用的是你之前配置 VNC 密码的时候使用的那个用户名。VNC 服务本身使用的是5900端口。鉴于有不同的用户使用 VNC ，每个人的连接都会获得不同的端口。配置文件名里面的数字告诉 VNC 服务器把服务运行在5900的子端口上。在我们这个例子里，第一个 VNC 服务会运行在5901（5900 + 1）端口上，之后的依次增加，运行在5900 + x 号端口上。其中 x 是指之后用户的配置文件名 vncserver@:x.service 里面的 x 。在建立连接之前，我们需要知道服务器的 IP 地址和端口。IP 地址是一台计算机在网络中的独特的识别号码。我的服务器的 IP 地址是96.126.120.92，VNC 用户端口是1。 附件1：账户密码表表 2 系统用户表Hostname 角色 登录名/密码 备注Master root root/123456Slave1 root root/123456 系统管理员Slave2 root root/123456 系统管理员Slave3 root root/123456 系统管理员注*：大小写敏感 表 3 数据库用户表数据库 角色 登录名/密码 作用域 备注MySQL DBA root/ localhostMySQL DBA zhjx/1234 %注*：大小写敏感 表 4 服务用户表服务名称 角色 登录名/密码 作用域 备注Ambari superuser admin/admin HUE DBA root/123456 %注：大小写敏感]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jira/confluence安装]]></title>
    <url>%2F2021%2F04%2F10%2Fjira-confluence%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1 运行环境Centos 7.9jira 8.13.4confluence 7.4.5postgres 10.16 2 说明本文中的安装方案采用jira+confluence认证方式，先安装jira，再安装confluence。 3 数据库安装Postgresql yum安装方式，具体安装过程，参考官网教程。创建 jira confluence 2个数据库，设置数据库远程访问和本地访问密码验证，设置postgres 用户密码，这里密码为123456 4 jira安装4.1 官网下周jira linux安装包atlassian-jira-software-8.13.4-x64.bin 4.2 安装4.2.1 赋权并执行安装[root@jira soft]# chmod +x atlassian-jira-software-8.13.4-x64.bin [root@jira soft]# . /atlassian-jira-software-8.13.4-x64.bin • 安装完成后jira自动启动 4.2.2 开放8080端口 4.2.3 执行破解破解文件网上自行百度搜索下载，整理使用的破解文件版本：atlassian-extras-3.2.jar• 将默认的atlassian-extras-3.2.jar移除到其他目录，将破解好的atlassian-extras-3.2.jar放入进去放置目录/opt/atlassian/jira/atlassian-jira/WEB-INF/lib重启jira服务/etc/init.d/jira stop/start 4.2.4 访问页面• JIRA默认端口8080 • 这里需要等待程序进行数据初始化，需要一段时间• 总共创建了264张表 • 因为我们没有正式的license，所以需要我们在jira官网注册一个账号，然后利用这个账号申请一个可以试用30天的license，点击生成jira许可证。如下 • 自动跳转到刚才填写KEY的界面，会自动将许可证填入进去，点击下一步 4.2.5 检查破解是否生效• 破解jira，其实我们已经破解了在前面复制atlassian-extras-3.1.2.jar到/opt/atlassian/jira/atlassian-jira/WEB-INF/lib/目录下时，再次启动jira时就已经破解了• 我们现在登陆到jira中查看授权信息，如下 • 通过上文章来源(Source)：https://www.dqzboy.com图，我们可以很明显的看到jira我们可以使用到2033年，到此有关jira的安装、破解就已经全部结束。 4.3 插件安装旧系统插件在新系统进行安装破节后才能做数据迁移。 4.3.1 在线安装旧系统插件直接在线市场安装 4.3.2 插件破解jira插件授权更新：将atlassian-universal-plugin-manager-plugin-2.22.4.jar替换掉/opt/atlassian/jira/atlassian-jira/WEB-INF/atlassian-bundled-plugins/下对应jar包，重启jira服务即可。 这里使用破解文件版本 atlassian-universal-plugin-manager-plugin-4.0.2.jar文件直接百度搜索下载。备份默认 atlassian-universal-plugin-manager-plugin-XXX.jar 文件，替换破解文件，重启jira服务。 点击使用插件——获取许可证——登录官网申请临时key——拷贝key激活，默认就破解了，可以查看插件破解信息。 4.4 数据备份恢复-xml方式4.4.1 备份数据管理员登录jira, 管理——系统——导入导出——备份系统，导出jira数据。拷贝 /var/atlassian/application-data/jira/data/路径下的attachments和avatars目录，到新服务器相应目录，赋予目录及下级 jira用户权限 1chown jira:jira -R attachments avatars attachments 项目数据avatars 头像等数据 4.4.2 恢复数据管理员登录jira, 管理——系统——导入导出——恢复系统 拷贝备份的zip文件到/var/atlassian/application-data/jira/import 下，输入文件名恢复系统。拷贝attachments avatars 目录到/var/attassian/application-data/jira/data/下，并服务jira用户和组权限，重启jira服务。 4.4.3 重建索引 恢复完成。 4.5 出现问题4.5.1 问题1头像不显示原因：avatars目录数据没拷贝或放置位置有问题，权限有问题。拷贝到 application-data/jira/data/ 下。 4.5.2 问题2图片不显示解决办法：拷贝 application-data/jira/logos 数据到新系统相应路径。 4.5.3 问题3问题：用户登录后切换页面会要求重新登录认证原因：未找到解决办法：登录时候勾选 记住登录，可解决。下次登录不会再出现问题。 5 conflucese安装参考https://www.dqzboy.com/atlassian-confluence%e5%ae%89%e8%a3%85%e5%92%8c%e7%a0%b4%e8%a7%a3 5.1 下载官网下载安装包 5.2 安装5.2.1 下载和安装程序123456789101112131415161718192021222324252627282930313233343536373839404142[root@wiki-test soft]# wget https://product-downloads.atlassian.com/software/confluence/downloads/atlassian-confluence-7.2.0-x64.bin [root@wiki-test soft]# chmod +x atlassian-confluence-7.2.0-x64.bin [root@wiki-test soft]# ./atlassian-confluence-7.2.0-x64.bin This will install Confluence 7.2.0 on your computer. OK [o, Enter], Cancel [c] o # 输入o或者直接回车 Click Next to continue, or Cancel to exit Setup. Choose the appropriate installation or upgrade option. Please choose one of the following: Express Install (uses default settings) [1], Custom Install (recommended for advanced users) [2, Enter], Upgrade an existing Confluence installation [3] 1 # 输入1 See where Confluence will be installed and the settings that will be used. Installation Directory: /opt/atlassian/confluence Home Directory: /var/atlassian/application-data/confluence HTTP Port: 8090 RMI Port: 8000 Install as service: Yes Install [i, Enter], Exit [e] i # 输入i或者直接回车 Extracting files ... Please wait a few moments while we configure Confluence. Installation of Confluence 7.2.0 is complete Start Confluence now? Yes [y, Enter], No [n] y # 输入y或直接回车 Please wait a few moments while Confluence starts up. Launching Confluence ... Installation of Confluence 7.2.0 is complete Your installation of Confluence 7.2.0 is now ready and can be accessed via your browser. Confluence 7.2.0 can be accessed at http://localhost:8090 Finishing installation ... # 安装完成会自动启动 5.2.2 查看端口和放行端口1234567[root@wiki-test soft]# ss -tnlp |grep 8090LISTEN 0 10 :::8090 :::* users:(("java",pid=18657,fd=45)) [root@wiki-test soft]# firewall-cmd --permanent --zone=public --add-port=8090/tcpsuccess[root@wiki-test soft]# firewall-cmd --reloadsuccess 5.2.3 访问网页• http://ip:8090 5.2.4 进行破解123#破解需要两部，一是破解文件，二是获取授权码#注意，本地运行破解程序需要JAVA环境，直接在oraclejdk官网下载windows版本的exe程序安装即可[root@wiki-test ~]# cd /opt/atlassian/confluence/confluence/WEB-INF/lib/ • 将该目录下的atlassian-extras-decoder-v2-3.4.1.jar拷贝到自己的电脑上并进行重命名为atlassian-extras-2.4.jar • 将该.jar文件跟破解工具放在一起，然后运行破解工具• 选择.patch!找到刚才重命名的那个文件打开 • 打开后在当前目录下可以看到atlassian-extras-2.4.jar和atlassian-extras-2.4.bak两个文件，这里atlassian-extras-2.4.jar已经是破解好的了，将atlassian-extras-2.4.jar名字改回来atlassian-extras-decoder-v2-3.4.1.jar • 上传到服务器上的/opt/atlassian/confluence/confluence/WEB-INF/lib/目录，覆盖原来的atlassian-extras-decoder-v2-3.4.1.jar 1[root@wiki-test lib]# cp atlassian-extras-decoder-v2-3.4.1.jar / 123# 注意：覆盖文件后，一定到重启服务[root@wiki-test lib]# /etc/init.d/confluence stop[root@wiki-test lib]# /etc/init.d/confluence start 5.2.5 获取授权码• 查看网页中的服务器ID，运行破解工具confluence_keygen.jar，破解复制Key到Confluence里，然后点击下一步 5.2.6 配置数据库` 12345678910#安装MySQL5.7驱动 [root@wiki-test ~]# wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.48.tar.gz #将 .jar 文件放入 /opt/atlassian/confluence/confluence/WEB-INF/lib，然后重启Confluence [root@wiki-test ~]# tar -xf mysql-connector-java-5.1.48.tar.gz [root@wiki-test ~]# cd mysql-connector-java-5.1.48/ [root@wiki-test mysql-connector-java-5.1.48]# cp *.jar /opt/atlassian/confluence/confluence/WEB-INF/lib [root@wiki-test lib]# /etc/init.d/confluence stop [root@wiki-test lib]# /etc/init.d/confluence start • 刷新网页，注意数据库字符集必须为UTF8 • 注意：总共会创建117张表，会比较慢，需要等待一段时间 等待程序配置好数据库后即可完成安装了 5.3 数据备份恢复5.3.1 备份管理员登录conflucese——站点管理——一般配置——管理——备份与还原——导出网站 5.3.2 恢复拷贝备份zip包到新服务器/var/atlassian/application-data/confluence/restore 目录管理员登录conflucese——站点管理——一般配置——管理——备份与还原——选中文件——导入 导入完成，恢复成功。 5.4 问题5.4.1 问题1问题：恢复数据失败 原因：XML 备份包含导入尝试插入uniq_lwr_username user_mapping表的重复记录 = 用户 1（如上文所示）。 5.4.1.1 解决办法参考 https://community.atlassian.com/t5/Confluence-questions/Unable-to-import-XML-space-to-Confluence-Server-exported-earlier/qaq-p/1307536https://confluence.atlassian.com/confkb/confluence-site-xml-import-fails-with-duplicate-key-error-438993638.html第 1 步：查找重复的用户键运行下面的查询以获取重复用户的用户密钥： 12345SELECT * FROM user_mapping WHERE username IS NULL OR lower_username IS NULL OR lower_username NOT LIKE lower(username); 结果应如下： 第 2 步：删除重复的用户删除 user_mapping表中的重复用户 需要先删除content表中的用户记录第 3 步：删除content表中的用户记录根据第一步查询到的 “user_key”,查询content表中username字段进行匹配删除记录，然后返回第二步删除user_mapping表中的相应用户记录（如果删除中提示其他表字段关联，在其他表进行user_key匹配，删除相应记录）第 4 步：重新备份清理历史系统中的缓存数据，管理登录后台——管理——缓存管理——缓存统计——清除缓存 重新执行数据备份恢复步骤，问题解决。 5.4.2 问题2confluence使用jira用户认证。 5.4.2.1 jira配置（历史系统恢复后已经配置好，我这里重新进行了配置）管理员后台——用户管理——jira用户服务器——添加应用程序——设置认证信息——保存 5.4.2.2 confluence配置管理员后台——用户&amp;安全——用户目录——下移Rmote jira Directory(可新建，这里修改原来的)——编辑——修改认证地址和认证信息——测试并保存——上移到第一位，同步——重新登录，认证成功。 6 内存优化修改/bin/setenv.sh 文件默认配置。参考https://www.cnblogs.com/kevingrace/p/9413353.html 7 附件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>jira</tag>
        <tag>confluence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-compose示例1]]></title>
    <url>%2F2021%2F04%2F10%2Fdocker-compose%E7%A4%BA%E4%BE%8B1%2F</url>
    <content type="text"><![CDATA[install.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148#! /bin/bashlj=$(pwd)# usageusage()&#123; echo "##############################" echo -e "\033[31m 用法：$0 当前服务器IP地址 \033[0m" echo -e "\033[31m 示例：$0 192.168.1.10 \033[0m" echo "##############################"&#125;# portport()&#123; if docker ps -a --no-trunc --format "table &#123;&#123;.Names&#125;&#125;"| grep -i $&#123;2&#125;; then echo -e "\033[31m ERROR: $&#123;2&#125; container already exists !!! | $&#123;2&#125; 容器已经存在，请手动执行相关命令！！！\033[0m" exit else if [ "$(/usr/sbin/lsof -i :$&#123;1&#125;|grep -v "PID" | awk '&#123;print $2&#125;')" != "" ] then echo -e "\033[31m ERROR: $&#123;1&#125; port is already in use !!! | 错误: $&#123;1&#125; 端口已经被使用！！！\033[0m" exit else echo -e "\033[32m $&#123;1&#125; ready !!! | $&#123;1&#125; 空闲 !!! \033[0m" fi fi &#125;# 检测命令执行格式if [ $# -ne 1 ]; then usage exit 1else if [[ $1 =~ ^[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;$ ]]; then echo -e "\033[31m 服务器IP：$1 \033[0m" else usage exit 1 fifi# install lsofif rpm -qa|grep lsof; then echo -e "\033[32m ## lsof 已经安装 \033[0m"else echo -e "\033[32m ## install lsof \033[0m" yum localinstall -y $&#123;lj&#125;/docker/lsof/lsof-4.87-6.el7.x86_64.rpmfi # install dockerecho "################################################################################"cd $&#123;lj&#125;echo -e "\033[32m ### install docker \033[0m"cd dockersh install-docker.sh docker-18.09.5.tgz cp docker-compose /usr/local/bin/cd $&#123;lj&#125; # edit mapserv test fileecho "################################################################################"echo -e "\033[32m ### edit mapserv main.js file::: $&#123;lj&#125;/mapdata/client/js/main.js \033[0m"sed -i "21c url: 'http://$&#123;1&#125;:10000/gdw/wms/demo?/'," $&#123;lj&#125;/data/client/js/main.jsecho "SERVER_IP=$&#123;1&#125;" &gt; .env # Import imageecho "################################################################################"echo -e "\033[32m ### import docker image \033[0m"gunzip -c $&#123;lj&#125;/docker/postgresql.tar.gz | docker loadgunzip -c $&#123;lj&#125;/docker/redis.tar.gz | docker loadgunzip -c $&#123;lj&#125;/docker/gdwserv.tar.gz | docker loadgunzip -c $&#123;lj&#125;/docker/httpd.tar.gz | docker load# 检测端口及容器占用port 6379 crawler_redisport 5432 crawler_postgisport 10000 crawler_crawlerport 8199 crawler_crawlerport 2999 crawler_httpd # create network zhjx#if [ "$(docker network list | awk '&#123;print $2&#125;'|grep zhjx)" != "" ]; then# echo -e "\033[32m ## zhjx network already exists &amp;&amp; zhjx 网络已经存在 \033[0m"#else# echo -e "\033[32m ### create zhjx network \033[0m"# docker network create zhjx#fi # start postgresql#echo "################################################################################"#if docker ps -a --no-trunc --format "table &#123;&#123;.Names&#125;&#125;"|grep crawler_postgis; then# echo -e "\033[31m ERROR: crawler_postgis container already exists | crawler_postgis 容器 已经存在，请手动执行相关命令！！！ \033[0m"# exit#else# if [ "$(/usr/sbin/lsof -i :5432|grep -v "PID" | awk '&#123;print $2&#125;')" != "" ]# then# echo -e "\033[31m ERROR: 5432 port is already in use !!! | 错误：5432 端口已经被使用！！！\033[0m"# exit# else# echo "start run crawler"# docker run --network zhjx --name crawler_postgis --restart always -d -p 5432:5432 -e POSTGRES_PASSWORD=1qazxsw2 -v $&#123;lj&#125;/pgdata:/var/lib/postgresql/data mdillon/postgis:10# fi#fi# start redis#echo "################################################################################"#if docker ps -a --no-trunc --format "table &#123;&#123;.Names&#125;&#125;"| grep -i crawler_redis; then# echo -e "\033[31m ERROR: crawler_redis container already exists !!! | crawler_redis 容器已经存在，请手动执行相关命令！！！\033[0m"# exit#else# if [ "$(/usr/sbin/lsof -i :6379|grep -v "PID" | awk '&#123;print $2&#125;')" != "" ]# then# echo -e "\033[31m ERROR: 6379 port is already in use !!! | 错误：6379 端口已经被使用！！！\033[0m"# exit# else# echo -e "\033[32m start run redis \033[0m"# docker run --network zhjx --name crawler_redis --restart always -d -p 6379:6379 -v $&#123;lj&#125;/redis:/data redis:5.0.9# fi#fi## start crawler#echo "################################################################################"#if docker ps -a --no-trunc --format "table &#123;&#123;.Names&#125;&#125;"|grep crawler_mapser; then# echo -e "\033[31m ERROR: crawler_mapser container already exists | crawler_mapser 容器 已经存在，请手动执行相关命令！！！ \033[0m"# exit#else# if [ "$(/usr/sbin/lsof -i :8080|grep -v "PID" | awk '&#123;print $2&#125;')" != "" ]# then# echo -e "\033[31m ERROR: 8080 port is already in use !!! | 错误：8080 端口已经被使用！！！\033[0m"# exit# else# if [ "$(/usr/sbin/lsof -i :10000|grep -v "PID" | awk '&#123;print $2&#125;')" != "" ]# then# echo -e "\033[31m ERROR: 10000 port is already in use !!! | 错误：10000 端口已经被使用！！！\033[0m"# exit# else# echo -e "\033[32m start run crawler \033[0m"## docker run -u root --name crawler_mapser --network zhjx --restart always -d -p 10000:80 -p 8080:8080 -e LANG="en_US.utf8" -e JAVA_OPTS="-server -Xms2048m -Xmx2048m -Xss512k -Dfile.encoding="UTF8" -Dfile.encoding="UTF8"" -e REDIS_IP=$&#123;1&#125; -e REDIS_PORT=6379 -e REDIS_TOKENS=5 -e BASEURL=http://$&#123;1&#125;:10000/gdw -dti -v $&#123;lj&#125;/services/:/tomcat/webapps -v $&#123;lj&#125;/mapdata/:/usr/local/gdw/data gdwserv:v0.1_tomcat8# docker run -u root --name crawler_mapser --network zhjx -d -p 10000:80 -p 8080:8080 -e LANG="en_US.utf8" -e JAVA_OPTS="-server -Xms2048m -Xmx4096m -Xss512k -Dfile.encoding="UTF8"" -e REDIS_IP=$&#123;1&#125; -e REDIS_PORT=6379 -e REDIS_TOKENS=5 -e BASEURL=http://$&#123;1&#125;:10000/gdw -dti -v $&#123;lj&#125;/service/:/tomcat/webapps -v $&#123;lj&#125;/data/:/usr/local/gdw/data/ -v $&#123;lj&#125;/service_data/root/crawler/:/root/crawler/ -v $&#123;lj&#125;/service_data/datafile/crawler:/datafile/crawler -v /service_data/data/ftp/:/data/ftp/ -v $&#123;lj&#125;/service_data/icons/crawler://icons/crawler -v $&#123;lj&#125;/service_data/vsicurl/:/vsicurl/ -v $&#123;lj&#125;/service_data/vsizip:/vsizip -v $&#123;lj&#125;/service_data/vsizip:/vsizip -v $&#123;lj&#125;/service_data/home/data/insight/:/home/data/insight/ gdwserv:v0.1_tomcat8# fi# fi#fi# start crawlerdocker-compose up -dsleep 2docker psecho "----------------------------------------------------------------------------------------"echo -e "\033[32m ALL Services install finish, Please view from the browser. http://$&#123;1&#125;:8080/crawler \033[0m"echo -e "\033[32m 所有服务安装完成，请通过浏览器访问.http://$&#123;1&#125;:8080/crawler \033[0m"echo "----------------------------------------------------------------------------------------" docker-compose.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152version: "3"services: crawler: image: gdwserv:v0.1_tomcat8 restart: always depends_on: - postgis - redis ports: - "8199:8080" - "10000:80" volumes: - "./service/:/tomcat/webapps" - "./data/:/usr/local/gdw/data" - "./service_data/root/crawler/:/root/crawler/" - "./service_data/datafile/crawler:/datafile/crawler" - "./service_data/data/ftp/:/data/ftp/" - "./service_data/vsicurl/:/vsicurl" - "./service_data/vsizip:/vsizip" - "./service_data/home/data/insight:/home/data/insight" environment: - LANG="en_US.utf8"# - JAVA_OPTS="-server -Xms2048m -Xmx4096m -Xss512k -Dfile.encoding="UTF8""# 采用当前目录,默认 .env 文件内的变量 - REDIS_IP=$&#123;SERVER_IP&#125; - REDIS_PORT=6379 - REDIS_TOKENS=5 - BASEURL=http://$&#123;SERVER_IP&#125;:10000/gdw postgis: image: mdillon/postgis:10 restart: always ports: - "5432:5432"# 采用指定文件内的变量 env_file: - .post_pass volumes: - "./pgdata:/var/lib/postgresql/data" redis: image: redis:6 restart: always ports: - "6379:6379" volumes: - "./redis:/data" httpd: image: httpd:2.4.46-alpine restart: always ports: - "2999:80" volumes: - "./service_data/home/data/insight:/usr/local/apache2/htdocs" uninstall.sh1234567891011121314151617181920212223242526272829303132#! /bin/bashlj=$(pwd)echo "################################################################################"# del servicesecho -e "\033[32m ### del services \033[0m"docker-compose down # del imagesecho -e "\033[32m ### del images \033[0m"docker rmi gdwserv:v0.1_tomcat8docker rmi mdillon/postgis:10docker rmi redis:6docker rmi httpd:2.4.46-alpine # del network zhjx#echo -e "\033[32m ### del docker-network \033[0m"#docker network rm zhjx # del dockerecho -e "\033[32m ### del docker service \033[0m"systemctl stop docker.serviceSYSTEMDDIR=/usr/lib/systemd/systemSERVICEFILE=docker.serviceDOCKERDIR=/usr/bin/COMPOSEDIR=/usr/local/bin/rm -rf $&#123;DOCKERDIR&#125;/docker*rm -rf $&#123;SYSTEMDDIR&#125;/$&#123;SERVICEFILE&#125;rm -rf $&#123;COMPOSEDIR&#125;/docker-composesystemctl daemon-reload echo -e "\033[32m ALL Services delete \033[0m"echo "----------------------------------------------------------------------------------------" 1 1 1 1 1 1 1 1 1 1 1 1]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker-compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgresql+pgpool双机热备-高可用]]></title>
    <url>%2F2020%2F07%2F01%2Fpostgresql-pgpool%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87-%E9%AB%98%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、 主机 主机 IP 服务 master 192.168.0.56 postgresql、postgis、pgpool slave 192.168.0.58 postgresql、postgis、pgpool 二、安装postgresql过程忽略 三、配置白名单与流复制1. master配置PostgreSQL流复制默认是异步的。在主服务器上提交事务和从服务器上变化可见之间有一个小的延迟，这个延迟远小于基于文件日志传送，通常1秒能完成。如果主服务器突然崩溃，可能会有少量数据丢失。同步复制必须等主服务器和从服务器都写完WAL后才能提交事务。这样在一定程度上会增加事务的响应时间。配置同步复制仅需要一个额外的配置步骤： synchronous_standby_names 必须设置为一个非空值。synchronous_commit 也必须设置为 on。这里部署的是异步的流复制 1.1 配置白名单123456vim /pgdata/pg_hba.conf# 在配置文件末尾添加pg_hba.confhost all all 0.0.0.0/0 md5host replication replica 192.168.0.58/32 md5 1.2 修改配置文件1234567891011121314151617vim /pgdata/postgresql.conf# postgresql.confdata_directory = '/pgdata' # 自定义data目录archive_mode = on # 允许归档archive_command = 'cp %p /pg_archive/%f'wal_level = hot_standby # 选择热备max_wal_senders = 16 # 最多多少各流复制链接wal_keep_segments = 256 # 流复制保留最多的xlog数wal_sender_timeout = 60s # 流复制主机发送数据超时时间max_connections = 1000 # 从库的max_connections必须大于主库的full_page_writes = on # 使用pg_rewind命令同步数据库要用wal_log_hints = on # 使用pg_rewind命令同步数据库要用hot_standby = on # 使用pg_rewind命令同步数据库要用listen_addresses = '*' # 修改监听 1.3 创建data目录，赋权并修改启动文件123mkdir -p /pgdata &amp;&amp; chown postgres:postgres /pgdata &amp;&amp; chmod 700 /pgdatamkdir -p /pg_archive &amp;&amp; chown postgres:postgres /pg_archive &amp;&amp; chmod 700 pg_archive初始化数据库：su - postgres -c "/usr/pgsql-10/bin/initdb -D /pgdata 1.4 修改启动文件1vim /usr/lib/systemd/system/postgresql-10.service 1.5 启动postgresql并创建replica用户，密码replica1234systemctl start postgresql-10.servicesu postgrespsqlCREATE ROLE replica login replication encrypted password 'replica'; 1.6 在slave节点测试链接主节点slave节点执行：psql -h 192.168.0.56 -U postgres (首先设置好postgres用户密码后测试) 2 slave配置2.1 配置白名单123456vim /pgdata/pg_hba.conf# 在配置文件末尾添加pg_hba.confhost all all 0.0.0.0/0 md5host replication replica 192.168.0.56/32 md5 2.2 创建data目录12mkdir -p /pgdata &amp;&amp; chown postgres:postgres /pgdata &amp;&amp; chmod 700 /pgdatamkdir -p /pg_archive &amp;&amp; chown postgres:postgres /pg_archive &amp;&amp; chmod 700 pg_archive 2.3 备份数据1234# 切换用户su postgres# 备份主库的数据库到从库/pgdatapg_basebackup -h 192.168.0.56 -U replica -D /pgdata -X stream –P 2.4 修改启动文件1vim /usr/lib/systemd/system/postgresql-10.service 2.5 配置recovery.conf123456789su postgrescp /usr/pgsql-10/share/recovery.conf.sample /pgdata/recovery.confvim /pgdata/recovery.conf# recovery.confstandby_mode = on # 该节点为从primary_conninfo = 'host=192.168.123.180 port=5432 user=replica password=replica' #主服务器的ip、userrecovery_target_timeline = 'latest'trigger_file = '/tmp/trigger_file0' 2.6 配置postgresql.conf1234567891011121314vim /pgdata/postgresql.conf# postgresql.confdata_directory = '/pgdata' # 自定义data目录max_connections = 10000 # 从库的max_connections必须大于主库的max_standby_streaming_delay = 30swal_receiver_status_interval = 10shot_standby_feedback = onfull_page_writes = on # 使用pg_rewind命令同步数据库要用wal_log_hints = on # 使用pg_rewind命令同步数据库要用hot_standby = on # 使用pg_rewind命令同步数据库要用listen_addresses = '*' 2.7 启动postgresql1systemctl start postgresql-10.services 3 验证流复制3.1 在master上登陆psql查看状态: 1select client_addr,sync_state from pg_stat_replication; 3.2 创建test库1Create database test; 3.3 slave上登陆psql查看库 发现已同步。 4 主从切换及恢复4.1 模拟主故障关闭postgresql 4.2 升级从库为主库12345[root@test3 log]# su postgresbash-4.2$ /usr/pgsql-10/bin/pg_ctl promote -D /pgdatawaiting for server to promote.... 完成server promotedbash-4.2$ 升级主库后，recovery.conf 会变成recovery.done， 配置文件失效 4.3 通过pg_rewind命令同步数据，主库变成从库123456789101112131415161718192021222324252627[root@test2 log]# su postgres# 同步主库（原备库）数据 ！！！ 同步前，本地postgresl要处于关闭状态bash-4.2$ /usr/pgsql-10/bin/pg_rewind --target-pgdata=/pgdata --source-server='host=192.168.0.58 port=5432 user=postgres password=zhjx123 dbname=postgres' -P .....received chunk for file "pg_wal/00000003000000000000000E", offset 10000000, size 1000000received chunk for file "pg_wal/00000003000000000000000E", offset 11000000, size 1000000received chunk for file "pg_wal/00000003000000000000000E", offset 12000000, size 1000000received chunk for file "pg_wal/00000003000000000000000E", offset 13000000, size 1000000received chunk for file "pg_wal/00000003000000000000000E", offset 14000000, size 1000000received chunk for file "pg_wal/00000003000000000000000E", offset 15000000, size 1000000received chunk for file "pg_wal/00000003000000000000000E", offset 16000000, size 777216received chunk for file "pg_xact/0000", offset 0, size 8192received chunk for file "postgresql.auto.conf", offset 0, size 88received chunk for file "postgresql.conf", offset 0, size 23826received chunk for file "recovery.done", offset 0, size 5986已复制100084/100084 kB (100%)正在创建备份标签并且更新控制文件正在同步目标数据目录同步数据到磁盘...成功完成！bash-4.2$ su root[root@test2 pgdata]# lsbackup_label.old log pg_ident.conf pg_replslot pg_stat_tmp PG_VERSION postgresql.confbase pg_commit_ts pg_logical pg_serial pg_subtrans pg_wal postmaster.optscurrent_logfiles pg_dynshmem pg_multixact pg_snapshots pg_tblspc pg_xact postmaster.pidglobal pg_hba.conf pg_notify pg_stat pg_twophase postgresql.auto.conf recovery.done 修改修改recovery.done 1[root@test2 pgdata]# mv recovery.done recovery.conf 修改主库连接地址 启动本地数据库 1[root@test2 pgdata]# systemctl start postgresql-10.serivce 4.4 检测集群状态在主库（原从库）执行查询命令 查看状态:select client_addr,sync_state from pg_stat_replication; 1234567postgres=# select client_addr,sync_state from pg_stat_replication;client_addr | sync_state--------------+------------192.168.0.56| async(1 row)postgres=# 四、配置pgpool (经测试不稳定)参考：https://www.jianshu.com/p/ef183d0a9213pgpool-II是PostgreSQL服务器之间一种有效的中间件和PostgreSQL数据库客户端。它提供了以下功能。连接池pgpool-II保存到PostgreSQL服务器的连接，当一个相同新连接(如用户名、数据库、协议版本)进来时，重用他们。它减少了连接开销，提高了系统的整体吞吐量。复制pgpool-II可以管理多个PostgreSQL服务器。使用复制功能可以使2个或更多的物理磁盘上创建一个实时备份，这样服务不会因服务器的磁盘故障而中断。负载平衡如果数据库是复制的，在任何服务器上执行一个SELECT查询会返回相同的结果。pgpool-II复制特性的优势在于减少每个PostgreSQL服务器上的负载，因为它可以使用分布在多个服务器之间进行SELECT查询，从而提高系统的整体吞吐量。最好是查询和PostgreSQL服务器数量成一定比例，多用户同时执行多查询达到负载均衡最好的效果。限制连接数PostgreSQL的最大并发连接数有一定限制的，当超过限制的连接数后，连接会被拒绝。然而，设置增加最大连接数又会增加资源消耗，影响系统性能。pgpool-II也有最大连接数限制，但超过的连接进来时是进行立即排队，而不是返回一个错误。pgpool-II交互PostgreSQL的后端和前端协议时，起着继电器的作用。因此，数据库应用程序(前端)认为pgpool-II是真实的PostgreSQL服务器，服务器(后端)认为pgpool-II是它的客户端之一。因为pgpool-II在服务器和客户端是透明的，所以pgpool-II可以使用现有的数据库应用程序而做到几乎不修改它们。 1 各节点免密登录关闭系统selinux防火墙，否则失败，同事修改SSH免密登录，使配置支持。安装之前先配置密钥使master和slave这两台虚拟机的postgres用户能免密连接, 先修改postgres的密码，在root用户下 12passwd postgres# 新密码 123456 1234567891011# Master到slave1的免密码登陆：# 在master上切换至postgres用户，生成密钥su postgres ssh-keygen -t rsa然后全输入回车 # 切换到postgres用户：su postgres ssh-copy-id -i /var/lib/pgsql/.ssh/id_rsa 192.168.0.58然后ssh 192.168.0.58 成功，实现master到slave的免密码登陆-----------------------------------------------------------------------------# Slave1到master的免密码登陆：流程同上 2 安装pgpool（主从节点都安装）1yum install pgpool-II-10.x86_64 3 配置（主从节点均配置）3.1 配置pool_hba.confpool_hba.conf是对登录用户进行验证的，要和pg的pg_hba.conf保持一致，要么都是trust，要么都是md5验证方式，这里采用了md5验证方式如下设置： 123456789[postgres@master ~]$ cd /opt/pgpool/etc[postgres@etc~]$ cp pool_hba.conf.sample pool_hba.conf[postgres@etc~]$ vim pool_hba.conf#编辑内容如下# "local" is for Unix domain socket connections onlylocal all all md5# IPv4 local connections:host all all 0.0.0.0/0 md5host all all 0/0 md5 3.2 配置pcp.confpcp.conf配置用于pgpool自己登陆管理使用的，一些操作pgpool的工具会要求提供密码等，配置如下： 1234567891011121314[postgres@master ~]$ cd /etc/pgpool-II-10[postgres@etc~]$ cp pcp.conf.sample pcp.conf# 使用pg_md5生成配置的用户名密码，nariadmin 是pgpool的用户密码，随意自定义[postgres@etc~]$ pg_md5 nariadmin 6b07583ba8af8e03043a1163147faf6a# pcp.conf是pgpool管理器自己的用户名和密码，用于管理集群。[postgres@etc~]$ vim pcp.conf# 编辑内容如下。postgres也是定义的用户，自定义postgres:6b07583ba8af8e03043a1163147faf6a# 保存退出！# 在pgpool中添加pg数据库的用户名和密码[postgres@etc~]$ pg_md5 -p -m -u postgres pool_passwd# 数据库登录用户是postgres,这里输入登录密码，不能出错# 输入密码后，在/etc/pgpool-II-10 目录下会生成一个pool_passwd文件 3.3 配置系统命令权限配置ifconfig, arping 执行权限 ，执行failover_stream.sh需要用到，可以让其他普通用户执行。 12[root@master ~]# chmod u+s /sbin/ifconfig [root@master ~]# chmod u+s /usr/sbin 3.4 配置master上的pgpool.conf123[postgres@master ~]$ cd /opt/pgpool/etc[postgres@etc~]$ cp pgpool.conf.sample pgpool.conf[postgres@etc~]$ vim pgpool.conf 编辑内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876# ----------------------------# pgPool-II configuration file# ----------------------------## This file consists of lines of the form:## name = value## Whitespace may be used. Comments are introduced with "#" anywhere on a line.# The complete list of parameter names and allowed values can be found in the# pgPool-II documentation.## This file is read on server startup and when the server receives a SIGHUP# signal. If you edit the file on a running system, you have to SIGHUP the# server for the changes to take effect, or use "pgpool reload". Some# parameters, which are marked below, require a server shutdown and restart to# take effect.##------------------------------------------------------------------------------# CONNECTIONS#------------------------------------------------------------------------------# - pgpool Connection Settings -# 修改监听listen_addresses = '*' # Host name or IP address to listen on: # '*' for all, '' for no TCP/IP connections # (change requires restart)port = 9999 # Port number # (change requires restart)socket_dir = '/tmp' # Unix domain socket path # The Debian package defaults to # /var/run/postgresql # (change requires restart)listen_backlog_multiplier = 2 # Set the backlog parameter of listen(2) to # num_init_children * listen_backlog_multiplier. # (change requires restart)serialize_accept = off # whether to serialize accept() call to avoid thundering herd problem # (change requires restart)reserved_connections = 0 # Number of reserved connections. # Pgpool-II does not accept connections if over # num_init_chidlren - reserved_connections.# - pgpool Communication Manager Connection Settings -# 修改监听pcp_listen_addresses = '*' # Host name or IP address for pcp process to listen on: # '*' for all, '' for no TCP/IP connections # (change requires restart)pcp_port = 9898 # Port number for pcp # (change requires restart)pcp_socket_dir = '/tmp' # Unix domain socket path for pcp # The Debian package defaults to # /var/run/postgresql # (change requires restart)# - Backend Connection Settings -# pgpool集群后端主机信息# 修改为本端主机名，要在hosts中加入解析backend_hostname0 = 'test2' # Host name or IP address to connect to for backend 0backend_port0 = 5432 # Port number for backend 0backend_weight0 = 1 # Weight for backend 0 (only in load balancing mode)# 修改postgresql的数据目录backend_data_directory0 = '/pgdata' # Data directory for backend 0backend_flag0 = 'ALLOW_TO_FAILOVER' # Controls various backend behavior # ALLOW_TO_FAILOVER, DISALLOW_TO_FAILOVER # or ALWAYS_MASTER# 添加内容，backend_hostname1 = 'test3'backend_port1 = 5432backend_weight1 = 1backend_data_directory1 = '/pgdata'backend_flag1 = 'ALLOW_TO_FAILOVER'# 添加结束backend_application_name0 = 'server0' # walsender's application_name, used for "show pool_nodes" command#backend_hostname1 = 'host2'#backend_port1 = 5433#backend_weight1 = 1#backend_data_directory1 = '/data1'#backend_flag1 = 'ALLOW_TO_FAILOVER'#backend_application_name1 = 'server1'# - Authentication -# 激活pgpool认证方式enable_pool_hba = on # Use pool_hba.conf for client authenticationpool_passwd = 'pool_passwd' # File name of pool_passwd for md5 authentication. # "" disables pool_passwd. # (change requires restart)authentication_timeout = 60 # Delay in seconds to complete client authentication # 0 means no timeout.allow_clear_text_frontend_auth = off # Allow Pgpool-II to use clear text password authentication # with clients, when pool_passwd does not # contain the user password# - SSL Connections -ssl = off # Enable SSL support # (change requires restart)#ssl_key = './server.key' # Path to the SSL private key file # (change requires restart)#ssl_cert = './server.cert' # Path to the SSL public certificate file # (change requires restart)#ssl_ca_cert = '' # Path to a single PEM format file # containing CA root certificate(s) # (change requires restart)#ssl_ca_cert_dir = '' # Directory containing CA root certificate(s) # (change requires restart)ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # Allowed SSL ciphers # (change requires restart)ssl_prefer_server_ciphers = off # Use server's SSL cipher preferences, # rather than the client's # (change requires restart)ssl_ecdh_curve = 'prime256v1' # Name of the curve to use in ECDH key exchangessl_dh_params_file = '' # Name of the file containing Diffie-Hellman parameters used # for so-called ephemeral DH family of SSL cipher.#------------------------------------------------------------------------------# POOLS#------------------------------------------------------------------------------# - Concurrent session and pool size -num_init_children = 32 # Number of concurrent sessions allowed # (change requires restart)max_pool = 4 # Number of connection pool caches per connection # (change requires restart)# - Life time -child_life_time = 300 # Pool exits after being idle for this many secondschild_max_connections = 0 # Pool exits after receiving that many connections # 0 means no exitconnection_life_time = 0 # Connection to backend closes after being idle for this many seconds # 0 means no closeclient_idle_limit = 0 # Client is disconnected after being idle for that many seconds # (even inside an explicit transactions!) # 0 means no disconnection#------------------------------------------------------------------------------# LOGS#------------------------------------------------------------------------------# - Where to log -log_destination = 'stderr' # Where to log # Valid values are combinations of stderr, # and syslog. Default to stderr.# - What to log -log_line_prefix = '%t: pid %p: ' # printf-style string to output at beginning of each log line.log_connections = off # Log connectionslog_hostname = off # Hostname will be shown in ps status # and in logs if connections are loggedlog_statement = off # Log all statementslog_per_node_statement = off # Log all statements # with node and backend informationslog_client_messages = off # Log any client messageslog_standby_delay = 'none' # Log standby delay # Valid values are combinations of always, # if_over_threshold, none# - Syslog specific -syslog_facility = 'LOCAL0' # Syslog local facility. Default to LOCAL0syslog_ident = 'pgpool' # Syslog program identification string # Default to 'pgpool'# - Debug -#log_error_verbosity = default # terse, default, or verbose messages#client_min_messages = notice # values in order of decreasing detail: # debug5 # debug4 # debug3 # debug2 # debug1 # log # notice # warning # error#log_min_messages = warning # values in order of decreasing detail: # debug5 # debug4 # debug3 # debug2 # debug1 # info # notice # warning # error # log # fatal # panic#------------------------------------------------------------------------------# FILE LOCATIONS#------------------------------------------------------------------------------# pid文件路径 这里不做修改pid_file_name = '/var/run/pgpool-II-10/pgpool.pid' # PID file name # Can be specified as relative to the" # location of pgpool.conf file or # as an absolute path # (change requires restart)# pgpool集群状态信息文件，这里保持默认logdir = '/var/log/pgpool-II-10' # Directory of pgPool status file # (change requires restart)#------------------------------------------------------------------------------# CONNECTION POOLING#------------------------------------------------------------------------------connection_cache = on # Activate connection pools # (change requires restart) # Semicolon separated list of queries # to be issued at the end of a session # The default is for 8.3 and laterreset_query_list = 'ABORT; DISCARD ALL' # The following one is for 8.2 and before#reset_query_list = 'ABORT; RESET ALL; SET SESSION AUTHORIZATION DEFAULT'#------------------------------------------------------------------------------# REPLICATION MODE#------------------------------------------------------------------------------# 复制模式保持关闭replication_mode = off # Activate replication mode # (change requires restart)replicate_select = off # Replicate SELECT statements # when in replication mode # replicate_select is higher priority than # load_balance_mode.insert_lock = on # Automatically locks a dummy row or a table # with INSERT statements to keep SERIAL data # consistency # Without SERIAL, no lock will be issuedlobj_lock_table = '' # When rewriting lo_creat command in # replication mode, specify table name to # lock# - Degenerate handling -# replication_stop_on_mismatch = off # On disagreement with the packet kind # sent from backend, degenerate the node # which is most likely "minority" # If off, just force to exit this sessionfailover_if_affected_tuples_mismatch = off # On disagreement with the number of affected # tuples in UPDATE/DELETE queries, then # degenerate the node which is most likely # "minority". # If off, just abort the transaction to # keep the consistency#------------------------------------------------------------------------------# LOAD BALANCING MODE#------------------------------------------------------------------------------# 激活负载均衡模式load_balance_mode = on # Activate load balancing mode # (change requires restart)ignore_leading_white_space = on # Ignore leading white spaces of each querywhite_function_list = '' # Comma separated list of function names # that don't write to database # Regexp are acceptedblack_function_list = 'currval,lastval,nextval,setval' # Comma separated list of function names # that write to database # Regexp are acceptedblack_query_pattern_list = '' # Semicolon separated list of query patterns # that should be sent to primary node # Regexp are accepted # valid for streaming replicaton mode only.database_redirect_preference_list = '' # comma separated list of pairs of database and node id. # example: postgres:primary,mydb[0-4]:1,mydb[5-9]:2' # valid for streaming replicaton mode only.app_name_redirect_preference_list = '' # comma separated list of pairs of app name and node id. # example: 'psql:primary,myapp[0-4]:1,myapp[5-9]:standby' # valid for streaming replicaton mode only.allow_sql_comments = off # if on, ignore SQL comments when judging if load balance or # query cache is possible. # If off, SQL comments effectively prevent the judgment # (pre 3.4 behavior).disable_load_balance_on_write = 'transaction' # Load balance behavior when write query is issued # in an explicit transaction. # Note that any query not in an explicit transaction # is not affected by the parameter. # 'transaction' (the default): if a write query is issued, # subsequent read queries will not be load balanced # until the transaction ends. # 'trans_transaction': if a write query is issued, # subsequent read queries in an explicit transaction # will not be load balanced until the session ends. # 'always': if a write query is issued, read queries will # not be load balanced until the session ends.statement_level_load_balance = off # Enables statement level load balancing#------------------------------------------------------------------------------# MASTER/SLAVE MODE#------------------------------------------------------------------------------# 激活主从模式master_slave_mode = on # Activate master/slave mode # (change requires restart)master_slave_sub_mode = 'stream' # Master/slave sub mode # Valid values are combinations stream, slony # or logical. Default is stream. # (change requires restart)# - Streaming -# 流复制检查周期sr_check_period = 5 # Streaming replication check period # Disabled (0) by default# 流复制检查用户，replica是数据库的用户sr_check_user = 'replica' # Streaming replication check user # This is necessary even if you disable # streaming replication delay check with # sr_check_period = 0# 流复制检查用户密码sr_check_password = 'replica' # Password for streaming replication check user. # Leaving it empty will make Pgpool-II to first look for the # Password in pool_passwd file before using the empty password# 流复制检查数据库sr_check_database = 'postgres' # Database name for streaming replication checkdelay_threshold = 0 # Threshold before not dispatching query to standby node # Unit is in bytes # Disabled (0) by default# - Special commands -follow_master_command = '' # Executes this command after master failover # Special values: # %d = failed node id # %h = failed node host name # %p = failed node port number # %D = failed node database cluster path # %m = new master node id # %H = new master node hostname # %M = old master node id # %P = old primary node id # %r = new master port number # %R = new master database cluster path # %N = old primary node hostname # %S = old primary node port number # %% = '%' character#------------------------------------------------------------------------------# HEALTH CHECK GLOBAL PARAMETERS#------------------------------------------------------------------------------# 激活健康检查，周期10shealth_check_period = 10 # Health check period # Disabled (0) by defaulthealth_check_timeout = 20 # Health check timeout # 0 means no timeout# 健康检查用户，数据库管理员用户health_check_user = 'postgres' # Health check user# 健康检查用户密码health_check_password = 'zhjx123' # Password for health check user # Leaving it empty will make Pgpool-II to first look for the # Password in pool_passwd file before using the empty password# 健康检查数据库# 必须设置，否则primary数据库down了，pgpool不知道，不能及时切换。从库流复制还在连接数据，报连接失败。# 只有下次使用pgpool登录时，发现连接不上，然后报错，这时候，才知道挂了，pgpool进行切换。health_check_database = 'postgres' # Database name for health check. If '', tries 'postgres' frist, then 'template1'health_check_max_retries = 0 # Maximum number of times to retry a failed health check before giving up.health_check_retry_delay = 1 # Amount of time to wait (in seconds) between retries.connect_timeout = 10000 # Timeout value in milliseconds before giving up to connect to backend. # Default is 10000 ms (10 second). Flaky network user may want to increase # the value. 0 means no timeout. # Note that this value is not only used for health check, # but also for ordinary conection to backend.#------------------------------------------------------------------------------# HEALTH CHECK PER NODE PARAMETERS (OPTIONAL)#------------------------------------------------------------------------------#health_check_period0 = 0#health_check_timeout0 = 20#health_check_user0 = 'nobody'#health_check_password0 = ''#health_check_database0 = ''#health_check_max_retries0 = 0#health_check_retry_delay0 = 1#connect_timeout0 = 10000#主备切换的命令行配置#------------------------------------------------------------------------------# FAILOVER AND FAILBACK#------------------------------------------------------------------------------# 启动故障转移failover_command = '/etc/pgpool-II-10/failover_stream.sh %H' # Executes this command at failover # Special values: # %d = failed node id # %h = failed node host name # %p = failed node port number # %D = failed node database cluster path # %m = new master node id # %H = new master node hostname # %M = old master node id # %P = old primary node id # %r = new master port number # %R = new master database cluster path # %N = old primary node hostname # %S = old primary node port number # %% = '%' characterfailback_command = '' # Executes this command at failback. # Special values: # %d = failed node id # %h = failed node host name # %p = failed node port number # %D = failed node database cluster path # %m = new master node id # %H = new master node hostname # %M = old master node id # %P = old primary node id # %r = new master port number # %R = new master database cluster path # %N = old primary node hostname # %S = old primary node port number # %% = '%' characterfailover_on_backend_error = on # Initiates failover when reading/writing to the # backend communication socket fails # If set to off, pgpool will report an # error and disconnect the session.detach_false_primary = off # Detach false primary if on. Only # valid in streaming replicaton # mode and with PostgreSQL 9.6 or # after.search_primary_node_timeout = 300 # Timeout in seconds to search for the # primary node when a failover occurs. # 0 means no timeout, keep searching # for a primary node forever.auto_failback = off # Dettached backend node reattach automatically # if replication_state is 'streaming'.auto_failback_interval = 60 # Min interval of executing auto_failback in # seconds.#------------------------------------------------------------------------------# ONLINE RECOVERY#------------------------------------------------------------------------------recovery_user = 'nobody' # Online recovery userrecovery_password = '' # Online recovery password # Leaving it empty will make Pgpool-II to first look for the # Password in pool_passwd file before using the empty passwordrecovery_1st_stage_command = '' # Executes a command in first stagerecovery_2nd_stage_command = '' # Executes a command in second stagerecovery_timeout = 90 # Timeout in seconds to wait for the # recovering node's postmaster to start up # 0 means no waitclient_idle_limit_in_recovery = 0 # Client is disconnected after being idle # for that many seconds in the second stage # of online recovery # 0 means no disconnection # -1 means immediate disconnection#------------------------------------------------------------------------------# WATCHDOG#------------------------------------------------------------------------------# - Enabling -# 激活看门狗use_watchdog = on # Activates watchdog # (change requires restart)# -Connection to up stream servers -trusted_servers = '' # trusted server list which are used # to confirm network connection # (hostA,hostB,hostC,...) # (change requires restart)ping_path = '/bin' # ping command path # (change requires restart)# - Watchdog communication Settings -# 本端地址wd_hostname = 'test2' # Host name or IP address of this watchdog # (change requires restart)wd_port = 9000 # port number for watchdog service # (change requires restart)wd_priority = 1 # priority of this watchdog in leader election # (change requires restart)wd_authkey = '' # Authentication key for watchdog communication # (change requires restart)wd_ipc_socket_dir = '/tmp' # Unix domain socket path for watchdog IPC socket # The Debian package defaults to # /var/run/postgresql # (change requires restart)# - Virtual IP control Setting -# 激活虚拟VIP，vip在hosts里面设置解析，否则直接填写ipdelegate_IP = 'vip' # delegate IP address # If this is empty, virtual IP never bring up. # (change requires restart)if_cmd_path = '/sbin' # path to the directory where if_up/down_cmd exists # If if_up/down_cmd starts with "/", if_cmd_path will be ignored. # (change requires restart)#if_up_cmd = '/usr/bin/sudo /sbin/ip addr add $_IP_$/24 dev eth0 label eth0:0'# 执行添加vip地址命令if_up_cmd = '/sbin/ifconfig ens192:0 inet $_IP_$ netmask 255.255.254.0' # startup delegate IP command # (change requires restart)#if_down_cmd = '/usr/bin/sudo /sbin/ip addr del $_IP_$/24 dev eth0'# 执行删除vip地址命令if_down_cmd = '/sbin/ifconfig ens192:0 down' # shutdown delegate IP command # (change requires restart)arping_path = '/usr/sbin' # arping command path # If arping_cmd starts with "/", if_cmd_path will be ignored. # (change requires restart)arping_cmd = '/usr/bin/sudo /usr/sbin/arping -U $_IP_$ -w 1 -I ens192' # arping command # (change requires restart)# - Behaivor on escalation Setting -clear_memqcache_on_escalation = on # Clear all the query cache on shared memory # when standby pgpool escalate to active pgpool # (= virtual IP holder). # This should be off if client connects to pgpool # not using virtual IP. # (change requires restart)wd_escalation_command = '' # Executes this command at escalation on new active pgpool. # (change requires restart)wd_de_escalation_command = '' # Executes this command when master pgpool resigns from being master. # (change requires restart)# - Watchdog consensus settings for failover -failover_when_quorum_exists = on # Only perform backend node failover # when the watchdog cluster holds the quorum # (change requires restart)failover_require_consensus = on # Perform failover when majority of Pgpool-II nodes # aggrees on the backend node status change # (change requires restart)allow_multiple_failover_requests_from_node = off # A Pgpool-II node can cast multiple votes # for building the consensus on failover # (change requires restart)enable_consensus_with_half_votes = off # apply majority rule for consensus and quorum computation # at 50% of votes in a cluster with even number of nodes. # when enabled the existence of quorum and consensus # on failover is resolved after receiving half of the # total votes in the cluster, otherwise both these # decisions require at least one more vote than # half of the total votes. # (change requires restart)# - Lifecheck Setting -# -- common --wd_monitoring_interfaces_list = '' # Comma separated list of interfaces names to monitor. # if any interface from the list is active the watchdog will # consider the network is fine # 'any' to enable monitoring on all interfaces except loopback # '' to disable monitoring # (change requires restart)wd_lifecheck_method = 'heartbeat' # Method of watchdog lifecheck ('heartbeat' or 'query' or 'external') # (change requires restart)wd_interval = 10 # lifecheck interval (sec) &gt; 0 # (change requires restart)# -- heartbeat mode --# 激活心跳侦测wd_heartbeat_port = 9694 # Port number for receiving heartbeat signal # (change requires restart)wd_heartbeat_keepalive = 2 # Interval time of sending heartbeat signal (sec) # (change requires restart)wd_heartbeat_deadtime = 30 # Deadtime interval for heartbeat signal (sec) # (change requires restart)# 对端地址heartbeat_destination0 = 'test3' # Host name or IP address of destination 0 # for sending heartbeat signal. # (change requires restart)heartbeat_destination_port0 = 9694 # Port number of destination 0 for sending # heartbeat signal. Usually this is the # same as wd_heartbeat_port. # (change requires restart)# 执行侦测的网卡heartbeat_device0 = 'ens192' # Name of NIC device (such like 'eth0') # used for sending/receiving heartbeat # signal to/from destination 0. # This works only when this is not empty # and pgpool has root privilege. # (change requires restart)#heartbeat_destination1 = 'host0_ip2'#heartbeat_destination_port1 = 9694#heartbeat_device1 = ''# -- query mode --wd_life_point = 3 # lifecheck retry times # (change requires restart)wd_lifecheck_query = 'SELECT 1' # lifecheck query to pgpool from watchdog # (change requires restart)wd_lifecheck_dbname = 'template1' # Database name connected for lifecheck # (change requires restart)wd_lifecheck_user = 'nobody' # watchdog user monitoring pgpools in lifecheck # (change requires restart)wd_lifecheck_password = '' # Password for watchdog user in lifecheck # Leaving it empty will make Pgpool-II to first look for the # Password in pool_passwd file before using the empty password # (change requires restart)# - Other pgpool Connection Settings -# 激活其他pgpool服务器节点连接信息#other_pgpool_hostname0 = 'host0'other_pgpool_hostname0 = 'test3' # Host name or IP address to connect to for other pgpool 0 # (change requires restart)# pgpool连接端口改为9999other_pgpool_port0 = 9999 # Port number for other pgpool 0 # (change requires restart)other_wd_port0 = 9000 # Port number for other watchdog 0 # (change requires restart)#other_pgpool_hostname1 = 'host1'#other_pgpool_port1 = 5432#other_wd_port1 = 9000#------------------------------------------------------------------------------# OTHERS#------------------------------------------------------------------------------relcache_expire = 0 # Life time of relation cache in seconds. # 0 means no cache expiration(the default). # The relation cache is used for cache the # query result against PostgreSQL system # catalog to obtain various information # including table structures or if it's a # temporary table or not. The cache is # maintained in a pgpool child local memory # and being kept as long as it survives. # If someone modify the table by using # ALTER TABLE or some such, the relcache is # not consistent anymore. # For this purpose, cache_expiration # controls the life time of the cache.relcache_size = 256 # Number of relation cache # entry. If you see frequently: # "pool_search_relcache: cache replacement happend" # in the pgpool log, you might want to increate this number.check_temp_table = catalog # Temporary table check method. catalog, trace or none. # Default is catalog.check_unlogged_table = on # If on, enable unlogged table check in SELECT statements. # This initiates queries against system catalog of primary/master # thus increases load of master. # If you are absolutely sure that your system never uses unlogged tables # and you want to save access to primary/master, you could turn this off. # Default is on.enable_shared_relcache = on # If on, relation cache stored in memory cache, # the cache is shared among child process. # Default is on. # (change requires restart)relcache_query_target = master # Target node to send relcache queries. Default is master (primary) node. # If load_balance_node is specified, queries will be sent to load balance node.#------------------------------------------------------------------------------# IN MEMORY QUERY MEMORY CACHE#------------------------------------------------------------------------------memory_cache_enabled = off # If on, use the memory cache functionality, off by default # (change requires restart)memqcache_method = 'shmem' # Cache storage method. either 'shmem'(shared memory) or # 'memcached'. 'shmem' by default # (change requires restart)memqcache_memcached_host = 'localhost' # Memcached host name or IP address. Mandatory if # memqcache_method = 'memcached'. # Defaults to localhost. # (change requires restart)memqcache_memcached_port = 11211 # Memcached port number. Mondatory if memqcache_method = 'memcached'. # Defaults to 11211. # (change requires restart)memqcache_total_size = 67108864 # Total memory size in bytes for storing memory cache. # Mandatory if memqcache_method = 'shmem'. # Defaults to 64MB. # (change requires restart)memqcache_max_num_cache = 1000000 # Total number of cache entries. Mandatory # if memqcache_method = 'shmem'. # Each cache entry consumes 48 bytes on shared memory. # Defaults to 1,000,000(45.8MB). # (change requires restart)memqcache_expire = 0 # Memory cache entry life time specified in seconds. # 0 means infinite life time. 0 by default. # (change requires restart)memqcache_auto_cache_invalidation = on # If on, invalidation of query cache is triggered by corresponding # DDL/DML/DCL(and memqcache_expire). If off, it is only triggered # by memqcache_expire. on by default. # (change requires restart)memqcache_maxcache = 409600 # Maximum SELECT result size in bytes. # Must be smaller than memqcache_cache_block_size. Defaults to 400KB. # (change requires restart)memqcache_cache_block_size = 1048576 # Cache block size in bytes. Mandatory if memqcache_method = 'shmem'. # Defaults to 1MB. # (change requires restart)memqcache_oiddir = '/var/log/pgpool/oiddir' # Temporary work directory to record table oids # (change requires restart)white_memqcache_table_list = '' # Comma separated list of table names to memcache # that don't write to database # Regexp are acceptedblack_memqcache_table_list = '' # Comma separated list of table names not to memcache # that don't write to database # Regexp are accepted 3.5 配置slave上的pgpool.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876# ----------------------------# pgPool-II configuration file# ----------------------------## This file consists of lines of the form:## name = value## Whitespace may be used. Comments are introduced with "#" anywhere on a line.# The complete list of parameter names and allowed values can be found in the# pgPool-II documentation.## This file is read on server startup and when the server receives a SIGHUP# signal. If you edit the file on a running system, you have to SIGHUP the# server for the changes to take effect, or use "pgpool reload". Some# parameters, which are marked below, require a server shutdown and restart to# take effect.##------------------------------------------------------------------------------# CONNECTIONS#------------------------------------------------------------------------------# - pgpool Connection Settings -# 修改监听listen_addresses = '*' # Host name or IP address to listen on: # '*' for all, '' for no TCP/IP connections # (change requires restart)port = 9999 # Port number # (change requires restart)socket_dir = '/tmp' # Unix domain socket path # The Debian package defaults to # /var/run/postgresql # (change requires restart)listen_backlog_multiplier = 2 # Set the backlog parameter of listen(2) to # num_init_children * listen_backlog_multiplier. # (change requires restart)serialize_accept = off # whether to serialize accept() call to avoid thundering herd problem # (change requires restart)reserved_connections = 0 # Number of reserved connections. # Pgpool-II does not accept connections if over # num_init_chidlren - reserved_connections.# - pgpool Communication Manager Connection Settings -# 修改监听pcp_listen_addresses = '*' # Host name or IP address for pcp process to listen on: # '*' for all, '' for no TCP/IP connections # (change requires restart)pcp_port = 9898 # Port number for pcp # (change requires restart)pcp_socket_dir = '/tmp' # Unix domain socket path for pcp # The Debian package defaults to # /var/run/postgresql # (change requires restart)# - Backend Connection Settings -# pgpool集群后端主机信息# 修改为本端主机名，要在hosts中加入解析backend_hostname0 = 'test2' # Host name or IP address to connect to for backend 0backend_port0 = 5432 # Port number for backend 0backend_weight0 = 1 # Weight for backend 0 (only in load balancing mode)# 修改postgresql的数据目录backend_data_directory0 = '/pgdata' # Data directory for backend 0backend_flag0 = 'ALLOW_TO_FAILOVER' # Controls various backend behavior # ALLOW_TO_FAILOVER, DISALLOW_TO_FAILOVER # or ALWAYS_MASTER# 添加内容，backend_hostname1 = 'test3'backend_port1 = 5432backend_weight1 = 1backend_data_directory1 = '/pgdata'backend_flag1 = 'ALLOW_TO_FAILOVER'# 添加结束backend_application_name0 = 'server0' # walsender's application_name, used for "show pool_nodes" command#backend_hostname1 = 'host2'#backend_port1 = 5433#backend_weight1 = 1#backend_data_directory1 = '/data1'#backend_flag1 = 'ALLOW_TO_FAILOVER'#backend_application_name1 = 'server1'# - Authentication -# 激活pgpool认证方式enable_pool_hba = on # Use pool_hba.conf for client authenticationpool_passwd = 'pool_passwd' # File name of pool_passwd for md5 authentication. # "" disables pool_passwd. # (change requires restart)authentication_timeout = 60 # Delay in seconds to complete client authentication # 0 means no timeout.allow_clear_text_frontend_auth = off # Allow Pgpool-II to use clear text password authentication # with clients, when pool_passwd does not # contain the user password# - SSL Connections -ssl = off # Enable SSL support # (change requires restart)#ssl_key = './server.key' # Path to the SSL private key file # (change requires restart)#ssl_cert = './server.cert' # Path to the SSL public certificate file # (change requires restart)#ssl_ca_cert = '' # Path to a single PEM format file # containing CA root certificate(s) # (change requires restart)#ssl_ca_cert_dir = '' # Directory containing CA root certificate(s) # (change requires restart)ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # Allowed SSL ciphers # (change requires restart)ssl_prefer_server_ciphers = off # Use server's SSL cipher preferences, # rather than the client's # (change requires restart)ssl_ecdh_curve = 'prime256v1' # Name of the curve to use in ECDH key exchangessl_dh_params_file = '' # Name of the file containing Diffie-Hellman parameters used # for so-called ephemeral DH family of SSL cipher.#------------------------------------------------------------------------------# POOLS#------------------------------------------------------------------------------# - Concurrent session and pool size -num_init_children = 32 # Number of concurrent sessions allowed # (change requires restart)max_pool = 4 # Number of connection pool caches per connection # (change requires restart)# - Life time -child_life_time = 300 # Pool exits after being idle for this many secondschild_max_connections = 0 # Pool exits after receiving that many connections # 0 means no exitconnection_life_time = 0 # Connection to backend closes after being idle for this many seconds # 0 means no closeclient_idle_limit = 0 # Client is disconnected after being idle for that many seconds # (even inside an explicit transactions!) # 0 means no disconnection#------------------------------------------------------------------------------# LOGS#------------------------------------------------------------------------------# - Where to log -log_destination = 'stderr' # Where to log # Valid values are combinations of stderr, # and syslog. Default to stderr.# - What to log -log_line_prefix = '%t: pid %p: ' # printf-style string to output at beginning of each log line.log_connections = off # Log connectionslog_hostname = off # Hostname will be shown in ps status # and in logs if connections are loggedlog_statement = off # Log all statementslog_per_node_statement = off # Log all statements # with node and backend informationslog_client_messages = off # Log any client messageslog_standby_delay = 'none' # Log standby delay # Valid values are combinations of always, # if_over_threshold, none# - Syslog specific -syslog_facility = 'LOCAL0' # Syslog local facility. Default to LOCAL0syslog_ident = 'pgpool' # Syslog program identification string # Default to 'pgpool'# - Debug -#log_error_verbosity = default # terse, default, or verbose messages#client_min_messages = notice # values in order of decreasing detail: # debug5 # debug4 # debug3 # debug2 # debug1 # log # notice # warning # error#log_min_messages = warning # values in order of decreasing detail: # debug5 # debug4 # debug3 # debug2 # debug1 # info # notice # warning # error # log # fatal # panic#------------------------------------------------------------------------------# FILE LOCATIONS#------------------------------------------------------------------------------# pid文件路径 这里不做修改pid_file_name = '/var/run/pgpool-II-10/pgpool.pid' # PID file name # Can be specified as relative to the" # location of pgpool.conf file or # as an absolute path # (change requires restart)# pgpool集群状态信息文件，这里保持默认logdir = '/var/log/pgpool-II-10' # Directory of pgPool status file # (change requires restart)#------------------------------------------------------------------------------# CONNECTION POOLING#------------------------------------------------------------------------------connection_cache = on # Activate connection pools # (change requires restart) # Semicolon separated list of queries # to be issued at the end of a session # The default is for 8.3 and laterreset_query_list = 'ABORT; DISCARD ALL' # The following one is for 8.2 and before#reset_query_list = 'ABORT; RESET ALL; SET SESSION AUTHORIZATION DEFAULT'#------------------------------------------------------------------------------# REPLICATION MODE#------------------------------------------------------------------------------# 复制模式保持关闭replication_mode = off # Activate replication mode # (change requires restart)replicate_select = off # Replicate SELECT statements # when in replication mode # replicate_select is higher priority than # load_balance_mode.insert_lock = on # Automatically locks a dummy row or a table # with INSERT statements to keep SERIAL data # consistency # Without SERIAL, no lock will be issuedlobj_lock_table = '' # When rewriting lo_creat command in # replication mode, specify table name to # lock# - Degenerate handling -# replication_stop_on_mismatch = off # On disagreement with the packet kind # sent from backend, degenerate the node # which is most likely "minority" # If off, just force to exit this sessionfailover_if_affected_tuples_mismatch = off # On disagreement with the number of affected # tuples in UPDATE/DELETE queries, then # degenerate the node which is most likely # "minority". # If off, just abort the transaction to # keep the consistency#------------------------------------------------------------------------------# LOAD BALANCING MODE#------------------------------------------------------------------------------# 激活负载均衡模式load_balance_mode = on # Activate load balancing mode # (change requires restart)ignore_leading_white_space = on # Ignore leading white spaces of each querywhite_function_list = '' # Comma separated list of function names # that don't write to database # Regexp are acceptedblack_function_list = 'currval,lastval,nextval,setval' # Comma separated list of function names # that write to database # Regexp are acceptedblack_query_pattern_list = '' # Semicolon separated list of query patterns # that should be sent to primary node # Regexp are accepted # valid for streaming replicaton mode only.database_redirect_preference_list = '' # comma separated list of pairs of database and node id. # example: postgres:primary,mydb[0-4]:1,mydb[5-9]:2' # valid for streaming replicaton mode only.app_name_redirect_preference_list = '' # comma separated list of pairs of app name and node id. # example: 'psql:primary,myapp[0-4]:1,myapp[5-9]:standby' # valid for streaming replicaton mode only.allow_sql_comments = off # if on, ignore SQL comments when judging if load balance or # query cache is possible. # If off, SQL comments effectively prevent the judgment # (pre 3.4 behavior).disable_load_balance_on_write = 'transaction' # Load balance behavior when write query is issued # in an explicit transaction. # Note that any query not in an explicit transaction # is not affected by the parameter. # 'transaction' (the default): if a write query is issued, # subsequent read queries will not be load balanced # until the transaction ends. # 'trans_transaction': if a write query is issued, # subsequent read queries in an explicit transaction # will not be load balanced until the session ends. # 'always': if a write query is issued, read queries will # not be load balanced until the session ends.statement_level_load_balance = off # Enables statement level load balancing#------------------------------------------------------------------------------# MASTER/SLAVE MODE#------------------------------------------------------------------------------# 激活主从模式master_slave_mode = on # Activate master/slave mode # (change requires restart)master_slave_sub_mode = 'stream' # Master/slave sub mode # Valid values are combinations stream, slony # or logical. Default is stream. # (change requires restart)# - Streaming -# 流复制检查周期sr_check_period = 5 # Streaming replication check period # Disabled (0) by default# 流复制检查用户，replica是数据库的用户sr_check_user = 'replica' # Streaming replication check user # This is necessary even if you disable # streaming replication delay check with # sr_check_period = 0# 流复制检查用户密码sr_check_password = 'replica' # Password for streaming replication check user. # Leaving it empty will make Pgpool-II to first look for the # Password in pool_passwd file before using the empty password# 流复制检查数据库sr_check_database = 'postgres' # Database name for streaming replication checkdelay_threshold = 0 # Threshold before not dispatching query to standby node # Unit is in bytes # Disabled (0) by default# - Special commands -follow_master_command = '' # Executes this command after master failover # Special values: # %d = failed node id # %h = failed node host name # %p = failed node port number # %D = failed node database cluster path # %m = new master node id # %H = new master node hostname # %M = old master node id # %P = old primary node id # %r = new master port number # %R = new master database cluster path # %N = old primary node hostname # %S = old primary node port number # %% = '%' character#------------------------------------------------------------------------------# HEALTH CHECK GLOBAL PARAMETERS#------------------------------------------------------------------------------# 激活健康检查，周期10shealth_check_period = 10 # Health check period # Disabled (0) by defaulthealth_check_timeout = 20 # Health check timeout # 0 means no timeout# 健康检查用户，数据库管理员用户health_check_user = 'postgres' # Health check user# 健康检查用户密码health_check_password = 'zhjx123' # Password for health check user # Leaving it empty will make Pgpool-II to first look for the # Password in pool_passwd file before using the empty password# 健康检查数据库# 必须设置，否则primary数据库down了，pgpool不知道，不能及时切换。从库流复制还在连接数据，报连接失败。# 只有下次使用pgpool登录时，发现连接不上，然后报错，这时候，才知道挂了，pgpool进行切换。health_check_database = 'postgres' # Database name for health check. If '', tries 'postgres' frist, then 'template1'health_check_max_retries = 0 # Maximum number of times to retry a failed health check before giving up.health_check_retry_delay = 1 # Amount of time to wait (in seconds) between retries.connect_timeout = 10000 # Timeout value in milliseconds before giving up to connect to backend. # Default is 10000 ms (10 second). Flaky network user may want to increase # the value. 0 means no timeout. # Note that this value is not only used for health check, # but also for ordinary conection to backend.#------------------------------------------------------------------------------# HEALTH CHECK PER NODE PARAMETERS (OPTIONAL)#------------------------------------------------------------------------------#health_check_period0 = 0#health_check_timeout0 = 20#health_check_user0 = 'nobody'#health_check_password0 = ''#health_check_database0 = ''#health_check_max_retries0 = 0#health_check_retry_delay0 = 1#connect_timeout0 = 10000#主备切换的命令行配置#------------------------------------------------------------------------------# FAILOVER AND FAILBACK#------------------------------------------------------------------------------# 启动故障转移failover_command = '/etc/pgpool-II-10/failover_stream.sh %H' # Executes this command at failover # Special values: # %d = failed node id # %h = failed node host name # %p = failed node port number # %D = failed node database cluster path # %m = new master node id # %H = new master node hostname # %M = old master node id # %P = old primary node id # %r = new master port number # %R = new master database cluster path # %N = old primary node hostname # %S = old primary node port number # %% = '%' characterfailback_command = '' # Executes this command at failback. # Special values: # %d = failed node id # %h = failed node host name # %p = failed node port number # %D = failed node database cluster path # %m = new master node id # %H = new master node hostname # %M = old master node id # %P = old primary node id # %r = new master port number # %R = new master database cluster path # %N = old primary node hostname # %S = old primary node port number # %% = '%' characterfailover_on_backend_error = on # Initiates failover when reading/writing to the # backend communication socket fails # If set to off, pgpool will report an # error and disconnect the session.detach_false_primary = off # Detach false primary if on. Only # valid in streaming replicaton # mode and with PostgreSQL 9.6 or # after.search_primary_node_timeout = 300 # Timeout in seconds to search for the # primary node when a failover occurs. # 0 means no timeout, keep searching # for a primary node forever.auto_failback = off # Dettached backend node reattach automatically # if replication_state is 'streaming'.auto_failback_interval = 60 # Min interval of executing auto_failback in # seconds.#------------------------------------------------------------------------------# ONLINE RECOVERY#------------------------------------------------------------------------------recovery_user = 'nobody' # Online recovery userrecovery_password = '' # Online recovery password # Leaving it empty will make Pgpool-II to first look for the # Password in pool_passwd file before using the empty passwordrecovery_1st_stage_command = '' # Executes a command in first stagerecovery_2nd_stage_command = '' # Executes a command in second stagerecovery_timeout = 90 # Timeout in seconds to wait for the # recovering node's postmaster to start up # 0 means no waitclient_idle_limit_in_recovery = 0 # Client is disconnected after being idle # for that many seconds in the second stage # of online recovery # 0 means no disconnection # -1 means immediate disconnection#------------------------------------------------------------------------------# WATCHDOG#------------------------------------------------------------------------------# - Enabling -# 激活看门狗use_watchdog = on # Activates watchdog # (change requires restart)# -Connection to up stream servers -trusted_servers = '' # trusted server list which are used # to confirm network connection # (hostA,hostB,hostC,...) # (change requires restart)ping_path = '/bin' # ping command path # (change requires restart)# - Watchdog communication Settings -# 本端地址，与master节点相反wd_hostname = 'test3' # Host name or IP address of this watchdog # (change requires restart)wd_port = 9000 # port number for watchdog service # (change requires restart)wd_priority = 1 # priority of this watchdog in leader election # (change requires restart)wd_authkey = '' # Authentication key for watchdog communication # (change requires restart)wd_ipc_socket_dir = '/tmp' # Unix domain socket path for watchdog IPC socket # The Debian package defaults to # /var/run/postgresql # (change requires restart)# - Virtual IP control Setting -# 激活虚拟VIP，vip在hosts里面设置解析，否则直接填写ipdelegate_IP = 'vip' # delegate IP address # If this is empty, virtual IP never bring up. # (change requires restart)if_cmd_path = '/sbin' # path to the directory where if_up/down_cmd exists # If if_up/down_cmd starts with "/", if_cmd_path will be ignored. # (change requires restart)#if_up_cmd = '/usr/bin/sudo /sbin/ip addr add $_IP_$/24 dev eth0 label eth0:0'# 执行添加vip地址命令if_up_cmd = '/sbin/ifconfig ens192:0 inet $_IP_$ netmask 255.255.254.0' # startup delegate IP command # (change requires restart)#if_down_cmd = '/usr/bin/sudo /sbin/ip addr del $_IP_$/24 dev eth0'# 执行删除vip地址命令if_down_cmd = '/sbin/ifconfig ens192:0 down' # shutdown delegate IP command # (change requires restart)arping_path = '/usr/sbin' # arping command path # If arping_cmd starts with "/", if_cmd_path will be ignored. # (change requires restart)arping_cmd = '/usr/bin/sudo /usr/sbin/arping -U $_IP_$ -w 1 -I ens192' # arping command # (change requires restart)# - Behaivor on escalation Setting -clear_memqcache_on_escalation = on # Clear all the query cache on shared memory # when standby pgpool escalate to active pgpool # (= virtual IP holder). # This should be off if client connects to pgpool # not using virtual IP. # (change requires restart)wd_escalation_command = '' # Executes this command at escalation on new active pgpool. # (change requires restart)wd_de_escalation_command = '' # Executes this command when master pgpool resigns from being master. # (change requires restart)# - Watchdog consensus settings for failover -failover_when_quorum_exists = on # Only perform backend node failover # when the watchdog cluster holds the quorum # (change requires restart)failover_require_consensus = on # Perform failover when majority of Pgpool-II nodes # aggrees on the backend node status change # (change requires restart)allow_multiple_failover_requests_from_node = off # A Pgpool-II node can cast multiple votes # for building the consensus on failover # (change requires restart)enable_consensus_with_half_votes = off # apply majority rule for consensus and quorum computation # at 50% of votes in a cluster with even number of nodes. # when enabled the existence of quorum and consensus # on failover is resolved after receiving half of the # total votes in the cluster, otherwise both these # decisions require at least one more vote than # half of the total votes. # (change requires restart)# - Lifecheck Setting -# -- common --wd_monitoring_interfaces_list = '' # Comma separated list of interfaces names to monitor. # if any interface from the list is active the watchdog will # consider the network is fine # 'any' to enable monitoring on all interfaces except loopback # '' to disable monitoring # (change requires restart)wd_lifecheck_method = 'heartbeat' # Method of watchdog lifecheck ('heartbeat' or 'query' or 'external') # (change requires restart)wd_interval = 10 # lifecheck interval (sec) &gt; 0 # (change requires restart)# -- heartbeat mode --# 激活心跳侦测wd_heartbeat_port = 9694 # Port number for receiving heartbeat signal # (change requires restart)wd_heartbeat_keepalive = 2 # Interval time of sending heartbeat signal (sec) # (change requires restart)wd_heartbeat_deadtime = 30 # Deadtime interval for heartbeat signal (sec) # (change requires restart)# 对端地址，与master节点相反heartbeat_destination0 = 'test2' # Host name or IP address of destination 0 # for sending heartbeat signal. # (change requires restart)heartbeat_destination_port0 = 9694 # Port number of destination 0 for sending # heartbeat signal. Usually this is the # same as wd_heartbeat_port. # (change requires restart)# 执行侦测的网卡heartbeat_device0 = 'ens192' # Name of NIC device (such like 'eth0') # used for sending/receiving heartbeat # signal to/from destination 0. # This works only when this is not empty # and pgpool has root privilege. # (change requires restart)#heartbeat_destination1 = 'host0_ip2'#heartbeat_destination_port1 = 9694#heartbeat_device1 = ''# -- query mode --wd_life_point = 3 # lifecheck retry times # (change requires restart)wd_lifecheck_query = 'SELECT 1' # lifecheck query to pgpool from watchdog # (change requires restart)wd_lifecheck_dbname = 'template1' # Database name connected for lifecheck # (change requires restart)wd_lifecheck_user = 'nobody' # watchdog user monitoring pgpools in lifecheck # (change requires restart)wd_lifecheck_password = '' # Password for watchdog user in lifecheck # Leaving it empty will make Pgpool-II to first look for the # Password in pool_passwd file before using the empty password # (change requires restart)# - Other pgpool Connection Settings -# 激活其他pgpool服务器节点连接信息#other_pgpool_hostname0 = 'host0'other_pgpool_hostname0 = 'test2' # Host name or IP address to connect to for other pgpool 0 # (change requires restart)# pgpool连接端口改为9999other_pgpool_port0 = 9999 # Port number for other pgpool 0 # (change requires restart)other_wd_port0 = 9000 # Port number for other watchdog 0 # (change requires restart)#other_pgpool_hostname1 = 'host1'#other_pgpool_port1 = 5432#other_wd_port1 = 9000#------------------------------------------------------------------------------# OTHERS#------------------------------------------------------------------------------relcache_expire = 0 # Life time of relation cache in seconds. # 0 means no cache expiration(the default). # The relation cache is used for cache the # query result against PostgreSQL system # catalog to obtain various information # including table structures or if it's a # temporary table or not. The cache is # maintained in a pgpool child local memory # and being kept as long as it survives. # If someone modify the table by using # ALTER TABLE or some such, the relcache is # not consistent anymore. # For this purpose, cache_expiration # controls the life time of the cache.relcache_size = 256 # Number of relation cache # entry. If you see frequently: # "pool_search_relcache: cache replacement happend" # in the pgpool log, you might want to increate this number.check_temp_table = catalog # Temporary table check method. catalog, trace or none. # Default is catalog.check_unlogged_table = on # If on, enable unlogged table check in SELECT statements. # This initiates queries against system catalog of primary/master # thus increases load of master. # If you are absolutely sure that your system never uses unlogged tables # and you want to save access to primary/master, you could turn this off. # Default is on.enable_shared_relcache = on # If on, relation cache stored in memory cache, # the cache is shared among child process. # Default is on. # (change requires restart)relcache_query_target = master # Target node to send relcache queries. Default is master (primary) node. # If load_balance_node is specified, queries will be sent to load balance node.#------------------------------------------------------------------------------# IN MEMORY QUERY MEMORY CACHE#------------------------------------------------------------------------------memory_cache_enabled = off # If on, use the memory cache functionality, off by default # (change requires restart)memqcache_method = 'shmem' # Cache storage method. either 'shmem'(shared memory) or # 'memcached'. 'shmem' by default # (change requires restart)memqcache_memcached_host = 'localhost' # Memcached host name or IP address. Mandatory if # memqcache_method = 'memcached'. # Defaults to localhost. # (change requires restart)memqcache_memcached_port = 11211 # Memcached port number. Mondatory if memqcache_method = 'memcached'. # Defaults to 11211. # (change requires restart)memqcache_total_size = 67108864 # Total memory size in bytes for storing memory cache. # Mandatory if memqcache_method = 'shmem'. # Defaults to 64MB. # (change requires restart)memqcache_max_num_cache = 1000000 # Total number of cache entries. Mandatory # if memqcache_method = 'shmem'. # Each cache entry consumes 48 bytes on shared memory. # Defaults to 1,000,000(45.8MB). # (change requires restart)memqcache_expire = 0 # Memory cache entry life time specified in seconds. # 0 means infinite life time. 0 by default. # (change requires restart)memqcache_auto_cache_invalidation = on # If on, invalidation of query cache is triggered by corresponding # DDL/DML/DCL(and memqcache_expire). If off, it is only triggered # by memqcache_expire. on by default. # (change requires restart)memqcache_maxcache = 409600 # Maximum SELECT result size in bytes. # Must be smaller than memqcache_cache_block_size. Defaults to 400KB. # (change requires restart)memqcache_cache_block_size = 1048576 # Cache block size in bytes. Mandatory if memqcache_method = 'shmem'. # Defaults to 1MB. # (change requires restart)memqcache_oiddir = '/var/log/pgpool/oiddir' # Temporary work directory to record table oids # (change requires restart)white_memqcache_table_list = '' # Comma separated list of table names to memcache # that don't write to database # Regexp are acceptedblack_memqcache_table_list = '' # Comma separated list of table names not to memcache # that don't write to database # Regexp are accepted 配置文件里，故障处理配置的是failover_command = ‘/etc/pgpool-II-10/failover_stream.sh %H ‘，因此，需要在/opt/pgpool目录中写个failover_stream.sh脚本： 123[postgres@master ~]$ cd /etc/pgpool-II-10[postgres@pgpool~]$ touch failover_stream.sh[postgres@pgpool~]$ vim failover_stream.sh 123456789101112#! /bin/sh # Failover command for streaming replication. # Arguments: $1: new master hostname. new_master=$1trigger_command="/usr/pgsql-10/bin/pg_ctl promote -D /pgdata"# Prompte standby database. # 升级从数据库为主节点/usr/bin/ssh -T $new_master $trigger_commandexit 0; 如果是其他用户创建的，需要赋予postgres可执行权限，例如 12[root@opt ~]$ chown -R postgres.postgres /opt/pgpool[root@opt ~]]$ chmod 777 /opt/pgpool/failover_stream.sh 3.6 启动集群分别启动postgrsql 1systemctl start postgresql-10.service 分别启动pgpool 1systemctl start pgpool-II-10.service 启动pgpool后，查看集群节点状态: 12345678910111213[postgres@master ~]$ psql -h vip -p 9999psql (9.6.1)#提示输入密码：Type "help" for help.postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role | select_cnt | load_balance_node | replication_delay ---------+----------+------+--------+-----------+---------+------------+-------------------+------------------- 0 | master | 5432 | up | 0.500000 | primary | 0 | false | 0 1 | slave | 5432 | up | 0.500000 | standby | 0 | true | 0(2 rows)#在slave上节点也是psql -h vip -p 9999，双pgpool使用虚拟ip，做到高可用。 发现当前主备节点都是正常的up状态。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis高可用集群配置]]></title>
    <url>%2F2020%2F07%2F01%2Fredis%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[一 主机 角色 IP 服务 master 192.168.0.56 redis slave 192.168.0.58 redis redis-Sentinel 二 安装redis1yum install redis 三 配置主从 master节点 1234567vim redis.conf# 开启所有地址连接（根据实际情况配置）bind 0.0.0.0# redis添加认证密码test123 （可不设置，如果设置各节点密码保持一致）requirepass test123# 连接master节点密码 （如果设置认证密码，配置该项）masterauth test123 slave节点 123456789vim redis.conf# 开启所有地址连接（根据实际情况配置）bind 0.0.0.0# redis添加认证密码test123 （可不设置，如果设置各节点密码保持一致）requirepass test123# 配置为从节点(配置为集群从节点只需这一句)slaveof 192.168.0.56 6379# 连接master节点密码 （如果设置认证密码，配置该项）masterauth test123 检查主从节点 master节点 123456789101112131415master节点redis信息[root@test2 ~]# systemctl start redis[root@test2 ~]# redis-cli127.0.0.1:6379&gt; AUTH test123OK127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0127.0.0.1:6379&gt; slave节点 1234567891011121314151617181920212223slave节点redis信息[root@test3 etc]# systemctl start redis[root@test3 etc]# redis-cli127.0.0.1:6379&gt; AUTH test123OK127.0.0.1:6379&gt; info replication# Replicationrole:slavemaster_host:192.168.0.56master_port:6379master_link_status:upmaster_last_io_seconds_ago:0master_sync_in_progress:0slave_repl_offset:15slave_priority:100slave_read_only:1connected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0127.0.0.1:6379&gt; 添加键值测试同步 四 配置redis哨兵（Sentinel）实现高可用redis-sentinel是redis自带的功能，安装redis服务自带该服务。启动服务方式：systemctl start redis-sentinel.service 123456789vim redis-sentinel.conf# 允许所有地址连接sentinelbind 0.0.0.0# 侦测master节点，1表示当有一个sentinel服务发现master宕机后开始切换master节点sentinel monitor mymaster 192.168.0.56 6379 1# 连接master节点的密钥sentinel auth-pass mymaster test123# 5秒内master没有响应，就认为SDOWN （可不添加）sentinel down-after-milliseconds mymaster 5000 五 redis客户端连接集群redisk客户端连接方式redis客户端连接redis集群，连接的不是redis服务器的地址，而是Sentinel服务的地址和端口,当master节点故障时，Sentinel会自动设置slave为master并连接到master，就实现了redis的高可用]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[greenplum部署-单机版]]></title>
    <url>%2F2020%2F07%2F01%2Fgreenplum%E9%83%A8%E7%BD%B2-%E5%8D%95%E6%9C%BA%E7%89%88%2F</url>
    <content type="text"><![CDATA[1 修改Linux内核参数 123456789101112131415161718# vi /etc/sysctl.conf net.ipv4.ip_forward = 0net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 1 kernel.core_uses_pid = 1net.ipv4.tcp_syncookies = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.sem = 250 64000 100 512kernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_max_syn_backlog = 4096 net.core.netdev_max_backlog = 10000 vm.overcommit_memory = 2net.ipv4.conf.all.arp_filter = 1 2 修改Linux最大限制12345678# vi /etc/security/limits.conf # greenplum configs* soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072 3 关闭selinuxvim /etc/selinux/conf123SELINUX=disabled# setenforce 0 4 greenplum安装4.1 创建数据库用户12345groupadd -g 530 gpadminuseradd -g 530 -u530 -m -d /home/gpadmin -s /bin/bash gpadminpasswd gpadmin # gpadmin用户密码gpadmin 4.2 修改hosts设置集群解析 123# vim /etc/hosts 192.168.0.174 mdw 4.3 修改主机名 mdw1hostnamectl set-hostname mdw 4.4 下载安装包下载合适的版本rpm包和postgis插件 官网 https://network.pivotal.io/products/pivotal-gpdb#/releases/1683 4.5 rpm安装 12345yum install greenplum-db-6.2.1-rhel7-x86_64.rpm# 默认安装到/usr/local，授权给gpadminchown -R gpadmin /usr/local/greenplum*chgrp -R gpadmin /usr/local/greenplum* 4.6 创建instance需要的目录123456789mkdir -p /home/gpdata/mastermkdir -p /home/gpdata/gp1mkdir -p /home/gpdata/gp2mkdir -p /home/gpdata/gp3mkdir -p /home/gpdata/gp4# 修改目录属主 chown -R gpadmin:gpadmin /home/gpdata chown -R gpadmin:gpadmin /home/gpdata/master chown -R gpadmin:gpadmin /home/gpdata/gp* 4.7 切换用户gpadmin1su gpadmin 4.8 修改gpadmin用户环境变量修改/home/gpadmin/.bashrc 和 bash_profile 文件，增加如下内容。 12345source /usr/local/greenplum-db-5.5.0/greenplum_path.sh # greenplum安装目录的变量（自带）export MASTER_DATA_DIRECTORY=/home/gpadmin/master/gpseg-1 # 数据目录export PGPORT=5432 # 数据库端口，可不设置，如果变更端口设置export PGUSER=gpadmin # 数据库管理员用户export PGDATABASE=gpdb # 自动创建的数据库 4.9 设置节点host12345vim /home/gpadmin/all_hosts写入mdw# 添加一行mdw（单机版只有一个host） 4.10 配置用户SSH无密码登录，单机版也要设置1234[root@gpmaster ~]# su - gpadmin[gpadmin@gpmaster .ssh]$ ssh-keygen -t rsa[gpadmin@gpmaster .ssh]$ ssh mdw cat /home/gpadmin/.ssh/id_rsa.pub &gt;&gt; authorized_keys [gpadmin@gpmaster .ssh]$chmod 600 authorized_keys 4.11 权限互通1gpssh-exkeys -f all_hosts 4.12 创建、编辑初始化文件123456789vim initgp_config写入：SEG_PREFIX=gpsegPORT_BASE=33000declare -a DATA_DIRECTORY=(/home/gpadmin/gp1 /home/gpadmin/gp2 /home/gpadmin/gp3 /home/gpadmin/gp4)MASTER_HOSTNAME=mdwMASTER_PORT=5432MASTER_DIRECTORY=/home/gpadmin/master DATABASE_NAME=gpdb 4.13 设置节点服务器1234vim seg_hosts写入：mdw# 本例单机，只有mdw这一台 4.14 初始化GP1gpinitsystem -c initgp_config -h seg_hosts 5 连接GP5.1 psql登录修改密码（gpadmin用户操作）12345# su gpadmin# psql修改数据库密码为zhjx123 alter role gpadmin with password 'zhjx123'; \q 5.2 远程连接配置12345678vim /home/gpdata/master/gpseg-1/postgresql.conf修改：#listen_addresses = '*'，去#注释 vim /home/gpdata/master/gpseg-1/pg_hba.conf添加：host all gpadmin 0.0.0.0/0 md5 5.3 重新加载配置文件1gpstop -u 5.4 其他命令1234gpstart # 正常启动gpstop # 正常关闭gpstop -M fast # 快速关闭gpstop –r # 重启]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenVPN配置参数详解]]></title>
    <url>%2F2020%2F03%2F12%2FOpenVPN%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[OpenVPN 配置参数详解:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141# #号和;号开头的都是注释# 设置监听 IP，默认是监听所有 IP#local 116.6.45.23#Openvpn 服务器监听端口port 2194# 设置用 TCP 还是 UDP 协议？;proto tcpproto udp# 设置创建 tun 的路由 IP 通道，还是创建 tap 的以太网通道# 路由 IP 容易控制，所以推荐使用它；但如果如 IPX 等必须# 使用第二层才能通过的通讯，则可以用 tap 方式，tap 也# 就是以太网桥接dev tun# 配置 VPN 使用的网段，OpenVPN 会自动提供基于该网段的 DHCP# 服务，但不能和任何一方的局域网段重复，保证唯一# server 端 ip 默认会设为.1 的地址。server 10.9.0.0 255.255.255.0# 为客户端创建对应的路由,以另其通达公司网内部服务器# 但记住，公司网内部服务器也需要有可用路由返回到客户端push "route 172.18.2.0 255.255.255.0"# 维持一个客户端和 virtual IP 的对应表，以方便客户端重新# 连接可以获得同样的 IPifconfig-pool-persist /usr/local/etc/ipp.txt# 用 OpenVPN 的 DHCP 功能为客户端提供指定的 DNS、WINS 等push "dhcp-option DNS 172.18.2.23"push "dhcp-option DNS 202.96.128.86"# 这里是重点，必须指定 SSL/TLS root certificate (ca),# certificate(cert), and private key (key)# ca 文件是服务端和客户端都必须使用的，但不需要 ca.key# 服务端和客户端指定各自的.crt 和.key# 请注意路径,可以使用以配置文件开始为根的相对路径,# 也可以使用绝对路径# 请小心存放.key 密钥文件ca /usr/local/etc/keys/ca.crtcert /usr/local/etc/keys/server.crtkey /usr/local/etc/keys/server.key# 指定 Diffie hellman parameters.dh /usr/local/etc/keys/dh1024.pem#用于吊销客户证书crl-verify /usr/local/etc/keys/vpncrl.pem#增强安全性# Generate with:# openvpn --genkey --secret ta.key## The server and each client must have# a copy of this key.# The second parameter should be 0# on the server and 1 on the clients.tls-auth /usr/local/etc/keys/ta.key 0# 设置服务端检测的间隔和超时时间 每 10 秒 ping 一次，如果 120 秒没有回应则认为对方已经 downkeepalive 10 120# 使用 lzo 压缩的通讯,服务端和客户端都必须配置comp-lzo# 输出短日志,每分钟刷新一次,以显示当前的客户端status /var/log/openvpn-status.log#设置日志要记录的级别。#0 只记录错误信息。#4 能记录普通的信息。#5 和 6 在连接出现问题时能帮助调试#9 是极端的，所有信息都会显示，甚至连包头等信息都显示（像 tcpdump）verb 4#相同信息的数量，如果连续出现 20 条相同的信息，将不记录到日志中。mute 20# 让 OpenVPN 以 nobody 用户和组来运行（安全）user nobodygroup nobody# The persist options will try to avoid# accessing certain resources on restart# that may no longer be accessible because# of the privilege downgrade.# 重启时仍保留一些状态persist-keypersist-tun ###################其他参数 ####################################### 为特定的客户端指定 IP 或指定路由,该路由通常是客户端后面的# 内网网段,而不是服务端连接的网段# ccd 是/etc/openvpn 下的目录，其中建有希望限制的客户端 Common# Name 为文件名的文件,并通过下面的命令写入固定 IP 地址# 例如 Common Name 为 client1,则在/etc/openvpn/ccd/client1 写有：# ifconfig-push 10.9.0.1 10.9.0.2client-config-dir /usr/local/etc/ccd# 若客户端希望所有的流量都通过 VPN 传输,则可以使用该语句# 其会自动改变客户端的网关为 VPN 服务器,推荐关闭# 一旦设置，请小心服务端的 DHCP 设置问题;push "redirect-gateway"# 如果您希望有相同 Common Name 的客户端都可以登陆# 也可以注释下面的语句,推荐每个客户端都使用不用的 Common Name# 常用于测试;duplicate-cn# 设置最大用户数#max-clients 3# 打开管理界面,可以定义监控的 IP 和端口management localhost 7505# 缺省日志会记录在系统日志中，但也可以导向到其他地方# 建议调试的使用先不要设置,调试完成后再定义;log /var/log/openvpn/openvpn.log;log-append /var/log/openvpn/openvpn.log# 配置为以太网桥模式,但需要使用系统的桥接功能# 这里不需要使用;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100#记录日志，每次重新启动 openvpn 后删除原有的 log 信息log /var/log/openvpn.log#和 log 一致，每次重新启动 openvpn 后保留原有的 log 信息，新信息追加到文件最后;log-append openvpn.log#定义运行 openvpn 的用户user nobodygroup nobody#Run script or shell command cmd to validate client#virtual addresses or routes. 具体查看 manual;learn-address ./script#其它的一些需要 PUSH 给 Client#用于记录某个 Client 获得的 IP 地址，类似于 dhcpd.lease 文件，#防止 openvpn 重新启动后“忘记”Client 曾经使用过的 IP 地址ifconfig-pool-persist ipp.txt#Bridge 状态下类似 DHCPD 的配置，为客户分配地址，由于这里工作在路由模式，所以不使用;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100# 随机选择一个 Server 连接，否则按照顺序从上到下依次连接;remote-random# 始终重新解析 Server 的 IP 地址（如果 remote 后面跟的是域名） ，# 保证 Server IP 地址是动态的使用 DDNS 动态更新 DNS 后，Client 在自动重新连接时重新解析 Server 的IP 地址# 这样无需人为重新启动，即可重新接入 VPNresolv-retry infinite# 在本机不邦定任何端口监听 incoming 数据，Client 无需此操作，除非一对一的 VPN 有必要nobind# 如果你使用 HTTP 代理连接 VPN Server，把 Proxy 的 IP 地址和端口写到下面# 如果代理需要验证，使用 http-proxy server port [authfile] [auth-method]# 其中 authfile 是一个 2 行的文本文件，用户名和密码各占一行，auth-method 可以省略，详细信息查看 Manual;http-proxy-retry # retry on connection failures;http-proxy [proxy server] [proxy port #]# Server 使用 build-key-server 脚本什成的，在 x509 v3 扩展中加入了 ns-cert-type 选项# 防止 VPN client 使用他们的 keys ＋ DNS hack 欺骗 vpn client 连接他们假冒的 VPN Server# 因为他们的 CA 里没有这个扩展ns-cert-type servera.定义 tun 为使用路由方式的 VPNb.小心处理证书的路径，.key 文件要保存好，特别是 ca.key。（ca.key 不需要在 OpenVPN 中用到，可以另外保存）注意，每个虚拟 tun 网卡都是成对的，只有 inet addr 标识的才是用于 VPN 通讯。并且必须在/30 网段]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>openvpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[history命令优化]]></title>
    <url>%2F2019%2F12%2F27%2Fhistory%E5%91%BD%E4%BB%A4%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[history命令添加 时间/IP/用户 123# history命令添加时间/ip/用户USER_IP=`who -u am i 2&gt;/dev/null| awk '&#123;print $NF&#125;'|sed -e 's/[()]//g'`HISTTIMEFORMAT="[%F %T] [`whoami`] [$&#123;USER_IP&#125;] " 效果：优点只添加时间/ip/用户，可以显示操作过的所有命令，汇总显示。记录文件系统默认路径：用户/.bash_history 。例如：vim /root/.bash_history history命令根据用户/ip/时间生成操作日志文件123456789101112131415161718192021222324# 设置命令显示时间/ip/用户USER_IP=`who -u am i 2&gt;/dev/null| awk '&#123;print $NF&#125;'|sed -e 's/[()]//g'` export HISTTIMEFORMAT="[%F %T][`whoami`][$&#123;USER_IP&#125;] "# history 日志根据用户及登录时间生成操作日志LOGIP=`who -u am i 2&gt;/dev/null| awk '&#123;print $NF&#125;'|sed -e 's/[()]//g'` LOG_DIR=/var/log/history if [ -z $LOGIP ] then LOGIP=`hostname` fi if [ ! -d $LOG_DIR ] then mkdir -p $LOG_DIR chmod 777 $LOG_DIR fiif [ ! -d $LOG_DIR/$&#123;LOGNAME&#125; ] then mkdir -p $LOG_DIR/$&#123;LOGNAME&#125; chmod 777 $LOG_DIR/$&#123;LOGNAME&#125; fiexport HISTSIZE=4096 LOGTM=`date +"%Y%m%d_%H%M%S"` export HISTFILE="$LOG_DIR/$&#123;LOGNAME&#125;/$&#123;LOGIP&#125;-$LOGTM" chmod 777 $LOG_DIR/$&#123;LOGNAME&#125;/*-* 2&gt;/dev/null 效果：优点添加时间/ip/用户，可以显示操作过的命令，每次终端登录退出生成一个操作记录文件，history命令只能显示当前终端登录所作过的操作，查看以往登录的操作记录，需要单独去记录文件种查看。适用与较为严谨的生成环境。记录文件位置：/var/log/history/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>小知识</tag>
        <tag>history</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JMeter之POST上传图片测试]]></title>
    <url>%2F2019%2F10%2F18%2FJMeter%E4%B9%8BPOST%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[1 JMeter配置1.1 创建线程组 1.2 创建HTTP请求 1.3 创建报告 2 配置2.1 配置并发数 2.2 配置HTTP请求配置连接地址、端口号、接口路径，客户端实现方式，选择JAVA方式。参考下图。 参数部分的设置，根据抓包工具得到的参数进行添加 2.2.1 使用Fiddler工具抓包根据Fiddler抓包工具，抓取登录web系统时的/GCPI/login消息内容 3 执行结果 3 参考资料https://blog.csdn.net/ab_2016/article/details/78249686]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>JMeter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议详解]]></title>
    <url>%2F2019%2F08%2F14%2FHTTP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/TankXiao/archive/2012/02/13/2342672.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jmeter测试报错]]></title>
    <url>%2F2019%2F08%2F13%2Fjmeter%E6%B5%8B%E8%AF%95%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[1 java.net.BindException: Address already in use: connect 当线程数持续上升到一个点的时候，运行脚本的时候有很多报错，如图：java.net.BindException: Address already in use: connect通过 netstat -a 命令查看，有一大堆状态为 TIME_WAIT 的占用连接不被释放 开始以为是单机运行脚本运行不过来，所以另加了一台负载机同时运行脚本分布式环境部署参考：https://www.cnblogs.com/whitewasher/p/6946207.html 但是依然还是会报错，后面查阅了相关资料后发现，是因为windows本身提供的端口访问机制的问题。Windows XP提供给 TCP/IP链接的端口为 1024-5000，并且要四分钟来循环回收他们。就导致我们在短时间内跑大量的请求时将端口占满了。 解决方案为： cmd中，用regedit命令打开注册表 在 HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\Tcpip\Parameters下，右击parameters，添加新的DWORD（如果是分布式运行的话，控制机器和负载机器都需要这样操作哦)- 新建`DWORD`值，name：`TcpTimedWaitDe`，value：`30（十进制）` –&gt; 设置为30秒，默认是240秒 - 新建 `DWORD`值，name：`MaxUserPort`，value：`65534（十进制）`–&gt; 设置最大连接数65534 修改配置完毕之后记得重启机器才会生效 参考资料：https://www.cnblogs.com/pgf622/p/9109521.htmlhttps://blog.csdn.net/zoulonglong/article/details/80940411https://blog.csdn.net/lifuxiangcaohui/article/details/40188847 2 java.net.SocketException: Socket closed如题，jmeter报出java.net.SocketException: Socket closed，我查询了下，服务器是正常的，可以返回数据，基本确定问题出在我这边jmeter。查询原因，看到有人说：该异常在客户端和服务器均可能发生。异常的原因是己方主动关闭了连接后（调用了Socket的close方法）再对网络连接进行读写操作。 Socket closed Non HTTP response code: org.apache.http.NoHttpResponseException (the target server failed to respond) 资料查询：https://wiki.apache.org/jmeter/JMeterSocketClosed 问题原因：在JMeter下，发送http 请求时，一般都是默认选择了use keepAlive（这个是什么？看后面资料），这个是连接协议，JMeter坑就在这里，默认勾选了这个（如果不勾选的话，也不会保存），但其配置JMeter.properties中的时间设置默认却是注销的，也是是说，不会等待，一旦连接空闲，则立马断开了，导致我们压测中出现了事务失败的情形。 解决方案：修改httpclient4.idletimeout=&lt;time in ms&gt; 设置成自己觉得合理的时间，一般可设置成10-60s（表示连接空闲10s后才会断开），注意这边单位是ms。修改完成后再次压测，错误不再有了。 3 java.net.SocketException: Connection reset参考资料：https://www.cnblogs.com/shoren/p/httpclient-connectionreset.html]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>jmeter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解tomcat的连接数与线程池]]></title>
    <url>%2F2019%2F08%2F13%2F%E8%AF%A6%E8%A7%A3tomcat%E7%9A%84%E8%BF%9E%E6%8E%A5%E6%95%B0%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/kismetv/p/7806063.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解Tomcat配置文件server.xml]]></title>
    <url>%2F2019%2F08%2F13%2F%E8%AF%A6%E8%A7%A3Tomcat%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6server-xml%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/kismetv/p/7228274.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[svn远程触发jenkins构建]]></title>
    <url>%2F2019%2F08%2F09%2Fsvn%E8%BF%9C%E7%A8%8B%E8%A7%A6%E5%8F%91jenkins%E6%9E%84%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[1 jenkins 配置在jenkins平台上配置job，开启远程触发构建 2 svn配置仓库触发在仓库/hook/目录创建post-commit文件，写入脚本 1234567891011121314151617181920212223242526#!/bin/sh# 设置默认字符集，否则post信息到钉钉时中文乱码export LANG=en_US.UTF-8# svn中变量1为仓库路径，2为提交版本号REPOS="$1"REV="$2"# 下方svnlook命令获取相应的结果#mailer.py commit "$REPOS" "$REV" /path/to/mailer.conftime=$(date +%F/%T)AUTHOR=$(/usr/bin/svnlook author -r $&#123;REV&#125; $&#123;REPOS&#125;)CHANGEDDIRS=$(/usr/bin/svnlook dirs-changed $REPOS)MESSAGE=$(/usr/bin/svnlook log -r $REV $REPOS)# 设置CONTENT的结果CONTENT=提交时间：$&#123;time&#125;\\n提交版本：$&#123;REV&#125;\\n作者：$&#123;AUTHOR&#125;\\n提交备注：$&#123;MESSAGE&#125;\\n修改目录：$&#123;CHANGEDDIRS&#125;# 进入程序目录执行命令，发送提交结果到钉钉群cd /home//usr/bin/java Request 130883d5c8dc420d8deaa5cdabe2c95adf9b780dbxxxxxxxxxxxxxx $CONTENT# 上面都是发送svn更新到钉钉通知的设置，与发送触发到jenkins无关，下方配置jenkins触发# jenkins触发命令curl -u svnjenkins:svnjenkins http://192.168.0.120/jenkins/job/PU2017001_%E6%B4%9E%E8%A7%81_CI.CD_1.222/build?token=dongjian 命令格式svnjenkins:svnjenkins # jenkins用户及密码http://192.168.0.120/jenkins/ # jenkins访问urljob/PU2017001_%E6%B4%9E%E8%A7%81_CI.CD_1.222/build?token= # job地址，看图一绿色框内地址dongjian # token的值，jenkins设置的token名字 3 svn远程触发jenkins，增加筛选12345678910111213141516171819202122#! /bin/bashexport LANG=en_US.UTF-8REPOS="$1"REV="$2"#mailer.py commit "$REPOS" "$REV" /path/to/mailer.conftime=$(date +%F/%T)AUTHOR=$(/usr/bin/svnlook author -r $&#123;REV&#125; $&#123;REPOS&#125;)CHANGEDDIRS=$(/usr/bin/svnlook dirs-changed $REPOS)MESSAGE=$(/usr/bin/svnlook log -r $REV $REPOS)CONTENT=提交时间：$&#123;time&#125;\\n提交版本：$&#123;REV&#125;\\n作者：$&#123;AUTHOR&#125;\\n提交备注：$&#123;MESSAGE&#125;\\n修改目录：$&#123;CHANGEDDIRS&#125;cd /home//usr/bin/java Request 130883d5c8dc420d8deaa5cdabe2c95adf9b780dbd36bxxxxxxxxxxxxxxxxxx $CONTENT# 筛选规则b=$(echo $&#123;MESSAGE&#125; | grep build)if [[ "$b" != "" ]]then curl -u svnjenkins:svnjenkins http://192.168.0.120/jenkins/job/PU2017001_%E6%B4%9E%E8%A7%81_CI.CD_1.222/build?token=dongjian sleep 5 curl -u svnjenkins:svnjenkins http://192.168.0.120/jenkins/view/%E5%AE%A1%E6%9F%A5js/job/PU2017001_dongjian_js/build?token=dongjianfi 脚本说明根据${MESSAGE}的结果，筛选是否包含build字符，如果包含，触发远程构建，如果不包含，不做任何操作。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
        <tag>svn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yaml例子]]></title>
    <url>%2F2019%2F08%2F09%2Fyaml%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[mysql 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: apps/v1beta1kind: Deploymentmetadata: name: mysqlspec: replicas: 1 template: metadata: labels: run: mysql spec: containers: - name: mysql image: mysql:5.6 env: - name: MYSQL_ROOT_PASSWORD value: zhjx@123 ports: - containerPort: 3306 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql - name: mysql-config mountPath: /etc/mysql/conf.d/docker.cnf subPath: docker.cnf volumes: - name: mysql-data hostPath: path: /scsidisk/data/mysql - name: mysql-config configMap: name: mysql-config nodeSelector: node: node1---apiVersion: v1kind: Servicemetadata: name: mysql-svcspec: type: NodePort selector: run: mysql ports: - protocol: TCP nodePort: 30000 port: 3306 targetPort: 3306 iwhereearth123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657apiVersion: apps/v1beta1kind: Deploymentmetadata: name: iwhereearthspec: replicas: 1 template: metadata: labels: run: earth spec: hostAliases: - ip: 192.168.1.221 hostnames: - iwhereearthurl containers: - name: earth image: 192.168.1.118:5000/zhjx/iwhereearth:20180817 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 env: - name: MYSQL_URL value: mysql-svc - name: MYSQL_USER value: root - name: MYSQL_USER_PASSWORD value: zhjx@123# volumeMounts:# - name: earth-data# mountPath: /tomcat/webapps# - name: tomcat-config# mountPath: /usr/local/tomcat/conf/server.xml# subPath: server.xml# volumes:# - name: earth-data# hostPath:# path: /scsidisk/update/iWhereEarth/nfzq_data# - name: tomcat-config# configMap:# name: req-tomcat-config# nodeSelector:# node: node1---apiVersion: v1kind: Servicemetadata: name: iwhereearth-svcspec: type: NodePort selector: run: earth ports: - protocol: TCP nodePort: 30202 port: 8080 targetPort: 8080 iwherelink12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#apiVersion: v1#kind: Namespace#metadata:# name: iwherelink#---apiVersion: apps/v1beta1kind: Deploymentmetadata: name: iwherelink# namespace: iwherelinkspec: replicas: 1 template: metadata: labels: run: link spec: hostAliases: - ip: 192.168.1.9 hostnames: - redis containers: - name: link image: 192.168.1.118:5000/zhjx/iwherelink:201808171448 imagePullPolicy: IfNotPresent ports: - containerPort: 80 env: - name: MYSQL_URL value: mysql-svc - name: MYSQL_USER value: root - name: MYSQL_USER_PASSWORD value: zhjx@123---apiVersion: v1kind: Servicemetadata: name: iwherelink-svc# namespace: iwherelinkspec: type: NodePort selector: run: link ports: - protocol: TCP nodePort: 30102 port: 80 targetPort: 80 iwherevisual123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384apiVersion: v1kind: Namespacemetadata: name: iwherevisual---apiVersion: apps/v1beta1kind: Deploymentmetadata: name: postdb-visual namespace: iwherevisualspec: replicas: 1 template: metadata: labels: run: postdb-visual spec: containers: - name: postdb-visual image: postgres:9.6.5 imagePullPolicy: IfNotPresent env: - name: POSTGIS_PASSWORD value: admin ports: - containerPort: 5432 volumeMounts: - mountPath: /var/lib/postgresql/data name: postdb-visual volumes: - name: postdb-visual hostPath: path: /scsidisk/data/postdb_visual nodeSelector: node: node1---apiVersion: v1kind: Servicemetadata: name: postdb-visual-svc namespace: iwherevisualspec: type: NodePort selector: run: postdb-visual ports: - protocol: TCP nodePort: 30001 port: 5432 targetPort: 5432---apiVersion: apps/v1beta1kind: Deploymentmetadata: name: iwherevisual namespace: iwherevisualspec: replicas: 1 template: metadata: labels: run: visual spec: containers: - name: visual image: 192.168.1.118:5000/zhjx/iwherevisual:201904301601 imagePullPolicy: IfNotPresent ports: - containerPort: 7001---apiVersion: v1kind: Servicemetadata: name: iwherevisual-svc namespace: iwherevisualspec: type: NodePort selector: run: visual ports: - protocol: TCP nodePort: 30101 port: 7001 targetPort: 7001 洞见12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#apiVersion: v1#kind: Namespace#metadata:# name: iwherelink#---apiVersion: apps/v1beta1kind: Deploymentmetadata: name: dongjian namespace: zhjx-basespec: replicas: 1 template: metadata: labels: run: dongjian spec: containers: - name: tomcat image: tomcat:8-jre8 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 volumeMounts: - mountPath: /usr/local/tomcat/webapps name: dongjian-tomcat - mountPath: /home/data/insight/upload name: dongjian-upload - name: httpd image: httpd:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - mountPath: /usr/local/apache2/htdocs/upload name: dongjian-httpd volumes: - name: dongjian-tomcat hostPath: path: /opt/data/PU2017001_洞见_CI.CD - name: dongjian-upload hostPath: path: /opt/data/PU2017001_洞见_CI.CD/upload - name: dongjian-httpd hostPath: path: /opt/data/PU2017001_洞见_CI.CD/upload nodeSelector: node: node2# env:# - name: MYSQL_URL# value: mysql-svc# - name: MYSQL_USER# value: root# - name: MYSQL_USER_PASSWORD# value: zhjx@123---apiVersion: v1kind: Servicemetadata: name: dongjian-tomcat-svc namespace: zhjx-basespec: type: NodePort selector: run: dongjian ports: - protocol: TCP nodePort: 30109 port: 8080 targetPort: 8080---apiVersion: v1kind: Servicemetadata: name: dongjian-httpd-svc namespace: zhjx-basespec: type: NodePort selector: run: dongjian ports: - protocol: TCP nodePort: 30110 port: 80 targetPort: 80]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>yaml</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix钉钉通知_插件版]]></title>
    <url>%2F2019%2F08%2F08%2Fzabbix%E9%92%89%E9%92%89%E9%80%9A%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[1 安装插件 下载插件webhook-zabbix-robot-64到/usr/lib/zabbix/alertscripts/路径 2 创建报警媒介类型脚本名称：插件或脚本的名字脚本参数：-webhook=https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-msg={ALERT.MESSAGE}-url=http://192.168.0.10/zabbix/-log=/tmp/dingding.log 3 配置用户告警3.1 配置告警媒介添加告警媒介，使用户可接收告警信息，及设置接收信息重要性 3.2 配置权限用户只接收有权限的主机告警 4 创建动作 触发条件 操作消息内容： 1234567891011121314151617&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;root&gt; &lt;from&gt;&#123;HOST.NAME&#125;&lt;/from&gt; &lt;time&gt;&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;&lt;/time&gt; &lt;level&gt;&#123;TRIGGER.SEVERITY&#125;&lt;/level&gt; &lt;name&gt;&#123;TRIGGER.NAME&#125;&lt;/name&gt; &lt;key&gt;&#123;TRIGGER.KEY1&#125;&lt;/key&gt; &lt;value&gt;&#123;ITEM.VALUE&#125;&lt;/value&gt; &lt;now&gt;&#123;ITEM.LASTVALUE&#125;&lt;/now&gt; &lt;id&gt;&#123;EVENT.ID&#125;&lt;/id&gt; &lt;ip&gt;&#123;HOST.IP&#125;&lt;/ip&gt; &lt;url&gt;http://192.168.0.10/zabbix&lt;/url&gt; &lt;age&gt;&#123;EVENT.AGE&#125;&lt;/age&gt; &lt;status&gt;&#123;EVENT.STATUS&#125;&lt;/status&gt;&lt;acknowledgement&gt; &#123;EVENT.ACK.STATUS&#125; &lt;/acknowledgement&gt;&lt;acknowledgementhistory&gt; &#123;EVENT.ACK.HISTORY&#125;&lt;/acknowledgementhistory&gt;&lt;/root&gt; 恢复操作消息内容： 1234567891011121314151617&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;root&gt; &lt;from&gt;&#123;HOST.NAME&#125;&lt;/from&gt; &lt;time&gt;&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;&lt;/time&gt; &lt;level&gt;&#123;TRIGGER.SEVERITY&#125;&lt;/level&gt; &lt;name&gt;&#123;TRIGGER.NAME&#125;&lt;/name&gt; &lt;key&gt;&#123;TRIGGER.KEY1&#125;&lt;/key&gt; &lt;value&gt;&#123;ITEM.VALUE&#125;&lt;/value&gt; &lt;now&gt;&#123;ITEM.LASTVALUE&#125;&lt;/now&gt; &lt;id&gt;&#123;EVENT.ID&#125;&lt;/id&gt; &lt;ip&gt;&#123;HOST.IP&#125;&lt;/ip&gt; &lt;color&gt;FF4A954A&lt;/color&gt; &lt;url&gt;http://192.168.0.10/zabbix&lt;/url&gt; &lt;age&gt;&#123;EVENT.AGE&#125;&lt;/age&gt; &lt;recoveryTime&gt;&#123;EVENT.RECOVERY.DATE&#125; &#123;EVENT.RECOVERY.TIME&#125;&lt;/recoveryTime&gt; &lt;status&gt;OK&lt;/status&gt;&lt;/root&gt; 告警确认或更新 消息内容：123456789报警确认：&#123;HOST.NAME&#125;------------确认人：&#123;USER.FULLNAME&#125; 时间：&#123;ACK.DATE&#125; &#123;ACK.TIME&#125; 确认信息如下："&#123;ACK.MESSAGE&#125;"问题服务器IP：&#123;HOSTNAME1&#125;问题ID：&#123;EVENT.ID&#125;当前的问题是： &#123;TRIGGER.NAME&#125; 备注这个告警使用的使python脚本，非通过插件，可观察上图仅送到信息，本文结尾上传插件和脚本，同时添加告警媒介python版脚本参数：{ALERT.MESSAGE}{ALERT.SENDTO}{ALERT.SUBJECT} 5 附件=dingding.py=webhook-zabbix-robot-64]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
        <tag>钉钉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix监控项自定义]]></title>
    <url>%2F2019%2F08%2F08%2Fzabbix%E7%9B%91%E6%8E%A7%E9%A1%B9%E8%87%AA%E5%AE%9A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[1 防火墙状态监控1.1 在被监控主机配置监控脚本 1.1.1 监控脚本在/usr/local/zabbix/scripts/下创建脚本firewalld_status.sh内容如下：123456789#!/bin/bashstat=`systemctl status firewalld|grep Active|awk '&#123;print $3&#125;'|cut -c 2-8`if [ $stat == running ];then echo 1else echo 0fi 脚本含义：打印防火墙状态，截取运行状态字符，如果是running输出1，否则输出0 1.1.2 设置键值在/etc/zabbix/zabbix_agentd.d目录下创建配置文件配置文件后缀必须是.conf #创建配置文件firewalld_status.conf #添加如下内容,设置键值status.firewalld1UserParameter=status.firewalld,/usr/local/zabbix/scripts/firewalld_status.sh 1.2 创建监控项自定义名称，输入上方定义的键值为status.firewalld 1.3 创建触发器检测键值输出，如果输出为0，则触发告警。 2 端口监控2.1 创建监控项net.tcp.listen[port] 检查 TCP 端口 是否处于侦听状态。返回 0 - 未侦听；1 - 正在侦听 2.2 创建触发器检测最后的返回结果，如果是0触发告警 3 进程监控3.1 创建监控项proc.num[&lt;name&gt;,&lt;user&gt;,&lt;state&gt;,&lt;cmdline&gt;] 进程数。返回整数 3.2 创建触发器如果进程数量为0,触发告警]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins安装及优化]]></title>
    <url>%2F2019%2F08%2F07%2Fjenkins%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1. 安装jdk、tomcat安装如下: /usr/local/ 配置环境变量：vim /etc/profile 2. 部署jenkins下载http://mirrors.jenkins.io/war-stable/2.138.4/Jenkins 2.138.4版本，较高的版本汉化不完全，中英文混合 2.1. 将jenkins.war放到/tomcat/webapps目录2.2. 增加跳转页面,实现访问ip地址直接调转到jenkins1vim tomcat/webapps/Root/index.html 1234567&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Language" content="zh-CN"&gt;&lt;meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=gb2312"&gt;&lt;meta http-equiv="refresh" content="0;url=http://192.168.0.120/jenkins/"&gt;&lt;/head&gt;&lt;/html&gt; 2.3. 修改tomcat端口为80修改tomcat配置文件server.xml，修改8080为80启动tomcat，安装jenkins 3. 安装插件除了安装推荐插件外，需要额外安装如下插件Maven Integration plugin //maven构建插件aCoCo plugin //展示界面SonarQube Scanner for Jenkins //sonarqube集成插件SSH Slaves plugin //ssH 传输文件及命令插件Dingding[钉钉] Plugin //钉钉通知NodeJS Plugin //node构建插件Cppcheck Plug-in //收集项目的Cppcheck分析结果Python Plugin //支持python脚本Python Wrapper Plugin 4. 安装mysql解压缩版mysql-5.6.39-linux-glibc2.12-x86_64.tar.gz 123456789101112131415161718shell&gt; groupadd mysqlshell&gt; useradd -r -g mysql -s /bin/false mysqlshell&gt; cd /usr/localshell&gt; tar zxvf /path/to/mysql-VERSION-OS.tar.gzshell&gt; ln -s full-path-to-mysql-VERSION-OS mysqlshell&gt; cd mysqlshell&gt; chown -R mysql .shell&gt; chgrp -R mysql .shell&gt; scripts/mysql_install_db --user=mysqlshell&gt; chown -R root .shell&gt; chown -R mysql data# Next command is optionalshell&gt; cp support-files/my-medium.cnf /etc/my.cnfshell&gt; bin/mysqld_safe --user=mysql &amp;# Next command is optionalshell&gt; cp support-files/mysql.server /etc/init.d/mysql.server 创建sonar数据库，创建本地用户，用户名密码 sonar/sonar，赋予管理权限。 5. 安装sonarqube-6.5 解压sonarqube-6.5.tar.gz到/usr/local/目录 修改配置文件conf/sonar.properties 配置数据库连接信息 conf/wrapper.conf 指定java执行文件的路径，避免开启启动检测不到java环境变量造成启动失败 启动sonar/usr/local/sonarqube-6.5/bin/linux-x86-64/sonar.sh start默认用户名密码：admin/admin 访问端口：9000 5.1. 扩展：sonar使用LDAP认证1vim sonar.properties 123456789101112131415161718#添加如下内容# LDAP configuration# General Configurationsonar.security.realm=LDAPldap.url=ldap://192.168.0.129:389ldap.bindDn=cn=admin,dc=zhjx,dc=comldap.bindPassword=2hjx@123# User Configurationldap.user.baseDn=ou=People,dc=zhjx,dc=comldap.user.request=(&amp;(objectClass=inetOrgPerson)(uid=&#123;login&#125;))ldap.user.realNameAttribute=cnldap.user.emailAttribute=mail# Group Configurationldap.group.baseDn=ou=Group,dc=zhjx,dc=comldap.group.request=(&amp;(objectClass=posixGroup)(memberUid=&#123;uid&#125;))ldap.group.idAttribute=cn 6. 安装cppcheck安装Cppcheck工具下载：https://sourceforge.net/projects/cppcheck/files/cppcheck/ 进入/usr/local，解压：tar –zxvf cppcheck-1.88.tar.gz 进入/usr/local/cppcheck 1.88，编译：make CFGDIR=/usr/local/cppcheck 1.88/cfg/ 安装：make install 配置cppcheck的环境变量，修改/etc/profile如下：新增CPPCHECK_HOME=/usr/local/cppcheck 1.88在PATH后追加:$CPPCHECK_HOME在CLASSPATH后追加:$CPPCHECK_HOME运行source /etc/profile命令，使新的环境变量生效 7. 配置jenkins7.1. 系统管理-系统设置7.1.1. SonarQube servers设置本地sonar 名字，URL地址，连接到sonar的token地址，token在sonar上面创建。登陆sonar，点击登陆账号-我的账号-安全 7.1.2. Jenkins Location设置Jenkins URL 7.1.3. Publish over SSH添加远端服务器，设置连接凭证及地址 7.2. 全局工具配置7.2.1. SonarQube Scanner使用自动安装，安装到jenkins主目录tools下 7.2.2. Maven使用本地安装的版本 7.2.3. NodeJS使用自动安装的版本 7.3. 全局安全配置7.3.1. LDAP认证 7.3.2. 授权策略：结合LDAP实现用户组权限配置 8. 优化8.1. SonarQube scanner扫描结果发送到钉钉 8.1.1. 步骤在jenkins服务器执行如下操作 8.1.1.1. 安装pip12$ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py # 下载安装脚本$ sudo python get-pip.py # 运行安装脚本 8.1.1.2. 安装模块123pip install requestspip install python-jenkinspip install json262 8.1.1.3. 在jenkins构建添加python脚本 脚本内容：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#!/usr/bin/python# -*- coding: UTF-8 -*-import sysreload(sys)sys.setdefaultencoding('utf8')import requests,json,jenkins,os,time# 接受jenkins当前JOB_NAME参数projectName = os.getenv("JOB_NAME")def sendding(Dingtalk_access_token,content,title,messageUrl): url = Dingtalk_access_token pagrem = &#123; "msgtype": "link", "link": &#123; 'title':title, "text": content, 'picUrl': messageUrl, 'messageUrl':'http://192.168.0.243:9000/dashboard?id='+ projectName &#125; &#125; headers = &#123; 'Content-Type': 'application/json', &#125; # 发送消息 requests.post(url, data=json.dumps(pagrem), headers=headers)def notification(): # 钉钉hook地址 Dingtalk_access_token = 'https://oapi.dingtalk.com/robot/send?access_token=10c79fc688ed7679ced8ac600c4357f23c339e03da838c9dea10181b939b5e1c' # sonar API sonar_Url = 'http://192.168.0.243:9000/api/measures/search?projectKeys='+ projectName +'&amp;metricKeys=alert_status%2Cbugs%2Creliability_rating%2Cvulnerabilities%2Csecurity_rating%2Ccode_smells%2Csqale_rating%2Cduplicated_lines_density%2Ccoverage%2Cncloc%2Cncloc_language_distribution' # 获取sonar指定项目结果，请求带认证 resopnse = requests.get(sonar_Url,auth=('root','zhjx_1058')).text # 转换成josn result = json.loads(resopnse) bug = 0 leak = 0 code_smell = 0 coverage = 0 density = 0 status = '' statusStr = '' # 解析sonar json结果 for item in result['measures']: if item['metric']=="bugs": bug = item['value'] elif item['metric']=="vulnerabilities": leak = item['value'] elif item['metric']=='code_smells': code_smell = item['value'] elif item['metric']=='coverage': coverage = item['value'] elif item['metric']=='duplicated_lines_density': density = item['value'] elif item['metric']=='alert_status': status = item['value'] else: pass # 判断新代码质量阀状态 if status == 'ERROR': # 错误图片 messageUrl = 'http://www.iconsdb.com/icons/preview/soylent-red/x-mark-3-xxl.png' statusStr = '失败' elif status == 'OK': statusStr = '成功' # 正确图片 messageUrl = 'http://icons.iconarchive.com/icons/paomedia/small-n-flat/1024/sign-check-icon.png' # 消息内容。如果太长只会部分展示 code_reslut= "Bug数:" + bug + "个，" + \ "漏洞数:" + leak + "个，" + \ "可能存在问题代码："+ code_smell + "行，" + \ "覆盖率:" + coverage + "%，" + \ "重复率:" + density + "%" print("静态代码扫描统计："+"状态:"+ status +","+code_reslut) # 连接jenkins server=jenkins.Jenkins(url="http://192.168.0.243:8080/jenkins/",username='root',password="zhjx_1058") # 获取指定项目最后编译number get_number = server.get_job_info(projectName)['lastBuild']['number'] print("BUILD_NUMBER："+ str(get_number)) sendding(Dingtalk_access_token, content=code_reslut, title=projectName+"#"+str(get_number)+"新代码扫描" + statusStr,messageUrl=messageUrl)if __name__=="__main__": # 等待10秒,确保SonarQube刷新结果 time.sleep(10) notification() 8.2. jmeter测试报告8.2.1. 安装jmeter再jenkins服务器 解压jmeter安装包到/usr/local/jmeter 配置环境变量 1234# jmeterJMETER_HOME=/usr/local/jmeterCLASSPATH=.:$JMETER_HOME/lib/ext/ApacheJMeter_core.jar:$JMETER_HOME/lib/jorphan.jarPATH=$JMETER_HOME/bin:$PATH 测试是否安装成功 8.2.2. Jenkins安装插件HTML Publisher plugin //此插件发布HTML报告Performance Plugin //此插件允许根据从流行的测试工具(Apache JMeter、JUnit、金牛座)读取的结果跟踪性能 KPI 8.2.3. Jenkins流程配置123456# 删除jmeter结果文件rm -rf /home/jmeter.jtl# 删除jemeter 静态页面报告rm -rf /home/jmeter/html/*# 执行测试/usr/local/jmeter/bin/jmeter.sh -n -t /home/Test_Plan.jmx -j /home/jmeter.log -l /home/jmeter.jtl -e -o /home/jmeter/html/ Jmeter 的命令参数如下，参数使用的格式如上： 8.2.4. 解决jenkins下使用HTML Publisher插件后查看html报告显示不正常在jenkins后使用html publisher查看html报告时，发现显示不全，很多东西显示不了。在查看官方文档后，这原来是安全问题所导致的。Jenkins安全默认是将以下功能都关闭了1、javascript2、html上的内置插件3、内置css或从其它站的css4、从其它站的图处5、AJAX我的网页使用的是css，所以显示不全。解决方法如下： 在jenkins系统管理中输入以下脚本运行： System.setProperty(“hudson.model.DirectoryBrowserSupport.CSP”, “”) 如下图： 再次查看，显示正确注意：此方法只适用于 HTML Publisher Plugin to version 1.10以上的版本]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openldap部署]]></title>
    <url>%2F2019%2F08%2F07%2Fopenldap%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1 安装环境OS: Centos 7.5OpenLDAP: 2.4.44Phpldapadmin: 1.2.3主1：192.168.0.130主2：192.168.0.10 2 安装OpenLDAP（主1）2.1 Yum命令安装1yum -y install openldap compat-openldap openldap-clients openldap-servers openldap-devel 2.2 安装包说明 2.3 配置方式说明这里就是重点中的重点了，从openldap2.4.23版本开始，所有配置说句都保存在slapd.d目录下的cn=config文件夹内，不再使用slapd.conf作为配置文件。配置文件的后缀为ldif，且每个配置文件都是通过命令自动生成的，任意打开一个配置文件，在开头都会有一行注释，说明此为自动生成的文件，请勿编辑，使用ldapmodify命令进行修改 # AUTO-GENERATED FILE - DO NOT EDIT!! Use ldapmodify. 有两种配置方式，一种是通过修改旧版配置文件slapd.conf，再用slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d/导入到数据库中，这种方式非常麻烦，不建议使用。一种是通过编辑ldif文件，再用ldapadd命令导入到数据库中，这种方式是动态配置，不需要重新启动服务端slapd进程。 2.4 查看安装版本1slapd -VV 2.5 启动OpenLDAP1234567891011# 复制一个默认配置到指定目录下,并授权，这一步一定要做，然后再启动服务，不然生产密码时会报错cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG# 授权给ldap用户,此用户yum安装时便会自动创建chown -R ldap. /var/lib/ldap/DB_CONFIG # 启动服务，先启动服务，配置后面再进行修改systemctl start slapdsystemctl enable slapd # 查看状态，正常启动则oksystemctl status slapd 2.6 设置管理员密码安装openldap后，会有三个命令用于修改配置文件，分别为ldapadd,ldapmodify, ldapdelete，顾名思义就是添加，修改和删除。而需要修改或增加配置时，则需要先写一个ldif后缀的配置文件，然后通过命令将写的配置更新到slapd.d目录下的配置文件中去，完整的配置过程如下，跟着我做就可以了： 123456789# 生成管理员密码,记录下这个密码，后面需要用到[root@LDAP ldap]# slappasswd New password: 2hjx@123Re-enter new password: 2hjx@123&#123;SSHA&#125;r1uEavrA6S6uINzNXHcPGK+YFLNGB3VV# 新增修改密码文件,ldif为后缀，文件名随意，不要在/etc/openldap/slapd.d/目录下创建类似文件# 生成的文件为需要通过命令去动态修改ldap现有配置，如下，我在家目录下，创建文件cd /etc/openldap/zidingyi.confvim changepwd.ldif 1234567----------------------------------------------------------------------dn: olcDatabase=&#123;0&#125;config,cn=configchangetype: modifyadd: olcRootPWolcRootPW: &#123;SSHA&#125;r1uEavrA6S6uINzNXHcPGK+YFLNGB3VV---------------------------------------------------------------------- 这里解释一下这个文件的内容: 第一行执行配置文件，这里就表示指定为 cn=config/olcDatabase={0}config 文件。你到/etc/openldap/slapd.d/目录下就能找到此文件第二行 changetype 指定类型为修改第三行 add表示添加 olcRootPW配置项第四行指定 olcRootPW配置项的值 #在执行下面的命令前，你可以先查看原本的olcDatabase={0}config文件，里面是没有olcRootPW这个项的，执行命令后，你再看就会新增了olcRootPW项，而且内容是我们文件中指定的值加密后的字符串 #执行命令，修改ldap配置，通过-f执行文件ldapadd -Y EXTERNAL -H ldapi:/// -f changepwd.ldif 执行修改命令后，有如下输出则为正常：查看slapd.d/cn\=config/olcDatabase\=\{0\}config.ldif内容,新增了一个olcRootPW项。上面就是一个完整的修改配置的过程，切记不能直接修改/etc/openldap/slapd.d/目录下的配置。 2.7 导入Schema1234567891011121314# 我们需要向 LDAP 中导入一些基本的 Schema。这些 Schema 文件位于 /etc/openldap/schema/ 目录中，schema控制着条目拥有哪些对象类和属性，可以自行选择需要的进行导入，# 依次执行下面的命令，导入基础的一些配置,我这里将所有的都导入一下，其中core.ldif是默认已经加载了的，不用导入ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/collective.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/corba.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/duaconf.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/dyngroup.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/java.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/misc.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/openldap.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/pmi.ldifldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/ppolicy.ldif 2.8 修改域名#修改域名，新增changedomain.ldif, 这里我自定义的域名为 zhjx.com，管理员用户账号为admin。 #如果要修改，则修改文件中相应的dc=zhjx,dc=com为自己的域名 1vim changedomain.ldif 1234567891011121314151617181920212223242526272829-------------------------------------------------------------------------dn: olcDatabase=&#123;1&#125;monitor,cn=configchangetype: modifyreplace: olcAccessolcAccess: &#123;0&#125;to * by dn.base="gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth" read by dn.base="cn=admin,dc=zhjx,dc=com" read by * none dn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyreplace: olcSuffixolcSuffix: dc=zhjx,dc=com dn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyreplace: olcRootDNolcRootDN: cn=admin,dc=zhjx,dc=com dn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyreplace: olcRootPWolcRootPW: &#123;SSHA&#125;r1uEavrA6S6uINzNXHcPGK+YFLNGB3VV dn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyadd: olcAccessolcAccess: &#123;0&#125;to attrs=userPassword,shadowLastChange by dn="cn=admin,dc=zhjx,dc=com" write by anonymous auth by self write by * noneolcAccess: &#123;1&#125;to dn.base="" by * readolcAccess: &#123;2&#125;to * by dn="cn=admin,dc=zhjx,dc=com" write by * read ------------------------------------------------------------------------- #执行命令，修改配置1ldapmodify -Y EXTERNAL -H ldapi:/// -f changedomain.ldif 空白行检测有没有空格，否则报错2.9 启用memberof功能 #新增add-memberof.ldif, #开启memberof支持并新增用户支持memberof配置vim add-memberof.ldif12345678910111213141516171819202122-------------------------------------------------------------[root@LDAP zidingyi.conf]# vim add-memberof.ldif dn: cn=module&#123;0&#125;,cn=configcn: modulle&#123;0&#125;objectClass: olcModuleListobjectclass: topolcModuleload: memberof.laolcModulePath: /usr/lib64/openldapdn: olcOverlay=&#123;0&#125;memberof,olcDatabase=&#123;2&#125;hdb,cn=configobjectClass: olcConfigobjectClass: olcMemberOfobjectClass: olcOverlayConfigobjectClass: topolcOverlay: memberofolcMemberOfDangling: ignoreolcMemberOfRefInt: TRUEolcMemberOfGroupOC: groupOfUniqueNamesolcMemberOfMemberAD: uniqueMemberolcMemberOfMemberOfAD: memberOf------------------------------------------------------------- 12345678 # 新增refint1.ldif文件vim refint1.ldif-------------------------------------------------------------dn: cn=module&#123;0&#125;,cn=configadd: olcmoduleloadolcmoduleload: refint------------------------------------------------------------- 12345678910# 新增refint2.ldif文件-------------------------------------------------------------dn: olcOverlay=refint,olcDatabase=&#123;2&#125;hdb,cn=configobjectClass: olcConfigobjectClass: olcOverlayConfigobjectClass: olcRefintConfigobjectClass: topolcOverlay: refintolcRefintAttribute: memberof uniqueMember manager owner------------------------------------------------------------- #依次执行下面命令，加载配置，顺序不能错 12345ldapadd -Q -Y EXTERNAL -H ldapi:/// -f add-memberof.ldif ldapmodify -Q -Y EXTERNAL -H ldapi:/// -f refint1.ldif ldapadd -Q -Y EXTERNAL -H ldapi:/// -f refint2.ldif 2.10 创建组织及admin组织角色到此，配置修改完了，在上述基础上，我们来创建一个叫做 zhjx 的组织，并在其下创建一个 admin 的组织角色（该组织角色内的用户具有管理整个 LDAP 的权限）和 People 和 Group 两个组织单元： #新增配置文件 1vim zhjx.ldif 1234567891011121314151617181920----------------------------------------------------------dn: dc=zhjx,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: zhjx Companydc: zhjxdn: cn=admin,dc=zhjx,dc=comobjectClass: organizationalRolecn: admindn: ou=People,dc=zhjx,dc=comobjectClass: organizationalUnitou: Peopledn: ou=Group,dc=zhjx,dc=comobjectClass: organizationalRolecn: Group---------------------------------------------------------- #执行命令，添加配置, 这里要注意修改域名为自己配置的域名，然后需要输入上面我们生成的密码 1ldapadd -x -D cn=admin,dc=yaobili,dc=com -W -f zhjx.ldif 通过以上的所有步骤，我们就设置好了一个 LDAP 目录树：其中基准 dc=zhjx,dc=com 是该树的根节点，其下有一个管理域 cn=admin,dc=zhjx,dc=com和两个组织单元 ou=People,dc=zhjx,dc=com及 ou=Group,dc=zhjx,dc=com。 2.11 关闭匿名访问在以下两个文件添加如下属性1234/etc/openldap/cn=config/cn=config.ldif添加：olcDisallows: bind_anonolcRequires: authc 123/etc/openldap/cn=config/olcDatabase=&#123;-1&#125;frontend.ldif添加：olcRequires: authc 新建disabled_anon.ldif, 1vim disabled_anon.ldif 1234567891011121314151617------------------------------------dn: cn=configchangetype: modifyadd: olcDisallowsolcDisallows: bind_anondn: cn=configchangetype: modifyadd: olcRequiresolcRequires: authcdn: olcDatabase=&#123;-1&#125;frontend,cn=configchangetype: modifyadd: olcRequiresolcRequires: authc----------------------------------- 1ldapadd -Y EXTERNAL -H ldapi:/// -f zidingyi/disabled_anon.ldif 2.12 安装phpldapadmin ldap装好后，下面安装web界面phpldapadmin。 1234# yum安装时，会自动安装apache和php的依赖。# 注意： phpldapadmin很多没更新了，只支持php5，如果你服务器的环境是php7，则会有问题，页面会有各种报错yum -y install epel-releaseyum install -y phpldapadmin 修改apache配置 1234567891011# 修改apache的phpldapadmin配置文件# 修改如下内容，放开外网访问，这里只改了2.4版本的配置，因为centos7 默认安装的apache为2.4版本。所以只需要改2.4版本的配置就可以了# 如果不知道自己apache版本，执行 rpm -qa|grep httpd 查看apache版本 vim /etc/httpd/conf.d/phpldapadmin.conf----------------------------------------------------------------- &lt;IfModule mod_authz_core.c&gt; # Apache 2.4 Require all granted &lt;/IfModule&gt;----------------------------------------------------------------- 修改phpldapadmin配置 12345678910111213141516171819202122# 修改配置用DN登录ldapvim /etc/phpldapadmin/config.php-----------------------------------------------------------------# 398行，默认是使用uid进行登录，我这里改为cn，也就是用户名$servers-&gt;setValue('login','attr','cn'); # 460行，关闭匿名登录，否则任何人都可以直接匿名登录查看所有人的信息$servers-&gt;setValue('login','anon_bind',false); # 519行，设置用户属性的唯一性，这里我将cn,sn加上了，以确保用户名的唯一性$servers-&gt;setValue('unique','attrs',array('mail','uid','uidNumber','cn','sn'));# 538行，设置登陆信息，连接多个服务器可添加多个$servers-&gt;setValue('server','name','众合景轩-LDAP Server');$servers-&gt;setValue('server','host','127.0.0.1');$servers-&gt;setValue('server','port',389);$servers-&gt;setValue('server','base',array('dc=zhjx,dc=com'));$servers-&gt;setValue('login','auth_type','cookie');$servers-&gt;setValue('login','bind_id','cn=admin,dc=zhjx,dc=com');$servers-&gt;setValue('login','bind_pass','2hjx@123');$servers-&gt;setValue('server','tls',false);----------------------------------------------------------------- 启动apache 12systemctl start httpdsystemctl enable httpd 3 主2安装配置主2服务，安装过程参考 2.1——2.8步骤。参考2.12安装phpldapadmin 3.1 管理员密码信息1234[root@localhost ~]# slappasswdNew password: 2hjx@123Re-enter new password: 2hjx@123 &#123;SSHA&#125;AFKgWr+wyQ47fZMdgeUU6b5fcytXgA8W 4 配置双主同步4.1 配置双主复制，在主1和主2上执行下面的步骤4.1.1 添加syncprov模块4.1.1.1 mod_syncprov.ldif123456789101112131415[root@test1] ~/ldif$ vim mod_syncprov.ldif # create newdn: cn=module,cn=configobjectClass: olcModuleListcn: moduleolcModulePath: /usr/lib64/openldapolcModuleLoad: syncprov.la[root@test1 ~]# ldapadd -Y EXTERNAL -H ldapi:/// -f mod_syncprov.ldif SASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "cn=module,cn=config 4.1.1.2 syncprov.ldif12345678910111213[root@test1] ~/ldif$ vim syncprov.ldif # create newdn: olcOverlay=syncprov,olcDatabase=&#123;2&#125;hdb,cn=configobjectClass: olcOverlayConfigobjectClass: olcSyncProvConfigolcOverlay: syncprovolcSpSessionLog: 100[root@test1 ~]# ldapadd -Y EXTERNAL -H ldapi:/// -f syncprov.ldif SASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0adding new entry "olcOverlay=syncprov,olcDatabase=&#123;2&#125;hdb,cn=config" 4.2 在主1和主2上执行下面的步骤，但是注意需要替换olcServerID和provider的值4.2.1 主1 master01.ldif模块配置1234567891011121314151617181920212223242526272829303132333435363738394041[root@test1] ~/ldif$ vim master01.ldif # create newdn: cn=configchangetype: modifyreplace: olcServerID# specify uniq ID number on each serverolcServerID: 0 #主2上替换为1，使用不同IDdn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyadd: olcSyncReplolcSyncRepl: rid=001 provider=ldap://192.168.0.10:389/ #主2 IP，主2的配置上改为主1IP bindmethod=simple binddn="cn=admin,dc=zhjx,dc=com" # 绑定用户名字，这里用管理员，双主密码一样 credentials=2hjx@123 #明文密码 可以选择加密的 searchbase="dc=zhjx,dc=com" #搜索根域，主1和2一样 scope=sub schemachecking=on type=refreshAndPersist retry="30 5 300 3" interval=00:00:05:00-add: olcMirrorModeolcMirrorMode: TRUEdn: olcOverlay=syncprov,olcDatabase=&#123;2&#125;hdb,cn=configchangetype: addobjectClass: olcOverlayConfigobjectClass: olcSyncProvConfigolcOverlay: syncprov####[root@test1 ~]# ldapmodify -Y EXTERNAL -H ldapi:/// -f master01.ldif SASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0modifying entry "cn=config"modifying entry "olcDatabase=&#123;2&#125;hdb,cn=config"adding new entry "olcOverlay=syncprov,olcDatabase=&#123;2&#125;hdb,cn=config" 4.2.2 主2 master02.ldif模块配置参考主1 master01.ldif模块配置,修改相应参数. 4.3 重启服务主1和主2重启openLDAP服务,重启后生效 1systemctl restart slapd 此时，配置成功，在任意节点添加/删除/修改都能实时同步。 4.4 LDAP高可用4.4.1 安装keepalived，分别在主1/主2上安装，配置通过VIP地址实现LDAP高可用1yum install keepalived.x86_64 4.4.2 修改配置文件1vim /etc/keepalived/keepalived.conf 4.4.2.1 主1配置12345678910111213141516171819202122232425262728293031323334353637383940global_defs &#123; router_id LVS_LDAP&#125;vrrp_script CheckK8sMaster &#123; script "curl -k ldap://192.168.0.129:389" # 这里侦测VIP连接地址 interval 3 timeout 9 fall 2 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER #主节点用MASTER,备节点用BACKUP interface ens192 # 网卡名字 virtual_router_id 61 # 主节点权重最高 依次减少 priority 100 # 主节点权重高于备节点 advert_int 1 #修改为本地IP mcast_src_ip 192.168.0.130 # 本机IP nopreempt authentication &#123; auth_type PASS auth_pass sqP05dQgMSlzrxHj &#125; unicast_peer &#123; #注释掉本地IP # 主备节点IP #192.168.1.221 192.168.0.130 192.168.0.10 &#125; virtual_ipaddress &#123; 192.168.0.129/23 #VIP 地址 &#125; track_script &#123; CheckK8sMaster &#125;&#125; 4.4.2.2 主2配置12345678910111213141516171819202122232425262728293031323334353637383940global_defs &#123; router_id LVS_LDAP&#125;vrrp_script CheckK8sMaster &#123; script "curl -k ldap://192.168.0.129:389" interval 3 timeout 9 fall 2 rise 2&#125;vrrp_instance VI_1 &#123; state BACKUP # 备节点用BACKUP interface enp3s0 # 网卡名字 virtual_router_id 61 # 主节点权重最高 依次减少 priority 99 # 备节点权重低于主节点 advert_int 1 #修改为本地IP mcast_src_ip 192.168.0.10 # 本机IP nopreempt authentication &#123; auth_type PASS auth_pass sqP05dQgMSlzrxHj &#125; unicast_peer &#123; #注释掉本地IP # 主备节点IP #192.168.1.221 192.168.0.130 192.168.0.10 &#125; virtual_ipaddress &#123; 192.168.0.129/23 #VIP 地址 &#125; track_script &#123; CheckK8sMaster &#125;&#125; 4.4.3 重启服务1systemctl restart keepalived.service 4.4.3.1 测试VIP1) ip a 命令查看IP 2) 关闭主1 keepalived服务，查看ip，当主1失去连接，vip转移到主2节点 配置成功。 5 pholdapadmin使用5.1 创建组类似部门分组，没有gid. 5.2 创建角色组举例：在jenkins组里面创建jenkins-admin角色组，拥有gid,主要用于与其他平台对接，根据不同gid内的用户，区分权限 建组时没有添加用户，后期添加，需要添加memberUid属性后添加用户 5.3 创建用户用户统一放到ou=People,dc=zhjx,dc=com下]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>openldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器的重启策略及--restart选项详解]]></title>
    <url>%2F2019%2F05%2F10%2FDocker%E5%AE%B9%E5%99%A8%E7%9A%84%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5%E5%8F%8A-restart%E9%80%89%E9%A1%B9%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[创建容器时没有添加参数 –restart=always ，导致的后果是：当 Docker 重启时，容器未能自动启动。 现在要添加该参数怎么办呢，方法有二： 1 重启策略及–restart选项详解1. Docker容器的重启策略Docker容器的重启策略是面向生产环境的一个启动策略，在开发过程中可以忽略该策略。 Docker容器的重启都是由Docker守护进程完成的，因此与守护进程息息相关。 Docker容器的重启策略如下： no，默认策略，在容器退出时不重启容器 on-failure，在容器非正常退出时（退出状态非0），才会重启容器 on-failure:3，在容器非正常退出时重启容器，最多重启3次 always，在容器退出时总是重启容 unless-stopped，在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器 2. Docker容器的退出状态码docker run的退出状态码如下： 0，表示正常退出 非0，表示异常退出（退出状态码采用chroot标准） 125，Docker守护进程本身的错误 126，容器启动后，要执行的默认命令无法调用 127，容器启动后，要执行的默认命令不存在 其他命令状态码，容器启动后正常执行命令，退出命令时该命令的返回状态码作为容器的退出状态码 3. docker run的–restart选项通过--restart选项，可以设置容器的重启策略，以决定在容器退出时Docker守护进程是否重启刚刚退出的容器。--restart选项通常只用于detached模式的容器。 --restart选项不能与--rm选项同时使用。显然，-restart选项适用于detached模式的容器，而--rm选项适用于foreground模式的容器。 在docker ps查看容器时，对于使用了--restart选项的容器，其可能的状态只有Up或Restarting两种状态。 示例： 12docker run -d --restart=always bba-208docker run -d --restart=on-failure:10 bba-208 补充： 查看容器重启次数 1docker inspect -f "&#123;&#123; .RestartCount &#125;&#125;" bba-208 查看容器最后一次的启动时间 1docker inspect -f "&#123;&#123; .State.StartedAt &#125;&#125;" bba-208 2、Docker 命令修改1docker container update --restart=always 容器名字 1234567891011121314操作实例如下：[root@localhost mnt]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES46cdfc60b7a6 nginx "nginx -g 'daemon ..." About a minute ago Up 42 seconds 80/tcp n379d55a734c26 nginx "nginx -g 'daemon ..." About a minute ago Up 42 seconds 80/tcp n2f7b2206c019d nginx "nginx -g 'daemon ..." About a minute ago Up 46 seconds 80/tcp n1[root@localhost mnt]# docker container update --restart=always n1n1[root@localhost mnt]# systemctl restart docker [root@localhost mnt]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES46cdfc60b7a6 nginx "nginx -g 'daemon ..." 2 minutes ago Exited (0) 5 seconds ago n379d55a734c26 nginx "nginx -g 'daemon ..." 2 minutes ago Exited (0) 5 seconds ago n2f7b2206c019d nginx "nginx -g 'daemon ..." 2 minutes ago Up 2 seconds 80/tcp n1 3、直接改配置文件（经测试后无效，修改配置文件后，启动容器后，该参数有自动变成了no，修改不生效） 首先停止容器，不然无法修改配置文件配置文件路径为：/var/lib/docker/containers/容器ID在该目录下找到一个文件 hostconfig.json，找到该文件中关键字 RestartPolicy修改前配置：&quot;RestartPolicy&quot;:{&quot;Name&quot;:&quot;no&quot;,&quot;MaximumRetryCount&quot;:0}修改后配置：&quot;RestartPolicy&quot;:{&quot;Name&quot;:&quot;always&quot;,&quot;MaximumRetryCount&quot;:0}最后启动容器。 4 修改docker容器的挂载路径 停止所有docker容器 1sudo docker stop $(docker ps -a | awk '&#123; print $1&#125;' | tail -n +2) 停止docker服务 1sudo service docker stop 修改mysql路径 12cd ~sudo cp -r mysql/ /home/server/ 备份容器配置文件 123cd /var/lib/docker/containers/de9c6501cdd3cp hostconfig.json hostconfig.json.bakcp config.v2.json config.v2.json.bak 修改hostconfig的冒号前的配置路径 123vi hostconfig.json"Binds": ["/home/server/mysql/conf/my.cnf:/etc/mysql/my.cnf", "/home/server/mysql/logs:/logs", "/home/server/mysql/data:/mysql_data"], 修改config的Source的配置路径 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293vi config.v2.json "MountPoints": &#123; "/etc/mysql/my.cnf": &#123; "Source": "/home/server/mysql/conf/my.cnf", "Destination": "/etc/mysql/my.cnf", "RW": true, "Name": "", "Driver": "", "Relabel": "", "Propagation": "rprivate", "Named": false, "ID": "" &#125;, "/logs": &#123; "Source": "/home/server/mysql/logs", "Destination": "/logs", "RW": true, "Name": "", "Driver": "", "Relabel": "", "Propagation": "rprivate", "Named": false, "ID": "" &#125;, "/mysql_data": &#123; "Source": "/home/server/mysql/data", "Destination": "/mysql_data", "RW": true, "Name": "", "Driver": "", "Relabel": "", "Propagation": "rprivate", "Named": false, "ID": "" &#125;, "/var/lib/mysql": &#123; "Source": "", "Destination": "/var/lib/mysql", "RW": true, "Name": "85d91bff7012b57606af819480ce267449084e81ab386737c80ace9fe75f6621", "Driver": "local", "Relabel": "", "Propagation": "", "Named": false, "ID": "897cd0152dd152166cb2715044ca4a3915a1b66280e0eb096eb74c2d737d7f77" &#125; &#125;, 5 修改docker默认的存储位置docker 的所有images及相关信息存储位置为：/var/lib/docker 查看默认的docker存储路径 123docker info |grep 'Docker Root Dir'WARNING: No swap limit supportDocker Root Dir: /var/lib/docker 停止所有docker容器 1sudo docker stop $(docker ps -a | awk '&#123; print $1&#125;' | tail -n +2) 停止docker服务 12sudo service docker stopcd /var/lib 打包docker目录 123sudo tar -czvf /usr/docker.tar.gz docker/cd /usr/sudo tar -xzvf docker.tar.gz 修改docker默认的存储位置 12345sudo vim /etc/docker/daemon.json&#123; "graph": "/home/server/docker"&#125; 启动docker服务 1sudo service docker start 启动所有docker容器 1sudo docker start $(docker ps -a | awk '&#123; print $1&#125;' | tail -n +2) 查看修改后docker存储路径 123docker info |grep 'Docker Root Dir'WARNING: No swap limit supportDocker Root Dir: /usr/docker 本文转自：https://www.cnblogs.com/zhuochong/p/10070516.html ！！！！！！]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyISAM与InnoDB两者之间区别与选择]]></title>
    <url>%2F2019%2F04%2F28%2FMyISAM%E4%B8%8EInnoDB%E4%B8%A4%E8%80%85%E4%B9%8B%E9%97%B4%E5%8C%BA%E5%88%AB%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[1、MyISAM：默认表类型，它是基于传统的ISAM类型，ISAM是Indexed Sequential Access Method (有索引的顺序访问方法) 的缩写，它是存储记录和文件的标准方法。不是事务安全的，而且不支持外键，如果执行大量的select，insert MyISAM比较适合。 2、InnoDB：支持事务安全的引擎，支持外键、行锁、事务是他的最大特点。如果有大量的update和insert，建议使用InnoDB，特别是针对多个并发和QPS较高的情况。 一、表锁差异MyISAM:myisam只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。也可以通过lock table命令来锁表，这样操作主要是可以模仿事务，但是消耗非常大，一般只在实验演示中使用。 InnoDB ：Innodb支持事务和行级锁，是innodb的最大特色。 事务的ACID属性：atomicity,consistent,isolation,durable。 并发事务带来的几个问题：更新丢失，脏读，不可重复读，幻读。 事务隔离级别：未提交读(Read uncommitted)，已提交读(Read committed)，可重复读(Repeatable read)，可序列化(Serializable) 四种隔离级别的比较查看mysql的默认事务隔离级别“show global variables like ‘tx_isolation’;” Innodb的行锁模式有以下几种：共享锁，排他锁，意向共享锁(表锁)，意向排他锁(表锁)，间隙锁。 注意：当语句没有使用索引，innodb不能确定操作的行，这个时候就使用的意向锁，也就是表锁 关于死锁：什么是死锁？当两个事务都需要获得对方持有的排他锁才能完成事务，这样就导致了循环锁等待，也就是常见的死锁类型。 解决死锁的方法： 1、 数据库参数 2、 应用中尽量约定程序读取表的顺序一样 3、 应用中处理一个表时，尽量对处理的顺序排序 4、 调整事务隔离级别（避免两个事务同时操作一行不存在的数据，容易发生死锁） 二、数据库文件差异MyISAM ：myisam属于堆表 myisam在磁盘存储上有三个文件，每个文件名以表名开头，扩展名指出文件类型。 .frm用于存储表的定义 .MYD用于存放数据 .MYI 用于存放表索引 myisam表还支持三种不同的存储格式： 静态表(默认，但是注意数据末尾不能有空格，会被去掉) 动态表 压缩表 InnoDB ：innodb属于索引组织表 innodb有两种存储方式，共享表空间存储和多表空间存储 两种存储方式的表结构和myisam一样，以表名开头，扩展名是.frm。 如果使用共享表空间，那么所有表的数据文件和索引文件都保存在一个表空间里，一个表空间可以有多个文件，通过innodb_data_file_path和innodb_data_home_dir参数设置共享表空间的位置和名字，一般共享表空间的名字叫ibdata1-n。 如果使用多表空间，那么每个表都有一个表空间文件用于存储每个表的数据和索引，文件名以表名开头，以.ibd为扩展名。 三、索引差异1、关于自动增长myisam引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 innodb引擎的自动增长咧必须是索引，如果是组合索引也必须是组合索引的第一列。 2、关于主键myisam允许没有任何索引和主键的表存在， myisam的索引都是保存行的地址。 innodb引擎如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见) innodb的数据是主索引的一部分，附加索引保存的是主索引的值。 3、关于count()函数myisam保存有表的总行数，如果select count(*) from table;会直接取出出该值 innodb没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了wehre 条件后，myisam和innodb处理的方式都一样。 4、全文索引myisam支持FULLTEXT类型的全文索引 innodb不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。（sphinx 是一个开源软件，提供多种语言的API接口，可以优化mysql的各种查询） 5、delete from table使用这条命令时，innodb不会从新建立表，而是一条一条的删除数据，在innodb上如果要清空保存有大量数据的表，最 好不要使用这个命令。(推荐使用truncate table，不过需要用户有drop此表的权限) 6、索引保存位置myisam的索引以表名+.MYI文件分别保存。 innodb的索引和数据一起保存在表空间里。 四、开发的注意事项 可以用show create table tablename 命令看表的引擎类型。 对不支持事务的表做start/commit操作没有任何效果，在执行commit前已经提交。 可以执行以下命令来切换非事务表到事务（数据不会丢失），innodb表比myisam表更安全：alter table tablename type=innodb;或者使用 alter table tablename engine = innodb; 默认innodb是开启自动提交的，如果你按照myisam的使用方法来编写代码页不会存在错误，只是性能会很低。如何在编写代码时候提高数据库性能呢？ a、尽量将多个语句绑到一个事务中，进行提交，避免多次提交导致的数据库开销。 b、在一个事务获得排他锁或者意向排他锁以后，如果后面还有需要处理的sql语句，在这两条或者多条sql语句之间程序应尽量少的进行逻辑运算和处理，减少锁的时间。 c、尽量避免死锁 d、sql语句如果有where子句一定要使用索引，尽量避免获取意向排他锁。 f、针对我们自己的数据库环境，日志系统是直插入，不修改的，所以我们使用混合引擎方式，ZION_LOG_DB照旧使用myisam存储引擎，只有ZION_GAME_DB，ZION_LOGIN_DB，DAUM_BILLING使用Innodb引擎。 五、究竟该怎么选择下面先让我们回答一些问题： ◆ 你的数据库有外键吗？ ◆ 你需要事务支持吗？ ◆ 你需要全文索引吗？ ◆ 你经常使用什么样的查询模式？ ◆ 你的数据有多大？ myisam只有索引缓存innodb不分索引文件数据文件innodb buffermyisam只能管理索引，在索引数据大于分配的资源时，会由操作系统来cache；数据文件依赖于操作系统的cache。innodb不管是索引还是数据，都是自己来管理 思考上面这些问题可以让你找到合适的方向，但那并不是绝对的。如果你需要事务处理或是外键，那么InnoDB可能是比较好的方式。如果你需要全文索引，那么通常来说 MyISAM是好的选择，因为这是系统内建的，然而，我们其实并不会经常地去测试两百万行记录。所以，就算是慢一点，我们可以通过使用Sphinx从InnoDB中获得全文索引。 数据的大小，是一个影响你选择什么样存储引擎的重要因素，大尺寸的数据集趋向于选择InnoDB方式，因为其支持事务处理和故障恢复。数据库的在小决定了故障恢复的时间长短，InnoDB可以利用事务日志进行数据恢复，这会比较快。而MyISAM可能会需要几个小时甚至几天来干这些事，InnoDB只需要几分钟。 操作数据库表的习惯可能也会是一个对性能影响很大的因素。比如： COUNT() 在 MyISAM表中会非常快，而在InnoDB表下可能会很痛苦。而主键查询则在InnoDB下会相当相当的快，但需要小心的是如果我们的主键太长了也会导致性能问题。大批的inserts语句在 MyISAM下会快一些，但是updates 在InnoDB下会更快一些——尤其在并发量大的时候。 所以，到底你检使用哪一个呢？根据经验来看，如果是一些小型的应用或项目，那么MyISAM 也许会更适合。当然，在大型的环境下使用MyISAM 也会有很大成功的时候，但却不总是这样的。如果你正在计划使用一个超大数据量的项目，而且需要事务处理或外键支持，那么你真的应该直接使用InnoDB方式。但需要记住InnoDB 的表需要更多的内存和存储，转换100GB 的MyISAM表到InnoDB 表可能会让你有非常坏的体验。 对于支持事务的InnoDB类型的表，影响速度的主要原因是AUTOCOMMIT默认设置是打开的，而且程序没有显式调用BEGIN 开始事务，导致每插入一条都自动Commit，严重影响了速度。可以在执行sql前调用begin，多条sql形成一个事务（即使autocommit打开也可以），将大大提高性能。 InnoDBInnoDB 给 MySQL 提供了具有事务(commit)、回滚(rollback)和崩溃修复能力 (crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。InnoDB提供了行锁(locking on row level)，提供与 Oracle 类型一致的不加锁读取(non- locking read in SELECTs)。这些特性均提高了多用户并发操作的性能表现。在InnoDB表中不需要扩大锁定 (lock escalation)，因为InnoDB的列锁定(row level locks)适宜非常小的空间。 InnoDB 是 MySQL 上第一个提供外键约束(FOREIGN KEY constraints)的表引擎。 InnoDB 的设计目标是处理大容量数据库系统，它的 CPU 利用率是其它基于磁盘的关系数据库引擎所不能比的。在技术上，InnoDB 是一套放在 MySQL 后台的完整数据库系统，InnoDB 在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 InnoDB 把数据和索引存放在表空间里，可能包含多个文件，这与其它的不一样，举例来说，在 MyISAM 中，表被存放在单独的文件中。InnoDB 表的大小只受限于操作系统的文件大小，一般为 2 GB。InnoDB所有的表都保存在同一个数据文件ibdata1 中（也可能是多个文件，或者是独立的表空间文件）,相对来说比较不好备份，免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump。 MyISAMMyISAM 是MySQL缺省存贮引擎 .每张MyISAM表被存放在三个文件 。frm 文件存放表格定义。 数据文件是MYD (MYData)。 索引文件是 MYI (MYIndex) 引伸。因为MyISAM相对简单所以在效率上要优于InnoDB..小型应用使用MyISAM是不错的选择.MyISAM表是保存成文件的形式,在跨平台的数据转移中使用MyISAM存储会省去不少的麻烦 以下是一些细节和具体实现的差别： InnoDB不支持FULLTEXT类型的索引。 InnoDB中不保存表的具体行数，也就是说，执行select count() from table时，InnoDB要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。注意的是，当count()语句包含 where条件时，两种表的操作是一样的。 对于AUTO_INCREMENT类型的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中，可以和其他字段一起建立联合索引。 DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除。 LOAD TABLE FROM MASTER操作对InnoDB是不起作用的，解决方法是首先把InnoDB表改成MyISAM表，导入数据后再改成InnoDB表，但是对于使用的额外的InnoDB特性（例如外键）的表不适用。 另外，InnoDB表的行锁也不是绝对的，如果在执行一个SQL语句时MySQL不能确定要扫描的范围，InnoDB表同样会锁全表，例如update table set num=1 where name like &quot;%aaa%&quot;; 任何一种表都不是万能的，只用恰当的针对业务类型来选择合适的表类型，才能最大的发挥MySQL的性能优势。 六、重复地总结一遍 MyISAM不支持事务，InnoDB是事务类型的存储引擎，当我们的表需要用到事务支持的时候，那肯定是不能选择MyISAM了。 MyISAM只支持表级锁，BDB支持页级锁和表级锁默认为页级锁，而InnoDB支持行级锁和表级锁默认为行级锁 表级锁：直接锁定整张表，在锁定期间，其他进程无法对该表进行写操作，如果设置的是写锁，那么其他进程读也不允许 MyISAM是表级锁定的存储引擎，它不会出现死锁问题 对于write，表锁定原理如下： 如果表上没有锁，在其上面放置一个写锁，否则，把锁定请求放在写锁队列中。 对于read，表锁定原理如下 ： 如果表上没有写锁定，那么把一个读锁放在其上面，否则把锁请求放在读锁定队列中 当一个锁定被释放时，表可被写锁定队列中的线程得到，然后才是读锁定队列中的线程。这意味着，如果你在一个表上有许多更新，那么你的SELECT语句将等到所有的写锁定线程执行完。 行级锁：只对指定的行进行锁定，其他进程还是可以对表中的其他行进行操作的。 行级锁是Mysql粒度最小的一种锁，它能大大的减少数据库操作的冲突，但是粒度越小实现成本也越大。 行级锁可能会导致“死锁”，那到底是怎么导致的呢，分析原因：Mysql行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，那么Mysql就会锁定这个主键索引，如果sql语句操作的是非主键索引，那么Mysql会先锁定这个非主键索引，再去锁定主键索引。 在UPDATE 和 DELETE操作时Mysql不仅会锁定所有WHERE 条件扫描过得索引，还会锁定相邻的键值。 “死锁”举例分析： 表Test：（ID,STATE,TIME） 主键索引：ID 非主键索引：STATE 当执行”UPDATE STATE =1011 WHERE STATE=1000“ 语句的时候会锁定STATE索引，由于STATE 是非主键索引,所以Mysql还会去请求锁定ID索引 当另一个SQL语句与语句1几乎同时执行时：“UPDATE STATE=1010 WHERE ID=1” 对于语句2 Mysql会先锁定ID索引，由于语句2操作了STATE字段，所以Mysql还会请求锁定STATE索引。这时。彼此锁定着对方需要的索引，又都在等待对方释放锁定。所以出现了”死锁”的情况。 行级锁的优点： 有许多线程访问不同的行时，只存在少量的冲突。 回滚时只有少量的更改 可以长时间锁定单一的行 行级锁缺点： 相对于页级锁和表级锁来说占用了更多的内存 当表的大部分行在使用时，比页级锁和表级锁慢，因为你必须获得更多的锁 当在大部分数据上经常使用GROUP BY操作，肯定会比表级锁和页级锁慢。 页级锁：表级锁速度快，但是冲突多；行级锁速度慢，但冲突少；页级锁就是他俩折中的，一次锁定相邻的一组记录。 MyISAM引擎不支持外键，InnoDB支持外键 MyISAM引擎的表在大量高并发的读写下会经常出现表损坏的情况 我们以前做的项目就遇到这个问题，表的INSERT 和 UPDATE操作很频繁，原来用的MyISAM引擎，导致表隔三差五就损坏，后来更换成了InnoDB引擎。 其他容易导致表损坏原因： 服务器突然断电导致数据文件损坏，强制关机（mysqld未关闭情况下）导致表损坏 mysqld进程在写入操作的时候被杀掉 磁盘故障 表损坏常见症状： 查询表不能返回数据或返回部分数据 打开表失败： Can’t open file: ‘×××.MYI’ (errno: 145) 。 Error: Table ‘p’ is marked as crashed and should be repaired 。 Incorrect key file for table: ‘…’. Try to repair it Mysql表的恢复： 对于MyISAM表的恢复： 可以使用Mysql自带的myisamchk工具： myisamchk -r tablename 或者 myisamchk -o tablename（比前面的更保险） 对表进行修复 对于count()查询来说MyISAM更有优势 因为MyISAM存储了表中的行数记录，执行SELECT COUNT() 的时候可以直接获取到结果，而InnoDB需要扫描全部数据后得到结果。 但是注意一点：对于带有WHERE 条件的SELECT COUNT()语句两种引擎的表执行过程是一样的，都需要扫描全部数据后得到结果 InnoDB是为处理巨大数据量时的最大性能设计，它的CPU效率可能是任何其它基于磁盘的关系数据库引擎所不能匹敌的。 MyISAM支持全文索引（FULLTEXT），InnoDB不支持 MyISAM引擎的表的查询、更新、插入的效率要比InnoDB高 网上截取了前辈们测试结论： 测试方法：连续提交10个query， 表记录总数：38万 ， 时间单位 s 12345678910111213引擎类型 MyISAM InnoDB 性能相差 count 0.0008357 3.0163 3609 查询主键 0.005708 0.1574 27.57 查询非主键 24.01 80.37 3.348 更新主键 0.008124 0.8183 100.7 更新非主键 0.004141 0.02625 6.338 插入 0.004188 0.3694 88.21 (1) 加了索引以后，对于MyISAM查询可以加快：4 206.09733倍，对InnoDB查询加快510.72921倍，同时对MyISAM更新速度减慢为原来的1/2，InnoDB的更新速度减慢为原来的1/30。要看情况决定是否要加索引，比如不查询的log表，不要做任何的索引。 (2) 如果你的数据量是百万级别的，并且没有任何的事务处理，那么用MyISAM是性能最好的选择。 (3）InnoDB表的大小更加的大，用MyISAM可省很多的硬盘空间。 123456789101112在我们测试的这个38w的表中，表占用空间的情况如下： 引擎类型 MyISAM InnoDB 数据 53,924 KB 58,976 KB 索引 13,640 KB 21,072 KB 占用总空间 67,564 KB 80,048 KB 另外一个176W万记录的表， 表占用空间的情况如下： 引擎类型 MyIsam InnorDB 数据 56,166 KB 90,736 KB 索引 67,103 KB 88,848 KB 占用总空间 123,269 KB 179,584 KB 七、性能对比测试的版本是mysql Ver 14.14 Distrib 5.1.49, for debian-linux-gnu (i686)，使用的是Innodb plugin 1.0.8（官方称比built-in版本性能更好）和默认的MyISAM。 测试机器是笔记本，配置如下：Intel 酷睿2双核 P8600，4G DDR3 1066内存，320G硬盘5400转。 测试一：数据插入性能测试，这里我分别对innodb_flush_log_at_trx_commit参数打开和关闭都测了了一下，每次测试都是运行40s，表中数字都是实际插入条数。 12345678910 MyISAM Innodb (打开) Innodb(关闭)单线程，逐个插入 120000 60000 600004线程，逐个插入 40000*4 20000*4 40000*4单线程，批量100条/次插入 3600*100 800*100 3000*100单线程，批量200条/次插入 1800*200 400*200 1600*200 可以发现批量插入的性能远高于单条插入，但是一次批量的大小对性能影响不大。每条记录是否都刷新日志的参数对innodb性能的影响巨大。总体上来说，MyISAM性能更优一点。这里有一点需要注意，在插入测试过程中，我对系统资源进行了监控，发现MyISAM对系统资源占用很低，但是Innodb对磁盘占用却很高，应该是对事务控制多了很多需要记录的日志。 测试二：数据读取性能测试。每次随机读取1000条记录，反复进行读取。 123456 MyISAM Innodb单线程，200次读取 5.7s 16.7s4线程，200次读取 12s 40.8s 可以看出MyISAM的读取性能非常恐怖，性能差距在3倍的样子。 以上两个测试发现MyISAM在无事务的需求下几乎完胜，但是要知道它是表锁，Innodb是行锁，那么在并发读写同时存在的情况下，那结果会是怎么样呢？！ 测试三：两个线程并发写入，2个线程并发读取。 123456 MyISAM Innodb逐个插入 写入40s：10000*2 读取200次*2：14s 写入40s：60000*2 读取200次*2：50s批量100条/次插入 写入40s：1000*100*2 读取200次*2：10s 写入40s：1500*100*2 读取200次*2：50s 这下立刻显示出Innodb在并发情况下强劲的性能，几乎没有什么性能衰减。而MyISAM单条插入速度变得非常慢，批量插入也下降了40%性能。 总结一下，在写多读少的应用中还是Innodb插入性能更稳定，在并发情况下也能基本，如果是对读取速度要求比较快的应用还是选MyISAM。 转自于：http://blog.csdn.net/wjtlht928/article/details/46641865]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL自带的性能压力测试工具mysqlslap]]></title>
    <url>%2F2019%2F04%2F28%2FMySQL%E8%87%AA%E5%B8%A6%E7%9A%84%E6%80%A7%E8%83%BD%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7mysqlslap%2F</url>
    <content type="text"><![CDATA[1 mysqlslap介绍 mysqlslap是从MySQL5.1.4版就开始官方提供的压力测试工具。 通过模拟多个并发客户端并发访问MySQL来执行压力测试，同时提供了较详细的SQL执行数据性能报告，并且能很好的对比多个存储引擎（MyISAM，InnoDB等）在相同环境下的相同并发压力下的性能差别。 mysqlslap 官方介绍：http://dev.mysql.com/doc/refman/5.6/en/mysqlslap.html 1.1 常用参数 [options] 详解--host=host_name: -h host_name 连接到的MySQL服务器的主机名（或IP地址），默认为本机localhost; --user=user_name: -u user_name 连接MySQL服务时用的用户名; --password[=password]: -p[password] 连接MySQL服务时用的密码; --create-schema: 代表自定义的测试库名称，测试的schema，MySQL中schema也就是database;(没指定使用哪个数据库时，可能会遇到错误mysqlslap: Error when connecting to server: 1049 Unknown database ‘mysqlslap’) `–query=name`: -q 使用自定义脚本执行测试（可以是SQL字符串或脚本），例如可以调用自定义的一个存储过程或者sql语句来执行测试。 --create: 创建表所需的SQL（可以是SQL字符串或脚本） --concurrency=N: -c N 表示并发量，也就是模拟多少个客户端同时执行query。可指定多个值，以逗号或者–delimiter参数指定的值做为分隔符。例如：–concurrency=100,200,500（分别执行100、200、500个并发）。 --iterations=N: -i N测试执行的迭代次数，代表要在不同的并发环境中，各自运行测试多少次；多次运行以便让结果更加准确。 --number-of-queries=N: 总的测试查询次数(并发客户数×每客户查询次数) --engine=engine_name: -e engine_name 代表要测试的引擎，可以有多个，用分隔符隔开。例如：–engines=myisam,innodb,memory。 --auto-generate-sq: -a 自动生成测试表和数据，表示用mysqlslap工具自己生成的SQL脚本来测试并发压力。 --auto-generate-sql-load-type=type: 测试语句的类型。代表要测试的环境是读操作还是写操作还是两者混合的。取值包括：read (scan tables), write (insert into tables), key (read primary keys), update (update primary keys), or mixed (half inserts, half scanning selects). 默认值是：mixed. --auto-generate-sql-add-auto-increment: 代表对生成的表自动添加auto_increment列，从5.1.18版本开始支持。 --number-char-cols=N: -x N 自动生成的测试表中包含多少个字符类型的列，默认1 --number-int-cols=N: -y N自动生成的测试表中包含多少个数字类型的列，默认1 --commint=N: 多少条DML后提交一次。 --compress: -C 如果服务器和客户端支持都压缩，则压缩信息传递。 --only-print: 只打印测试语句而不实际执行。 --detach=N: 执行N条语句后断开重连。 --debug-info: -T 打印内存和CPU的相关信息。 1.2 测试范例：1mysqlslap -uroot -p --socket /tmp/mysql3306.sock --concurrency=1 --iterations=1 --create-schema='test' --query='SELECT id,unionid,current_num,total_num FROM invite_join WHERE unionid="Cmo" AND active_id="3" AND is_deleted =0 ORDER BY id DESC LIMIT 1;' --number-of-queries=1000000 各种测试参数实例（-p后面跟的是mysql的root密码）： 单线程测试。测试做了什么。 1# mysqlslap -a -uroot -p123456 多线程测试。使用–concurrency来模拟并发连接。 1# mysqlslap -a -c 100 -uroot -p123456 迭代测试。用于需要多次执行测试得到平均值。 1# mysqlslap -a -i 10 -uroot -p123456 1234567# mysqlslap ---auto-generate-sql-add-autoincrement -a -uroot -p123456# mysqlslap -a --auto-generate-sql-load-type=read -uroot -p123456# mysqlslap -a --auto-generate-secondary-indexes=3 -uroot -p123456# mysqlslap -a --auto-generate-sql-write-number=1000 -uroot -p123456# mysqlslap --create-schema world -q "select count(*) from City" -uroot -p123456# mysqlslap -a -e innodb -uroot -p123456# mysqlslap -a --number-of-queries=10 -uroot -p123456 测试同时不同的存储引擎的性能进行对比： 1# mysqlslap -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 --engine=myisam,innodb --debug-info -uroot -p123456 执行一次测试，分别50和100个并发，执行1000次总查询： 1# mysqlslap -a --concurrency=50,100 --number-of-queries 1000 --debug-info -uroot -p123456 50和100个并发分别得到一次测试结果(Benchmark)，并发数越多，执行完所有查询的时间越长。为了准确起见，可以多迭代测试几次: 1# mysqlslap -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 --debug-info -uroot -p123456]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql优化]]></title>
    <url>%2F2019%2F04%2F25%2Fmysql%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1 缓存优化之QueryCache 从 MySQL4开始，出现了QueryCache查询缓存，如果使用了QueryCache，当查询接收到一个和之前同样的查询，服务器将会从查询缓存种检索结果，而不是再次分析和执行上次的查询。这样就大大提高了性能，节省时间，非常有用。 打开查询缓存，是通过几个步骤来设置的，例如：虽然你设置Mysql允许查询缓存，但是如果你设置的查询缓存大小为了0，这和没有允许没什么区别。所以必须是几个步骤的设置才能真正打开查询缓存这个功能。 1.1 查询开启状态 一般，我们会把 query_cache_type 设置为 ON,默认情况下应该是ON(其实5.6默认是OFF) 1mysql&gt; select @@query_cache_type; # 查询开启状态 +——————–+| @@query_cache_type |+——————–+| ON |+——————–+这样 当我们执行 select id,name from tableName; 这样就会用到查询缓存。在 query_cache_type 打开的情况下，如果你不想使用缓存，需要指明select sql_no_cache id,name from tableName;当然也可以禁用查询缓存：mysql&gt; set session uery_cache_type=off;这里我们不讨论这个，我们演示常用的设置。 介绍 query_cache_size: 主要用来缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集，所以仅仅只能针对select语句。当我们打开了 Query Cache功能，MySQL在接受到一条select语句的请求后，如果该语句满足Query Cache的要求(未显式说明不允许使用Query Cache，或者已经显式申明需要使用Query Cache)，MySQL会直接根据预先设定好的HASH算法将接受到的select语句以字符串方式进行hash，然后到Query Cache中直接查找是否已经缓存。也就是说，如果已经在缓存中，该select请求就会直接将数据返回，从而省略了后面所有的步骤(如SQL语句的解析，优化器优化以及向存储引擎请求数据等)，极大的提高性能。根据MySQL用户手册，使用查询缓冲最多可以达到238%的效率。 当然，Query Cache也有一个致命的缺陷，那就是当某个表的数据有任何任何变化，都会导致所有引用了该表的select语句在Query Cache中的缓存数据失效。所以，当我们的数据变化非常频繁的情况下，使用Query Cache可能会得不偿失 Query Cache的使用需要多个参数配合，其中最为关键的是query_cache_size和query_cache_type，前者设置用于缓存 ResultSet的内存大小，后者设置在何场景下使用Query Cache。在以往的经验来看，如果不是用来缓存基本不变的数据的MySQL数据库，query_cache_size一般256MB是一个比较合适的大小。当然，这可以通过计算Query Cache的命中率(Qcache_hits/(Qcache_hits+Qcache_inserts)*100))来进行调整. query_cache_type可以设置为0(OFF)，1(ON)或者2(DEMOND)，分别表示完全不使用query cache，除显式要求不使用query cache(使用sql_no_cache)之外的所有的select都使用query cache，只有显示要求才使用query cache(使用sql_cache)。 如果Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲. 如果Qcache_hits的值也非常大，则表明查询缓冲使用非常频繁，此时需要增加缓冲大小； 根据命中率(Qcache_hits/(Qcache_hits+Qcache_inserts)*100))进行调整，一般不建议太大，256MB可能已经差不多了，大型的配置型静态数据可适当调大. 可以通过命令：show status like &#39;Qcache_%&#39;;查看目前系统Query catch使用大小 | Qcache_hits | 1892463 | | Qcache_inserts | 35627 命中率98.17%=1892463/(1892463 +35627 )*100 1.2 系统变量 have_query_cache 设置查询缓存是否可用1mysql&gt; show variables like 'have_query_cache'; +——————+——-+| Variable_name | Value |+——————+——-+| have_query_cache | YES |+——————+——-+上面的显示，表示设置查询缓存是可用的。 1.3 系统变量 query_cache_size表示查询缓存大小，也就是分配内存大小给查询缓存，如果你分配大小为0，那么 第一步 和 第二步 起不到作用，还是没有任何效果。 1mysql&gt; select @@global.query_cache_size; +—————————+| @@global.query_cache_size |+—————————+| 16777216 |+—————————+上面是 mysql6.0设置默认的，之前的版本好像默认是0的，那么就要自己设置下。设置set @@global.query_cache_size=1000000; 这里是设置1M左右，900多K。再次查看下 select @@global.query_cache_size;+—————————+| @@global.query_cache_size |+—————————+| 999424 |+—————————+显示我们设置新的大小，表示设置成功。 1.4 query_cache_limit 控制缓存查询结果的最大值例如： 如果查询结果很大， 也缓存？？？？这个明显是不可能的。MySql 可以设置一个最大的缓存值，当你查询缓存数结果数据超过这个值就不会进行缓存。缺省为1M，也就是超过了1M查询结果就不会缓存。 1mysql&gt; select @@global.query_cache_limit; +—————————-+| @@global.query_cache_limit |+—————————-+| 1048576 |+—————————-+这个是默认的数值，如果需要修改，就像设置缓存大小一样设置，使用set重新指定大小。好了，通过4个步骤就可以 打开了查询缓存，具体值的大小和查询的方式 这个因不同的情况来指定了。 1.5 缓存合理性，优化1.5.1 query_cache_size 优化通过调节以下几个参数可以知道query_cache_size设置得是否合理 Qcache_insertsQcache_hitsQcache_lowmem_prunesQcache_free_blocks 如果Qcache_lowmem_prunes 的值非常大，则表明经常出现缓存不够的情况，如果Qcache_hits的值非常大，则表明查询缓存冲使用非常频繁，如果该值较小反而影响效率，那么可以考虑不用查询缓存； Qcache_free_blocks 值非常大，则表明缓存区中的碎片很多，可能需要需找合适的机会进行整理 Qcache_hits 表示多少次命中，通过这个参数我们可以查看到 Query Cache的基本效果； Qcache_inserts 表示多少次未命中然后插入，通过Qcache_hits 和 Qcache_inserts 两个参数可以算出Query Cache的命中率 Query Cache命中率 = Qcache_hits / (Qcache_hits + Qcache_inserts) Qcache_lowmem_prunes 表示多少条Query 因为内存不足而被清除出Query Cache，通过Qcache_lowmem_prunes和Qcache_free_memory相互结合，能够更清楚地了解到系统中Query Cache的内存大小是否真的足够，是否频繁出现因为内存不足而有Query被换出的情况 1.5.2 query_cache_min_res_unit 优化开启了数据库缓存后用 1show status like 'qcache%'; ## 查看缓存`query_cache_min_res_unit` 默认是4k 发现 Qcache_free_blocks 数目大 说明可能有碎片。Qcache_free_blocks: 表示查询缓存中目前还有多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内存碎片过多了，可能在一定的时间进行整理。 减少碎片： 合适的query_cache_min_res_unit可以减少碎片，这个参数最合适的大小和应用程序查询结果的平均大小直接相关. 可以通过内存实际消耗（ query_cache_size - Qcache_free_memory ）除以 Qcache_queries_in_cache 计算平均缓存大小。 其中 Qcache_free_memory 和 Qcache_queries_in_cache 在上面已将有了 分别是119423544和 13205query_cache_size 是自己设置的可以通过 1SHOW VARIABLES LIKE '%query_cache%'; # 查看 （ query_cache_size - Qcache_free_memory ）除以 Qcache_queries_in_cache就是 （134217728 - 119423544）/13205=1120.34所以在设置的的时候 query_cache_min_res_unit可以设置成2k修改my.cnf,配置如下： query_cache_min_res_unit= 2k 然后在查看他的Qcache_free_blocks运行一段时间有没有减少 2 缓存优化之 Innodb当使用innoDB存储引擎的时候，innodb_buffer_pool_size参数可能是影响性能的最为关键的一个参数了，用来设置用于缓存innoDB索引及数据块的内存区域大小，更像是Oracle数据库的db_cache_size。简单来说，当操作一个InnoDB表的时候，返回的所有数据或者查询过程中用到的任何一个索引块，都会在这个内存区域中区查询一遍。 和key_buffer_size 对于MyISAM引擎一样，innodb_buffer_pool_size设置了InnoDB存储引擎需求最大的一块内存区域的大小，直接关系到InnoDB存储引擎的性能，所有如果有足够的内存，尽可将该参数设置到足够大，将尽可能多的InnoDB的索引及数据都放入到该缓存区域中，直至全部。 可以通过(innodb_buffer_pool_read_requests - innodb_buffer_pool_reads) / innodb_buffer_pool_read_requests * 100%计算缓存命中率，并根据命中率来调整innodb_buffer_pool_size参数大小进行优化。 3 缓存优化之table_cache另外，table_cache是一个非常重要的MySQL性能参数，主要用于设置table高速缓存的**数量**，由于每个用户端链接都会至少访问一个表，因此该参数是与max_connections有关。当某一连接访问一个表时，MySQL会检查当前已缓存表的数量。如果该表已经在缓存中打开，则会直接访问缓存中的表以加快查询速度； 如果该表未被缓存，则会将当前的表添加进缓存并进行查询。在执行缓存操作前，table_cache参数用于限制缓存表的最大数目；如果当前已经缓存的表未达到table_cache数目，则会将新表添加进去，若已经达到此值，MySQL将根据缓存表的最后查询时间，查询率，等规则释放之前的缓存。 1show global status like 'open%_tables'; 来查看这两个参数的值。其中Open_tables是当前正在打开表的数量，Opened_tables是所有已经打开表的数量。 参考地址：https://blog.csdn.net/qq_40460909/article/details/81236624https://blog.csdn.net/z13615480737/article/details/82621116https://www.cnblogs.com/onlysun/p/4513029.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下run包制作]]></title>
    <url>%2F2019%2F03%2F28%2FLinux%E4%B8%8Brun%E5%8C%85%E5%88%B6%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1 Run软件包介绍 run 程序安装包实质上是一个安装脚本加要安装的程序，如下所示： |—————–——|| || 安装脚本 || ||—————–——|| || 程序 || ||—————–——| 这样整个 run 安装包结构就一目了然了，实际上因为实际需要结构多少有点变动但这个无关紧要，只需要明白原理就行了。安装文件的优点： （1）只有一个包文件； （2）可以直接运行在 Linux上，因为它是 sh（它的前半部分是sh）； （3）在 sh 中可以包含需要用户接收的协议信息，而且提示用户接收，如果用户不接收，安装退出。 2 制作run安装包2.1 压缩环境包1tar -zcvf app.tar.gz app/ 2.2 制作安装脚本install.sh 12345678910111213#! /bin/bashlines=13 #这个值是指这个脚本的行数加 1，这个脚本共有 12 行tail -n +$lines $0 &gt; /tmp/app.tar.gz # $0 表示脚本本身，这个命令用来把从 $lines 开始的内容写入一个 /tmp 目录的 scan.tar.gz 文件里。tar zxvf /tmp/app.tar.gzcp -pR app /opt/rm -rf app/echo "********************************************************************************************************************"echo "*****Install Success **********************************************************************************************"echo "*****Installation path: /opt/app **********************************************************************************"echo "*****Explain: Modify the start.sh configuration parameter, and then execute the start.sh script startup program.****"echo "********************************************************************************************************************"exit 0 3 生成run文件1cat install.sh app.tar.gz &gt; app.run 这样就得到了 app.run 文件，它的结构如下：|—————–———| 第1行| || install.sh || | 第12行|—————–———|| app.tar.gz | 第13行| ||—————–———| 结尾 可通过vi/vim app.run 查看脚本内容 在运行 apprun 时，运行到第 12 行的 exit 0 ，脚本就会自动退出了，不会去运行第 13 行以下的二进制数据（即 app.tar.gz 文件），这样 shell 就不会因为识别不了二进制数据而出错了。这里我们巧妙地使用了 tail 命令，把第 12 行以下的数据重新生成了一个app.tar.gz文件，然后再执行安装。运行超级简单，使用 sh app.run 或赋予可执行权限然后直接执行 ./app.run 就可以安装了。 run 安装包制作较小的程序包是很好的选择，但是它也有缺点，做逻辑比较复杂的安装包，写的安装脚本将会很麻烦，因此此时还是用其他的安装包更好。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>run</tag>
        <tag>打包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Postgresql_Postgis解压版安装]]></title>
    <url>%2F2019%2F03%2F28%2FPostgresql-Postgis%E8%A7%A3%E5%8E%8B%E7%89%88%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1．软件下载 postgresql-9.6.1-1-windows-x64-binaries.zip https://www.postgresql.org/download/windows/ postgis-bundle-pg96-2.3.1x64.zip http://download.osgeo.org/postgis/windows/pg96/ 2. 将postgresql.zip解压解压postgresql-9.6.1-1-windows-x64-binaries.zip到你想要的安装目录（D:\GreenSoftware\PostgreSQL961），主要最好不要有中文或者空格， 3. 创建数据存放目录D:\GreenSoftware\PostgreSQL961\data 4. 初始化数据库D:\GreenSoftware\PostgreSQL961\bin\ initdb.exe -D D:\GreenSoftware\PostgreSQL961\data -E UTF8 --locale=Chinese 5. 启动数据库，有两种方式5.1 第一种方式：注册为windows服务方式5.1.1 注册服务D:\GreenSoftware\PostgreSQL961\bin\ pg_ctl.exe register -D D:\GreenSoftware\PostgreSQL961\data -Npgsql备注：-N表示windows服务名称为pgsql； 5.1.2 启动服务1net start pgsql 如果你的安装没有错误，现在就应该可以起来了。 5.1.3 关闭服务1net stop pgsql 5.1.4 卸载服务D:\GreenSoftware\PostgreSQL961\bin\ pg_ctl.exe unregister -D D:\GreenSoftware\PostgreSQL961\data –Npgsql 5.2 第二种方式：直接启动方式5.2.1 启动D:\GreenSoftware\PostgreSQL961\bin\ pg_ctl.exe start -w -D D:\GreenSoftware\PostgreSQL961\data 5.2.2 关闭D:\GreenSoftware\PostgreSQL961\bin\ pg_ctl.exe stop -W -D D:\GreenSoftware\PostgreSQL961\data 6 创建数据库D:\GreenSoftware\PostgreSQL961\bin\ createdb.exe -E UTF8 geodb D:\GreenSoftware\PostgreSQL961\bin\ dropdb.exe geodb 7 创建用户D:\GreenSoftware\PostgreSQL961\bin\ createuser.exe -s -r postgres 会有是否创建superuser的选项，创建一个名为postgres的超级用户； 使用方法: createuser [选项]... [用户名] 选项: -c, –connection-limit=N 角色的连接限制(缺省: 没有限制) -d, –createdb 此角色可以创建新数据库 -D, –no-createdb 此角色不可以创建新数据库 -e, –echo 显示发送到服务端的命令 -E, –encrypted 口令加密存储 -i, –inherit 角色能够继承它所属角色的权限 （这是缺省情况) -I, –no-inherit 角色不继承权限 -l, –login 角色能够登录(这是缺省情况) -L, –no-login 角色不能登录 -N, –unencrypted 口令不加密存储 -P, –pwprompt 给新角色指定口令 -r, –createrole 这个角色可以创建新的角色 -R, –no-createrole 这个角色没有创建其它角色的权限 -s, –superuser 角色将是超级用户 -S, –no-superuser 角色不能是超级用户 –help 显示此帮助信息, 然后退出 –version 输出版本信息, 然后退出 联接选项: -h, –host=HOSTNAM 数据库服务器所在机器的主机名或套接字目录 -p, –port=PORT 数据库服务器端口号 -U, –username=USERNAME 联接用户 (不是要创建的用户名) -w, -no-password 永远不提示输入口令 -W, –password 强制提示输入口令 如果 -d, -D, -r, -R, -s, -S 和 ROLENAME 一个都没有指定,将使用交互式提示 你. 臭虫报告至 &#112;&#103;&#x73;&#x71;&#108;&#45;&#x62;&#x75;&#103;&#115;&#x40;&#112;&#x6f;&#x73;&#116;&#x67;&#114;&#x65;&#115;&#x71;&#x6c;&#46;&#111;&#114;&#x67;. 例子1:&gt;createuser -P -d -U postgres dan 解释:-P(大写)说的是为新用户指定口令;-d说的该角色是否可以创建数据库;-U(大写)当前的操作是哪个用户发出的;最后的dan是新用户的名字。 补充： 查看系统中的所用用户：select * from pg_user; 删除一个用户：drop user dan;其中dan为用户名 D:\GreenSoftware\PostgreSQL961\bin\dropuser.exe postgres 7.1 修改用户密码7.1.1第一种方式：应用psql命令D:\GreenSoftware\PostgreSQL961\bin\ psql.exe postgres 123postgres=# alter user postgres with password 'xxx';postgres-# \q 7.1.2第二种方式：为使用pgAdmin修改用pgAdmin连接到服务器，可以直接修改密码； 8 将postgis-bundle-pg96-2.3.1x64.zip解压解压postgis-bundle-pg96-2.3.1x64.zip到没有中文或者空格的目录。 9 修改makepostgisdb_using_extensions.bat文件 10 将空间数据导入PostGIS中 11 显示PostGIS中空间数据 12处理外网访问1. 修改D:\GreenSoftware\PostgreSQL961\data\pg_hba.conf文件加入如下的文字： host all all 192.168.1.0/24 md5 2.修改D:\GreenSoftware\PostgreSQL961\data\postgresql.conf文件加入如下的文字： 将 #listen_addresses = &#39;127.0.0.1&#39; 改为： listen_addresses = &#39;*&#39;]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgresql部署]]></title>
    <url>%2F2019%2F03%2F01%2Fpostgresql%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1 Postgres部署手册1.1 安装 1.1.1安装软件库12yum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpmyum install epel-release #安装postgis时提示部分依赖无法安装 1.1.2 安装软件1yum install postgresql10 &amp;&amp; yum install postgresql10-server 1.1.3 初始化及开机启动123/usr/pgsql-10/bin/postgresql-10-setup initdbsystemctl enable postgresql-10systemctl start postgresql-10 1.1.4 设置postgres密码12Sudo –u postgres psqlpostgres=# ALTER USER postgres WITH PASSWORD 'zhjx123' 1.1.5 开启远程访问12345678vim /var/lib/pgsql/10/data/postgresql.conf # 修改#listen_addresses = 'localhost' 为 listen_addresses='*' 当然，此处‘*’也可以改为任何你想开放的服务器IPvim /var/lib/pgsql/10/data/pg_hba.conf 修改如下内容，信任指定服务器连接# IPv4 local connections:host all all 192.168.0.0/23（需要连接的服务器IP） md5 1.2 安装postgis插件（补充）1yum install postgis25_10 2 greenplum部署手册2.1修改Linux内核参数123456789101112131415161718# vi /etc/sysctl.confnet.ipv4.ip_forward = 0net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 1kernel.core_uses_pid = 1net.ipv4.tcp_syncookies = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.sem = 250 64000 100 512kernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_max_syn_backlog = 4096net.core.netdev_max_backlog = 10000vm.overcommit_memory = 2net.ipv4.conf.all.arp_filter = 1 2.2 修改Linux最大限制1234567# vi /etc/security/limits.conf#greenplum configs* soft nofile 65536* hard nofile 65536* soft nproc 131072 * hard nproc 131072 2.3 关闭selinux12345# vim /etc/selinux/conf修改如下：SELINUX=disabled# setenforce 0 # 临时关闭，重启恢复。 2.4 greenplum安装2.4.1 创建数据库用户123# groupadd -g 530 gpadmin# useradd -g 530 -u530 -m -d /home/gpadmin -s /bin/bash gpadmin# passwd gpadmin 2.4.2 修改hosts设置集群解析 12# vim /etc/hosts192.168.0.174 mdw sdw 2.4.3 下载安装包官网https://network.pivotal.io/products/pivotal-gpdb#/releases/1683 2.4.3 赋权及安装1234# unzip greenplum-db-5.9.0-rhel7-x86_64.zip# chmod +x greenplum-db-5.9.0-rhel7-x86_64.bin# ./ greenplum-db-5.9.0-rhel7-x86_64.bin一路yes,安装完成 默认目录/usr/loca/greenplum-db 2.4.4 设置gpadmin用户环境12345678910# cd /home/gpadmin# vi .bashrc# vi .bash_profile.bashrc和.bash_profile最后都添加下面两行source /usr/local/greenplum-db/greenplum_path.shexport MASTER_DATA_DIRECTORY=/home/gpadmin/gpdata/gpmaster/gpseg-1export PGPORT=5433 # 应为本机有postsql，端口占用，所以使用5433端口#export PGDATABASE=testDB 2.4.4 准备节点服务器信息文件后面的批量安装会用到这两个文件，如果all_host和all_segment内容一样，可以只创建一个文件 123456[root@mdw ~]# cd /home/gp[root@mdw~ ]# touch all_host[root@mdw ~]# touch all_segmentall_host和all_segment内容：mdwsdw 2.4.5 建立节点服务器间的信任1gpssh-exkeys -f /opt/gpinit/all_host 2.4.6 批量安装1gpseginstall -f /home/gpadmin/all_host -u gpadmin -p gpadmin 2.4.7 检查批量安装情况1gpssh -f /usr/local/greenplum-db/all_host -e ls -l $GPHOME #检查安装情况 2.4.8 创建存储目录123456789101112131415161718192021222324252627282930313233343536373839404142[gpadmin@mdw conf]$ gpssh -f /home/gpadmin/all_host=&gt; mkdir gpdata[sdw3][ mdw][sdw2][sdw1]=&gt; cd gpdata[sdw3][ mdw][sdw2][sdw1]=&gt; mkdir gpmaster gpdatap1 gpdatap2 gpdatam1 gpdatam2[sdw3][ mdw][sdw2][sdw1]=&gt; ll[sdw3] 总用量 20[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam1[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam2[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap1[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap2[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpmaster[ mdw] 总用量 20[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam1[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam2[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap1[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap2[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpmaster[sdw2] 总用量 20[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam1[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam2[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap1[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap2[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpmaster[sdw1] 总用量 20[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam1[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam2[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap1[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap2[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpmaster=&gt; exit 2.4.9 配置.bash_profile环境变量1234567[gpadmin@mdw ~]$ vim /home/gpadmin/.bash_profilesource /opt/greenplum/greenplum-db/greenplum_path.shexport MASTER_DATA_DIRECTORY=/home/gpadmin/gpdata/gpmaster/gpseg-1export PGPORT=5433# export PGDATABASE=testDB[gpadmin@mdw ~]$ source .bash_profile(让环境变量生效) 2.4.10 创建初始化配置文件1234567891011121314[gpadmin@mdw ~]$ vim /home/gpadmin/gpinitsystem_configARRAY_NAME="Greenplum"SEG_PREFIX=gpsegPORT_BASE=33000declare -a DATA_DIRECTORY=(/home/gpadmin/gpdata/gpdatap1 /home/gpadmin/gpdata/gpdatap2)MASTER_HOSTNAME=mdwMASTER_DIRECTORY=/home/gpadmin/gpdata/gpmasterMASTER_PORT=5433TRUSTED_SHELL=/usr/bin/sshMIRROR_PORT_BASE=43000REPLICATION_PORT_BASE=34000MIRROR_REPLICATION_PORT_BASE=44000declare -a MIRROR_DATA_DIRECTORY=(/home/gpadmin/gpdata/gpdatam1 /home/gpadmin/gpdata/gpdatam2)MACHINE_LIST_FILE=/home/gpadmin/seg_hosts 2.4.11 初始化数据库1[gpadmin@mdw ~]$ gpinitsystem -c /home/gpadmin/gpinitsystem_config -s mdw 2.4.12 启动/关闭/状态1Gpadmin用户执行命令gpstart/gpstop/gpstate 3 安装postgis插件1gppkg -i postgis-2.1.5+pivotal.1-gp5-rhel7-x86_64.gppkg]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vsftp示例1]]></title>
    <url>%2F2019%2F01%2F21%2Fvsftp%E7%A4%BA%E4%BE%8B1%2F</url>
    <content type="text"><![CDATA[1 部署要求在阿里云服务器上部署FTP.服务要求：FTP用户A、B可互相访问，A对B有写权限，B对A只有读权限。 2 步骤2.1 安装vsftpd1yum install -y vsftpd 2.2 修改配置文件1234567891011121314vim /etc/vsftpd/vsftpd.conf配置如下：anonymous_enable=NOchroot_local_user=YESchroot_list_enable=NOchroot_list_file=/etc/vsftpd/chroot_listlocal_root=/home/vsftpdallow_writeable_chroot=YESpasv_enable=YESpasv_min_port=8022pasv_max_port=8030pasv_promiscuous=YES 2.3 创建FTP根目录1mkdir -p /home/vsftpd 2.4 创建系统用户12useradd ftpsg -d /home/vsftpd/ftpsguseradd ftpxc -d /home/vsftpd/ftpxc -g ftpsg 警告：1、创建用户必须赋予登陆权限，不可使用useradd vsftpd -d /home/vsftpd -s /bin/false 禁用登陆；2、两个用户在一个用户组里面，后面分配权限更安全。 2.5 修改目录权限12chmod 770 /home/ftpsgchmod 750 /home/ftpxc 信息：通过设置目录权限，达到限制用户权限的目的。ftpxc用户可读写ftpsg目录，ftpsg用户可读不可写ftpxc目录，其他用户无法访问2个目录。备注：同时设置r-x权限，才可以访问目录。 3 FTP客户端访问使用FileZilla客户端访问FTP服务器，出现如下问题 状态: 连接建立，等待欢迎消息…状态: 已登录状态: 读取目录列表…状态: 服务器发回了不可路由的地址。使用服务器地址代替 编辑-设置注释：因阿里云服务器事vps环境，访问FTP服务器，服务器返回的是内网地址，客户端无法连接；使用主动模式，可以直接通过互联网地址访问FTP。 4 参考资料重要!!!讲解一个重要配置组合。vsftp 实现不同用户不同权限配置]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat优化]]></title>
    <url>%2F2018%2F12%2F21%2Ftomcat%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1 常用配置详解 1.1 目录结构 /bin：脚本文件目录。 /common/lib：存放所有web项目都可以访问的公共jar包（使用Common类加载器加载）。 /conf：存放配置文件，最重要的是server.xml。 /logs：存放日志文件。 /server/webapps：来管理Tomcat-web服务用的。仅对TOMCAT可见，对所有的WEB APP都不可见（使用Catalina类加载器加载）。 /shared/lib：仅对所有WEB APP可见，对TOMCAT不可见（使用Shared类加载器加载）。 /temp：Tomcat运行时候存放临时文件用的。 /webapps：web应用发布目录。 /work：Tomcat把各种由jsp生成的servlet文件放在这个目录下。删除后，启动时会自动创建。 1.2 配置文件 server.xml：主要的配置文件。 web.xml：缺省的web app配置，WEB-INF/web.xml会覆盖该配置。 context.xml：不清楚跟server.xml里面的context是否有关系。 server.xml配置 ==server标签== port：指定一个端口，这个端口负责监听关闭tomcat的请求。 shutdown：指定向端口发送的命令字符串。 ==service标签== name：指定service的名字。 ==Connector(表示客户端和service之间的连接)标签== port：指定服务器端要创建的端口号，并在这个端口监听来自客户端的请求。 minProcessors：服务器启动时创建的处理请求的线程数。 maxProcessors：最大可以创建的处理请求的线程数。 enableLookups：如果为true，则可以通过调用request.getRemoteHost()进行DNS查询来得到远程客户端的实际主机名，若为false则不进行DNS查询，而是返回其ip地址。 redirectPort：指定服务器正在处理http请求时收到了一个SSL传输请求后重定向的端口号。 acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。 connectionTimeout：指定超时的时间数(以毫秒为单位)。 ==Engine(表示指定service中的请求处理机，接收和处理来自Connector的请求)标签== defaultHost：指定缺省的处理请求的主机名，它至少与其中的一个host元素的name属性值是一样的。 ==Context(表示一个web应用程序，通常为WAR文件，关于WAR的具体信息见servlet规范)标签== docBase：该web应用的文档基准目录（Document Base，也称为Context Root），或者是WAR文件的路径。可以使用绝对路径，也可以使用相对于context所属的Host的appBase路径。 path：表示此web应用程序的url的前缀，这样请求的url为http://localhost:8080/path/****。 reloadable：这个属性非常重要，如果为true，则tomcat会自动检测应用程序的/WEB-INF/lib和/WEB-INF/classes目录的变化，自动装载新的应用程序，我们可以在不重起tomcat的情况下改变应用程序。 useNaming：如果希望Catalina为该web应用使能一个JNDI InitialContext对象，设为true。该InitialialContext符合J2EE平台的约定，缺省值为true。 workDir：Context提供的临时目录的路径，用于servlet的临时读/写。利用javax.servlet.context.tempdir属性，servlet可以访问该目录。如果没有指定，使用$CATALINA_HOME/work下一个合适的目录。 swallowOutput：如果该值为true，System.out和System.err的输出被重定向到web应用的logger。如果没有指定，缺省值为false debug：与这个Engine关联的Logger记录的调试信息的详细程度。数字越大，输出越详细。如果没有指定，缺省为0。 ==host(表示一个虚拟主机)标签== name：指定主机名。 appBase：应用程序基本目录，即存放应用程序的目录。 unpackWARs：如果为true，则tomcat会自动将WAR文件解压，否则不解压，直接从WAR文件中运行应用程序。 ==Logger(表示日志，调试和错误信息)标签== className：指定logger使用的类名，此类必须实现org.apache.catalina.Logger接口。 prefix：指定log文件的前缀。 suffix：指定log文件的后缀。 timestamp：如果为true，则log文件名中要加入时间，如下例:localhost_log.2001-10-04.txt。 ==Realm(表示存放用户名，密码及role的数据库)标签== className：指定Realm使用的类名，此类必须实现org.apache.catalina.Realm接口。 ==Valve(功能与Logger差不多，其prefix和suffix属性解释和Logger 中的一样)标签== className：指定Valve使用的类名，如用org.apache.catalina.valves.AccessLogValve类可以记录应用程序的访问信息。directory：指定log文件存放的位置。 pattern：有两个值，common方式记录远程主机名或ip地址，用户名，日期，第一行请求的字符串，HTTP响应代码，发送的字节数。combined方式比common方式记录的值更多。 1.3 配置虚拟目录 直接部署到webapps目录下面访问。 修改conf/server.xml文件。在&lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; xmlValidation=&quot;false&quot; xmlNamespaceAware=&quot;false&quot;&gt;&lt;/host&gt;中加入&lt;Context path=&quot;/test&quot; docBase=&quot;webdemo&quot; debug=&quot;0&quot; reloadable=&quot;true&quot; /&gt;。docBase目录默认使用appBase=”webapps”这个目录。也可以是绝对路径。配置主目录，可以将path=””。 当项目没有放在webapps目录下时，可以在conf/Catalina/localhost新建一个XXX.XML文件。里面加入&lt;Context docBase=&quot;E:webdemo&quot; debug=&quot;0&quot; reloadable=&quot;true&quot; /&gt;。 注意： 这里的path属性不需要设置，设置了也不会起作用的。 也可以使用该方法建立主目录指向另一个目录，例如：&lt;Context docBase=&quot;E:webdemo&quot; debug=&quot;0&quot; reloadable=&quot;true&quot; /&gt;命名为ROOT.xml，这样默认访问的主目录就被修改过了。 配置连接数maxThreads：Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。minSpareThreads：Tomcat初始化时创建的线程数。maxSpareThreads：一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket线程。enableLookups：是否反查域名，取值为：true或false。为了提高处理能力，应设置为falseconnectionTimeout：网络连接超时，单位：毫秒。设置为0表示永不超时，这样设置有隐患的。默认可设置为20000毫秒。 web server允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。 配置内存大小修改bin/catalina.bat中的set CATALINA_OPTS=-Xms64m -Xmx128m。Xms指最小内存，Xmx指最大内存。 安全配置1）将&lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt;SHUTDOWN修改为其他一些字符串。否则就容易被人给停止掉了。 2）对应tomcat3.1中，屏蔽目录文件自动列出修改conf/web.xml中的 12345678910111213&lt;servlet&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.catalina.servlets.DefaultServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;debug&lt;/param-name&gt; &lt;param-value&gt;0&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;listings&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt;&lt;!-- 改成false --&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; 3）访问日志设置 在server.xml中加入 123&lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log." suffix=".txt" pattern="common" resolveHosts="false"/&gt; 这样访问日志会记录到Logs中。 4）修改用户名、密码 conf/tomcat-users.xml 5）屏蔽后台管理入口 方法一：从控制用户和权限着手。废掉要管理权限的用户就可以了。 方法二：将conf/Catalina/localhost/manager.xml改名。 6）配置403,404,500错误页面 默认情况下，报出HTTP错误的时候会暴露tomcat版本号。如果不想暴露的话，就需要重新定义错误跳转页面。 123456789101112&lt;error-page&gt; &lt;error-code&gt;401&lt;/error-code&gt; &lt;location&gt;/401.jsp&lt;/location&gt;&lt;/error-page&gt;&lt;error-page&gt; &lt;error-code&gt;404&lt;/error-code&gt; &lt;location&gt;/404.jsp&lt;/location&gt;&lt;/error-page&gt;&lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/500.jsp&lt;/location&gt;&lt;/error-page&gt; 注意： 在测试的时候碰到一个奇怪的现象，平时项目里面的时候测试正常的。可是今天在tomcat目录里面新建一个测试目录测试并不能跳转到指定错误页面。暂时不知道为什么。 配置Log4j日志记录项目中抛出的异常，抛到tomcat中的异常会被tomcat记录下来，存放至logs/localhost.yyyy-MM-dd.log文件中。平时我们在项目中使用的log4j记录日志跟tomcat是没有任何关系的，是独立的一个程序，记录的文件是自定义的。我们可以在tomcat中定义一个log4j的公共日志处理方式，这样在项目中就不需要在定义log4j的配置了。1）将log4j-1.2.15.jar加入到commonlib目录。2）将log4j.properties加入到commonclasses目录。内容例如： Output pattern : date [thread] priority category - messagelog4j.rootLogger=DEUBG, stdout, logfilelog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d [%t] %-5p [%c] -%m%nlog4j.appender.logfile=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.logfile.File=${catalina.home}/logs/tomcat_app.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d [%t] %-5p [%c] -%m%n #3rd party library levellog4j.logger.org.hibernate.cache=ERROR 注意： 我们项目中使用e.printStackTrace();输出的异常会在控制台输出来，但是，不会记录到tomcat日志中。 而且，也不会记录到log4j的日志中。要想记录到log4j日志中，必须使用log4j输出来。 所以，实际上web项目中进行异常处理应该将e.printStackTrace();写写法多改成log4j的形式才对！ 但是，实际项目中很多项目多偷懒使用了e.printStackTrace();方式输出异常。当出现异常的时候在控制台上查看一下就可以了，也不考虑实际运行时候的维护。假如有人不小心关了控制台，那么，你不就看不到异常了吗？ 个人介意使用log4j的形式记入web异常！ Tomcat5乱码问题Tomcat5跟Tomcat4对参数处理是不一样的，在Tomcat4中get与post的编码是一样的，所以只要在过滤器中通过request.setCharacterEncoding()设定一次就可以解决get与set的问题。然而，在Tomcat5中，get与post的处理是分开的，对get请求使用URIEncoding进行处理，对post使用request.setCharacterEncoding()处理。Tomcat5中，在server.xml的Connector元素增加了以下配置参数：URIEncoding：用来设定通过URI传递的 2 tomcat优化配置参数2.1 内存优化优化内存，主要是在bin/catalina.bat/sh 配置文件中进行。linux上，在catalina.sh中添加： 1JAVA_OPTS="-server -Xms1G -Xmx2G -Xss256K -Djava.awt.headless=true -Dfile.encoding=utf-8 -XX:MaxPermSize=256m -XX:PermSize=128M -XX:MaxPermSize=256M" 其中： • -serve：启用jdk的server版本。• -Xms：虚拟机初始化时的最小堆内存。• -Xmx：虚拟机可使用的最大堆内存。 #-Xms与-Xmx设成一样的值，避免JVM因为频繁的GC导致性能大起大落• -XX:PermSize：设置非堆内存初始值,默认是物理内存的1/64。• -XX:MaxNewSize：新生代占整个堆内存的最大值。• -XX:MaxPermSize：Perm（俗称方法区）占整个堆内存的最大值，也称内存最大永久保留区域。 2.1.1 错误提示：java.lang.OutOfMemoryError:Java heap spaceTomcat默认可以使用的内存为128MB，在较大型的应用项目中，这点内存是不够的，有可能导致系统无法运行。常见的问题是报Tomcat内存溢出错误，Outof Memory(系统内存不足)的异常，从而导致客户端显示500错误，一般调整Tomcat的-Xms和-Xmx即可解决问题，通常将-Xms和-Xmx设置成一样，堆的最大值设置为物理可用内存的最大值的80%。 1set JAVA_OPTS=-Xms512m-Xmx512m 2.1.2 错误提示：java.lang.OutOfMemoryError: PermGenspacePermGenspace的全称是Permanent Generationspace,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGenspace中，它和存放类实例(Instance)的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGenspace进行清理，所以如果你的应用中有很CLASS的话,就很可能出现PermGen space错误，这种错误常见在web服务器对JSP进行precompile的时候。如果你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。解决方法： 1setJAVA_OPTS=-XX:PermSize=128M 2.1.3 垃圾回收机制在使用-Xms和-Xmx调整tomcat的堆大小时，还需要考虑垃圾回收机制。如果系统花费很多的时间收集垃圾，请减小堆大小。一次完全的垃圾收集应该不超过3-5 秒。如果垃圾收集成为瓶颈，那么需要指定代的大小，检查垃圾收集的详细输出，研究垃圾收集参数对性能的影响。一般说来，你应该使用物理内存的 80% 作为堆大小。当增加处理器时，记得增加内存，因为分配可以并行进行，而垃圾收集不是并行的。 2.2 连接数优化：#优化连接数，主要是在conf/server.xml配置文件中进行修改。 2.2.1 优化线程数找到Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;，增加maxThreads和acceptCount属性（使acceptCount大于等于maxThreads），如下： 1&lt;Connector port="8080" protocol="HTTP/1.1"connectionTimeout="20000" redirectPort="8443"acceptCount="500" maxThreads="400" /&gt; 其中： • maxThreads：tomcat可用于请求处理的最大线程数，默认是200• minSpareThreads：tomcat初始线程数，即最小空闲线程数• maxSpareThreads：tomcat最大空闲线程数，超过的会被关闭• acceptCount：当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理.默认100 2.2.2 使用线程池在server.xml中增加executor节点，然后配置connector的executor属性，如下： 12&lt;Executor name="tomcatThreadPool" namePrefix="req-exec-"maxThreads="1000" minSpareThreads="50"maxIdleTime="60000"/&gt;&lt;Connector port="8080" protocol="HTTP/1.1"executor="tomcatThreadPool"/&gt; 其中： •namePrefix：线程池中线程的命名前缀• maxThreads：线程池的最大线程数• minSpareThreads：线程池的最小空闲线程数• maxIdleTime：超过最小空闲线程数时，多的线程会等待这个时间长度，然后关闭• threadPriority：线程优先级注：当tomcat并发用户量大的时候，单个jvm进程确实可能打开过多的文件句柄，这时会报java.net.SocketException:Too many open files错误。可使用下面步骤检查：•ps -ef |grep tomcat查看tomcat的进程ID，记录ID号，假设进程ID为10001• lsof -p 10001|wc -l 查看当前进程id为10001的 文件操作数• 使用命令：ulimit -a 查看每个用户允许打开的最大文件数 2.3 Tomcat Connector三种运行模式（BIO, NIO, APR）2.3.1 三种模式比较：1）BIO：一个线程处理一个请求。缺点：并发量高时，线程数较多，浪费资源。Tomcat7或以下在Linux系统中默认使用这种方式。 2）NIO：利用Java的异步IO处理，可以通过少量的线程处理大量的请求。Tomcat8在Linux系统中默认使用这种方式。Tomcat7必须修改Connector配置来启动（conf/server.xml配置文件）： 1&lt;Connector port="8080"protocol="org.apache.coyote.http11.Http11NioProtocol" connectionTimeout="20000"redirectPort="8443"/&gt; 3）APR(Apache Portable Runtime)：从操作系统层面解决io阻塞问题。Linux如果安装了apr和native，Tomcat直接启动就支持apr。 2.3.2 apr模式安装apr以及tomcat-native 1yum -y install apr apr-devel 进入tomcat/bin目录，比如： 12345cd /opt/local/tomcat/bin/tar xzfv tomcat-native.tar.gzcd tomcat-native-1.1.32-src/jni/native./configure --with-apr=/usr/bin/apr-1-configmake &amp;&amp; make install 注意： #最新版本的tomcat自带tomcat-native.war.gz，不过其版本相对于yum安装的apr过高，configure的时候会报错。 解决：yum remove apr apr-devel –y,卸载yum安装的apr和apr-devel,下载最新版本的apr源码包，编译安装;或者下载低版本的tomcat-native编译安装 安装成功后还需要对tomcat设置环境变量，方法是在catalina.sh文件中增加1行： 1CATALINA_OPTS="-Djava.library.path=/usr/local/apr/lib" #apr下载地址：http://apr.apache.org/download.cgi #tomcat-native下载地址：http://tomcat.apache.org/download-native.cgi 修改8080端对应的conf/server.xml 123456789protocol="org.apache.coyote.http11.Http11AprProtocol"&lt;Connector executor="tomcatThreadPool"port="8080"protocol="org.apache.coyote.http11.Http11AprProtocol"connectionTimeout="20000"enableLookups="false"redirectPort="8443"URIEncoding="UTF-8" /&gt; PS:启动以后查看日志 显示如下表示开启 apr 模式 12Sep 19, 2016 3:46:21 PM org.apache.coyote.AbstractProtocol startINFO: Starting ProtocolHandler ["http-apr-8081"] 3 tomcat常见面试题3.1 Tomcat的缺省是多少，怎么修改Tomcat的缺省端口号是8080.修改Tomcat端口号： 找到Tomcat目录下的conf文件夹 进入conf文件夹里面找到server.xml文件 打开server.xml文件 在server.xml文件里面找到下列信息 1maxThreads=”150″ minSpareThreads=”25″ maxSpareThreads=”75″ enableLookups=”false” redirectPort=”8443″ acceptCount=”100″ connectionTimeout=”20000″ disableUploadTimeout=”true” /&gt; 5.把port=”8080″改成port=”8888″，并且保存 6.启动Tomcat，并且在IE浏览器里面的地址栏输入http://127.0.0.1:8888/ 7、tomcat默认采用的BIO模型，在几百并发下性能会有很严重的下降。tomcat自带还有NIO的模型，另外也可以调用APR的库来实现操作系统级别控制。NIO模型是内置的，调用很方便，只需要将上面配置文件中protocol修改成 org.apache.coyote.http11.Http11NioProtocol，重启即可生效。如下面的参数配置，默认的是HTTP/1.1。 123&lt;Connector port=”8080″ protocol=”org.apache.coyote.http11.Http11NioProtocol” connectionTimeout=”20000″ redirectPort=”8443″ maxThreads=”500″ minSpareThreads=”20″ acceptCount=”100″ disableUploadTimeout=”true”enableLookups=”false” URIEncoding=”UTF-8″ /&gt; 3.2 tomcat 如何优化？3.2.1优化连接配置这里以tomcat7的参数配置为例，需要修改conf/server.xml文件，修改连接数，关闭客户端dns查询。 参数解释： URIEncoding=”UTF-8″ :使得tomcat可以解析含有中文名的文件的url，真方便，不像apache里还有搞个mod_encoding，还要手工编译 maxSpareThreads : 如果空闲状态的线程数多于设置的数目，则将这些线程中止，减少这个池中的线程总数。 minSpareThreads : 最小备用线程数，tomcat启动时的初始化的线程数。 enableLookups : 这个功效和Apache中的HostnameLookups一样，设为关闭。 connectionTimeout : connectionTimeout为网络连接超时时间毫秒数。 maxThreads: maxThreads Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数，即最大并发数。 acceptCount: acceptCount是当线程数达到maxThreads后，后续请求会被放入一个等待队列，这个acceptCount是这个队列的大小，如果这个队列也满了，就直接refuse connection maxProcessors与minProcessors : 在 Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。多线程帮助程序员写出CPU最 大利用率的高效程序，使空闲时间保持最低，从而接受更多的请求。 通常Windows是1000个左右，Linux是2000个左右。 useURIValidationHack: 我们来看一下tomcat中的一段源码： 1234567891011121314151617181920212223【security】if (connector.getUseURIValidationHack()) &#123;String uri = validate(request.getRequestURI());if (uri == null) &#123;res.setStatus(400);res.setMessage(“Invalid URI”);throw new IOException(“Invalid URI”);&#125; else &#123;req.requestURI().setString(uri);// Redoing the URI decodingreq.decodedURI().duplicate(req.requestURI());req.getURLDecoder().convert(req.decodedURI(), true); 可以看到如果把useURIValidationHack设成”false”，可以减少它对一些url的不必要的检查从而减省开销。 enableLookups=”false” ： 为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。 disableUploadTimeout ：类似于Apache中的keeyalive一样 给Tomcat配置gzip压缩(HTTP压缩)功能 1compression=”on” compressionMinSize=”2048″ compressableMimeType=”text/html,text/xml,text/javascript,text/css,text/plain” HTTP 压缩可以大大提高浏览网站的速度，它的原理是，在客户端请求网页后，从服务器端将网页文件压缩，再下载到客户端，由客户端的浏览器负责解压缩并浏览。相对于普通的浏览过程HTML,CSS,Javascript , Text ，它可以节省40%左右的流量。更为重要的是，它可以对动态生成的，包括CGI、PHP , JSP , ASP , Servlet,SHTML等输出的网页也能进行压缩，压缩效率惊人。 1)compression=”on” 打开压缩功能 2)compressionMinSize=”2048″ 启用压缩的输出内容大小，这里面默认为2KB 3)noCompressionUserAgents=”gozilla, traviata” 对于以下的浏览器，不启用压缩 4)compressableMimeType=”text/html,text/xml” 压缩类型 最后不要忘了把8443端口的地方也加上同样的配置，因为如果我们走https协议的话，我们将会用到8443端口这个段的配置，对吧？｛ tomcat设置https端口时,8443和443区别: 8443端口在访问时需要加端口号,相当于http的8080,不可通过域名直接访问,需要加上端口号;https://yuming.com:8443。 443端口在访问时不需要加端口号,相当于http的80,可通过域名直接访问;例:https://yuming.com。 *问:https使用域名访问网站,而不显示端口号? 答:将端口号设置为443,即可通过域名直接访问网站 1234567｝&lt;!–enable tomcat ssl–&gt;&lt;Connector port=”8443″ protocol=”HTTP/1.1″ URIEncoding=”UTF-8″ minSpareThreads=”25″ maxSpareThreads=”75″ enableLookups=”false” disableUploadTimeout=”true” connectionTimeout=”20000″ acceptCount=”300″ maxThreads=”300″ maxProcessors=”1000″ minProcessors=”5″ useURIValidationHack=”false” compression=”on” compressionMinSize=”2048″ compressableMimeType=”text/html,text/xml,text/javascript,text/css,text/plain”SSLEnabled=”true”scheme=”https” secure=”true”clientAuth=”false” sslProtocol=”TLS”keystoreFile=”d:/tomcat2/conf/shnlap93.jks” keystorePass=”aaaaaa”/&gt; 好了，所有的Tomcat优化的地方都加上了。 3.2.2优化JDKTomcat默认可以使用的内存为128MB,Windows下,在文件{tomcat_home}/bin/catalina.bat，Unix下，在文件$CATALINA_HOME/bin/catalina.sh的前面，增加如下设置： 1JAVA_OPTS=”‘$JAVA_OPTS” -Xms[初始化内存大小] -Xmx[可以使用的最大内存] 或 设置环境变量：export JAVA_OPTS=””$JAVA_OPTS” -Xms[初始化内存大小] -Xmx[可以使用的最大内存]” 一般说来，你应该使用物理内存的 80% 作为堆大小。如果本机上有Apache服务器，可以先折算Apache需要的内存，然后修改堆大小。建议设置为70％；建议设置[[初始化内存大小]等于[可以使用的最大内存]，这样可以减少平凡分配堆而降低性能。 本例使用加入环境变量的方式： 1# vi /etc/profile 加入：export JAVA_OPTS=””$JAVA_OPTS” -Xms700 —Xmx700 1# source /etc/profile 【参数说明】 -Xms 是指设定程序启动时占用内存大小。一般来讲，大点，程序会启动的 快一点，但是也可能会导致机器暂时间变慢。 -Xmx 是指设定程序运行期间最大可占用的内存大小。如果程序运行需要占 用更多的内存，超出了这个设置值，就会抛出OutOfMemory 异常。 -Xss 是指设定每个线程的堆栈大小。这个就要依据你的程序，看一个线程 大约需要占用多少内存，可能会有多少线程同时运行等。 -XX:PermSize设置非堆内存初始值，默认是物理内存的1/64 。 -XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。 3.3 tomcat 有那几种Connector 运行模式？tomcat的运行模式有3种.修改他们的运行模式.3种模式的运行是否成功,可以看他的启动控制台,或者启动日志.或者登录他们的默认页面http://localhost:8080/查看其中的服务器状态。 1)bio 默认的模式,性能非常低下,没有经过任何优化处理和支持. 2)nio 利用java的异步io护理技术,no blocking IO技术. 想运行在该模式下，直接修改server.xml里的Connector节点,修改protocol为 &lt;Connector port=”80″ protocol=”org.apache.coyote.http11.Http11NioProtocol” connectionTimeout=”20000″ URIEncoding=”UTF-8″ useBodyEncodingForURI=”true” enableLookups=”false” redirectPort=”8443″ /&gt;启动后,就可以生效。 3)apr 安装起来最困难,但是从操作系统级别来解决异步的IO问题,大幅度的提高性能. 必须要安装apr和native，直接启动就支持apr。下面的修改纯属多余，仅供大家扩充知识,但仍然需要安装apr和native。如nio修改模式,修改protocol为org.apache.coyote.http11.Http11AprProtocol 3.4 tomcat调优 JVM参数调优-Xms&lt;size&gt; 表示JVM初始化堆的大小，-Xmx&lt;size&gt;表示JVM堆的最大值。这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。在catalina.bat中，设置JAVA_OPTS=&#39;-Xms256m -Xmx512m&#39;，表示初始化内存为256MB，可以使用的最大内存为512MB。 禁用DNS查询 当web应用程序想要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。DNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值：Tomcat4 1&lt;Connector className="org.apache.coyote.tomcat4.CoyoteConnector" port="80" minProcessors="5" maxProcessors="75" enableLookups="false" redirectPort="8443" acceptCount="100" debug="0" connectionTimeout="20000" useURIValidationHack="false" disableUploadTimeout="true" /&gt; Tomcat5 1&lt;Connector port="80" maxThreads="150" minSpareThreads="25" maxSpareThreads="75" enableLookups="false" redirectPort="8443" acceptCount="100" debug="0" connectionTimeout="20000" disableUploadTimeout="true"/&gt; 调整线程数通过应用程序的连接器（Connector）进行性能控制的的参数是创建的处理请求的线程数。Tomcat使用线程池加速响应速度来处理请求。在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接受更多的请求。Tomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。minProcessors服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。maxProcessors的值也是有上限的，应防止流量不可控制（或者恶意的服务攻击），从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。web server允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。在Tomcat5对这些参数进行了调整，请看下面属性：maxThreads Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。acceptCount 指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。connnectionTimeout 网络连接超时，单位：毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。minSpareThreads Tomcat初始化时创建的线程数。maxSpareThreads 一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket线程。最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。 Tomcat作为Web服务器，它的处理性能直接关系到用户体验，下面是几种常见的优化措施： 一、掉对web.xml的监视，把jsp提前编辑成Servlet。有富余物理内存的情况，加大tomcat使用的jvm的内存 二、服务器资源 服务器所能提供CPU、内存、硬盘的性能对处理能力有决定性影响。 (1) 对于高并发情况下会有大量的运算，那么CPU的速度会直接影响到处理速度。 (2) 内存在大量数据处理的情况下，将会有较大的内存容量需求，可以用-Xmx -Xms -XX:MaxPermSize等参数对内存不同功能块进行划分。我们之前就遇到过内存分配不足，导致虚拟机一直处于full GC，从而导致处理能力严重下降。 (3) 硬盘主要问题就是读写性能，当大量文件进行读写时，磁盘极容易成为性能瓶颈。最好的办法还是利用下面提到的缓存。 三、利用缓存和压缩 对于静态页面最好是能够缓存起来，这样就不必每次从磁盘上读。这里我们采用了Nginx作为缓存服务器，将图片、css、js文件都进行了缓存，有效的减少了后端tomcat的访问。 另外，为了能加快网络传输速度，开启gzip压缩也是必不可少的。但考虑到tomcat已经需要处理很多东西了，所以把这个压缩的工作就交给前端的Nginx来完成。 除了文本可以用gzip压缩，其实很多图片也可以用图像处理工具预先进行压缩，找到一个平衡点可以让画质损失很小而文件可以减小很多。曾经我就见过一个图片从300多kb压缩到几十kb，自己几乎看不出来区别。 四、采用集群 单个服务器性能总是有限的，最好的办法自然是实现横向扩展，那么组建tomcat集群是有效提升性能的手段。我们还是采用了Nginx来作为请求分流的服务器，后端多个tomcat共享session来协同工作。 五、 优化tomcat参数 这里以tomcat7的参数配置为例，需要修改conf/server.xml文件，主要是优化连接配置，关闭客户端dns查询。 12345678910&lt;Connector port="8080" protocol="org.apache.coyote.http11.Http11NioProtocol" connectionTimeout="20000" redirectPort="8443" maxThreads="500" minSpareThreads="20" acceptCount="100" disableUploadTimeout="true" enableLookups="false" URIEncoding="UTF-8" /&gt; 3.5 Tomcat 部署项目的三种方法目录 1、下载 Tomcat 服务器2、启动并部署 Tomcat 服务器3、Tomcat 的目录结构4、部署项目的第一种方法（项目直接放入 webapps 目录中）5、部署项目的第二种方法（修改 conf/server.xml 文件 ）6、部署项目的第三种方法（apache-tomcat-7.0.52\conf\Catalina\localhost ） 3.5.1 下载 Tomcat 服务器 ①、官网下载地址：http://tomcat.apache.org/ ②、tomcat 8.0 64位百度云下载地址：http://pan.baidu.com/s/1slbKPsx 密码：ewui ③、tomcat 8.0 32位百度云下载地址：http://pan.baidu.com/s/1o8G28rS 密码：k11n 3.5.2 启动并部署 Tomcat 服务器 ①、解压 tomcat 安装包到一个非中文目录下 ②、配置环境变量。JAVA_HOME(指向 JDK 安装的根目录) ③、双击 apache-tomcat-6.0.16\bin 目录下的 startup.bat，启动服务器(如果一闪而过，那就是没有配置 JAVA_HOME 的环境变量) ④、在浏览器中输入 http://localhost:8080 注意：Tomcat 启动不了的时候注意配置 JAVA_HOME:C:\Program Files\Java\jdk1.6.0_43这是安装 JDK的根目录 3.5.3 Tomcat 的目录结构 3.5.4 部署项目的第一种方法（项目直接放入 webapps 目录中）1、将编写并编译好的web项目(注意要是编译好的，如果是 eclipse，可以将项目打成 war 包放入)，放入到 webapps 中2、启动tomcat服务器（双击 apache-tomcat-6.0.16\bin 目录下的 startup.bat，启动服务器）3、在浏览器输入：http://localhost:8080/项目名/访问的文件名 3.5.5 部署项目的第二种方法（修改 conf/server.xml 文件 ）①、打开tomcat下conf/server.xml，在 标签之间输入项目配置信息 1&lt;Context path="/WebProject" docBase="D:/WebProject" reloadable="true" /&gt; path:浏览器访问时的路径名 docBase:web项目的WebRoot所在的路径，注意是WebRoot的路径，不是项目的路径。其实也就是编译后的项目 reloadble:设定项目有改动时，tomcat是否重新加载该项目 ②、双击 startup.bat，启动 tomcat 服务器，然后在浏览器输入访问的项目名称路径注意：如果你配置的 path=”/xx”,那么访问的时候就是这样： 3.5.6 部署项目的第三种方法（apache-tomcat-7.0.52\conf\Catalina\localhost ）①、进入到 apache-tomcat-7.0.52\conf\Catalina\localhost 目录，新建一个 项目名.xml 文件②、在 那个新建的 xml 文件中，增加下面配置语句（和上面的是一样的,但是不需要 path 配置，加上也没什么用） 1&lt;Context docBase="D:/WebProject" reloadable="true" /&gt; ③、在浏览器输入路径：localhost:8080/xml文件名/访问的文件名总结： ①、第一种方法比较普通，但是我们需要将编译好的项目重新 copy 到 webapps 目录下，多出了两步操作 ②、第二种方法直接在 server.xml 文件中配置，但是从 tomcat5.0版本开始后，server.xml 文件作为 tomcat 启动的主要配置文件，一旦 tomcat 启动后，便不会再读取这个文件，因此无法再 tomcat 服务启动后发布 web 项目 ③、第三种方法是最好的，每个项目分开配置，tomcat 将以\conf\Catalina\localhost 目录下的 xml 文件的文件名作为 web 应用的上下文路径，而不再理会 中配置的 path 路径，因此在配置的时候，可以不写 path。 通常我们使用第三种方法]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux环境SVN钉钉通知]]></title>
    <url>%2F2018%2F12%2F11%2FLinux%E7%8E%AF%E5%A2%83SVN%E9%92%89%E9%92%89%E9%80%9A%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[1 参考资料 https://www.cnblogs.com/jianxuanbing/p/6835765.html 2 步骤 2.1 修改pre-commit1234567891011#"!/bin/shREPOS="$1"TXN="$2"SVNLOOK=/usr/bin/svnlook # 同pre-commit.tmpl文件中的SVNLOOKLOGMSG=`$SVNLOOK log -t "$TXN" "$REPOS" | grep "[a-zA-Z0-9]" | wc -c`if [ "$LOGMSG" -lt 10 ];then echo "提交失败： 注释不能低于10个字符" 1&gt;&amp;2 exit 1fi 2.2 修改post-commit12345678910111213141516#!/bin/sh# 设置默认字符集，否则post信息到钉钉时中文乱码export LANG=en_US.UTF-8# svn中变量1为仓库路径，2为提交版本号REPOS="$1"REV="$2"time=$(date +%F/%T)# 下方svnlook命令获取相应的结果AUTHOR=$(/usr/bin/svnlook author -r $&#123;REV&#125; $&#123;REPOS&#125;)CHANGEDDIRS=$(/usr/bin/svnlook dirs-changed $REPOS)MESSAGE=$(/usr/bin/svnlook log -r $REV $REPOS)CONTENT=提交时间：$&#123;time&#125;\\n提交版本：$&#123;REV&#125;\\n作者：$&#123;AUTHOR&#125;\\n提交备注：$&#123;MESSAGE&#125;\\n修改目录：$CHANGEDDIRScd /home//usr/local/jdk1.8.0_152/bin/java Request 92bfa50db10658ee1c37f9ab6b7a000e14ce94a901002170762e472af3754fbf $CONTENT 备注： 脚本需要设置LANG变量为UTF8,否则中文乱码 CONTENT变量赋值种的:为中文书写方式，输出的消息存在空格的效果。 Request为java请求文件编译后的class文件，下面讲解 效果对比： 3 配置Java请求文件由于钉钉提供的接口是https协议，curl需要支持https，因此通过java代码发起Post请求，打包成可运行的class文件，然后用post-commit调用，传入信息即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.wolf.util;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.OutputStream;import java.net.URL;import javax.net.ssl.HttpsURLConnection;public class Request &#123; public static void main(String[] args) throws Exception &#123; String token = args[0]; String content = args[1]; content = "&#123;\"msgtype\": \"text\",\"text\": &#123;\"content\": \""+content+"\"&#125;&#125;"; httpsRequest("https://oapi.dingtalk.com/robot/send?access_token="+token, "POST", content); System.out.println("OK"); System.exit(0); &#125; /** * 发送https请求 */ public static String httpsRequest(String requestUrl, String requestMethod, String outputStr) throws Exception &#123; HttpsURLConnection conn = null; BufferedReader bufferedReader = null; try &#123; URL url = new URL(requestUrl); conn = (HttpsURLConnection) url.openConnection(); conn.setDoOutput(true); conn.setDoInput(true); conn.setUseCaches(false); conn.setRequestMethod(requestMethod); conn.setRequestProperty("content-type", "application/json"); if (null != outputStr) &#123; OutputStream outputStream = conn.getOutputStream(); outputStream.write(outputStr.getBytes("utf-8")); outputStream.close(); &#125; bufferedReader = new BufferedReader(new InputStreamReader(conn.getInputStream(), "utf-8")); String str = null; StringBuffer buffer = new StringBuffer(); while ((str = bufferedReader.readLine()) != null) &#123; buffer.append(str); &#125; return buffer.toString(); &#125; catch (Exception e) &#123; throw e; &#125; finally &#123; if (conn != null) &#123; conn.disconnect(); &#125; if (bufferedReader != null) &#123; try &#123; bufferedReader.close(); &#125; catch (IOException e) &#123; &#125; &#125; &#125; &#125;&#125; 备注：将上面代码使用eclipse工具导出为java文件，然后javac Request.java命令生成class文件，java Request命令执行class文件。 3 问题汇总3.1 乱码问题参考上面讲述，及解决方法 3.2 消息：后空格问题参考上面代码：使用中文写法。 4 附件Request.classRequest.java]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>钉钉</tag>
        <tag>svn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mount挂载共享文件夹报错]]></title>
    <url>%2F2018%2F12%2F05%2Fmount%E6%8C%82%E8%BD%BD%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%E5%A4%B9%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[mount挂载报错 1 失败: 关键字已过期 原因：账号密码过期，更换账号或设置密码永不过期。]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>小知识</tag>
        <tag>基础运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nohup输出重定向]]></title>
    <url>%2F2018%2F09%2F20%2Fnohup%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%2F</url>
    <content type="text"><![CDATA[Linux shell中有三种输入输出，分别为标准输入，标准输出，错误输出，分别对应0，1，2。我们可以直接通过输出重定向&gt;（或&gt;&gt;，表示追加）将某种输出重定向到其他地方，如设备，文件，比如:123ls &gt; ls.log #标准输出重定向ls 2&gt; ls.log #标准错误重定向ls &gt; /dev/null #重定向到null设备，相当于直接忽略输出 但是，有时候，我们想把标准输出以及错误输出一起重定向某个文件，这是可以通过 2&gt;&amp;1 实现，也可以通过两个同时重定向到某个文件 12ls &gt;ls.log 2&gt;&amp;1 //标准输出重定向到ls.log,而错误又重定向到标准输出，这两个位置不可换ls 2&gt;&gt;ls.log 1&gt;&gt;ls.log 转载：http://mblog.sigma.me/2011/08/15/linux-output-redirect.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux输出记录到文件]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E8%BE%93%E5%87%BA%E8%AE%B0%E5%BD%95%E5%88%B0%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1 如何把命令运行的结果保存到文件当中 这个问题太简单了，大家都知道，用&gt; 把输出转向就可以了 例子: 1234567 [lhd@hongdi ~]$ ls &gt; ls.txt [lhd@hongdi ~]$ cat ls.txt 1.gtkrc-2.0 2009 a amsn_received a.tar.gz 说明: &gt; 是把输出转向到指定的文件，如文件已存在的话也会重新写入，文件原内容不会保留 &gt;&gt;是把输出附向到文件的后面，文件原内容会保留下来 2 如何能在输出信息的同时把信息记录到文件中 我们在上面的例子中可以看到，我们使用输出转向，命令在终端上的输出转向到了文件中，但如果我希望能同时在终端上看到输出信息怎么办？ 我们可以使用这个命令: tee解释一下tee的作用:read from standard input and write to standard output and files它从标准输入读取内容并将其写到标准输出和文件中 看例子: 123456789101112 [lhd@hongdi ~]$ ls | tee ls_tee.txt 1.gtkrc-2.0 2009 a amsn_received a.tar.gz [lhd@hongdi ~]$ cat ls_tee.txt 1.gtkrc-2.0 2009 a amsn_received a.tar.gz 备注：使用tee时,如果想保留目标文件原有的内容怎么办？ 可以使用-a参数-a, --appendappend to the given FILEs, do not overwrite附加至给出的文件，而不是覆盖它 3 多个命令的输出都需要记录，可以用script script这个命令很强大，可以记录终端的所有输出到相应的文件中 看例子: 12345678910111213141516 [lhd@hongdi ~]$ script Script. started, file is typescript [lhd@hongdi ~]$ ls 1.gtkrc-2.0 c.tar kmess-2.0alpha2.tar.gz secpanel-0.5.3-1.noarch.rpm 2009 DownZipAction.php kmesslog secpanel-0.5.4-2.noarch.rpm [lhd@hongdi ~]$ exit exit Script. done, file is typescript [lhd@hongdi ~]$ cat typescript Script. started on 2009年02月08日 星期日 18时56分52秒 [lhd@hongdi ~]$ ls 1.gtkrc-2.0 c.tar kmess-2.0alpha2.tar.gz secpanel-0.5.3-1.noarch.rpm 2009 DownZipAction.php kmesslog secpanel-0.5.4-2.noarch.rpm [lhd@hongdi ~]$ exit exit Script. done on 2009年02月08日 星期日 18时57分00秒 说明: 1. 我们在启动script时没有指定文件名，它会自动记录到当前目录下一个名为 typescript的文件中。也可以用-a参数 指定文件名 例子: 12 [lhd@hongdi ~]$ script. -a example.txt Script. started, file is example.txt 此时终端的输出内容被记录到 example.txt这个文件中 2. 退出script时，用exit 感到奇怪吗？事实上script就是启动了一个shell 看一下ps auxfww的信息就知道了 12345 lhd 17738 0.1 3.2 152028 33328 ? Sl 18:30 0:03 /usr/bin/konsole lhd 17740 0.0 0.1 6372 1720 pts/1 Ss 18:30 0:00 \_ /bin/bash lhd 17900 0.0 0.0 5344 628 pts/1 S 19:01 0:00 | \_ script lhd 17901 0.0 0.0 5348 464 pts/1 S 19:01 0:00 | \_ script lhd 17902 0.5 0.1 6372 1688 pts/2 Ss 19:01 0:00 | \_ bash -i 3. 查看typescript的内容，可以看到它同时记录下了script的启动和结束时间 4 用script录制并播放session的内容 我们可以用 script把整个终端会话的所有操作和输出录制下来，然后再用scriptreplay进行播放。 如果录制时记录下来了操作时的时间数据，那么播放时和操作时的使用时间完全相同。 **这个很有用吧，比如：我们可以把安装软件时编译的过程记录下来，然后给别人进行演示** **看例子:** undefined 说明: -t 2&gt;example.time -t是把时间数据输出到标准错误(standard error)，所以我们使用 2&gt;example.time 把数据转向到 example.time这个文件当中如何播放所记录的内容? 4.1 安装scriptreplay下载 1wget linux/utils/util-linux/util-linux-2.12r.tar.bz2"&gt;ftp://ftp.kernel.org/pub/linux/utils/util-linux/util-linux-2.12r.tar.bz2 解压 1tar -jxvf util-linux-2.12r.tar.bz2 之后复制文件到系统的命令目录中即可 12 [root@hongdi 下载]# cp util-linux-2.12r/misc-utils/scriptreplay.pl /usr/bin/scriptreplay [root@hongdi 下载]# chmod 755 /usr/bin/scriptreplay 备注: fedora 10的util-linux-ng-2.14.1-3.2.fc10.i386.rpm此包中已包含 scriptreplay,已无需另行安装 4.2 播放所录制的session内容1234567 [lhd@hongdi ~]$ scriptreplay example1.time example1.txt [lhd@hongdi ~]$ ls 1.gtkrc-2.0 c.tar jeffray_lee@hotmail.com pass [lhd@hongdi ~]$ abcd bash: abcd: command not found [lhd@hongdi ~]$ exit 转载：http://www.eetop.cn/blog/html/03/6503-25123.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用系统性能诊断工具]]></title>
    <url>%2F2018%2F09%2F14%2FLinux%E5%B8%B8%E7%94%A8%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E8%AF%8A%E6%96%AD%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[简介：Linux下的系统性能诊断工具特别的多，例如：top、iotop、tiptop、slabtop、iostat、mpstat、vmstat、dstat、pidstat、netstat、iptraf、nicstat、ss、sar、free、strace、ltrace、ftrace、dtrace、blktrace、perf、stap、ktap、ebpf、lttng等等，下面一张图宏观的标注出来，更加的方便记忆！]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>运维工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins报错]]></title>
    <url>%2F2018%2F09%2F13%2Fjenkins%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[1 ERROR: Exception when publishing, exception message [Exec timed out or was interrupted after XXX ms] 我在使用Jenkins进行远程部署时,执行的命令突然中断，包如下错误：ERROR: Exception when publishing, exception message [Exec timed out or was interrupted after 12,001 ms] 解决办法： 参考资料：https://blog.csdn.net/u013066244/article/details/52788407]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker自动登陆脚本]]></title>
    <url>%2F2018%2F09%2F13%2FDocker%E8%87%AA%E5%8A%A8%E7%99%BB%E9%99%86%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[安装expect命令 1yum install expect -y 示例脚本 docker自动登陆脚本12345678#！/usr/bin/expectset timeout 2 # 超时时间2sspawn docker login 192.168.0.200:5000 # 要执行的命令expect "Username" # 匹配字符Usernamesend "user\r" # 键入用户user \r 回车expect "password" # 匹配passwordsend "pass\r" # 键入密码pass \r 回车interact]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker清除日志脚本]]></title>
    <url>%2F2018%2F09%2F11%2FDocker%E6%B8%85%E9%99%A4%E6%97%A5%E5%BF%97%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[清除日志脚本Docker12345678910111213#!/bin/shecho "==================== start clean docker containers logs ==========================" logs=$(find /var/lib/docker/containers/ -name *-json.log) for log in $logs do echo "clean logs : $log" cat /dev/null &gt; $log done echo "==================== end clean docker containers logs =========================="]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache搭建环境经验与问题-WinX86版]]></title>
    <url>%2F2018%2F09%2F11%2FApache%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83%E7%BB%8F%E9%AA%8C%E4%B8%8E%E9%97%AE%E9%A2%98-WinX86%E7%89%88%2F</url>
    <content type="text"><![CDATA[1 搭建环境： OS: server 2008 Enterprise sp1（X86） , VMware-workstation-full-9.0.2-1031769下搭建 2 软件版本(X86)：1) VC11(VSU_4\vcredist_x86.exe) http://www.microsoft.com/en-us/download/details.aspx?id=30679 2) apache 2.4.10 (httpd-2.4.10-win32-VC11) http://www.apachelounge.com/download/ apache版本并不是越新越好，不同版本apache对系统要求不一样(旧版本支持XP 、2003，新版本不支持XP、2003) 3) PHP5.6.12(php-5.6.12-win32-VC11-X86) Thread safe版本 http://windows.php.net/download/ 搭建PHP与apache使用安全线程（Thread safe）搭建PHP与iis使用非安全线程版（NO Thread safe） 4) mysql5.6.26(mysql-5.6.26-win32.zip”免安装版”) http://dev.mysql.com/downloads/mysql/ 3 apache安装在安装中，apache修改httpd.conf文件正常，未有出错，注意”\”要改成“/”,结尾新增加语句注意有空格，按教程走没有任何错误问题。 3.1 出现问题1：在CMD窗口启动安装apache（如下图启动路径内的安装执行文件），出现 1234567Installing the Apache2. 2 serviceThe Apache2. 2 service is successfully installed testing httpd.conf..... Errors reported here must be corrected before the service can be started. Errors reported here must be corrected before the service can be started结尾，并不是错误，而是提示：如果这行下边出现错误提示则解决错误后再启动！（当时以为是出错了） 服务安装成功后在系统服务里面会有apache服务，如图 此时服务是未启动状态，可以右键启动，也可以去目录找到ApacheMonitor.exe打开执行 双击ApacheMonitor.exe 点击start出现弹窗错误，the requested operation has failed无法启动apache服务，后来是通过在cmd命令窗口执行netsh winsock reset命令重启后恢复正常。方法具体内容见结尾重启后打开apache,点击start按钮正常开启，如下图，图标变绿# 4 问题解决Apache无法启动解决 the requested operation has failed错误，方法页面 APACHE启动出现the requested operation has failed，别复制其他地方的答案啊，情况不一样，NETSTAT和httpd.exe -w -n &quot;Apache&quot; -k start都不好使了，80端口好象也没问题，2.2版本选的也是对的，还有其他什么可能么，系统启动起来的时候就提示有一个服务启动错误应该就是APACHE的 补充： httpd.exe -w -n &quot;Apache&quot; -k start提示的是没有服务，但是服务开不了啊 4.1 解决问题：原因一：80端口占用例如IIS，另外就是迅雷。我的apache服务器就是被迅雷害得无法启用！ 原因二：软件冲突装了某些软件会使apache无法启动如Dr.com你打开网络连接-&gt;TcpIp属性-&gt;高级-&gt;WINS标签 把netbios的lmhosts对勾去掉，禁用tcp/ip的netbios. 然后再启动应该就可以了。 原因三：httpd.conf配置错误如果apache的配置文件httpd.conf搞错了，在windows里启动它，会提示the requested operation has failed，这是比较郁闷的事，因为查错要看个半天。其实可以用命令行模式启动apache，并带上参数，apache会提示你哪句有误，然后就可以针对性的解决，命令如下： httpd.exe -w -n &quot;Apache2&quot; -k start 还有一种情况：即使你这次启动了，下次你都有可能启动失败在运行里输入：netsh winsock reset （本人通过这个命令解决问题）一会儿cmd会提示你重启，不用理会，现在APACHE已经可以启动了。其实就是一个winsock的修复 5 附件链接: http://pan.baidu.com/s/1bo3HYIN 密码: 11ky]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>httpd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay+keepalive+sersync+rsync主从同步]]></title>
    <url>%2F2018%2F09%2F11%2FLiferay-keepalive-sersync-rsync%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[1 环境 服务器 系统 软件 JAVA MYSQL IP Liferay-a Centos7 Liferay+keepalive+sersync 1.7.0_80 5.5.42 172.20.20.59 Liferay-b Centos7 Liferay+keepalive+rsync 1.7.0_80 5.5.42 172.20.20.60 虚拟IP: 172.20.20.58 2 安装liferay安装liferay官方版本6.2-ce4,方法自行度娘。恢复数据参考《liferay备份还原文档》 3 安装keepalive1Yum install keepalive 3.1 编辑配置文件1Yum install keepalive Liferay-a: Liferay-b: 启动keepalive，建立虚拟IP，主服务器当机，从服务器获得Ip. 4 mysql主从同步下载mysql-5.5.42-linux2.6-x86_64.tar.gz 解压到/usr/local,重命名为mysql 4.1 编辑配置文件1Vi /etc/my.cnf Liferay-a: Liferay-b 4.2 建立mysql主从同步 查看liferay-a(主服务器)，查看mysql（主）信息，并建立同步帐号： 1mysql&gt; GRANT ALL PRIVILEGES ON bitnami_liferay.* TO 'tongbu'@'%' IDENTIFIED BY 'De123456' WITH GRANT OPTION; 进入liferay-b（从服务器），在mysql中输入命令，建立连接 1mysql&gt; CHANGE MASTER TO MASTER_HOST='172.20.20.59', MASTER_USER='tongbu', MASTER_PASSWORD='De123456', MASTER_LOG_FILE='mysql-bin.000022',MASTER_LOG_POS=151424110; 输入命令，查看备服务器信息 1mysql&gt; SHOW SLAVE STATUS\G 通过mysql命令恢复bitnami_liferay数据库备份到lifreay-a(主服务器)，自动实时同步数据到从服务器。 5 目录同步5.1 安装rsync(liferay-b)只需在liferay-b服务器上安装rsync。 Liferay-b作为rsync服务器，lifray-a作为客户端，实时同步目录数据到liferay-b 1Yum install rsync 5.1.1 编辑配置文件1vi /etc/rsyncd.conf liferay-b: 5.1.2 新建rsyncd.pass1密码文件在/etc/目录文件格式 帐号:密码 设置文件权限为600（必须） 5.2 安装sersync（liferay-a）Liferay-a安装实时同步工具sersync2.5.4_64bit_binary_stable_final.tar.gz 下载sersync2.5.4_64bit_binary_stable_final.tar.gz 解压到/usr/local 5.2.1 编辑confxml.xml1vi confxml.xml 5.2.2 在/etc/目录创建密码文件rsyncd.pass1文件格式只填写密码（注意和同步服务器的密码文件内的密码一样） 5.2.3 启动sersync，开启实时同步1/usr/local/sersync/sersync2 -d -r -o /usr/local/sersync/confxml.xml]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>keepalived</tag>
        <tag>rsync</tag>
        <tag>liferay</tag>
        <tag>同步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openkm部署]]></title>
    <url>%2F2018%2F09%2F10%2FOpenkm%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1 环境： 系统：Centos7.1 软件：openkm-6.3.1-community-linux-x64-installer.run 辅助插件：Apache_OpenOffice_4.1.2_Linux_x86-64_install-rpm_zh-CN.tar.gz 数据库：mariadb-5.5.47-1.el7_2.x86_64 2 安装步骤2.1 数据库安装1[root@bogon openkm]# yum install mysql -y //yum安装数据库 //登录数据库 1MariaDB [(none)]&gt; CREATE DATABASE okmdb DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_bin; //创建okmdb数据库,支持utf8 12345MariaDB [(none)]&gt; grant all on okmdb.* to openkm@localhost identified by 'openkm';//创建本地用户并赋予数据库okmdb所有权限MariaDB [(none)]&gt; flush privileges; ///刷新权限 2.2 openKM安装12345[root@bogon openkm]# chmod 755 openkm-6.3.1-community-linux-x64-installer.run//赋予安装包权限[root@bogon openkm]# ./openkm-6.3.1-community-linux-x64-installer.run //安装软件 2.2.1 配置数据库迁移（数据库由HSQL迁移到mysql）1[root@bogon openkm]# vi /opt/openkm-6.3.1-community/tomcat/OpenKM.cfg //修改HSQLDialect（代表HSQL数据库）为MySQL5Dialect（代表mysql数据库）, none改为create(初始化数据库)。 1[root@bogon openkm]# vi /opt/openkm-6.3.1-community/tomcat/conf/server.xml //修改配置文件 //修改mysql相关 2.2.2 修改web浏览端口号1[root@bogon openkm]# vi /opt/openkm-6.3.1-community/tomcat/conf/server.xml 3 办公插件安装3.1 Openoffice安装软件下载地址：http://www.openoffice.org/download/index.html 123456789101112[root@bogon openkm]# wget http://freefr.dl.sourceforge.net/project/openofficeorg.mirror/4.1.2/binaries/zh-CN/Apache_OpenOffice_4.1.2_Linux_x86-64_install-rpm_zh-CN.tar.gz//下载安装包[root@bogon openkm]# tar -zxvf Apache_OpenOffice_4.1.2_Linux_x86-64_install-rpm_zh-CN.tar.gz//解压缩安装包[root@bogon openkm]# cd zh-CN/RPMS/ // 进入解压后的目录[root@bogon RPMS]# rpm -Uvih *rpm //安装新版本，默认将会安装/升级Apache OpenOffice到/opt目录 修改配置，指向openoffice安装目录，实现office文档预览。 3.2 swftools安装1234567891011121314151617[root@bogon openkm]# yum install gcc* automake zlib-devel libjpeg-devel giflib-devel freetype-devel//安装所需的库和组件[root@bogon openkm]# wget http://www.swftools.org/swftools-0.9.2.tar.gz//下载安装包[root@bogon openkm]# tar vxzf swftools-0.9.2.tar.gz //解压安装包[root@bogon openkm]# cd swftools-0.9.2/ //进入解压目录[root@bogon swftools-0.9.2]# ./configure --prefix=/usr/local/swftools//编辑文件，并指定安装位置[root@bogon swftools-0.9.2]# make install //安装 报错： 解决办法： 修改swftools-0.9.2/swfs下的 Makefile和Makefile.in文件； install:$(mkinstalldirs) $(pkgdatadir)$(mkinstalldirs) $(pkgdatadir)/swfs$(INSTALL_DATA) ./simple_viewer.swf $(pkgdatadir)/swfs/simple_viewer.swf$(INSTALL_DATA) ./keyboard_viewer.swf $(pkgdatadir)/swfs/keyboard_viewer.swf$(INSTALL_DATA) ./PreLoaderTemplate.swf $(pkgdatadir)/swfs/PreLoaderTemplate.swf$(INSTALL_DATA) ./tessel_loader.swf $(pkgdatadir)/swfs/tessel_loader.swf$(INSTALL_DATA) ./swft_loader.swf $(pkgdatadir)/swfs/swft_loader.swfrm -f $(pkgdatadir)/swfs/default_viewer.swf -o -L $(pkgdatadir)/swfs/default_viewer.swf$(LN_S) $(pkgdatadir)/swfs/simple_viewer.swf $(pkgdatadir)/swfs/default_viewer.swfrm -f $(pkgdatadir)/swfs/default_loader.swf -o -L $(pkgdatadir)/swfs/default_loader.swf$(LN_S) $(pkgdatadir)/swfs/tessel_loader.swf $(pkgdatadir)/swfs/default_loader.swf 将两个文件中的标记红色的-o -L 去掉； 然后删除安装目录/usr/local/swftools,重新编译安装即可； 123456[root@bogon swftools-0.9.2]# vi /etc/profile//编辑文件，添加如下内容，设置swftools环境变量，使pdf2swf成为一个可执行命令export PATH=$PATH:/usr/local/swftools/bin/ 安装xpdf语言包。下载xpdf-chinese-simplified.tar.gz文件，解压到/usr/local下，编辑add-to-xpdfrc文件，如下： 1[root@bogon ~]# vi /usr/local/xpdf-chinese-simplified/add-to-xpdfrc //编辑文件 12fontDir /usr/share/fonts/win displayCIDFontTT Adobe-GB1 /usr/share/fonts/win/simhei.ttf //管理-配置-编辑路径 3.3 ImageMagick安装1[root@bogon ~]# yum install ImageMagick –y //yum安装插件 //管理-配置-添加路径 4 资料linux-安装openkm6.3.doc xpdf-chinese-simplified.tar.gz swftools-0.9.2.tar.gz OpenKM_6_zh-CN.rar 链接: 所有相关文件 密码: iryc]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>openkm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OTRS部署及优化过程]]></title>
    <url>%2F2018%2F09%2F10%2FOTRS%E9%83%A8%E7%BD%B2%E5%8F%8A%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1 环境 系统：centos 7.2 1511 Otrs：otrs-5.0.14-02.noarch.rpm Mysql：5.5.50-MariaDB Apache: Apache/2.4.6 2 安装步骤2.1 禁用SELinux在文件/etc/selinux/config中配置SELINUX=disabled 1[root@drbd2 home]# vi /etc/selinux/config 重启系统。重启后确认命令getenforce返回为Disabled 2.2 安装数据库1[root@drbd2 home] yum -y install mariadb-server 这会在你的系统上使用默认选项安装MySQL，你需要修改默认设置以适用于OTRS。使用文本编辑器来 创建一个新文件/etc/my.cnf.d/zotrs.cnf，包含如下内容： 现在执行systemctl start mariadb来重启数据库服务器并激活刚才的修改内容。然后运行命令/usr/bin/mysql_secure_installation，并按照屏幕上的指令来设置数据库的root密码、移除匿名访问及删除test数据库。 2.3 安装otrs使用yum通过命令行来安装OTRS，它还会拉入一些依赖包如Apache WEB服务器和一些Perl模块。 备注确保你已经将OTRS RPM文件复制到了当前目录。(必须在otrsrpm安装包的目录运行安装命令)1[root@drbd2 home]# yum install --nogpgcheck otrs-5.0.14-02.noarch.rpm现在使用命令systemctl restart httpd.service重启Apache以载入为OTRS修改的配置。## 2.4 安装额外的perl模块除了通过RPM包安装的Perl模块外，OTRS还需要一些其它的Perl模块，你可以手动安装。通过执行位于目录/opt/otrs下的文件bin/otrs.CheckModules.pl来检查缺失的模块。一些模块只是可选的功能才需要，比如与IMAP服务器通讯或生成PDF。具体可参考《OTRS-4.0.5系统安装手册 otrs_5.0_管理员说明书》。## 2.5 使用Web安装器在安装完OTRS软件后，你可以使用OTRS的WEB安装器来设置和配置OTRS数据库。WEB安装器是一个能通过浏览器访问的WEB页面。WEB安装器的地址是：http://localhost/otrs/installer.pl 。启动WEB安装器后，请跟随下面的步骤来设置你的系统：1、 检验OTRS办公室信息并点击‘下一步’以继续（见下图）。2、 阅读GNU Affero通用公共许可证（见下图）并页面底部的相应按钮接受许可。3、 选择你要在OTRS中使用的数据库。如果你选择MySQL或PostgreSQL，你还能在这里选择是通过WEB安装器新建一个数据库还是使用你的数据库管理员已经创建好的空数据库。4、 根据你选择的数据库的不同，以及在上一步中是用WEB安装器新建数据库还是使用已有数据库，这个窗口可用有一点点差异。在这个窗口输入数据库认证信息。5、 创建一个新的数据库用户，选择一个数据库名称，并点击‘下一步’（见下图）。OTRS会为你生成一个强密码，当然如果你愿意也可以输入你自己的密码。这个密码会写入到配置文件Kernel/Config.pm，所以无需记住这个密码。6、 如果需要会创建数据库，并填充相应数据，如图所示。点击‘下一步’进入下一个窗口。7、 提供所有必填的系统设置，并点击‘下一步’（如下图）。8、 若需要，你可以提供需要的数据来配置收发邮件，或者点击窗口底部右边的按钮跳过这一步（如下图）。暂时先跳过这一步，后期在进行配置。9、 记录管理员密码，安装完成。## 2.6 登陆地址服务人员登录地址：http://172.20.22.108/otrs/index.pl客户登陆地址：http://172.20.22.108/otrs/customer.pl# 3 备份与恢复## 3.1 备份有两种类型的数据需要备份：应用程序文件（如/opt/otrs目录下的文件）和存储在数据库中的数据。为了简化备份，在每个OTRS安装中已经包括了脚本scripts/backup.pl。运行它可以备份所有重要的数据。123456789[root@drbd2 scripts]# ./backup.pl -d /home/Backup /home//2016-12-13_13-18/Config.tar.gz ... doneBackup /home//2016-12-13_13-18/Application.tar.gz ... doneDump MySQL rdbms ... doneCompress SQL-file... done所有数据都保存在目录/home/2016-12-13_13-18/下。另外，这些数据被保存到一个.tar.gz文件。## 3.2 恢复要恢复一个备份，保存的应用程序数据必须被写回到安装目录，如/opt/otrs。还必须要恢复数据库。每个OTRS安装都自带了一个脚本文件scripts/restore.pl（见下面的脚本），它简化了恢复过程，支持MySQL和PostgreSQL。恢复要求数据库为空 12345678910111213[root@drbd2 scripts]# ./restore.pl -b /home/2016-12-13_13-18 -d /opt/otrsRestore /home/2016-12-13_13-18//Config.tar.gz ...Restore /home/2016-12-13_13-18//Application.tar.gz ...create MySQLdecompresses SQL-file ...cat SQL-file into MySQL databasecompress SQL-file... 4 优化4.1 开启工单关注系统配置》搜索配置参数Ticket::watcher》激活功能。 4.2 PDF打印中文乱码解决方案 此方法适合于Linux，Win系列可以直接跳过拷贝文件设定配置即可解决，也可安装其它字体。 安装中文字体 从windows 7/vista/2008 系统中拷贝微软雅黑字体msyh.ttf文件 将字体文件上传至/usr/share/fonts/chinese/TrueType/msyh.ttf 打开OTRS配置页面 在otrs配置 core::PDF中添加字体文件的绝对路径/usr/share/fonts/chinese/TrueType/msyh.ttf Win下可直接设定字体名称，otrs系统会自动至Fonts目录查找指定的字体用于处理中文。 进入 OTRS to Admin &gt; SysConfig &gt; Framework &gt; Core::PDF并更新字体位置，为PDF打印选择指定的新字体 4.3 FAQ显示更多属性首先，在系统配置的FAQ中选择配置项Frontend::Agent::FAQ::ViewExplorer。 然后在打开的页面中找到配置项FAQ::Frontend::AgentFAQExplorer###ShowColumns。按照下图进行配置。 最后，我们返回到FAQ浏览器页面，就可以看到FAQ文章的更多属性。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>otrs</tag>
        <tag>工单</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix在php7下安装always-populate-raw-post-data=-1问题]]></title>
    <url>%2F2018%2F09%2F10%2Fzabbix%E5%9C%A8php7%E4%B8%8B%E5%AE%89%E8%A3%85always-populate-raw-post-data-1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[LNMP 平台 php7 ，zabbix 安装可能会出现的问题always-populate-raw-post-data = -1，解决方案： 1vim /目录/zabbix/include/classes/setup/CFrontendSetup.php 找到下面代码、关于always-populate-raw-post-data;添加 12345678910111213$current = -1; public function checkPhpAlwaysPopulateRawPostData() &#123; $current = ini_get('always_populate_raw_post_data'); $current = -1; return array( 'name' =&gt; _('PHP always_populate_raw_post_data'), 'current' =&gt; ($current != -1) ? _('on') : _('off'), 'required' =&gt; _('off'), 'result' =&gt; ($current != -1) ? self::CHECK_FATAL : self::CHECK_OK, 'error' =&gt; _('PHP always_populate_raw_post_data must be set to -1.') ); &#125; 重新刷新 zabbix 安装页面即可；]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7系统安装配置kvm软件步骤]]></title>
    <url>%2F2018%2F09%2F10%2Fcentos7%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEkvm%E8%BD%AF%E4%BB%B6%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[1 kvm相关安装包及其作用 qemu-kvm 主要的KVM程序包 python-virtinst 创建虚拟机所需要的命令行工具和程序库 virt-manager GUI虚拟机管理工具 virt-top 虚拟机统计命令 virt-viewer GUI连接程序，连接到已配置好的虚拟机 libvirt C语言工具包，提供libvirt服务 libvirt-client 为虚拟客户机提供的C语言工具包 virt-install 基于libvirt服务的虚拟机创建命令 bridge-utils 创建和管理桥接设备的工具 2 安装kvm 软件大致列下步骤： 1[root@361way ~]# yum -y install qemu-kvm libvirt virt-install bridge-utils virt-manager 2.1 查看是否加载kvm模块12345678910[root@kvm ~]# lsmod|grep kvmkvm_intel 138567 0kvm 441119 1 kvm_intel#如果没有这两条，可以用"modprobe kvm"加载；#相关命令"insmod;rmmod;modinfo" 2.2 启动libvirtd1234567[root@localhost ~]# systemctl start libvirtd # 启动[root@localhost ~]# systemctl enable libvirtd # 开机启动[root@localhost ~]# systemctl list-unit-files|grep libvirtd # 检测是否开机启动libvirtd.service enabled 3 置网卡桥接3.1 修改网卡文件eno167777361234 [root@kvm ~]# cd /etc/sysconfig/network-scripts/[root@kvm network-scripts]# echo "BRIDGE=br0" &gt;&gt; ifcfg-eno16777736 # 在ifcfg-e**原网卡文件中增加"BRIDGE=br0" 3.2 新建网桥文件ifcfg-br0(网桥名称),增加内容如下1234567891011121314151617[root@kvm network-scripts]# vi ifcfg-br0*************************************************DEVICE=br0 TYPE="Bridge" #大小写敏感，所以必须是BridgeBOOTPROTO="dhcp" ONBOOT="yes"DELAY="0"STP="yes" #这一行是来启动STP,和brctl命令行出来的结果有关************************************************* 3.3 重启NetworkManager及network服务：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@kvm network-scripts]# systemctl restart NetworkManager# 当你手动修改了网卡文件后，需要重启NetworkManager服务来重新接管网络配置# 网卡配置文件和NetworkManager配置冲突时，解决方案:｛1、重启NetworkManager;2、关闭NetworkManager｝ [root@kvm network-scripts]# systemctl restart network[root@kvm network-scripts]# ip a #ip命令用来查看和管理ip信息1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eno16777736: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master br0 state UP qlen 1000 link/ether 00:0c:29:61:5c:1d brd ff:ff:ff:ff:ff:ff3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN link/ether 52:54:00:b6:45:5b brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN qlen 500 link/ether 52:54:00:b6:45:5b brd ff:ff:ff:ff:ff:ff6: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP link/ether 00:0c:29:61:5c:1d brd ff:ff:ff:ff:ff:ff inet 192.168.0.32/24 brd 192.168.0.255 scope global dynamic br0 valid_lft 11979sec preferred_lft 11979sec inet6 fe80::20c:29ff:fe61:5c1d/64 scope link valid_lft forever preferred_lft forever7: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master br0 state UNKNOWN qlen 500 link/ether fe:54:00:cb:63:b2 brd ff:ff:ff:ff:ff:ff inet6 fe80::fc54:ff:fecb:63b2/64 scope link valid_lft forever preferred_lft forever 3.4 查看网桥连接123456789[root@kvm network-scripts]# brctl show #brctl是一个网桥连接管理命令bridge name bridge id STP enabled interfaces #如果上面不设置STP=yes，这里就会显示nobr0 8000.000c29615c1d yes eno16777736 vnet0virbr0 8000.525400b6455b yes virbr0-nic 4 selinux防火墙关闭12# setenforce 0 # sed -i 's/=enforcing/=disabled/g' /etc/selinux/config 5 打开图形界面 1[root@bogon ~]# virt-manager]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7创建分区及扩展分区]]></title>
    <url>%2F2018%2F09%2F10%2FCentos7%E5%88%9B%E5%BB%BA%E5%88%86%E5%8C%BA%E5%8F%8A%E6%89%A9%E5%B1%95%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[1 添加硬盘并挂载 1.1 查询硬盘信息（如果使用的是新加硬盘，可以在最下方看到硬盘信息，例子使用本地空闲空间） 例如： 1 [root@bogon /]# fdisk -l 1[root@bogon /]# df –h 1.2 Fdisk创建分区1[root@bogon /]# fdisk /dev/sda 1.3 查看硬盘信息1[root@bogon /]# fdisk –l 1.4 Partprobe刷新分区表（注：最好每做一步遇到找不到问题都刷新一下）1[root@bogon /]# partprobe 1.5 创建物理卷（PV）使用pvcreate 命令创建物理卷，pvdisplay 查看物理卷信息： 12345[root@bogon /]# pvcreate /dev/sda2 Physical volume "/dev/sda2" successfully created [root@bogon /]# pvdisplay 1.6 将PV加入卷组（VG）使用vgdisplay 查看卷组信息，下图显示卷组名为centos，空闲大小为0： 使用 vgextend 命令把/dev/vdb1加入到centos： 1[root@bogon /]# vgextend centos /dev/sda2 1.7 创建逻辑卷（LV）使用lvcreate 命令从卷组里划分一个新的逻辑卷，这里创建了名称为newlv，大小9GB的逻辑卷分区；使用lvdisplay 查看逻辑卷信息： 12345678[root@bogon /]# lvcreate -L 9G -n newlv centosWARNING: xfs signature detected on /dev/centos/newlv at offset 0. Wipe it? [y/n]: yWiping xfs signature on /dev/centos/newlv.Logical volume "newlv" created. 1[root@bogon /]# lvdisplay 1[root@bogon /]# vgdisplay 1.8 格式化逻辑卷并挂载新逻辑卷经过格式化就可以挂载到系统里存储数据了。使用mkfs.xfs 格式化为CentOS7的xfs文件系统： 1[root@bogon /]# mkfs.xfs /dev/centos/newlv 创建drbd目录，并挂载到/drbd目录，挂载后查询容量为9个G. 设置开机自动挂载，编辑 /etc/fstab文件，加入最后一行：]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows2003域控迁移到Windows2008域控]]></title>
    <url>%2F2018%2F09%2F10%2FWindows2003%E5%9F%9F%E6%8E%A7%E8%BF%81%E7%A7%BB%E5%88%B0Windows2008%E5%9F%9F%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[1 前期背景： 现有DNS、DHCP服务器域控为Windows 2003 系统，需要将Windows 2003 域控升级为2008. 2 环境：主域控 ：Server 2003 X86 迁移域控服务器：Server 2008 R2 X64 3 操作步骤3.1 配置Windows server 2008域控兼容环境 首先将Windows 2008系统加入 Windows 2003 域控的域中。这里为测试的，暂为 shdc.com。加入以后重启2008系统。 重启后用shdc\administrator登录2008系统。在测试进行之前，我们来查看一下， 现有的FSMO角色情况。（运行CMD，输入 etdom query fsmo），显示结果所有的角色都在2003的域控shdc-1中。 在升级域控到 Windows Server 2008之前，必须进行相关的扩展，这一点，与从Windows Server 2000域升级到Windows Server 2003域一样。在这里我们必须在原 Windows Server 2003 域控制 器上运行Windows Server 2008的 ADPREP工具，该工具位于 Windows Server 2008光盘中的 Source\adprep目录下，请复制 adprep目录到Windows Server 2003域控制上的任意磁盘分区中（计算机名为:shdc-1），本案例将此文件夹复制 到SHDC-1的磁盘分区C。特别说明，敬请留意：原 Windows Server 2000域升级到 Windows Server 2003域，只需对 Forest和 Domian进行扩展，但在 Windows Server 2003域升级到 Windows Server 2008域中，还必须对 RODC进行扩展，以便 Windows Server 2008能在基于 Windows Server 2003的域中担任域控制器类型角色 下面操作在shdc-1（域控制器）上进行操作。 开始 －运行 －CMD，进入C分区的ADPREP目录输入 adprep /forestprep根据提示，选择”C “，并按下Enter键继续。 完成 Forest扩展。接下来的是RODC请输入：adprep /rodcprep（从显示结果 来看，似乎这些扩展已经更新了，但之前尝试过跳过 RODC扩展时，不能继续。） 完成 RODC的扩展之后，接下来进行的是Domain的扩展。 输入：adprep /domainprep /gpprep 结果出现如下错误。 网络搜索发现，没有提升域模式，在Active Directory用户和计算机中点击右键，提升域功能级别。 选择 Windows 2000纯模式。点提升。 再次运行adprep /domainprep /gpprep ，如下图。 OK，我们已经做好了将 Windows Server2008（计算机名 Server2008）提升为域控 制器的准备工作。 3.2 提升Windows 2008 为域控制器 现在，以shdc\administrator身份登录成员服务器 Server 2008。 开始 －运行 － dcpromo（和Windows 2003中的类似） 出现”Active Directory 域服务安装向导“，在该向导中，我们选中”`使用高级模式 安装“，点击下一步`： 在” Active Directory 域服务安装向导“中，我们选中” 现有林“中的”`向现有域添加 域控制器”，并点击“下一步` ” 点击“ 下一步 ”继续 选择域，点击“ 下一步 ”继续 选择站点，点击“ 下一步 ”继续 “ 其他域控制器选项”，以这个选项中，默认的 角色为“ DNS服务器“和” 全局编录 “ ，而” 只读域控制器“，则为不可选，点击“ 下一步 ”继续 选择”是”继续。 默认选项，从现有的域控制器上接收更新数据。点击“下一步 ”继续 点击“ 下一步 ”继续 点击“ 下一步 ”继续 设置目录服务还原模式密码。 点击“ 下一步 ”开始提升 Server 2008为域控制器。 其过程如图： 因为选中了“ 完成后重新启动 “，故 DCPROMO完成之后，系统自动启动。让我们来看 Server 2008提升为域控制器之后，ADUC的对比情况。 3.3 角色转移 接下来的工作，就是传说中的 FSMO角色的转移，首先我们要进行的是对架构主机角色的转移，在这之前，我们必须使用 regsvr32 schmmgmt.dll来注册Active Directory 架构 。以便利用 MMC工具来添加 架构 管理控制台。这个地方要注意，要以管理员身份运行CMD，否则会出错。2008直接在运行里输CMD似乎权限不够 开始 －运行 －MMC，打开微软管理控制台。这里我们要添加 Activer Directory架构 管理控制台。如下图。 右键单击“ Active Directory 架构 “根部，并且选择 “ 更改Active Directory 域控制器 “ 选取要连接的域控制器 shdc-2.shdc.com。 再次右键单击“ Active Directory 架构 “根部，并且选择 “ 操作主机 “在对话框中，点击“ 更改“，系统会提示您” 你确实要更改操作主机？“点击” 是 ”继续 系统提示架构主机角色的已成功转移到 shdc-2.shdc.com 。 接下来的工作，就是分别将RID、PDC、基础结构主机角色转移到 shdc-2.com 。请在运行中输入dsa.msc打开 Active Directory 用户和计算机。 注意：在以下主机角色转移之前，请右键 shdc.com根部，选择“更改域控制器“， 并且选中 2008域控。 请右键 shdc.com根部，选择“ 操作主机“，依次将RID、PDC、Infrastructure主机角色转移到 shdc-2.com 最后我们需要做的是将域命名（Domain Name）主机，转移到 shdc-2.com上，打开 Active Directory域和信任关系，完成域命名主机的转移。 最后需要做的是取消 shdc-1.com的 GC（全局编录）角色，打开 Active Directory 站点和服务，依次选中 Site – Default First Site Name – Servers – shdc-1 ，右键单击 NTDS Setting，选择属性，然后将 全局编录 前面的勾去掉，只何留shdc-2.com（Windows 2008）为GC即可。 最后确定一下各种主机角色的状态，在运行里输入 cmd进行命令提示行，输入 1netdom query fsmo 至此 Windows Server 2003迁移到 Windows Server 2008完毕。 4 错误4.1 错误1：输入adprep /rodcprep报错 症状如下： 12345678Adprep could not contact a replica for partition DC=DomainDnsZones,DC=Contoso,DC=com Adprep failed the operation on partition DC=DomainDnsZones,DC=Contoso,DC=com Skipping to next partition. Adprep could not contact a replica for partition DC=ForestDnsZones,DC=Contoso,DC=com Adprep encountered an LDAP error. Error code: 0x0. Server extended error code: 0x0, Server error message: (null). Adprep failed the operation on partition DC=ForestDnsZones,DC=Contoso,DC=com Skipping to next partition. Adprep completed with errors. Not all partitions are updated. 原因是： 分区或在错误消息中不再存在引用分区 为引用的分区或分区结构主机已经强行降级或离线。 解决办法： 参考资料： https://support.microsoft.com/zh-cn/kb/949257 http://bbs.51cto.com/thread-1067016-1-1.html http://www.68idc.cn/help/jiabenmake/qita/2014040986015.html 备注：微软给出的fixfsmo.vbs脚本无需更改内容（当时纠结了一天，挺着心肝尝试了下，惨确定无须修改。)Fixfsmo.vbs脚本内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101'-------fixfsmo.vbs------------------const ADS_NAME_INITTYPE_GC = 3const ADS_NAME_TYPE_1779 = 1const ADS_NAME_TYPE_CANONICAL = 2 set inArgs = WScript.Arguments if (inArgs.Count = 1) then ' Assume the command line argument is the NDNC (in DN form) to use. NdncDN = inArgs(0)Else Wscript.StdOut.Write "usage: cscript fixfsmo.vbs NdncDN"End if if (NdncDN &lt;&gt; "") then ' Convert the DN form of the NDNC into DNS dotted form. Set objTranslator = CreateObject("NameTranslate") objTranslator.Init ADS_NAME_INITTYPE_GC, "" objTranslator.Set ADS_NAME_TYPE_1779, NdncDN strDomainDNS = objTranslator.Get(ADS_NAME_TYPE_CANONICAL) strDomainDNS = Left(strDomainDNS, len(strDomainDNS)-1) Wscript.Echo "DNS name: " &amp; strDomainDNS ' Find a domain controller that hosts this NDNC and that is online. set objRootDSE = GetObject("LDAP://" &amp; strDomainDNS &amp; "/RootDSE") strDnsHostName = objRootDSE.Get("dnsHostName") strDsServiceName = objRootDSE.Get("dsServiceName") Wscript.Echo "Using DC " &amp; strDnsHostName ' Get the current infrastructure fsmo. strInfraDN = "CN=Infrastructure," &amp; NdncDN set objInfra = GetObject("LDAP://" &amp; strInfraDN) Wscript.Echo "infra fsmo is " &amp; objInfra.fsmoroleowner ' If the current fsmo holder is deleted, set the fsmo holder to this domain controller. if (InStr(objInfra.fsmoroleowner, "\0ADEL:") &gt; 0) then ' Set the fsmo holder to this domain controller. objInfra.Put "fSMORoleOwner", strDsServiceName objInfra.SetInfo ' Read the fsmo holder back. set objInfra = GetObject("LDAP://" &amp; strInfraDN) Wscript.Echo "infra fsmo changed to:" &amp; objInfra.fsmoroleowner End if End if 分别运行运行脚本： 123cscript fixfsmo.vbs DC = ForestDnsZones，DC = dealeasy，DC = localcscript fixfsmo.vbs DC = DomainDnsZones，DC = dealeasy，DC = local 问题解决 5 附件：链接: http://pan.baidu.com/s/1kVBPuuV 密码: aevv]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>工作域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openvpn部署之部署基于AD域认证访问内网]]></title>
    <url>%2F2018%2F09%2F10%2Fopenvpn%E9%83%A8%E7%BD%B2%E4%B9%8B%E9%83%A8%E7%BD%B2%E5%9F%BA%E4%BA%8EAD%E5%9F%9F%E8%AE%A4%E8%AF%81%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%2F</url>
    <content type="text"><![CDATA[1 安装环境 Centos6.5 openvpn2.3.11 2 步骤2.1 添加fedora的yum源1rpm -ivh http://mirrors.ustc.edu.cn/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm 2.2 安装openvpn1234567yum install openvpn -yyum -y install openssl openssl-devel -y yum -y install lzo lzo-devel -y yum install -y libgcrypt libgpg-error libgcrypt-devel 2.3 安装openvpn认证插件（LDAP认证使用）1yum install openvpn-auth-ldap -y 2.4 安装easy-rsa由于openvpn2.3之后，在openvpn里面剔除了easy-rsa文件，所以需要单独安装 123yum install easy-rsa cp -rf /usr/share/easy-rsa/3.0.6 /etc/opevpn/easy-rsa 2.5 生成openvpn的key及证书修改/opt/openvpn/etc/easy-rsa/2.0/vars参数 123456789101112$ vi varsexport KEY_COUNTRY="CN" 国家export KEY_PROVINCE="ZJ" 省份export KEY_CITY="NingBo" 城市export KEY_ORG="TEST-VPN" 组织exportKEY_EMAIL="81367070@qq.com" 邮件export KEY_OU="baidu" 单位 保存退出 2.5.1 创建服务器证书./easyrsa clean-all 初始化目录，清理历史证书 ./easyrsa build-ca 创建ca证书,设置ca证书密码，必须记着密码，为服务起名 ./easyrsa gen-req server nopass 创建服务端证书，为server起名 ./easyrsa sign server server 签约服务端证书，输入上面ca证书的密码 ./easyrsa gen-dh 创建数据穿越密钥 2.5.2 创建客户端证书（使用客户端证书认证使用）123cd /etc/openvpn/client/cp -rf /usr/share/easy-rsa/3.06 /etc/opevpn/client/cd client/3.0.6 ./easyrsa init-pki 初始化目录 ./easyrsa build-ca 创建ca证书,记住密码，客户端登录要用 ./easyrsa gen-req client1 创建客户端证书，输入客户端ca证书密码，也是客户端登陆密码 cd /etc/openvpn/easy-rsa/3.0.6./easyrsa import-req /etc/openvpn/client/ 3.0.6/pki/reqs/client1.req client1 导入客户端证书 2.6 编辑openvpn服务端配置2.6.1 拷贝配置文件12345678cd /etc/openpvn/easy-rsa/3.0.6/pki/cp ca.crt /etc/openvpn/server/cp private/server.key /etc/openvpn/server/cp issued/server.crt /etc/openvpn/server/cp dh.pem /etc/openvpn/server/cp ca.crt /etc/openvpn/client/cp issued/client1.crt /etc/openvpn/client/cp /etc/openvpn/client/client/easyrsa3/pki/private/client1.key /etc/openvpn/client/ 12 cd /etc/openvpn/server$ ls 12cd /etc/openvpn/client$ ls 2.6.2 配置服务端配置文件1$ cat /etc/openvpn/server.conf 1234567891011121314151617181920212223242526272829port 1194proto tcpdev tun# 证书路径ca keys/ca.crtcert keys/server.crtkey keys/server.key # This file should be kept secret# 密钥路径dh keys/dh.pemserver 10.8.0.0 255.255.255.0 //客户端分配的ip地址push "route 172.20.17.0 255.255.255.0" //推送客户端的路由push "route 172.20.18.0 255.255.255.0"push "route 172.20.19.0 255.255.255.0"push "route 172.20.20.0 255.255.255.0"push "route 172.20.22.0 255.255.255.0"push "redirect-gateway" //修改客户端的网关，使其直接走vpn流量，不配置走客户端流量ifconfig-pool-persist ipp.txt keepalive 10 120comp-lzo //客户端和服务端保持一致persist-keypersist-tunstatus openvpn-status.log //登录日志verb 3# LDAP认证插件，使用时配置plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so "/etc/openvpn/auth/ldap.conf" # 禁用客户端证书认证client-cert-not-requiredusername-as-common-name log /var/log/openvpn.log 举例server.confLDAP方式配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334################################################## Sample OpenVPN 2.0 config file for ## multi-client server. ## ## This file is for the server side ## of a many-clients &lt;-&gt; one-server ## OpenVPN configuration. ## ## OpenVPN also supports ## single-machine &lt;-&gt; single-machine ## configurations (See the Examples page ## on the web site for more info). ## ## This config should work on Windows ## or Linux/BSD systems. Remember on ## Windows to quote pathnames and use ## double backslashes, e.g.: ## "C:\\Program Files\\OpenVPN\\config\\foo.key" ## ## Comments are preceded with '#' or ';' ################################################### Which local IP address should OpenVPN# listen on? (optional);local a.b.c.d# Which TCP/UDP port should OpenVPN listen on?# If you want to run multiple OpenVPN instances# on the same machine, use a different port# number for each one. You will need to# open up this port on your firewall.port 1194# TCP or UDP server?# ;proto tcp# proto udpproto tcp# "dev tun" will create a routed IP tunnel,# "dev tap" will create an ethernet tunnel.# Use "dev tap0" if you are ethernet bridging# and have precreated a tap0 virtual interface# and bridged it with your ethernet interface.# If you want to control access policies# over the VPN, you must create firewall# rules for the the TUN/TAP interface.# On non-Windows systems, you can give# an explicit unit number, such as tun0.# On Windows, use "dev-node" for this.# On most systems, the VPN will not function# unless you partially or fully disable# the firewall for the TUN/TAP interface.# ;dev tapdev tun# Windows needs the TAP-Win32 adapter name# from the Network Connections panel if you# have more than one. On XP SP2 or higher,# you may need to selectively disable the# Windows firewall for the TAP adapter.# Non-Windows systems usually don't need this.;dev-node MyTap# SSL/TLS root certificate (ca), certificate# (cert), and private key (key). Each client# and the server must have their own cert and# key file. The server and all clients will# use the same ca file.## See the "easy-rsa" directory for a series# of scripts for generating RSA certificates# and private keys. Remember to use# a unique Common Name for the server# and each of the client certificates.## Any X509 key management system can be used.# OpenVPN can also use a PKCS #12 formatted key file# (see "pkcs12" directive in man page).# 证书路径ca /etc/openvpn/ca.crt cert /etc/openvpn/server.crtkey /etc/openvpn/server.key # This file should be kept secret# Diffie hellman parameters.# Generate your own with:# openssl dhparam -out dh2048.pem 2048# dh dh2048.pem# 密钥路径dh /etc/openvpn/dh.pem# Network topology# Should be subnet (addressing via IP)# unless Windows clients v2.0.9 and lower have to# be supported (then net30, i.e. a /30 per client)# Defaults to net30 (not recommended);topology subnet# Configure server mode and supply a VPN subnet# for OpenVPN to draw client addresses from.# The server will take 10.8.0.1 for itself,# the rest will be made available to clients.# Each client will be able to reach the server# on 10.8.0.1. Comment this line out if you are# ethernet bridging. See the man page for more info.server 10.8.0.0 255.255.255.0# Maintain a record of client &lt;-&gt; virtual IP address# associations in this file. If OpenVPN goes down or# is restarted, reconnecting clients can be assigned# the same virtual IP address from the pool that was# previously assigned.ifconfig-pool-persist ipp.txt# Configure server mode for ethernet bridging.# You must first use your OS's bridging capability# to bridge the TAP interface with the ethernet# NIC interface. Then you must manually set the# IP/netmask on the bridge interface, here we# assume 10.8.0.4/255.255.255.0. Finally we# must set aside an IP range in this subnet# (start=10.8.0.50 end=10.8.0.100) to allocate# to connecting clients. Leave this line commented# out unless you are ethernet bridging.;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100# Configure server mode for ethernet bridging# using a DHCP-proxy, where clients talk# to the OpenVPN server-side DHCP server# to receive their IP address allocation# and DNS server addresses. You must first use# your OS's bridging capability to bridge the TAP# interface with the ethernet NIC interface.# Note: this mode only works on clients (such as# Windows), where the client-side TAP adapter is# bound to a DHCP client.;server-bridge# Push routes to the client to allow it# to reach other private subnets behind# the server. Remember that these# private subnets will also need# to know to route the OpenVPN client# address pool (10.8.0.0/255.255.255.0)# back to the OpenVPN server.;push "route 192.168.10.0 255.255.255.0";push "route 192.168.20.0 255.255.255.0"# 发送路由push "route 192.168.0.0 255.255.0.0"# To assign specific IP addresses to specific# clients or if a connecting client has a private# subnet behind it that should also have VPN access,# use the subdirectory "ccd" for client-specific# configuration files (see man page for more info).# EXAMPLE: Suppose the client# having the certificate common name "Thelonious"# also has a small subnet behind his connecting# machine, such as 192.168.40.128/255.255.255.248.# First, uncomment out these lines:;client-config-dir ccd;route 192.168.40.128 255.255.255.248# Then create a file ccd/Thelonious with this line:# iroute 192.168.40.128 255.255.255.248# This will allow Thelonious' private subnet to# access the VPN. This example will only work# if you are routing, not bridging, i.e. you are# using "dev tun" and "server" directives.# EXAMPLE: Suppose you want to give# Thelonious a fixed VPN IP address of 10.9.0.1.# First uncomment out these lines:;client-config-dir ccd;route 10.9.0.0 255.255.255.252# Then add this line to ccd/Thelonious:# ifconfig-push 10.9.0.1 10.9.0.2# Suppose that you want to enable different# firewall access policies for different groups# of clients. There are two methods:# (1) Run multiple OpenVPN daemons, one for each# group, and firewall the TUN/TAP interface# for each group/daemon appropriately.# (2) (Advanced) Create a script to dynamically# modify the firewall in response to access# from different clients. See man# page for more info on learn-address script.;learn-address ./script# If enabled, this directive will configure# all clients to redirect their default# network gateway through the VPN, causing# all IP traffic such as web browsing and# and DNS lookups to go through the VPN# (The OpenVPN server machine may need to NAT# or bridge the TUN/TAP interface to the internet# in order for this to work properly).;push "redirect-gateway def1 bypass-dhcp"# Certain Windows-specific network settings# can be pushed to clients, such as DNS# or WINS server addresses. CAVEAT:# http://openvpn.net/faq.html#dhcpcaveats# The addresses below refer to the public# DNS servers provided by opendns.com.;push "dhcp-option DNS 208.67.222.222";push "dhcp-option DNS 208.67.220.220"# 设置发送的DNSpush "dhcp-option DNS 114.114.114.114"# Uncomment this directive to allow different# clients to be able to "see" each other.# By default, clients will only see the server.# To force clients to only see the server, you# will also need to appropriately firewall the# server's TUN/TAP interface.# 开启客户端互相访问client-to-client# Uncomment this directive if multiple clients# might connect with the same certificate/key# files or common names. This is recommended# only for testing purposes. For production use,# each client should have its own certificate/key# pair.## IF YOU HAVE NOT GENERATED INDIVIDUAL# CERTIFICATE/KEY PAIRS FOR EACH CLIENT,# EACH HAVING ITS OWN UNIQUE "COMMON NAME",# UNCOMMENT THIS LINE OUT.;duplicate-cn# The keepalive directive causes ping-like# messages to be sent back and forth over# the link so that each side knows when# the other side has gone down.# Ping every 10 seconds, assume that remote# peer is down if no ping received during# a 120 second time period.# 心跳侦测keepalive 10 120# For extra security beyond that provided# by SSL/TLS, create an "HMAC firewall"# to help block DoS attacks and UDP port flooding.## Generate with:# openvpn --genkey --secret ta.key## The server and each client must have# a copy of this key.# The second parameter should be '0'# on the server and '1' on the clients.# tls-auth ta.key 0 # This file is secret# Select a cryptographic cipher.# This config item must be copied to# the client config file as well.# Note that v2.4 client/server will automatically# negotiate AES-256-GCM in TLS mode.# See also the ncp-cipher option in the manpage# 加密方式cipher AES-256-CBC# Enable compression on the VPN link and push the# option to the client (v2.4+ only, for earlier# versions see below);compress lz4-v2;push "compress lz4-v2"# For compression compatible with older clients use comp-lzo# If you enable it here, you must also# enable it in the client config file.# 和客户端保持一致comp-lzo# The maximum number of concurrently connected# clients we want to allow.;max-clients 100# 最大连接数max-clients 100# It's a good idea to reduce the OpenVPN# daemon's privileges after initialization.## You can uncomment this out on# non-Windows systems.;user nobody;group nobody# The persist options will try to avoid# accessing certain resources on restart# that may no longer be accessible because# of the privilege downgrade.persist-keypersist-tun# Output a short status file showing# current connections, truncated# and rewritten every minute.# 开启状态日志status openvpn-status.log# By default, log messages will go to the syslog (or# on Windows, if running as a service, they will go to# the "\Program Files\OpenVPN\log" directory).# Use log or log-append to override this default.# "log" will truncate the log file on OpenVPN startup,# while "log-append" will append to it. Use one# or the other (but not both).;log openvpn.log;log-append openvpn.log# 日志路径log /var/log/openvpn.log# Set the appropriate level of log# file verbosity.## 0 is silent, except for fatal errors# 4 is reasonable for general usage# 5 and 6 can help to debug connection problems# 9 is extremely verboseverb 3# Silence repeating messages. At most 20# sequential messages of the same message# category will be output to the log.;mute 20# Notify the client that when the server restarts so it# can automatically reconnect.# explicit-exit-notify 1# LDAP认证plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so "/etc/openvpn/auth/ldap.conf"# 关闭证书认证client-cert-not-required 2.7 修改openvpn-ldap-auth的配置文件：1vi /etc/openvpn/auth/ldap.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;LDAP&gt; # LDAP server URL #更改为AD服务器的ip URL ldap://172.20.20.10:389 # Bind DN (If your LDAP server doesn't support anonymous binds) # BindDN uid=Manager,ou=People,dc=example,dc=com #更改为域管理的dn,可以通过ldapsearch进行查询,-h的ip替换为服务器ip，-d换为管理员的dn，-b为基础的查询dn，*为所有 #ldapsearch -LLL -x -h 172.16.76.238 -D "administrator@xx.com" -W -b "dc=xx,dc=com" "*" BindDN " cn=administrator,cn=users,dc=dealeasy,dc=local" # Bind Password # Password SecretPassword #域管理员的密码 Password passwd # Network timeout (in seconds) Timeout 15 # Enable Start TLS TLSEnable no # Follow LDAP Referrals (anonymously) FollowReferrals no # TLS CA Certificate File #TLSCACertFile /usr/local/etc/ssl/ca.pem # TLS CA Certificate Directory #TLSCACertDir /etc/ssl/certs # Client Certificate and key # If TLS client authentication is required #TLSCertFile /usr/local/etc/ssl/client-cert.pem #TLSKeyFile /usr/local/etc/ssl/client-key.pem # Cipher Suite # The defaults are usually fine here # TLSCipherSuite ALL:!ADH:@STRENGTH&lt;/LDAP&gt; &lt;Authorization&gt; # Base DN #查询认证的基础dn BaseDN " ou=de,dc=dealeasy,dc=local" # User Search Filter #SearchFilter "(&amp;(uid=%u)(accountStatus=active))" #其中sAMAccountName=%u的意思是把sAMAccountName的字段取值为用户名，后面“memberof=CN=myvpn,DC=xx,DC=com”指向要认证的vpn用户组，这样任何用户使用vpn，只要加入这个组就好了 SearchFilter "( (&amp;(sAMAccountName=%u)(memberof=cn=myvpn,ou=vpn,ou=de,DC=dealeasy,DC=local" # Require Group Membership RequireGroup false # Add non-group members to a PF table (disabled) #PFTable ips_vpn_users &lt;Group&gt; #BaseDN "ou=Groups,dc=example,dc=com" #SearchFilter "(|(cn=developers)(cn=artists))" #MemberAttribute uniqueMember # Add group members to a PF table (disabled) #PFTable ips_vpn_eng BaseDN " ou=vpn,ou=de,dc=dealeasy,dc=local" SearchFilter " (cn=myvpn)" MemberAttribute "member" &lt;/Group&gt;&lt;/Authorization&gt; 例子：openldap配置示例使用了上面安装的openvpn-auth-ldap认证插件client-cert-not-required不再需要客户端证书，将改为使用OpenLDAP中的用户认证注意:上面的ldap.conf中RequireGroup true以及Group的配置实际我们期望是必须是LDAP中的名称为vpn的组下的用户才可以登录VPN。但根据这个ISSUE https://github.com/threerings/openvpn-auth-ldap/issues/7，当前2.0.3的openvpn-auth-ldap不支持。因此如果只想限制LDAP中某些用户可以使用VPN的话，只能设置`RequireGroup false，然后可以在SearchFilter中做一些文章，比如(&amp;(uid=%u)(ou=vpn))`即只有用户的ou字段为vpn的才可以。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;LDAP&gt; # LDAP server URL URL ldap://192.168.0.129:389 # Bind DN (If your LDAP server doesn't support anonymous binds) # BindDN uid=Manager,ou=People,dc=example,dc=com BindDN cn=admin,dc=zhjx,dc=com Password ******* # Bind Password # Password SecretPassword # Network timeout (in seconds) Timeout 15 # Enable Start TLS # TLSEnable yes TLSEnable no # Follow LDAP Referrals (anonymously) # FollowReferrals yes FollowReferrals no # TLS CA Certificate File # TLSCACertFile /usr/local/etc/ssl/ca.pem # TLS CA Certificate Directory # TLSCACertDir /etc/ssl/certs # Client Certificate and key # If TLS client authentication is required #TLSCertFile /usr/local/etc/ssl/client-cert.pem #TLSKeyFile /usr/local/etc/ssl/client-key.pem # Cipher Suite # The defaults are usually fine here # TLSCipherSuite ALL:!ADH:@STRENGTH&lt;/LDAP&gt;&lt;Authorization&gt; # Base DN BaseDN "ou=People,dc=zhjx,dc=com" # User Search Filter # SearchFilter "(&amp;(uid=%u)(accountStatus=active))" SearchFilter "(uid=%u)" # Require Group Membership RequireGroup false #RequireGroup true # Add non-group members to a PF table (disabled) #PFTable ips_vpn_users &lt;Group&gt; BaseDN "ou=Group,dc=zhjx,dc=com" SearchFilter "cn=openvpn" MemberAttribute memberUid # Add group members to a PF table (disabled) #PFTable ips_vpn_eng &lt;/Group&gt;&lt;/Authorization&gt; 2.8 拷贝/etc/openvpn/key目录下的ca.crt证书，以备客户端使用。注：客户端使用ca.crt和客户端配置文件即可正常使用openvpn了 2.8.1 配置客户端配置文件注意:客户端ca证书使用的是和服务端一样的ca证书，都是创建服务端证书用到的ca文件 1$ vi client.ovpn 123456789101112clientdev tunproto tcp //注意协议，跟服务器保持一致remote 172.20.20.25 1194 //xx.xx.com替换为你的服务器ipresolv-retry infinitenobindpersist-keypersist-tunca ca.crtauth-user-pass //客户端使用账户密码登陆的选项，用于客户端弹出认证用户的窗口comp-lzoverb 3 例子：openldap方式客户端配置示例，与上方server版示例对照 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141############################################### Sample client-side OpenVPN 2.0 config file ## for connecting to multi-client server. ## ## This configuration can be used by multiple ## clients, however each client should have ## its own cert and key files. ## ## On Windows, you might want to rename this ## file so it has a .ovpn extension ################################################ Specify that we are a client and that we# will be pulling certain config file directives# from the server.# 服务端类型client# Use the same setting as you are using on# the server.# On most systems, the VPN will not function# unless you partially or fully disable# the firewall for the TUN/TAP interface.# dev tap# 与服务端一致dev tun# Windows needs the TAP-Win32 adapter name# from the Network Connections panel# if you have more than one. On XP SP2,# you may need to disable the firewall# for the TAP adapter.#dev-node MyTap# Are we connecting to a TCP or# UDP server? Use the same setting as# on the server.# 与服务端一致proto tcp#proto udp# The hostname/IP and port of the server.# You can have multiple remote entries# to load balance between the servers.# 连接地址与端口remote XXX.imwork.net 1194#remote my-server-2 1194# Choose a random host from the remote# list for load-balancing. Otherwise# try hosts in the order specified.;remote-random# Keep trying indefinitely to resolve the# host name of the OpenVPN server. Very useful# on machines which are not permanently connected# to the internet such as laptops.resolv-retry infinite# Most clients don't need to bind to# a specific local port number.nobind# Downgrade privileges after initialization (non-Windows only);user nobody;group nobody# Try to preserve some state across restarts.persist-keypersist-tun# If you are connecting through an# HTTP proxy to reach the actual OpenVPN# server, put the proxy server/IP and# port number here. See the man page# if your proxy server requires# authentication.;http-proxy-retry # retry on connection failures;http-proxy [proxy server] [proxy port #]# Wireless networks often produce a lot# of duplicate packets. Set this flag# to silence duplicate packet warnings.;mute-replay-warnings# SSL/TLS parms.# See the server config file for more# description. It's best to use# a separate .crt/.key file pair# for each client. A single ca# file can be used for all clients.# ldap方式只使用ca证书ca ca.crt# cert client.crt# key client.key# Verify server certificate by checking that the# certicate has the correct key usage set.# This is an important precaution to protect against# a potential attack discussed here:# http://openvpn.net/howto.html#mitm## To use this feature, you will need to generate# your server certificates with the keyUsage set to# digitalSignature, keyEncipherment# and the extendedKeyUsage to# serverAuth# EasyRSA can do this for you.# remote-cert-tls server# If a tls-auth key is used on the server# then every client must also have the key.#tls-auth ta.key 1# Select a cryptographic cipher.# If the cipher option is used on the server# then you must also specify it here.# Note that v2.4 client/server will automatically# negotiate AES-256-GCM in TLS mode.# See also the ncp-cipher option in the manpage# 与服务的保持一致cipher AES-256-CBC# Enable compression on the VPN link.# Don't enable this unless it is also# enabled in the server config file.# 与服务的保持一致comp-lzo# Set log file verbosity.verb 3# Silence repeating messages;mute 20# auth-user-pass# 开启账户密码认证ns-cert-type serverauth-user-pass 2.9 开启路由转发1vi /etc/sysctl.conf 修改参数 net.ipv4.ip_forward = 1（默认为0，修改成1 表示开启路由转发，如果默认是空内容，请自行加上-腾讯云貌似就是空的） 重启sysctl生效路由转发： 1sysctl -p 2.9.1 配置防火墙及路由转发策略：1234567iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE #做NAT转换iptables -A INPUT -p TCP --dport 1194 -j ACCEPT #OpenVPN服务端口，可自定义，不可冲突service iptables saveservice iptables restart 2.10 开启 HTTP代理连接openvpn服务器通过此方法可以解决跨运营商连接中断及缓慢的问题，首先需要有一台三网HTTP代理服务器。公司使用的是景安的云服务器做HTTP代理。 参考资料：http://www.365mini.com/page/18.htm 1、 在景安云服务器部署代理软件CCProxy,并开启HTTP代理，端口443（可自定义）。 2、 在客户端配置文件添加如下语句。 1http-proxy 122.114.100.229 443 配置完成。可以正常连接使用。 3 用到的文件下载：=easy-rsa-master.zip=lzo-2.09.tar.gz=openvpn-2.3.11.tar.gz=openvpn2.3.exe=openvpn-auth-ldap-2.0.3-1.1.x86_64.rpm=openvpn-auth-ldap-2.0.3-9.fc17.i686.rpm=openvpn安装说明.docx]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>openvpn</tag>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7部署vpn服务器_PPTP]]></title>
    <url>%2F2018%2F09%2F10%2FCentos7%E9%83%A8%E7%BD%B2vpn%E6%9C%8D%E5%8A%A1%E5%99%A8-PPTP%2F</url>
    <content type="text"><![CDATA[1 环境 系统：centos 7.2 1511 2 详细步骤2.1 验证内核是否加载了MPPE模块：内核的MPPE模块用于支持Microsoft Point-to-Point Encryption。Windows自带的VPN客户端就是使用这种加密方式，主流的Linux Desktop也都有MPPE支持。其实到了我们这个内核版本，默认就已经加载了MPPE，只需要使用下面命令验证一下，显示MPPE ok即可： 1modprobe ppp-compress-18 &amp;&amp; echo MPPE is ok 2.2 安装所需的软件包：1yum install –y ppp pptpd iptables 2.3 修改ppp配置文件配置ppp需要编辑它的两个配置文件，一个是option（选项）文件，一个是用户账户文件。首先编辑option文件： 12345vi /etc/ppp/options.pptpdms-dns 8.8.8.8 #去掉“＃”，设置ＤＮＳms-dns 8.8.4.4 1vi /etc/ppp/chap-secrets 这个文件非常简单，其中用明文存储VPN客户的用户名、服务名称（option.pptpd文件name pptpd vpn的服务器名字）、密码和IP地址范围，每行一个账户： 123username1 pptpd passwd1 *username2 pptpd passwd2 * 其中第一第三列分别是用户名和密码；第二列应该和上面的文件/etc/ppp/options.pptpd中name后指定的服务名称一致；最后一列限制客户端IP地址，星号表示没有限制。 2.4 修改pptpd配置文件123456789vi /etc/pptpd.confoption /etc/ppp/options.pptpdlogwtmplocalip 192.168.0.1remoteip 192.168.0.207-217 其中option选项指定使用/etc/ppp/options.pptpd中的配置；logwtmp表示使用WTMP日志。 后面两行是比较重要的两行。VPN可以这样理解，Linux客户端使用一个虚拟网络设备ppp0（Windows客户端也可以理解成VPN虚拟网卡），连接到服务器的虚拟网络设备ppp0上，这样客户端就加入了服务器端ppp0所在的网络。localip就是可以分配给服务器端ppp0的IP地址，remoteip则是将要分配给客户端ppp0（或者虚拟网卡）的。 这两项都可以是多个IP，一般localip设置一个IP就行了，remoteip则视客户端数目，分配一段IP。其中remoteip的IP段需要和localip的IP段一致。 localip和remoteip所处的IP段可以随意些指定，但其范围内不要包含实际网卡eth0的IP地址。一般情况下，使用上面配置文件中的配置就好使了，你需要做的只是把192.168.0.207-217这个IP区间修改成你喜欢的192.168.0.a-b，其中1&lt;a&lt;b&lt;255。 2.5 打开内核的IP转发功能：12345vi /etc/sysctl.conf找到其中的行：net.ipv4.ip_forward = 0 修改为： 1net.ipv4.ip_forward = 1 然后执行下面命令使上述修改生效： 1sysctl –p 2.6 配置iptables防火墙放行和转发规则：最后，还需要配置防火墙。这里配置防火墙有三个目的：一是设置默认丢弃规则，保护服务器的安全；二是放行我们允许的数据包，提供服务；三是通过配置nat表的POSTROUTING链，增加NAT使得VPN客户端可以通过服务器访问互联网。总之我们的原则就是，只放行我们需要的服务，其他统统拒绝。 首先介绍跟PPTP VPN相关的几项： 允许GRE(Generic Route Encapsulation)协议，PPTP使用GRE协议封装PPP数据包，然后封装成IP报文 放行1723端口的PPTP服务 放行状态为RELATED,ESTABLISHED的入站数据包（正常提供服务的机器上防火墙应该都已经配置了这一项） 放行VPN虚拟网络设备所在的192.168.0.0/24网段与服务器网卡eth0之间的数据包转发 为从VPN网段192.168.0.0/24转往网卡eth0的出站数据包做NAT 如果你其他的防火墙规则已经配置好无需改动，只需要增加上述相关VPN相关的规则，那么执行下面几条命令即可（第三条一般不用执行，除非你原来的防火墙连这个规则都没允许，但是多执行一遍也无妨）： 1234567891011iptables -A INPUT -p gre -j ACCEPTiptables -A INPUT -p tcp -m tcp --dport 1723 -j ACCEPTiptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPTiptables -A FORWARD -s 192.168.0.0/24 -o eth0 -j ACCEPTiptables -A FORWARD -d 192.168.0.0/24 -i eth0 -j ACCEPTiptables -t nat -A POSTROUTING -s 192.168.0.0/24 -o eth0 -j MASQUERADE 3 问题总结使用centos 6.5部署vpn pptp 遇到连接错误，总是连接不上 3.1 日志错误1：12345678910[root@localhost log]#vi /var/log/messages Jun 13 14:00:25 localhost pptpd[9248]: CTRL: Client 101.69.242.170 control connection startedJun 13 14:00:25 localhost pptpd[9248]: CTRL: Starting call (launching pppd, opening GRE)Jun 13 14:00:25 localhost pppd[9249]: Warning: can't open options file /root/.ppprc: Permission deniedJun 13 14:00:25 localhost pppd[9249]: The remote system is required to authenticate itselfJun 13 14:00:25 localhost pppd[9249]: but I couldn't find any suitable secret (password) for it to use to do so.Jun 13 14:00:25 localhost pppd[9249]: (None of the available passwords would let it use an IP address.)Jun 13 14:00:25 localhost pptpd[9248]: GRE: read(fd=6,buffer=6124a0,len=8196) from PTY failed: status = -1 error = Input/output error, usually caused by unexpected termination of pppd, check option syntax and pppd logsJun 13 14:00:25 localhost pptpd[9248]: CTRL: PTY read or GRE write failed (pty,gre)=(6,7)Jun 13 14:00:25 localhost pptpd[9248]: CTRL: Client 101.69.242.170 control connection finished 相关配置文件 123456789101112131415161718192021[root@localhost ~]# more /etc/pptpd.conf |grep -v ^#option /etc/ppp/options.pptpddebug /var/log/pptpd.loglocalip 10.10.1.20remoteip 10.10.1.30-254[root@localhost ~]# more /etc/ppp/options.pptpd|grep -v ^#name pptpdrefuse-paprefuse-chaprefuse-mschaprequire-mschap-v2require-mppe-128ms-dns 221.228.255.1ms-dns 223.6.6.6proxyarplocknobsdcomp novjnovjccompnologfd 我使用的centos 6.5 64bit ，相关安装包如下： 123456789101112131415161718[root@localhost ~]# rpm -qa |grep "ppp*"libreport-plugin-rhtsupport-2.0.9-19.el6.centos.x86_64device-mapper-event-libs-1.02.79-8.el6.x86_64ppl-0.10.2-11.el6.x86_64pptpd-1.4.0-3.el6.x86_64abrt-addon-ccpp-2.0.8-21.el6.centos.x86_64device-mapper-1.02.79-8.el6.x86_64device-mapper-event-1.02.79-8.el6.x86_64tcp_wrappers-7.6-57.el6.x86_64cloog-ppl-0.15.7-1.2.el6.x86_64snappy-1.1.0-1.el6.x86_64kernel_ppp_mppe-1.0.2-3dkms.noarchpptp-1.7.2-8.1.el6.x86_64device-mapper-libs-1.02.79-8.el6.x86_64tcp_wrappers-libs-7.6-57.el6.x86_64cpp-4.4.7-4.el6.x86_64device-mapper-persistent-data-0.2.8-2.el6.x86_64ppp-2.4.5-5.el6.x86_64 现在不知道问题出在什么地方，麻烦各位帮忙看看，谢谢！！！ 已经解决，修改配置 123options.pptpd把require-mschap-v2require-mppe-128 这两行注释掉即可。如下图： 这里是关闭加密，跳过加密。此方法修改过没有用处。 3.2 错误2：1234567891011121314151617181920212223242526272829[root@webserver ~]# sysctl -pnet.ipv4.ip_forward = 1net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 0kernel.core_uses_pid = 1net.ipv4.tcp_syncookies = 1error: "net.bridge.bridge-nf-call-ip6tables" is an unknown keyerror: "net.bridge.bridge-nf-call-iptables" is an unknown keyerror: "net.bridge.bridge-nf-call-arptables" is an unknown keykernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296You have mail in /var/spool/mail/root 版本没错但是发现转发有错 加载模块然后重启解决 1234567891011121314151617181920212223[root@webserver ~]# modprobe bridge[root@webserver ~]# lsmod | grep bridgebridge 79078 0 stp 2218 1 bridgellc 5546 2 bridge,stp[root@webserver ~]# service pptpd restartShutting down pptpd: [ OK ]Starting pptpd: [ OK ]Warning: a pptpd restart does not terminate existing connections, so new connections may be assigned the same IP address and cause unexpected results. Use restart-kill to destroy existing connections during a restart. 此处是重新加载bridge模块，重新加载了，没有作用，后来实验，发现是否重新加载，都不影响vpn使用,最终就没管他，不知道是否有影响。 最终如何解决的，其实也没有做什么修改，就是重新yum remove –y ppp* pptp*后，重新yum install –y ppp* pptp* iptables* 后，多次重启，和重设置vpn帐号，又可以用了，I don’t know why. 最终解决方案： 后来发现一个问题，当客户端开启防火墙的时候就能连接，当关闭防火墙的时候就无法连接报错。I don’t know why.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>vpn</tag>
        <tag>pptp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双机热备Heartbeat+drbd]]></title>
    <url>%2F2018%2F09%2F07%2F%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87Heartbeat-drbd%2F</url>
    <content type="text"><![CDATA[1 环境 系统：Centos7.2 1151 软件：drbd84-utils-8.9.5-1.el7.elrepo.x86_64 主服务器：172.20.20.66 备服务器：172.20.20.35 虚拟IP（virtual_ip）：172.20.20.237 修改主服务器名称：masterNode 修改备服务器名称： backupNode 主备服务器添加下方地址到/etc/hosts文件： 172.20.20.66 masterNode 172.20.20.35 backupNode 2 简单说明主备服务器分别创建一个分区，挂载到新创建的目录drbd中，通过drbd目录完成数据同步。 只有主服务器挂载目录，备服务器不用挂载。 备服务器要挂载目录，需要先把主服务器设置成secondary（备机），备服务器设置成primary（主机），然后挂载到目录。 3 添加硬盘并挂载3.1 查询硬盘信息（如果使用的是新加硬盘，可以在最下方看到硬盘信息，例子使用本地空闲空间） 例如： 1[root@bogon /]# fdisk -l 1[root@bogon /]# df –h 3.2 Fdisk创建分区1[root@bogon /]# fdisk /dev/sda 注：下图设置分区格式可以省略，后面会进行格式化。 3.3 查看硬盘信息1[root@bogon /]# fdisk –l 3.4 Partprobe刷新分区表（注：最好每做一步遇到找不到问题都刷新一下）1[root@bogon /]# partprobe 4 安装配置drbd4.1 YUM安装drbd12345[root@localhost mapper]# rpm --import http://elrepo.org/RPM-GPG-KEY-elrepo.org[root@localhost mapper]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm[root@localhost mapper]# yum -y install drbd84-utils kmod-drbd84 4.1.1 问题1二次安装存在YUM安装失败，需要源码安装，源码安装包在http://oss.linbit.com/drbd/ 下载。 Rpm包在https://pkgs.org/centos-7/elrepo-x86_64/kmod-drbd84-8.4.6-1.el7.elrepo.x86_64.rpm.html https://pkgs.org/centos-7/elrepo-x86_64/drbd84-utils-8.9.5-1.el7.elrepo.x86_64.rpm/download/ 下载。 1[root@drbd2 home]# rpm -Uvh http://elrepo.org/linux/elrepo/el7/x86_64/RPMS/drbd84-utils-8.9.5-1.el7.elrepo.x86_64.rpm [root@htest2 home]# rpm -Uvh http://elrepo.org/linux/elrepo/el7/x86_64/RPMS/kmod-drbd84-8.4.6-1.el7.elrepo.x86_64.rpm 4.2 配置global_common.conf编辑全局配置： 1vi /etc/drbd.d/global_common.conf 确保文件中包含有下内容： 123456789101112131415global &#123; usage-count yes; &#125; common &#123; net &#123; protocol C; # 使用协议C.表示收到远程主机的写入确认后,则认为写入完成. &#125; &#125; 当然，还可以有其它配置，这是最基本的。 4.3 配置r0资源：创建r0资源： 注：r0可以随便命名。 1vi /etc/drbd.d/r0.res 写入文件内容： 123456789101112131415161718192021222324252627resource r0&#123; on masterNode&#123; device /dev/drbd1; #逻辑设备的路径 disk /dev/sda3; #物理设备 address 192.168.58.128:7788; meta-disk internal; &#125; on backupNode&#123; device /dev/drbd1; disk /dev/sda3; address 192.168.58.129:7788; meta-disk internal; &#125; &#125; 需要把上面用到的防火墙7788端口打开，这个端口是自定义的，如果嫌麻烦可以直接关掉防火墙。 注：masterNode/ backupNode是主机名字，根据实际情况书写。 说明： device 是自定义的物理设备的逻辑路径 disk 是磁盘设备，或者是逻辑分区 address 是机器监听地址和端口 meta-disk 这个还没弄明白，看到的资料都是设为：internal（局域网） 4.4 建立resource123modprobe drbd //载入 drbd 模块 lsmod | grep drbd //确认 drbd 模块是否载入 12345dd if=/dev/zero of=/dev/sda2 bs=1M count=100 //把一些资料塞到 sda3 內 (否则 create-md 时会报错) drbdadm create-md r0 //建立 drbd resource drbdadm up r0 // #启用资源 4.4.1 我遇到的问题1：12345[root@backupNode /]# drbdadm up r0 1: Failure: (104) Can not open backing device. Command 'drbdsetup attach 1 /dev/sda2 /dev/sda3 internal' terminated with exit code 原因是我之前已经挂在了/dev/sda3，需要先卸载/dev/sda3设备，解决办法: 1umount /dev/sda2 问题解决. 4.4.2 我遇到的问题2：123[root@backupNode /]# drbdadm create-md r0 ‘r0‘ not defined in your config (for this host). 原因是drbd.conf配置文件计算机名和本机的计算机名不一致。 修改配置文件计算机名。 问题解决。 4.4.3 我遇到的问题3：12345[root@htest2 drbd.d]# drbdadm create-md r0drbd.d/r0.res:3: no minor given nor device name contains a minor numberdrbd.d/r0.res:9: no minor given nor device name contains a minor number r0.res中drbd资源必须含有数字，猜测与分区模块有重名。 问题解决。 4.4.4 我遇到的问题4在centos 6.4上部署时，yum安装完成后加载模块报错。 1234正确安装drbd模块后，使用modprobe进行加载# modprobe drbdFATAL: Module drbd not found.出现如上错误 原因：这是因为系统默认的内核并不支持此模块，所以需要更新内核更新内核的方法：可以用 yum install kernel* 方式来更新。如果你要节约点时间的话可以只更新一下的几个包： kernel-devel kernel kernel-headers 更新后，记得要重新启动操作系统！！！ &gt; 注：以上每一步骤，都需要在主备服务器上进行配置设置。 4.5 设置Primary Node将masterNode设为主服务器(primary node)，在masterNode上执行： 1[root@backupNode /]# drbdadm primary --force r0 查看drbd状态： 1234567891011[root@backupNode /]# cat /proc/drbd version: 8.4.1 (api:1/proto:86-100) GIT-hash: 91b4c048c1a0e06777b5f65d312b38d47abaea80 build by root@masterNode, 2012-05-27 18:34:27 1: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- ns:4 nr:9504584 dw:9504588 dr:1017 al:1 bm:576 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0 已经变成了主服务器。 4.6 创建DRBD文件系统上面已经完成了/dev/drbd1的初始化，现在来把/dev/drbd1格式化成ext4格式的文件系统,在masterNode上执行： 1[root@masterNode /]# mkfs.ext3 /dev/drbd1 输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243mke2fs 1.41.12 (17-May-2010) 文件系统标签= 操作系统:Linux 块大小=4096 (log=2) 分块大小=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 589824 inodes, 2358959 blocks 117947 blocks (5.00%) reserved for the super user 第一个数据块=0 Maximum filesystem blocks=2415919104 72 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632 正在写入inode表: 完成 Creating journal (32768 blocks): 完成 Writing superblocks and filesystem accounting information: 完成 This filesystem will be automatically checked every 21 mounts or 180 days, whichever comes first. Use tune2fs -c or -i to override. 然后，将/dev/drbd1挂载到之前创建好的/drbd目录： 1[root@masterNode /]# mount /dev/drbd1 /drbd 现在只要把数据写入/drbd目录，drbd即会立刻把数据同步到backupNode的/dev/sda2分区上了。 4.7 DRBD同步测试1、首先，在主服务器上先将设备卸载，同时将主服务器降为备用服务器： 123[root@masterNode /]# umount /dev/drbd1 [root@masterNode /]# drbdadm secondary r0 2、然后，登录备用服务器，将备用服务器升为主服务器，同时挂载drbd1设备到 /drbd目录： 1234567 [root@masterNode /]# ssh backup Last login: Sun May 27 19:57:17 2012 from masternode [root@backupNode ~]# drbdadm primary r0 [root@backupNode ~]# mount /dev/drbd1 /drbd/ 3、最后，进入/drbd目录，就可以看到之前在另外一台机器上放入的数据了，如果没有看到，说明同步失败！ 4.8 设置开机启动 注：设置DRBD开机启动，需要手动设置主备级别，找到方法后补充。 1[root@drbd2 rc.d]# systemctl enable drbd 5 安装配置keepalived(使用Heartbeat代替，只做了解，功能和heartbeat一样，Heartbeat自带drbd检测脚本) 5.1 YUM安装keepalivedCentos 7 自带 keepalived，可以直接YUM安装。 1[root@drbd1 rc3.d]# yum install -y keepalived 5.2 配置keepalived.conf1[root@drbd1 /]# vi /etc/keepalived/keepalived.conf 5.2.1 主服务器配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; //定义vrrp实例 state MASTER //主节点 ，备用节点为BACKUP interface enp2s0 //绑定的网卡 virtual_router_id 51 //ID,默认就行 .同一实例下virtual_router_id必须相同 priority 100 //优先级，备用节点设置为比100小的值 advert_int 1 //MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS //验证类型和密码 auth_pass 1111 &#125; virtual_ipaddress &#123; 172.20.20.237 //设置虚拟IP地址，可以多个 &#125;&#125; 5.2.2 备服务器配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; //定义vrrp实例 state BACKUP //备节点 ，主用节点为MASTER interface ens192 //绑定的网卡 virtual_router_id 51 //ID,默认就行 .同一实例下virtual_router_id必须相同 priority 50 //优先级，备用节点设置为比100小的值 advert_int 1 //MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS //验证类型和密码 auth_pass 1111 &#125; virtual_ipaddress &#123; 172.20.20.237 //设置虚拟IP地址，可以多个 &#125;&#125; 5.3 启动服务1[root@drbd1 /]# systemctl start keepalived 5.4 查看网卡信息 当启动keepalived后，查看网卡信息，主服务器会显示挂载虚拟IP172.20.20.237成功，备服务器没有挂载虚拟IP，如果主备服务器同时显示挂载虚拟Ip,则属于脑裂故障，需要继续调试。如果主服务器keepalived程序挂掉后，备用服务器会自动升级为主服务器，并挂载虚拟ip，主服务器恢复后恢复主从关系。 1[root@drbd1 /]# ip a 5.5 验证测试 1、在主服务器上新建一个网页，内容为 172.20.20.66 2、在备用服务器上新建一个网页，内容为 172.20.20.35 3、启动主备服务器的http服务和Keepalived服务 4、通过浏览数，输入虚拟IP地址 172.20.20.237 页面显示为 `172.20.20.66` 5、关闭主服务器的Keepalived服务，通过浏览器输入IP地址172.20.20.237 页面显示为 `172.20.20.35` 6、再次启动主服务器的Keepalived服务，通过浏览器输入IP地址172.20.20.237 页面显示为 `172.20.20.66` 5.6 设置开机启动服务1[root@drbd2 rc.d]# systemctl enable keepalived 6 安装配置heartbeat6.1 创建用户和组先创建用户和组，否则glue安装报错。 123# groupadd -g 200 haclient #创建GID为200的用户组# useradd -g haclient -u 200 -s /bin/false -M hacluster #创建用户（具体什么意思没懂） YUM安装所需组件 1# yum install -y libtool libtool-ltdl-devel glib2-devel libxml2-devel libxml2 bzip2-devel e2fsprogs-devel libuuid-devel libxslt-devel asciidoc docbook-style-xsl libnet 6.2 安装glue源码安装heartbeat之前首先得源码安装glue。 下载glue：http://www.linux-ha.org/wiki/Download 进入源码目录安装：./autogen.sh 生成配置文件：./configure 编译安装：make &amp;&amp; make install ./autogen.sh安装报错及解决办法 1234567报错信息：You must have autoconf installed to compile the cluster-glue package.Download the appropriate package for your system,or get the source tarball at: ftp://ftp.gnu.org/pub/gnu/autoconf/ 解决办法： 1yum install libtool 6.3 安装agents不安装agents的话，在安装Heartbeat时会报错，具体什么错误忘记记录了。 下载http://www.linux-ha.org/wiki/Download 12345./autogen.sh./configuremake &amp;&amp; make install 6.4 安装heartbeat： 安装glue/agents/heartbeat,编译安装路径最好默认，自定义的话有报错。 下载heartbeat：http://www.linux-ha.org/wiki/Downloads 进入源码目录生成配置文件：`./ConfigureMe configure --disable-swig --disable-snmp-subagent` 编译安装：make &amp;&amp; make install 没有按照上述操作可能遇到的错误： 1234567libtoolize: putting libltdl files in `libltdl'. libtoolize: `COPYING.LIB' not found in `/usr/share/libtool/libltdl./bootstrap exiting due to error (sorry!). 解决办法： 1yum install libtool-ltdl-devel 6.5 同步时间同步两台节点的时间 1234567# rm -rf /etc/localtime# \cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime# yum install -y ntp# ntpdate -d cn.pool.ntp.org 6.6 配置主节点heartbeat总共有三个文件需要配置:ha.cf 监控配置文件haresources 资源管理文件authkeys 心跳线连接加密文件 配置文件位置 /usr/share/doc/heartbeat/ 1# cp ha.cf haresources authkeys /etc/ha.d/ #拷贝配置文件到/etc/ha.d 6.6.1 修改ha.cf1234567891011121314151617181920212223242526272829303132333435debugfile /var/log/ha-debug #用于记录 heartbeat 的调试信息logfile /var/log/ha-log #指名heartbeat的日志存放位置。logfacility local0 #如果未定义上述的日志文件,那么日志信息将送往local0(对应的#/var/log/messages),如果这 3 个日志文件都未定义,那么 heartbeat 默认情况下 将在/var/log 下建立 ha-debug 和 ha-log 来记录 相应的日志信息。 #bcast eth1 #指明心跳使用以太网广播方式，并且是在eth1接口上进行 广播。（实际操作中没有指明。）keepalive 2 #发送心跳报文的间隔,默认单位为秒,如果你毫秒为单位, 那么需要在后面跟 ms 单位,如 1500ms 即代表 1.5s deadtime 30 #指定若备用节点在30秒内没有收到主节点的心跳信 号，则立即接管主节点的服务资源。 warntime 10 #指定心跳延迟的时间为10秒。当10秒钟内备份节点不能接收到主节点的心跳信号时，就会往日志中写入一 个警告日志，但此时不会切换服务。发出最后的心跳 警告 信息的间隔。 initdead 120 #在某些系统上，系统启动或重启之后需要经过一段时间 网络才能正常工作，该选项用于解决这种情况产生 的时 间间隔。取值至少为deadtime的两倍。 udpport 694 #设置广播/单播通信使用的端口，694为默认使用的端口号 ucast eth0 192.168.60.132 #采用网卡eth0的udp单播来组织心跳，后面跟的 IP地址应为双机对方的IP地址。 对方IPauto_failback off #用来定义当主节点恢复后，是否将服务自动切回。如果不想启用，请设置为off，默认为on。heartbeat的两台主机分别为主节点和备份节点。主节点在正常情况下占用资源并运行所有的服务，遇到故障时把资源交给备份节点并由备份节点运行服务。在该选项设为on的情况下，一旦主节点恢复运行，则自动获取资源并取代备份节点；如果该选项设置为off，那么当主节点恢复后，将变为备份节点，而原来的备份节点成为主节点。根据实际情况，不能开启，选择offnode drbd1 #主节点主机名，可以通过命令"uanme -n"查看。 node drbd2 #备用节点主机名。 #ping 192.168.60.1 #选择ping的节点，ping节点选择的越好，HA集群就 越强壮，可以选择固定的路由器作为ping节点，或者 应用服务器但是 最好不要选择集群中的成员作为ping 节点，ping节点 仅仅用来测试网络连接。如果指定了多个ping节点如ping 192.168.0.1 192.168.0.2那么只有当能ping通所有ping节点 时才认为网络是连通的，否则则认为不连通.实际中我没有开通检测。respawn hacluster /usr/libexec/heartbeat/ipfail #该选项是可选配置， 意思 是以 hacluster 这 个用户身份运行/usr/lib/heartbeat/ ipfail 这个 插件 respawn列出与heartbeat一起启动和关闭的 进 程，该进程一般是和heartbeat集成的插件，这些 进程遇到故障可以自动重新启动。最常用的进程是 ipfail，此进程用于检测和处理网络故障，需要配合 ping或者ping_group语句,其中指定的ping node 来检测网络的连通性。在v2版本中，ipfail和crm有 冲突，不能同时使用，如果启用crm的情况下，可以 使用pingd插件代替ipfailapiauth ipfail gid=haclient uid=hacluster #指定对客户端 api 的访问控制,缺省为不可 访问，这里指定了 有权限访问 ipfail用户和组。(使用前边创建的用户和组)#apiauth default gid=haclient （实际中没有开启此项设置，因为创建的用户和组就是默认用户和组的名字）当配置了默认用户组时，其他所有api授权命令失效且该用户组中的成员可以访问任何api库如果不在ha.cf文件指定api库的访问权限，则默认的访问权限如下service default apiauthipfailuid=haclusterccmgid=haclientpinggid=haclientcl_statusgid=haclientlha-snmpagentuid=rootcrmuid=hacluster 6.6.2 修改haresources1234567drbd1 IPaddr::192.168.84.132/24/eno16777736 drbddisk::r0 Filesystem::/dev/drbd1::/drbddrbd1是主节点主机名，主备配置文件都一样。解说：node1 IPaddr::192.168.60.200/24/eth0/ Filesystem::/dev/sdb5::/webdata::ext3 httpd cp.sh db2::db2inst1 其中，node1是HA集群的主节点，IPaddr为heartbeat自带的一个执行脚步，Heartbeat首先将执行/etc/ha.d/resource.d/IPaddr 192.168.60.200/24 start的操作，也就是虚拟出一个子网掩码为255.255.255.0，IP为192.168.60.200的地址。此IP为Heartbeat对外提供服务的网络地址，同时指定此IP使用的网络接口为eth0。接着，Heartbeat将执行共享磁盘分区的挂载操作，"Filesystem::/dev/sdb5::/webdata::ext3"相当于在命令行下执行mount操作，即"mount -t ext3 /dev/sdb5 /webdata"，然后启动httpd，接下列执行cp.sh这个脚本文件之后以db2inst1的身份启动db2。其中cp.sh必须放置在/etc/ha.d/resource.d/或/etc/init.d/目录中。 注意主节点和备份节点中资源文件haresources要完全一样。 6.6.3 修改authkeys 此配置文件必须设置权限为600（固定的） 1Chmod 600 /etc/ha.d/authkeys 123456789auth 1 1 crc #2 sha1 sha1_any_password #3 md5 md5_any_password authkeys文件用于设定Heartbeat的认证方式，共有3种可用的认证方式，即crc、md5和sha1。3种认证方式的安全性依次提高，但是占用的系统资源也依次增加。如果Heartbeat集群运行在安全的网络上，可以使用crc方式；如果HA每个节点的硬件配置很高，建议使用sha1，这种认证方式安全级别最高；如果是处于网络安全和系统资源之间，可以使用md5认证方式。这里我们使用crc认证方式.需要说明的一点是：无论auth后面指定的是什么数字，在下一行必须作为关键字再次出现，例如指定了"auth 6"，下面一定要有一行"6 认证类型"。 最后确保这个文件的权限是600（即-rw-------）。 6.6.4 对配置文件进行软链接在/usr/etc/ha.d/目录创建软链接，在部署中通过查看日志文件发现是通过此目录运行软件的，具体没搞明白，网上说拷贝的/etc/ha.d目录就行了，不知道为什么，为了省事，干脆创建软链接，这样就没问题了。此功能好用。 12345678910111213ln -sv /etc/ha.d/shellfuncs /usr/etc/ha.d/shellfuncsln -sv /etc/ha.d/ha.cf /usr/etc/ha.d/ha.cfln -sv /etc/ha.d/authkeys /usr/etc/ha.d/authkeysln -sv /etc/ha.d/resource.d /usr/etc/ha.d/resource.dln -sv /etc/ha.d/haresources /usr/etc/ha.d/haresourcesln -sv /etc/ha.d/harc /usr/etc/ha.d/harcln -sv /etc/ha.d/rc.d /usr/etc/ha.d/rc.d 复制ocf文件，如果没有，启动的时候会报错 1234567cp /usr/lib/ocf/lib/heartbeat/ocf-binaries /usr/lib/ocf/resource.d/heartbeat/.ocf-binariescp /usr/lib/ocf/lib/heartbeat/ocf-directories /usr/lib/ocf/resource.d/heartbeat/.ocf-directoriescp /usr/lib/ocf/lib/heartbeat/ocf-returncodes /usr/lib/ocf/resource.d/heartbeat/.ocf-returncodescp /usr/lib/ocf/lib/heartbeat/ocf-shellfuncs /usr/lib/ocf/resource.d/heartbeat/.ocf-shellfuncs 这一块没有用到，发现默认情况下目录里面就存在这些文件，此处作为参考。 备用节点配置文件和主节点配置文件基本一样，只有一个地方有区别。就是 ucast eth0 192.168.60.132 IP地址是对方的IP 7 启动程序顺序第一步：启动主备节点drbd服务 1[root@drbd1 ha.d]# drbdadm up r0 第二步：设置升级主节点为primary，并挂载到目录 123[root@drbd1 ha.d]# drbdadm primary r0[root@drbd1 ha.d]# mount /dev/drbd1 /drbd/ 第三步：启动主备节点Heartbeat服务 1[root@drbd1 ha.d]# service heartbeat start 等待主备节点连接并配置完成，可以通过ip a 命令查看虚拟ip是否生效。以下环境部署完成。在进行主节点维护中，恢复的话必须手动操作，避免出错。 8 附件链接: http://pan.baidu.com/s/1mibp7Uo 密码: y99z]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>双机</tag>
        <tag>drbd</tag>
        <tag>heartbeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay服务器迁移(32位迁移至64位)]]></title>
    <url>%2F2018%2F09%2F07%2FLiferay%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%81%E7%A7%BB-32%E4%BD%8D%E8%BF%81%E7%A7%BB%E8%87%B364%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[1 原运行环境： 系统：CentOS 6.4 32位 Liferay:32位版本 bitnami-liferay-6.2-7-linux-installer 2 新运行环境：系统：CentOS7.2 64位 Liferay:由原软件目录直接拷贝到新系统，修改java、apache-tomcat目录为64位安装版目录，迁移数据和配置文件：/opt/liferay/apache-tomcat/temp /opt/liferay-6.2-7/apache-tomcat.bak/webapps/liferay/WEB-INF/classes/ portal-ext.properties 3 新系统运行32位软件需安装64位系统运行32位软件，需要安装32位运行库等。 123yum install glibc.i686yum install xulrunner.i686 安装后访问平台流程插件需要重新安装，否则流程工作台无法正常工作及显示。 Liferay 集成公司OA流程控制台方法： 用管理员登录liferay，进入应用程序管理页面，上传流程控制台插件，然后安装。如下图/1571819590646.png) 安装完后，可以在程序管理页面看到安装后的插件，如下图。/1571819598879.png) 切换到站点首页，添加页面内容，然后把任务提醒拉到要显示的位置，如下图：/1571819606383.png) ssh登录到服务器，将/opt/liferay-6.2-7/apache-tomcat/webapps/workflow-portlet目录备份更名，然后解压workflow-portlet20170208.tar.gz到/opt/liferay-6.2-7/apache-tomcat/webapps/workflow-portlet目录，如下图。 解压后，重启liferay。 /1571819614329.png) 在服务器上再次部署流程控制台web服务器tomcat。将tomcat程序复制到服务器/opt目录下，然后调整tomcat的端口，避免和liferay的tomcat端口冲突。修改/opt/tomcat7/conf/server.xml 文件。 修改位置： /1571819623044.png)/1571819631617.png)/1571819640024.png) tomcat配置好后，找研发人员要最新版本的流程开发程序包。文件名：DeWorkFlow.war。 将此文件放在/opt/tomcat7/webapps 目录下。如下图：/1571819648992.png) 备注：DeWorkFlow.war插件公司内部提供。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>liferay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay中文乱码(tomcat中文乱码)]]></title>
    <url>%2F2018%2F09%2F07%2FLiferay%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81-tomcat%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[/1571819676411.png)/1571819683261.png)/1571819691542.png)都是修改tomcat配置文件server.xml,添加URIEncoding=”UTF-8”解决问题。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
        <tag>liferay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK部署]]></title>
    <url>%2F2018%2F09%2F07%2FELK%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1 环境 系统：centos 7.2 x86_64 -1511 JDK: 1.8.0_111 ELK: elasticsearch kibana logstash https://www.elastic.co/cn/downloads 2 Elasticsearch部署下载并解压elasticsearch-5.3.2.tar.gz 1tar -zxvf elasticsearch-5.3.2.tar.gz -C /home/elk 2.1 配置elasticsearch.yml1[root@elk-node1 src]# cat /etc/elasticsearch/elasticsearch.yml | grep -v “#” 123456789101112131415161718192021cluster.name: elk #自定义集群名，相同集群内的节点设置相同的集群名node.name: elk-node1 #自定义节点名，建议统一采用节点hostnamepath.data: /var/lib/elasticsearch #data存储路径，可不取消注释，默认即此路径path.logs: /var/log/elasticsearch #log存储路径，可不取消注释，默认即此路径network.host: 0.0.0.0 #es监听地址，采用”0.0.0.0”，允许所有设备访问http.port: 9200 #es监听端口，可不取消注释，默认即此端口discovery.zen.ping.unicast.hosts: ["elk-node1", "elk-node2"] #集群节点发现列表，也可采用ip的形式discovery.zen.minimum_master_nodes: 2 #集群可做master的最小节点数#以下两项设置es5.x版本的head插件可以访问eshttp.cors.enabled: true #开启跨域访问支持，默认为falsehttp.cors.allow-origin: "*" #跨域访问允许的域名地址，使用正则表达式 2.2 配置head插件2.2.1 下载并配置nodejsnodejs官网：https://nodejs.org/en/ 12345678910[root@elk-node1 ~]# cd /home/elk[root@elk-node1 elk]# wget https://nodejs.org/dist/v6.9.5/node-v6.9.5-linux-x64.tar.xz[root@elk-node1 elk]# xz -d node-v6.10.2-linux-x64.tar.xz[root@elk-node1 elk]# tar -xvf node-v6.10.2-linux-x64.tar -C /usr/local/[root@elk-node1 ~]# ln -s /usr/local/node-v6.10.2-linux-x64/bin/node /usr/bin/node[root@elk-node1 ~]# ln -s /usr/local/node-v6.10.2-linux-x64/bin/npm /usr/bin/npm[root@elk-node1 ~]# node -vv6.10.2[root@elk-node1 ~]# npm -v3.10.10 2.2.2 安装head插件安装grunt 12[root@elk-node1 ~]# npm install -g grunt-cli[root@elk-node1 ~]# ln -s /usr/local/node-v6.10.2-linux-x64/lib/node_modules/grunt-cli/bin/grunt /usr/bin/grunt #grunt是一个方便的构建工具，可以进行打包压缩、测试、执行等等的工作，5.x里的head插件就是通过grunt启动的； #”-g”参数代表全局安装，一般安装到nodejs的”lib/node_modules”目录下；不带参数”-g”，则是本地安装，一般安装到运行npm命令时所在的目录，这里就需要安装到head插件目录； #为grunt命令建软链接，方便全局执行，或加入环境变量；； #如果镜像速度不理想，可提前切到国内的镜像，在安装grunt-cli前执行：npm config set registry https://registry.npm.taobao.org。 2.2.3 下载并配置head12[root@elk-node1 ~]# cd /home/elk/elasticsearch[root@elk-node1 elasticsearch]# git clone git://gitee.com/mobz/elasticsearch-head.git #配置elasticsearch.yml文件允许head插件跨域访问es，请见上述章节。 2.2.4 安装head12[root@elk-node1 ~]# cd /var/lib/elasticsearch/elasticsearch-head/[root@elk-node1 elasticsearch]# npm install # 安装完成后可能有一些报错，不影响使用，报错原因未知，本人对nodejs不了解，github上有相近的问题，但并未解决。解决方法如下： (1) 查看报错信息”Error: Cannot find module ‘/var/lib/elasticsearch/elasticsearch-head/node_modules/phantomjs-prebuilt/install.js’”，未找到” phantomjs-prebuilt/install.js”文件； (2) 采取比较土的办法，将完整的”phantomjs-prebuilt/”目录上传到相应位置，重新执行”npm install”，无报错。 # 同时有3个警告信息，忽略即可，其中“npm WARN elasticsearch-head@0.0.0 license should be a valid SPDX license expression”警告信息可做如下处理：http://www.itdadao.com/articles/c15a1031952p0.html 即修改”./elasticsearch-head”目录下“package.json”文件第17行的””Apache2“ “为”Apache-2.0“，涉及到开源软件与其他合作类软件的使用声明。 # 如果没有全局安装grunt二进制程序，可在”elasticsearch-head”目录下执行”npm install grunt --save“或” npm install grunt-cli“。 2.2.5 配置app.js和Guuntfile.js1234[root@elk-node1 ~]# cd /home/elk/elasticsearch-5.3.2/elasticsearch-head/[root@elk-node1 elasticsearch-head]# cd _site/[root@elk-node1 _site]# cp app.js app.js.bak[root@elk-node1 _site]# vim app.js #在4328行将原”http://localhost:9200“修改为”http://172.20.20.29:9200“，否则head插件不能获取节点状态信息。 123[root@elk-node1 ~]# cd /home/elk/elasticsearch-5.3.2/elasticsearch-head/[root@elk-node1 elasticsearch-head]# cp Gruntfile.js Gruntfile.js.bak[root@elk-node1 elasticsearch-head]# vim Gruntfile.js #在93行新增”hostname: 0.0.0.0’, “，确保能被访问。 2.3 启动并验证 创建elk用户 使用elk用户启动elasticsearch 1su - elk -c "/home/elk/elasticsearch-5.3.2/bin/elasticsearch &amp;" 启动head 12cd /home/elk/elasticsearch-5.3.2/elasticsearch-head/grunt server &amp; #启动head插件，需要到head目录下 访问https://172.20.20.29:9200 访问https://172.20.20.29:9100 3 Logstash部署3.1 下载logstash下载logstash-5.3.2 1tar -zxvf logstash-5.3.2 -C /home/elk 3.2 配置logstahs3.2.1 配置文件默认配置文件： /home/elk/logstash-5.3.2/config/logstash.yml 12345[root@elk-node1 ~]# cd /etc/logstash/[root@elk-node1 logstash]# cat logstash.yml | grep -v "#"path.data: /var/lib/logstashpath.config: /etc/logstash/conf.dpath.logs: /var/log/logstash *#其中默认配置已经明确数据，日志，logstash pipeline实例文件的存储位置，保持默认即可； #其中“http.host”参数指定数据输入主机，默认为localhost；“http.port”参数指定数据输入端口，默认为9600～9700（每实例占用1个），此验证暂时不做变更* 3.2.2 pipeline文件根据默认配置，pipeline实例文件默认应放置于/home/elk/logstash-5.3.2/config/目录，此时目录下无实例文件，可根据实际情况新建实例，以处理本机log信息为例，如下： 123456789101112131415161718[root@elk-node1 config]# vim messages.confinput &#123; file &#123; path =&gt; "/var/log/messages" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["172.20.20.29:9200","172..20.20.29:9200"] index =&gt; "messages-%&#123;+YYYY.MM.dd&#125;" &#125; stdout &#123;# codec =&gt; rubydebug &#125;&#125;[root@elk-node1 conf.d]# cd ..[root@elk-node1 logstash]# chown -R logstash:logstash conf.d/[root@elk-node1 ~]# chmod 644 /var/log/messages *#pipeline #pipeline实例文件以”input”，”output”，”filter”等区域组成，前两者为必选项； #”input”与”output”利用插件进行数据输入与输出，如这里”file”即输入插件，“elasticseach”与“stdout”即输出插件； #在各插件内再具体定义行为，如”input”定义了数据源，“elasticseach”定义了输出节点与数据输出的索引与格式； #“codec =&gt; rubydebug”会产生大量的debug文件至message（也可重定向到其他位置），此处注释掉； #请注意权限，这里message常规权限是400，logstash无法读取，如果无法调用，在logstash的启动日志中会有相应的记录。* 3.3 启动并验证1[root@elk-node1 ~]# /home/elk/logstash-5.3.2/bin/logstash -f /home/elk/logstash-5.3.2/conf.d/messages.conf &amp; &amp; 是在后台启动 1[root@elk-node1 ~]# netstat -tunlp 浏览器访问http://172.20.20.29:9100 4 Kibana部署4.1 解压下载的kibana1tar -zxvf kibana-5.3.2-linux-x86_64.tar.gz -C /home/elk 4.2 配置kibana123456[root@elk-node1 ~]# cd /home/elk/kibana-5.3.2-linux-x86_64/config[root@elk-node1 kibana]# vim kibana.yml[root@elk-node1 kibana]# cat /etc/kibana/kibana.yml | grep -v "^$" | grep -v "#"server.port: 5601 #默认即5601server.host: "0.0.0.0" #允许被访问elasticsearch.url: "http://172.20.20.29:9200" #es地址与端口 4.3 启动并验证1/home/elk/kibana-5.3.2-linux-x86_64/bin/kibana &amp; &amp; 在后台启动 1netstat -tunlp | grep 5601 4.4 kibana展示浏览器访问：http://172.20.20.29:5601 需要在“Index name or pattern”处创建索引名，elassticsearch中并没有以”logstash-*”命名的索引，则不能创建，新建索引对应logstash的pipeline输出插件定义的”index”，即”messages-*”，如下： 在“Index name or pattern”处填写入正确的索引名字，”@timestamp”会自动填充，点击”create”创建，见到如下界面即索引创建完成； 在“Discover”页面，可以搜索与浏览Elasticsearch中的数据，默认搜索的是最近15分钟的数据，可以自定义选择时间。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay+drbd双机热备部署]]></title>
    <url>%2F2018%2F09%2F07%2FLiferay-drbd%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1 主节点配置 1.1 添加硬盘为主服务器添加一块硬盘50G。 1.2 创建分区1fdisk /dev/sdb 为新硬盘创建分区sdb1,大小50G.分区格式lvm 1.3 创建物理卷1pvcreate /dev/sdb1 1.4 将物理卷添加入卷组1vgextend vg_cosliferay /dev/sdb1 卷组名通过vgdisplay查看。vg_cosliferay是卷组名。 1.5 创建逻辑卷1lvcreate -L 50G -n drbdlv vg_cosliferay 创建大小为50G，名字为drbdlv的逻辑卷。 1.6 安装drbd1yum -y install drbd84-utils kmod-drbd84 1.6.1 配置global_common.conf编辑全局配置： 1vi /etc/drbd.d/global_common.conf 确保文件中包含有下内容： 123456789101112global &#123; usage-count yes; &#125; common &#123; net &#123; protocol C; # 使用协议C.表示收到远程主机的写入确认后,则认为写入完成. &#125; &#125; 当然，还可以有其它配置，这是最基本的。 1.6.2 配置r0资源：创建r0资源： 注：r0可以随便命名。 1vi /etc/drbd.d/r0.res 写入文件内容： 123456789101112131415161718192021222324252627resource r0&#123; on masterNode&#123; device /dev/drbd1; #逻辑设备的路径 disk /dev/sda3; #物理设备 address 192.168.58.128:7788; meta-disk internal; &#125; on backupNode&#123; device /dev/drbd1; disk /dev/sda3; address 192.168.58.129:7788; meta-disk internal; &#125; &#125; 需要把上面用到的防火墙7788端口打开，这个端口是自定义的，如果嫌麻烦可以直接关掉防火墙。 注：masterNode/ backupNode是主机名字，根据实际情况书写。 说明： 1234567device 是自定义的物理设备的逻辑路径disk 是磁盘设备，或者是逻辑分区address 是机器监听地址和端口meta-disk 这个还没弄明白，看到的资料都是设为：internal（局域网） 1.6.3 建立resource123modprobe drbd //载入 drbd 模块 lsmod | grep drbd //确认 drbd 模块是否载入 12345dd if=/dev/zero of=/dev/sda2 bs=1M count=100 //把一些资料塞到 sda3 內 (否则 create-md 时会报错) drbdadm create-md r0 //建立 drbd resource drbdadm up r0 // #启用资源 2 备节点配置把以上主节点的操作在备节点上操作一次，操作过程完全一样。 2.1 设置Primary Node将masterNode设为主服务器(primary node)，在masterNode上执行： 1[root@backupNode /]# drbdadm primary --force r0 查看drbd状态： 123456789[root@backupNode /]# cat /proc/drbd version: 8.4.1 (api:1/proto:86-100) GIT-hash: 91b4c048c1a0e06777b5f65d312b38d47abaea80 build by root@masterNode, 2012-05-27 18:34:27 1: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- ns:4 nr:9504584 dw:9504588 dr:1017 al:1 bm:576 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0 已经变成了主服务器。 2.2 创建DRBD文件系统上面已经完成了/dev/drbd1的初始化，现在来把/dev/drbd1格式化成ext4格式的文件系统,在masterNode上执行： 1 [root@masterNode /]# mkfs.ext3 /dev/drbd1 3 安装配置keepalived3.1 YUM安装keepalived1[root@drbd1 rc3.d]# yum install -y keepalived 3.2 配置keepalived.conf1[root@drbd1 /]# vi /etc/keepalived/keepalived.conf 3.2.1 主服务器配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; //定义vrrp实例 state MASTER //主节点 ，备用节点为BACKUP interface enp2s0 //绑定的网卡 virtual_router_id 51 //ID,默认就行 .同一实例下virtual_router_id必须相同 priority 100 //优先级，备用节点设置为比100小的值 advert_int 1 //MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS //验证类型和密码 auth_pass 1111 &#125; virtual_ipaddress &#123; 172.20.20.237 //设置虚拟IP地址，可以多个 &#125;&#125; 3.2.2 备服务器配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; //定义vrrp实例 state BACKUP //备节点 ，主用节点为MASTER interface ens192 //绑定的网卡 virtual_router_id 51 //ID,默认就行 .同一实例下virtual_router_id必须相同 priority 50 //优先级，备用节点设置为比100小的值 advert_int 1 //MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS //验证类型和密码 auth_pass 1111 &#125; virtual_ipaddress &#123; 172.20.20.237 //设置虚拟IP地址，可以多个 &#125;&#125; 3.3 启动服务1[root@drbd1 /]# systemctl start keepalived 4 问题Liferay配置drbd双击热备中，在备机启动程序时，遇到以下问题。问题根本原因是原主机是32位系统安装32位程序，在备机64位系统运行32位程序没有运行库而报错。 4.1 问题1：123456789101112131415[09:55:50][root@liferay-backup opt]# /opt/liferay-6.2-7/ctlscript.sh start[09:55:50]/opt/liferay-6.2-7/mysql/bin/my_print_defaults: /opt/liferay-6.2-7/mysql/bin/my_print_defaults.bin: /lib/ld-linux.so.2: bad ELF interpreter: 没有那个文件或目录[09:55:50]/opt/liferay-6.2-7/mysql/bin/my_print_defaults:行12: /opt/liferay-6.2-7/mysql/bin/my_print_defaults.bin: 成功[09:55:50]/opt/liferay-6.2-7/mysql/bin/my_print_defaults: /opt/liferay-6.2-7/mysql/bin/my_print_defaults.bin: /lib/ld-linux.so.2: bad ELF interpreter: 没有那个文件或目录[09:55:50]/opt/liferay-6.2-7/mysql/bin/my_print_defaults:行12: /opt/liferay-6.2-7/mysql/bin/my_print_defaults.bin: 成功[09:55:50]161221 09:55:53 mysqld_safe Logging to '/opt/liferay-6.2-7/mysql/data/mysqld.log'.[09:55:50]161221 09:55:53 mysqld_safe Starting mysqld daemon with databases from /opt/liferay-6.2-7/mysql/data[09:55:50]161221 09:55:53 mysqld_safe mysqld from pid file /opt/liferay-6.2-7/mysql/data/mysqld.pid ended 解决办法： 1[root@liferay-backup opt]# yum install glibc.i686 安装后又报错： 123456789101112131415[root@liferay-backup data]# /opt/liferay-6.2-7/ctlscript.sh start[10:30:47]libgcc_s.so.1 must be installed for pthread_cancel to work[10:30:47]libgcc_s.so.1 must be installed for pthread_cancel to work[10:30:47]161221 10:30:50 mysqld_safe Logging to '/opt/liferay-6.2-7/mysql/data/mysqld.log'.[10:30:47]161221 10:30:50 mysqld_safe Starting mysqld daemon with databases from /opt/liferay-6.2-7/mysql/data[10:30:48]libgcc_s.so.1 must be installed for pthread_cancel to work[10:30:48]/opt/liferay-6.2-7/mysql/bin/mysqld_safe: 行 165: 19283 已放弃 (吐核)LD_LIBRARY_PATH=/opt/liferay-6.2-7/mysql/lib\:/opt/liferay-6.2-7/mysql/lib\:/opt/liferay-6.2-7/sqlite/lib\:/opt/liferay-6.2-7/apache2/lib\:/opt/liferay-6.2-7/common/lib\: nohup /opt/liferay-6.2-7/mysql/bin/mysqld --defaults-file=/opt/liferay-6.2-7/mysql/my.cnf --basedir=/opt/liferay-6.2-7/mysql --datadir=/opt/liferay-6.2-7/mysql/data --plugin-dir=/opt/liferay-6.2-7/mysql/lib/plugin --user=mysql --lower-case-table-names=1 --log-error=/opt/liferay-6.2-7/mysql/data/mysqld.log --pid-file=/opt/liferay-6.2-7/mysql/data/mysqld.pid --socket=/opt/liferay-6.2-7/mysql/tmp/mysql.sock --port=3306 &lt; /dev/null &gt;&gt; /opt/liferay-6.2-7/mysql/data/mysqld.log 2&gt;&amp;1[10:30:48]161221 10:30:51 mysqld_safe mysqld from pid file /opt/liferay-6.2-7/mysql/data/mysqld.pid ended 没有安装32位运行库 直接安装yum install glibc.i686无法成功，系统已安装64位运行库，无法在安装。需要 123456789Linux的有些软件需要32位运行库才能运行，如Dr.com客户端等yum在线安装： sudo yum install xulrunner.i686或者： sudo yum install ia32-libs.i686ubuntu下： sudo apt-get install ia32-libs 最终没有采用此方法，使用的方法为本博客内的Rsync+sersync实时双向同步 参考资料1：http://blog.csdn.net/tmy257/article/details/41013985 参考资料2：http://blog.csdn.net/attagain/article/details/17026433]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>双机</tag>
        <tag>liferay</tag>
        <tag>drbd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bitnami删除页面右下角Manage图标]]></title>
    <url>%2F2018%2F09%2F07%2Fbitnami%E5%88%A0%E9%99%A4%E9%A1%B5%E9%9D%A2%E5%8F%B3%E4%B8%8B%E8%A7%92Manage%E5%9B%BE%E6%A0%87%2F</url>
    <content type="text"><![CDATA[Bitnami info page How to remove the banner(如何删除旗帜) 如果你想删除的旗帜，你只需要使用工具bnconfig,执行下面命令。 Linux and OS X Systems 1sudo /opt/bitnami/apps/软件目录/bnconfig --disable_banner 1 然后重启APache: 1$ sudo /opt/bitnami/ctlscript.sh restart apache Windows Systems 进入算计路径：: cd apps\软件目录 1bnconfig.exe --disable_banner 1]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>bitnami</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bitnami软件修改URL方法]]></title>
    <url>%2F2018%2F09%2F07%2Fbitnami%E8%BD%AF%E4%BB%B6%E4%BF%AE%E6%94%B9URL%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1 更改首页页面 1install/apache2/conf/bitnami/bitnami.conf 修改bitnami.con文件，更改首页页面。 1234567&lt;VirtualHost _default_:80&gt; DocumentRoot "/opt/mediawiki-1.26.2-2/apache2/htdocs" &lt;Directory "/opt/mediawiki-1.26.2-2/apache2/htdocs"&gt; 改为&lt;VirtualHost _default_:80&gt; DocumentRoot "/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs" &lt;Directory "/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs"&gt; 2 更改应用访问地址后缀例如：http://172.20.20.37/moodle 改为http://172.20.20.37 1Install/apps/moodle/conf/httpd-prefix.conf 修改httpd-prefix.conf文件，更改应用页面更目录 1234567891011Alias /moodle/ "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login/"Alias /moodle "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login/" 改为 DocumentRoot "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login" #Alias /moodle/ "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login/"#Alias /moodle "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login/" 3 更改应用根目录。一般情况下，只需修改bitnami.con文件就可以达到URL后缀显示的问题，个别情况下需要修改apps下的httpd-prefix.conf文件。 还可以修改后缀名称。 例如：http://172.20.20.21/moodle 改为 http://172.20.20.21/mmkk 也是修改httpd-prefix.conf文件。 4 修改端口号：参考上面80位置1install/apache2/conf/bitnami/bitnami.conf 如果还不行，同时修改apache配置文件端口号。 1install/apache2/conf/httpd.conf]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>bitnami</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK部署过程中问题总结]]></title>
    <url>%2F2018%2F09%2F07%2FELK%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1 错误1： 启动elasticsearch-5.3.0报错 不能使用root用户启动。新建用户elk，用户组elk. 2 错误2：使用elk启动报错 改变elasticsearch文件夹所有者到当前用户 3 错误3：修改network-name : 0.0.0.0,后报错 注意： es启动要求提高一些系统参数配置，否则会报错 12345678910111213141516a. 增大vm.max_map_count到至少262144$ sudo vim /etc/sysctl.conf添加 vm.max_map_count=262144$ sudo sysctl -pb. 增大文件句柄数至少 65536 ulimit -a查看$ sudo vim /etc/security/limits.conf* soft nofile 65536* hard nofile 65536 4 错误4：启动elasticsearch报错 elasticsearch-head · 不能放在elasticsearch的plugins、modules 目录下 · 不能使用 elasticsearch-plugin install 移动elasticsearch-head,到别的目录，启动成功。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsyslog+loganalyzer+evtsys搭建日志服务器]]></title>
    <url>%2F2018%2F09%2F07%2Frsyslog-loganalyzer-evtsys%E6%90%AD%E5%BB%BA%E6%97%A5%E5%BF%97%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1 环境要求 将所有的服务器的日志都集中保存在一台rsyslog日志服务器中，mysql作为数据库，loganalyzer将服务器的日志数据进行分析展现，evtsys是windows服务器提交日志的客户端软件。 实验环境如下： 节点 OS IP 网络 rsyslog Server Centos6.4 192.168.200.106 单网卡 Linux Client Centos6.4 192.168.200.106 单网卡 Windows Client Windows2008x64 192.168.200.31 单网卡 2 rsyslog+loganalyzer日志服务器的部署2.1 配置rsyslog日志服务器因为rsyslog要把日志存到mysql中，所以要有mysql服务器，还要有rsyslog配置文件加载。 2.1.1 连接mysql的模块1[iyunv@rsyslog ~]# yum -y install rsyslog mysql-server rsyslog-mysql 2.1.2 配置数据库12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[iyunv@rsyslog ~]# rpm -ql rsyslog-mysql #首先查看rsyslog-mysql安装生成了那些文件/lib64/rsyslog/ommysql.so/usr/share/doc/rsyslog-mysql-5.8.10/usr/share/doc/rsyslog-mysql-5.8.10/createDB.sql #此sql文件就是需要导入到数据库中的数据文件#[iyunv@rsyslog ~]# service mysqld start #启动mysqld服务[iyunv@rsyslog ~]# mysql #连接mysqlWelcome to the MySQL monitor. Commands end with ; or g.Your MySQL connection id is 2Server version: 5.1.73 Source distributionCopyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or 'h' for help. Type 'c' to clear the current input statement.mysql&gt;mysql&gt;mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || test |+--------------------+3 rows in set (0.00 sec) #此时，只有3个库#mysql&gt; source /usr/share/doc/rsyslog-mysql-5.8.10/createDB.sql; #导入rsyslog的数据文件mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || Syslog || mysql || test |+--------------------+4 rows in set (0.01 sec)mysql&gt; use Syslog; #Syslog即是记录日志文件的数据库Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+------------------------+| Tables_in_Syslog |+------------------------+| SystemEvents || SystemEventsProperties |+------------------------+2 rows in set (0.00 sec)##接下来，即是为rsyslog服务器授权。此处一定是rsyslog服务器的IP#如果写成各服务器的IP，那就错了mysql&gt; grant all on Syslog.* to 'syslogroot'@'127.0.0.1' identified by 'syslogpass';Query OK, 0 rows affected (0.00 sec)mysql&gt; grant all on Syslog.* to 'syslogroot'@'192.168.200.106' identified by 'syslogpass';Query OK, 0 rows affected (0.04 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; qBye 2.1.3 修改rsyslog日志服务器配置文件1234567891011121314[iyunv@rsyslog ~]# grep -v "^$" /etc/rsyslog.conf | grep -v "^#"$ModLoad imuxsock$ModLoad imklog$ModLoad imudp #加载udp的模块$UDPServerRun 514 #允许接收udp 514的端口传来的日志$ModLoad imtcp #加载tcp的模块$InputTCPServerRun 514 #允许接收tcp 514的端口传来的日志$ModLoad ommysql #加载mysql的模块$ActionFileDefaultTemplateRSYSLOG_TraditionalFileFormat$IncludeConfig /etc/rsyslog.d/*.conf*.* :ommysql:192.168.200.106,Syslog,syslogroot,syslogpass #添加此行，所有设施的所有日志都记录到此数据库服务器的Syslog数据库中，以syslogroot用户，syslogpass密码访问数据库local7.* /var/log/boot.log$template SpiceTmpl,"%TIMESTAMP%.%TIMESTAMP:::date-subseconds% %syslogtag% %syslogseverity-text%:%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%":programname, startswith, "spice-vdagent" /var/log/spice-vdagent.log;SpiceTmpl 2.1.4 修改完成后，重启rsyslog服务123[iyunv@rsyslog ~]# service rsyslog restartShutting down system logger: [ OK ]Starting system logger: [ OK ] 2.2 配置rsyslog客户端2.2.1 修改配置文件123456789[iyunv@mariadb ~]# grep -v "^$" /etc/rsyslog.conf | grep -v "^#"$ModLoad imuxsock # provides support for local system logging (e.g. via logger command)$ModLoad imklog # provides kernel logging support (previously done by rklogd)$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat$IncludeConfig /etc/rsyslog.d/*.conf*.* @192.168.200.106*.* :ommysql:192.168.200.106,Syslog,syslogroot,syslogpass$template SpiceTmpl,"%TIMESTAMP%.%TIMESTAMP:::date-subseconds% %syslogtag% %syslogseverity-text%:%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%":programname, startswith, "spice-vdagent" /var/log/spice-vdagent.log;SpiceTmpl 2.2.2 修改完成后，重启rsyslog服务123[iyunv@rsyslog ~]# service rsyslog restartShutting down system logger: [ OK ]Starting system logger: [ OK ] 2.3 验证客户端日志文件的存放2.3.1 使用logger生成一条日志信息1[iyunv@mariadb ~]# logger -p info "I'm mariadb" 2.3.2 在rsyslog服务器上验证12345678910111213141516171819202122232425262728293031[iyunv@rsyslog ~]# mysqlmysql&gt; use Syslog;mysql&gt; select * from SystemEventsG*************************** 279. row ***************************ID: 279CustomerID: NULLReceivedAt: 2014-08-13 20:07:39DeviceReportedTime: 2014-08-13 20:07:40Facility: 1Priority: 6FromHost: mariadbMessage: I'm mariadb #我在做的时候，就是因为第二部的1的（2）中mysql授权时，写的是客户端IP，导致这里获取不到数据。NTSeverity: NULL #因此，在数据库授权的时候，要授权的是rsyslog日志服务器的IPImportance: NULLEventSource: NULLEventUser: NULLEventCategory: NULLEventID: NULLEventBinaryData: NULLMaxAvailable: NULLCurrUsage: NULLMinUsage: NULLMaxUsage: NULLInfoUnitID: 1SysLogTag: root:EventLogType: NULLGenericFileName: NULLSystemID: NULLprocessid:checksum: 0279 rows in set (0.00 sec) 到这里，rsyslog日志服务器就部署完成了，但此时日志处于rsyslog日志服务器的mysql数据库中，并不方便查看与管理，所以我们再部署一个loganalyzer日志分析器，来减小日志管理的复杂度。 2.4 部署loganalyzer日志分析器2.4.1 安装LAMP环境123[iyunv@rsyslog ~]# yum -y install httpd php php-mysql php-gd[iyunv@rsyslog ~]# mkdir /var/www/html/loganalyzer/mkdir: created directory `/var/www/html/loganalyzer/' 2.4.2 解压loganalyzer源码包123456789101112[iyunv@rsyslog ~]# tar xf loganalyzer-3.6.5.tar.gz[iyunv@rsyslog ~]# cd loganalyzer-3.6.5[iyunv@rsyslog loganalyzer-3.6.5]#[iyunv@rsyslog loganalyzer-3.6.5]# lsChangeLog contrib COPYING doc INSTALL src[iyunv@rsyslog loganalyzer-3.6.5]# mv src/* /var/www/html/loganalyzer/ #src下是php的网页文件[iyunv@rsyslog loganalyzer-3.6.5]# ls contrib/configure.sh secure.sh[iyunv@rsyslog loganalyzer-3.6.5]# mv contrib/* /var/www/html/loganalyzer/ #contrib目录下的两个脚本，可以打开看看#[iyunv@rsyslog loganalyzer-3.6.5]# cd /var/www/html/loganalyzer/[iyunv@rsyslog loganalyzer]# sh configure.sh #执行脚本 2.4.3 配置httpd修改DocumentRoot网页根目录 1234[iyunv@rsyslog ~]# vim /etc/httpd/conf/httpd.confDocumentRoot "/var/www/html/loganalyzer"#[iyunv@rsyslog ~]# service httpd start 2.4.4 配置httpd和mysql开机启动123[iyunv@rsyslog ~]# chkconfig mysqld on[iyunv@rsyslog ~]# chkconfig httpd on 2.4.5 创建loganalyzer数据库，并授权12345678[iyunv@rsyslog ~]# mysqlEnter password:mysql&gt; create database loganalyzer;Query OK, 1 row affected (0.04 sec)mysql&gt; grant all on loganalyzer.* to dianyi@'192.168.200.106' identified by 'dianyi123';Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 2.4.6 安装loganalyzer2.4.6.1 安装界面 安装完成，enjoy it 2.5 Windows客户端的安装配置2.5.1 安装Syslog日志客户端123456789下载Evtsys软件，分32位和64位安装方法一样。解压后将evtsys.dll、evtsys.exe复制到c:\windows\system32\下。 cd c:\windows\system32evtsys.exe -i -h 192.168.200.31 -p 514net start evtsys 2.2.2 开启系统相关审计打开windows组策略编辑器 (开始-&gt;运行 输入 gpedit.msc) 在windows 设置－&gt; 安全设置 －&gt; 本地策略－&gt;审核策略中，打开你需要记录的windows日志。evtsys会实时的判断是否有新的windows日志产生，然后把新产生的日志转换成syslogd可识别的格式,通过UDP端口发送给syslogd服务器。 3 Mysql数据库的优化的一些想法随着mysql数据库中的记录越来越多，查询的速度会越来越慢。我们可以定期到数据表中删除较早以前的记录来减少数据量从而起到优化的作用。可以定期执行如下SQL语句： 1delete from SystemEvents(数据表名) where ReceivedAt(日期字段名)&lt;curdate() - interval 3 month; //表示删除” SystemEvents”数据表” ReceivedAt”日期字段大于3个月的记录 也可以考虑使用其他数据库，比如具有循环功能的sqllite数据库，不过这样的话就要进行二次开发了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>rsyslog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mediawiki安装及LDAP认证问题]]></title>
    <url>%2F2018%2F09%2F07%2FMediawiki%E5%AE%89%E8%A3%85%E5%8F%8ALDAP%E8%AE%A4%E8%AF%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1 环境 安装环境：CentOS-7-x86_64-DVD-1511 Mediawiki: bitnami-mediawiki-1.26.2-2-linux-x64-installer.run LDAP扩展插件：LdapAuthentication-REL1_26-70ab129.tar.gz 2 安装mediawiki首先安装bitnami-mediawiki-1.26.2-2-linux-x64-installer.run 123#chmod 755 bitnami-mediawiki-1.26.2-2-linux-x64-installer.run //赋予读写权限#./bitnami-mediawiki-1.26.2-2-linux-x64-installer.run //执行安装 安装到最后出现mysql数据库错误： 1234567891011121314151617181920Error: Error running /opt/mediawiki-1.26.2-2/mysql/scripts/myscript.sh/opt/mediawiki-1.26.2-2/mysql ****: FATAL ERROR: please install the followingPerl modules before executing scripts/mysql_install_db:Data::DumperERROR 2002 (HY000): Can't connect to local MySQL server through socket'/opt/mediawiki-1.26.2-2/mysql/tmp/mysql.sock' (2)ERROR 2002 (HY000): Can't connect to local MySQL server through socket'/opt/mediawiki-1.26.2-2/mysql/tmp/mysql.sock' (2)ERROR 2002 (HY000): Can't connect to local MySQL server through socket'/opt/mediawiki-1.26.2-2/mysql/tmp/mysql.sock' (2) （根据提示，发现系统缺少Dumper模块） 根据提示安装Data-Dumper.x86_64 1# yum install perl-Data-Dumper.x86_64 //安装插件模块。 …………… OK, Mediawiki安装成功。 3 开启AD域用户认证登录Ldap认证需要php支持ldap,需要安装ldap支持模块（php-ldap），及开启php的ldap功能(修改php.ini文件)。 &gt; 备注：不安装php-ldap，LdapAuthentication插件不工作 1、 安装php-ldap模块： 1[root@localhost mediawiki]# yum install php-ldap 2、 开启ldap功能即修改php.ini文件，将extension=php_ldap.dll前的分号去掉。 1[root@localhost mediawiki]# vi /opt/mediawiki-1.26.2-2/php/etc/php.ini 3、下载LdapAuthentication插件：LdapAuthentication-REL1_26-70ab129.tar.gz 解压插件到extensions目录： 1# tar -xzf LdapAuthentication-REL1_26-70ab129.tar.gz /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions/ 4、编辑LocalSettings.php配置文件 在LocalSettings.php 末尾添加如下内容： 123456789101112131415161718192021222324252627282930313233343536373839require_once("extensions/LdapAuthentication/LdapAuthentication.php");$wgAuth= new LdapAuthenticationPlugin(); ## 这两行激活插件$wgLDAPDomainNames = array( "dealeasy" ); //域名简写$wgLDAPServerNames = array( "dealeasy"=&gt;"172.20.20.10" ); //域控域名或者ip$wgLDAPSearchStrings = array( "dealeasy"=&gt;"USER-NAME@dealeasy" ); //USER-NAME 不要修改它,默认用户名替代位置，特定字符$wgLDAPBaseDNs = array( "dealeasy"=&gt;"dc=dealeasy,dc=local");$wgLDAPSearchAttributes = array( "dealeasy"=&gt;"sAMAccountName"); //加上这两句就可以把DC上的用户名都同步过来了$wgLDAPUseLocal = false; 是否使用本地用户,ture代表使用，这里写入不使用$wgLDAPUpdateLDAP = false;$wgLDAPMailPassword = false;$wgMinimalPasswordLength = 1;$wgLDAPEncryptionType = array("dealeasy"=&gt;"clear");$wgShowSQLErrors = true;$wgDebugDumpSql = true;$wgShowDBErrorBacktrace = true; //这三行网上找到，因ldap用户登录出现数据库查询错误，添加这三行后，在出现错误时可以在页面上显示错误信息 Ok, LocalSettings.php编辑完成。 下面就可以直接使用域用户登录了。 4 Database错误 报错（前面LocalSettings.php添加后三行才能看到报错详情）： Database error12345678910111213141516171819202122232425262728293031323334353637383940414243444546A database query error has occurred. This may indicate a bug in the software.（翻译：数据库错误 一个数据库查询错误发生。这可能表明软件中的缺陷。）Query:SELECT domain FROM `ldap_domains` WHERE user_id = '4' LIMIT 1Function: LdapAuthenticationPlugin::loadDomainError: 1146 Table 'bitnami_mediawiki.ldap_domains' doesn't exist (localhost:3306)Backtrace:#0 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/db/Database.php(1076): DatabaseBase-&gt;reportQueryError('Table 'bitnami_...', 1146, 'SELECT domain ...', 'LdapAuthenticat...', false)#1 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/db/Database.php(1600): DatabaseBase-&gt;query('SELECT domain ...', 'LdapAuthenticat...')#2 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/db/Database.php(1689): DatabaseBase-&gt;select('ldap_domains', Array, Array, 'LdapAuthenticat...', Array, Array)#3 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions/LdapAuthentication/LdapAuthentication.php(2041): DatabaseBase-&gt;selectRow('ldap_domains', Array, Array, 'LdapAuthenticat...')#4 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions/LdapAuthentication/LdapAuthentication.php(2060): LdapAuthenticationPlugin::loadDomain(Object(User))#5 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions/LdapAuthentication/LdapAuthentication.php(1237): LdapAuthenticationPlugin::saveDomain(Object(User), 'dealeasy')#6 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specials/SpecialUserlogin.php(830): LdapAuthenticationPlugin-&gt;updateUser(Object(User))#7 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specials/SpecialUserlogin.php(958): LoginForm-&gt;authenticateUserData()#8 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specials/SpecialUserlogin.php(341): LoginForm-&gt;processLogin()#9 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specialpage/SpecialPage.php(384): LoginForm-&gt;execute(NULL)#10 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specialpage/SpecialPageFactory.php(553): SpecialPage-&gt;run(NULL)#11 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/MediaWiki.php(281): SpecialPageFactory::executePath(Object(Title), Object(RequestContext))#12 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/MediaWiki.php(714): MediaWiki-&gt;performRequest()#13 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/MediaWiki.php(508): MediaWiki-&gt;main()#14 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/index.php(41): MediaWiki-&gt;run()#15 &#123;main&#125; 最终，通过报错信息Error: 1146 Table &#39;bitnami_mediawiki.ldap_domains&#39; doesn&#39;t exist (localhost:3306)，发现不存在ldap_domains表，即bitnami_mediawiki不存在这个表，后来尝试在phpmyAdmin上为bitnami_mediawiki创建ldap_domains表，并在表内创建user_id和domain列，问题解决。 备注：根据多次实验和页面错误提示，才确认建立user_id和domain列 5 备份备份数据库、配置文件、附件目录images、插件目录extensions 5.1 备份脚本123456789101112131415161718192021Now=$(date +"%Y%m%d%H")File=bitnami_mediawiki-$Now.sqlecho **********start$Now************ &gt;&gt; /mnt/mediawiki-bak/log/$Now.log/opt/mediawiki-1.26.2-2/mysql/bin/mysqldump -uroot -pDe123456 bitnami_mediawiki &gt; /mnt/mediawiki-bak/$Fileecho **********cp LocalSettings.php********** &gt;&gt; /mnt/mediawiki-bak/log/$Now.logrsync -avzrtopgL --progress /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/LocalSettings.php /mnt/mediawiki-bak/LocalSettings.phpecho *********cp images ******** &gt;&gt; /mnt/mediawiki-bak/log/$Now.logrsync -avzrtopgL --progress /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/images /mnt/mediawiki-bak/echo ********cp extensions ********* &gt;&gt; /mnt/mediawiki-bak/log/$Now.logrsync -avzrtopgL --progress /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions /mnt/mediawiki-bak/ &gt;&gt; /mnt/mediawiki-bak/log/$Now.logecho ********end$Now********* &gt;&gt; /mnt/mediawiki-bak/log/$Now.log 6 安装包及插件等 安装包：bitnami_mediawiki（自己下载）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>wiki</tag>
        <tag>Mediawiki</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mediawiki优化]]></title>
    <url>%2F2018%2F09%2F07%2FMediawiki%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1 修改URL后缀 修改网址显示（http://172.20.20.21/mediawiki/首页改为http://172.20.20.21/首页） 修改/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/ LocalSettings.php文件 12345$wgArticlePath = "/mediawiki/$1";改为$wgArticlePath = "/$1"; 再修改/opt/mediawiki-1.26.2-2/apache2/conf/bitnami/bitnami.conf 12345678910111213&lt;VirtualHost _default_:80&gt;DocumentRoot "/opt/mediawiki-1.26.2-2/apache2/htdocs"&lt;Directory "/opt/mediawiki-1.26.2-2/apache2/htdocs"&gt;改为&lt;VirtualHost _default_:80&gt;DocumentRoot "/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs"&lt;Directory "/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs"&gt; 2 修改网站LOGO2.1 右上角logo方法一,替换图片文件。 1/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/resources/assets/wiki.png 方法二，修改LocalSettings.php文件，重新指定LOGO路径 1$wgLogo = "$wgScriptPath/resources/assets/huahe.png"; 2.2 左下角在配置文件LocalSettings.php中加入如下行即unset($wgFooterIcons[&#39;poweredby&#39;]); 3 修改目录悬浮加自动隐藏让目录悬浮起来，并且在不用时让它自动折叠起来，方便阅读和其他操作。自动折叠通过CSS的hover选择器实现，当鼠标移动到目录上时，目录框自动变大。 代码 先进入到下面页面（也许你需要将localhost替换成其他的）： http://localhost/mediawiki/index.php/MediaWiki:Common.css 在此页你可以设置全局的css样式，在这里加入如下： 12345678910111213141516171819202122232425262728293031323334353637383940#toc&#123; display: block; position: fixed; top: 100px; right: 0px; min-width: 100px; max-width: 350px; max-height: 20px; overflow-y: scroll; border: 1px solid #aaa; border-radius: 0 0 1px 1px; -moz-border-radius: 0 0 1px 1px; background: rgba(249,249,249,0.75); padding: 12px; box-shadow: 0 1px 8px #; -webkit-box-shadow: 0 1px 8px #; -moz-box-shadow: 0 1px 8px #;&#125; #toc:hover&#123; display: block; position: fixed; top: 100px; right: 0px; min-width: 100px; max-width: 350px; max-height: 500px; overflow-y: scroll; border: 1px solid #aaa; border-radius: 0 0 1px 1px; -moz-border-radius: 0 0 1px 1px; background: rgba(249,249,249,0.75); padding: 12px; box-shadow: 0 1px 8px #; -webkit-box-shadow: 0 1px 8px #; -moz-box-shadow: 0 1px 8px #; &#125; body &#123; overflow-x: hidden;&#125; 保存，清除浏览器缓存，看看如何！ 简直炫酷！。 关键点解释 1234567891011top: 100px; 目录框到顶部距离right: 0px; 目录框到右边框距离min-width: 100px; 目录框最小宽度max-width: 350px; 目录框最大宽度max-height: 500px; 目录框最大高度background: rgba(249,249,249,0.75); 背景色和透明度 MediaWiki版本 1.20.2 参考http://blog.klniu.com/post/mediawiki-floating-directory-and-scroll/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mediawiki</tag>
        <tag>wiki</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bitnami_Liferay系统设置去掉后缀]]></title>
    <url>%2F2018%2F09%2F07%2FBitnami-Liferay%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE%E5%8E%BB%E6%8E%89%E5%90%8E%E7%BC%80%2F</url>
    <content type="text"><![CDATA[1 修改配置文件 执行以下操作： 12345mv /opt/liferay-6.2-7/apache-tomcat/webapps/ROOT/ /opt/liferay-6.2-7/apache-tomcat/webapps/ROOT.backupmv /opt/liferay-6.2-7/apache-tomcat/webapps/liferay /opt/liferay-6.2-7/apache-tomcat/webapps/ROOTsed -i 's/portal.ctx/#portal.ctx/g' /opt/liferay-6.2-7/apache-tomcat/webapps/ROOT/WEB-INF/classes/portal-ext.properties 打开/opt/liferay-6.2-7/apps/liferay/conf/httpd-app.conf文件做如下修改： 12345678910111213141516171819202122232425262728293031323334353637将&lt;Location /liferay&gt; ProxyPass ajp://localhost:8009/liferay &lt;IfModule pagespeed_module&gt; ModPagespeedDisallow "*" &lt;/IfModule&gt;&lt;/Location&gt; 改为：&lt;Location /&gt; ProxyPass ajp://localhost:8009/ &lt;IfModule pagespeed_module&gt; ModPagespeedDisallow "*" &lt;/IfModule&gt;&lt;/Location&gt; 并且在最后加上：# App url redirectRewriteEngine OnRedirectMatch ^/$ / 2 修改数据库访问bitnami_liferay数据库，找到journalarticle表，用： 1update journalarticle set content=REPLACE (content,'/liferay/document','/document') 对表中content字段的记录进行批量替换 最后手工修改“规章制度”和“文档模板”的链接 现在访问http://172.20.20.58已经可以正常显示了 参考：https://wiki.bitnami.com/Applications/BitNami_Liferay#Manual_Approach]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>liferay</tag>
        <tag>bitnami</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay6.2-ce4官方原版安装简述]]></title>
    <url>%2F2018%2F09%2F07%2FLiferay6-2-ce4%E5%AE%98%E6%96%B9%E5%8E%9F%E7%89%88%E5%AE%89%E8%A3%85%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1 软件： Tomcat: apache-tomcat-7.0.61.tar.gz Java: java version “1.7.0_76” Liferay: liferay-portal-6.2-ce-ga4-20150416163831865.war 2 简单步骤 解压java、tomcat,设置环境变量 创建liferay目录，把java、tomcat放进这个目录 下载liferay依赖包，放到tomcat/lib/ext目录（重要） 新建setenc.sh 新建ROOT.xml 修改catalina.properties、server.xml]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>liferay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bitnami_Liferay自定义路径]]></title>
    <url>%2F2018%2F09%2F07%2FBitnami-Liferayx%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[1 修改文件存储路径 #配置文件portal-ext.properties 修改文件存储路径 1[root@htest2 classes]# vi /opt/liferay-6.2-7/apache-tomcat/webapps/liferay/WEB-INF/classes/portal-ext.properties 移动存储目录/opt/liferay-6.2-7/apps/liferay/data/document_library到/drbd,修改portal-ext.properties内dl.store.file.system.root.dir=参数（没有这项手动添加参数） 1dl.store.file.system.root.dir=/drbd/document_library 2 修改数据库路径移动/opt/liferay-6.2-7/mysql/data/到/home/mysql/data 2.1 修改my.cnf参数1[root@htest2 home]# vi /opt/liferay-6.2-7/mysql/my.cnf 修改my.cnf数据库路径参数 12345datadir=/opt/liferay-6.2-7/mysql/data改为datadir=/home/mysql/data 2.2 修改ctl.sh路径参数1[root@htest2 drbd]# vi /opt/liferay-6.2-7/mysql/scripts/ctl.sh 123456789101112131415161718192021MYSQL_PIDFILE=/ opt/liferay-6.2-7/mysql/data/mysqld.pidMYSQL_START="/opt/liferay-6.2-7/mysql/bin/mysqld_safe --defaults-file=/opt/liferay-6.2-7/mysql/my.cnf --port=3306 --socket=/opt/liferay-6.2-7/mysql/tmp/mysql.sock --datadir=/opt/liferay-6.2-7/mysql/data --log-error=/opt/liferay-6.2-7/mysql/data/mysqld.log --pid-file=$MYSQL_PIDFILE --lower-case-table-names=1 "改为MYSQL_PIDFILE=/home/mysql/data/mysqld.pidMYSQL_START="/opt/liferay-6.2-7/mysql/bin/mysqld_safe --defaults-file=/opt/liferay-6.2-7/mysql/my.cnf --port=3306 --socket=/opt/liferay-6.2-7/mysql/tmp/mysql.sock --datadir=/home/mysql/data --log-error=/home/mysql/data/mysqld.log --pid-file=$MYSQL_PIDFILE --lower-case-table-names=1 " 3 修改插件目录路径(暂不修改)3.1 修改catalina.bat参数移动/opt/liferay-6.2-7/apache-tomcat/temp到/drbd,修改catalina.bat参数 1[htest2@htest2 bin]$ vi /opt/liferay-6.2-7/apache-tomcat/bin/ catalina.bat 12345678910111213if not "%CATALINA_TMPDIR%" == "" goto gotTmpdirset "CATALINA_TMPDIR=%CATALINA_BASE%\temp":gotTmpdir改为if not "%CATALINA_TMPDIR%" == "" goto gotTmpdirset "CATALINA_TMPDIR=\drbd\temp":gotTmpdir 3.2 修改ctl.sh1[htest1@htest1 ~]$ vi /opt/liferay-6.2-7/apache-tomcat/scripts/ctl.sh 修改CATALINA_PID路径 12345CATALINA_PID=/opt/liferay-6.2-7/apache-tomcat/temp/catalina.pid改为CATALINA_PID=/drbd/temp/catalina.pid]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>liferay</tag>
        <tag>bitnami</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OTRS发送邮件设置]]></title>
    <url>%2F2018%2F09%2F07%2FOTRS%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[分三步: (图一)在系统配置里找到Framework -&gt; Core::Sendmail 配置你的 SMTP 信息, 如帐号, 密码, 邮件服务器地址直达地址:https://your.otrs.host/otrs/in … ework 如果发送邮件失败，按图中的位置把邮箱地址写上 (图二)在系统管理找到配置邮件发送地址管理, 增加系统邮件地址直达地址:https://your.otrs.host/otrs/in … dress (图三)在队列调用该邮件地址直达地址:https://your.otrs.host/otrs/in … D%3D1 创建工单, 测试发送邮件, 如果无法发送请抓系统日志来看. 可能遇到的问题, 通知邮件发送不出去, 请使用本网站的搜索功能, 此问题已经讨论过很多了. 如果你找很久(骂前面解决问题后失踪人)才找到, 那么请对有用的答案点赞, 这样就会影响问题的先后排序]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>otrs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JIRA6.3.6部署及破解]]></title>
    <url>%2F2018%2F09%2F05%2FJIRA6-3-6%E9%83%A8%E7%BD%B2%E5%8F%8A%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1 环境： 操作系统：CentOS7.2 1151 数据库：mysql 5.6.35 Java: java 1.8.0_65 平台软件：JIRA 6.3.6 2 安装步骤2.1 安装JDK安装JDK 1.7或1.8，本次部署直接YUM安装，版本1.8. 2.2 安装数据库安装mysql5.6.35,并创建jira数据库。 2.3 下载JIRA下载地址：https://www.atlassian.com/software/jira/download-archives 下载atlassian-jira-6.3.6.tar.gz 2.4 安装JIRA123[root@jira-24 home]# tar zxvf atlassian-jira-6.3.6.tar.gz[root@jira-24 home]# mv atlassian-jira-6.3.6 /opt/jira 2.5 修改端口修改端口号为80 1[root@jira-24 home]# vi /opt/jira/conf/server.xml 2.6 创建及配置jira_home1[root@jira-24 home]# mkdir /home/jira_home 配置路径 1[root@jira-24 home]# vi /opt/jira/atlassian-jira/WEB-INF/classes/jira-application.properties 2.7 启动JIRA1[root@jira-24 home]# /opt/jira/bin/start-jira.sh 打开http://172.20.20.24 配置完数据库之后，在新界面录入程序标题，点击“向后”按钮 输入临时授权码，进行注册： 注册完之后，填写管理员账户和密码 2.8 汉化汉化包位置：\\172.20.20.14\Software\Development\jira 中文软件包下载完毕后，我们需要登陆到jira系统找到Add-ons–Manage add-ons–upload add-on，如下： 中文软件包安装完毕后，我们现在配置jira，如下： 通过上图，我们可以很明显的看出，jira已经被中文语言了。 2.9 破解JIRA破解文件位置：\\172.20.20.14\Software\Development\jira 到了这一步，就是破解的过程了，此时操作如下： 1）先将JIRA服务关掉，不必关闭浏览器。 2）解压破解包到你的硬盘指定目录下，然后按如下指令操作。 1、用atlassian-extras-2.2.2.jar替换你的JIRA的安装目录的\atlassian-jira\WEB-INF\lib同名jar包。 2、用atlassian-universal-plugin-manager-plugin-2.10.1.jar替换你的JIRA的安装目录的\atlassian-jira\atlassian-bundled-plugins中的同名jar包。 3、根据自己的情况，按照keytpl.txt的格式填写好自己的license。 点击右上角齿轮形状的管理图标，选择“系统”，再选择“授权”，看到使用日期不到1个月，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879填写授权码，授权码参数范例如下：Description=JIRA: Commercial,CreationDate=你的安装日期，格式（yyyy-mm-dd）,jira.LicenseEdition=ENTERPRISE,Evaluation=false,jira.LicenseTypeName=COMMERCIAL,jira.active=true,licenseVersion=2,MaintenanceExpiryDate=你想设置的失效日期如：2099-12-31,Organisation=joiandjoin,SEN=你申请到的SEN注意没有前缀LID,ServerID=你申请到的ServerID,jira.NumberOfUsers=-1,LicenseID=LID你申请到的SEN，注意LID前缀不要丢掉,LicenseExpiryDate=你想设置的失效日期如：2099-12-31,PurchaseDate=你的安装日期，格式（yyyy-mm-dd） 本次安装授权码实例为：Description=JIRA: Commercial,CreationDate=2017-01-25,jira.LicenseEdition=ENTERPRISE,Evaluation=false,jira.LicenseTypeName=COMMERCIAL,jira.active=true,licenseVersion=2,MaintenanceExpiryDate=2099-12-31,Organisation=pl,SEN=SEN-L9178727,ServerID=BIBT-L882-5ZZ3-F2IZ,jira.NumberOfUsers=-1,LicenseID=AAABdA0ODAoPeNp9UU1PwkAUvO+vaOJFD9vQRS2SbCK0a1JTwNBqIvGyloeswrZ5u0X011tojaDgcd7HvJl5J2kJzq3UDjt3WKvrse75pRMkaQU8n7wggJ7nRQHoxioDbUBMlVW55mKYivHdOEoEGZbLZ8DR7N4AGk498qpQun+qdyVmc2kglBb4hp62PMouSEOcfhQwlEvgwWgwEOMg6sXfLbEuFH7s7DHK2iTItZWZFQOpFvxtPS+l/lTX3mXbzfIlSQBXgFHI+1E/pXGnw+jFZNKmNyya1AILzKdlZt0NoCaf2XeJ4FaMagXcYgn12HHfB9I5ZKLSpy1oqbMjRv5R8yfE5k7lK47CRAxpfOX5HZ/5pAJ8r/APbWIlWkA+kwsDZIQvUisjt/5C0YtFL3kkAcK28vtdi1rBQyVoM8/2YoDKKRaoTJNgCCZDVWyZb6Nxz0kaCc5p/aCzp64jVnJRbm/Vmo+94FC4u8d39344a/wFDcT8NjAsAhRCNRjYwj4n0JHDJmMZWChYSaegEgIUd1blNKrDbGRX6QLiCbEyzoyRJnA=X02i6,LicenseExpiryDate=2099-12-31,PurchaseDate=2017-01-25 将以上授权码信息填入授权码输入框，点击“增加”按钮，如下所示： 之后，看到授权信息更新了，就表示破解成功，会看到如下成功信息, 如下图： 2.10 插件破解插件破解和软件破解方法不一样，插件破解需要破解包内自带的Key. 破解Key在我百度云内。 参考资料：http://www.cnblogs.com/yangxia-test/p/4448002.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>jira</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsync和sersync实时双向同步]]></title>
    <url>%2F2018%2F09%2F04%2Frsync-sersync%E5%AE%9E%E6%97%B6%E5%8F%8C%E5%90%91%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[1 环境 操作系统 Centos7.1 511 主服务器 172.20.20.111 从服务器 172.20.22.99 测试目的：实现主服务器/rsync目录与从服务器/rsync目录实时双向同步. 参考资料：http://blog.sina.com.cn/s/blog_9f4962b10102vqua.html http://402753795.blog.51cto.com/10788998/1713179 2 以下操作主从服务器都要操作2.1 关闭selinux123456[root@server1]# vi /etc/selinux/config #编辑防火墙配置文件#SELINUX=enforcing #注释掉SELINUX=disabled #增加[root@server1]# setenforce 0 #立即生效 2.2 开启防火墙tcp 873端口（Rsync默认端口）123[root@server1 ]# firewall-cmd --zone=public --add-port=873/tcp --permanent[root@server1 ]# firewall-cmd --reload 主服务做rsync服务端，从服务器做客户端 3 安装rsync（服务端）1Yum install rsync 3.1 编辑配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253Vi /etc/rsyncd.confuid = root #/rsync目录用户属性gid = root #/rsync目录用户组属性port = 873 #rsync同步端口号，默认873address = 172.20.20.111 #本机IP地址use chroot = yes #是否禁锢用户read only = no # no客户端可上传文件,yes只读write only = no # no客户端可下载文件,yes不能下载#list = yeshosts allow = 172.20.22.99 #指定可以联系的客户端主机名或IPhosts deny = * #指定拒绝访问的客户端主机名或IPmax connections = 50 # 客户端最大连接数目（设置大些，否则同步中可能报错）motd file = /etc/rsyncd.motd pid file = /var/run/rsyncd.pid #启动后将进程PID放入此文件log file = /var/log/rsyncd.log # rsync使用syslog输出日志lock file = /var/run/rsync.lock #设置rsync锁文件transfer logging = yeslog format = %t%a%m%bsyslog facility = local3timeout = 300 #超时时间[liferay1] # 要同步的模块名path =/rsync # 要同步的目录（客户端同步文件到哪个目录）list = yes ignore errorsauth users = root # 登陆系统使用的用户名，没有默认为匿名（非主机用户，自定义）secrets file = /etc/rsyncd.pass1 ## 密码文件存放的位置comment = linuxsir liferay1 # 这个名名称无所谓，最后模块名一直 3.2 配置密码文件密码文件为配置文件中所写的文件/etc/rsyncd.secrets格式为**账户:密码** 1[root@server1 ]# vi /etc/rsyncd.pass1 输入帐号密码（自定义） 3.3 修改配置文件及密码文件权限(必须600)123 # chmod 600 /etc/rsyncd.conf # chmod 600 /etc/rsyncd.pass1 3.4 检查rsync是否启动123# lsof -i :873 或# netstat -an |grep 873 4 配置从服务器(客户端)4.1 设定密码文件配置密码文件 (注：为了安全，设定密码档案的属性为：600。rsync.pass1的密码一定要和Rsync 服务器端/etc/rsyncd.pass1的设定的密码一样) 1# vi /etc/rsyncd.pass1 密码文件可与服务端密码文件不一样，这里为了便于记忆，都设置为rsyncd.pass1 客户端密码文件只输入密码，不输入帐号。 4.2 赋予600权限1# chmod 600 /etc/rsyncd.pass1 # 必须修改权限 4.3 测试1$ rsync -avzP --password-file=/etc/rsyncd.pass1 /opt/liferay/data/ tongbu@172.20.20.111::liferay1 从客户端同步/rsync目录到服务端 4.4 安装sersync下载地址：https://code.google.com/archive/p/sersync/downloads 下载sersync2.5.4_64bit_binary_stable_final.tar 4.4.1 解压1Tar –xvf sersync2.5.4_64bit_binary_stable_final.tar 解压文件到/usr/local,重命名为serync 4.4.2 修改配置文件1Vi /usr/local/serync/confxml.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;&lt;head version="2.5"&gt; &lt;host hostip="localhost" port="8008"&gt;&lt;/host&gt; &lt;debug start="true"/&gt; &lt;fileSystem xfs="false"/&gt; &lt;filter start="false"&gt; #设置为true，开启同步过滤，这里不开启 &lt;exclude expression="(.*)\.svn"&gt;&lt;/exclude&gt; &lt;exclude expression="(.*)\.gz"&gt;&lt;/exclude&gt; &lt;exclude expression="^info/*"&gt;&lt;/exclude&gt; &lt;exclude expression="^static/*"&gt;&lt;/exclude&gt; &lt;/filter&gt; &lt;inotify&gt; &lt;delete start="true"/&gt; &lt;createFolder start="true"/&gt; &lt;createFile start="true"/&gt; &lt;closeWrite start="true"/&gt; &lt;moveFrom start="true"/&gt; &lt;moveTo start="true"/&gt; &lt;attrib start="false"/&gt; &lt;modify start="false"/&gt; &lt;/inotify&gt; &lt;sersync&gt; &lt;localpath watch="/rsync"&gt; &lt;remote ip="172.20.20.111" name="liferay1"/&gt; &lt;!--&lt;remote ip="192.168.8.39" name="tongbu"/&gt;--&gt; &lt;!--&lt;remote ip="192.168.8.40" name="tongbu"/&gt;--&gt; &lt;/localpath&gt; &lt;rsync&gt; &lt;commonParams params="-artuz"/&gt; &lt;auth start="true" users="root" passwordfile="/etc/rsyncd.pass1"/&gt; &lt;userDefinedPort start="false" port="874"/&gt;&lt;!-- port=874 --&gt; &lt;timeout start="false" time="100"/&gt;&lt;!-- timeout=100 --&gt; &lt;ssh start="false"/&gt; &lt;/rsync&gt; &lt;failLog path="/usr/local/sersync/rsync_fail_log.sh" timeToExecute="60"/&gt;&lt;!--default every 60mins execute once--&gt; &lt;crontab start="false" schedule="600"&gt;&lt;!--600mins--&gt; &lt;crontabfilter start="false"&gt; &lt;exclude expression="*.php"&gt;&lt;/exclude&gt; &lt;exclude expression="info/*"&gt;&lt;/exclude&gt; &lt;/crontabfilter&gt; &lt;/crontab&gt; &lt;plugin start="false" name="command"/&gt; &lt;/sersync&gt; &lt;plugin name="command"&gt; &lt;param prefix="/bin/sh" suffix="" ignoreError="true"/&gt; &lt;!--prefix /opt/tongbu/mmm.sh suffix--&gt; &lt;filter start="false"&gt; &lt;include expression="(.*)\.php"/&gt; &lt;include expression="(.*)\.sh"/&gt; &lt;/filter&gt; &lt;/plugin&gt; &lt;plugin name="socket"&gt; &lt;localpath watch="/opt/tongbu"&gt; 123456789101112131415161718修改的代码如下:&lt;delete start="true"/&gt;&lt;createFolder start="true"/&gt;&lt;createFile start="true"/&gt;&lt;closeWrite start="true"/ #对于大多数应用，可以尝试把createFile（监控文件事件选项）设置为false来提高性能，减少 rsync通讯。因为拷贝文件到监控目录会产生create事件与close_write事件，所以如果关闭create事件，只监控文件拷贝结束时的事件close_write，同样可以实现文件完整同步。 注意：强将createFolder保持为 true，如果将createFolder设为false，则不会对产生的目录进行监控，该目录下的子文件与子目录也不会被监控。所以除非特殊需要，请开启。默认情况下对创建文件（目录）事件与删除文件（目录）事件都进行监控，如果项目中不需要删除远程目标服务器的文件（目录），则可以将delete 参数设置为false，则不对删除事件进行监控。对于大多数应用，可以尝试把createFile（监控文件事件选项）设置为false来提高性能，减少 rsync通讯。因为拷贝文件到监控目录会产生create事件与close_write事件，所以如果关闭create事件，只监控文件拷贝结束时的事件close_write，同样可以实现文件完整同步。 注意：强将createFolder保持为 true，如果将createFolder设为false，则不会对产生的目录进行监控，该目录下的子文件与子目录也不会被监控。所以除非特殊需要，请开启。默认情况下对创建文件（目录）事件与删除文件（目录）事件都进行监控，如果项目中不需要删除远程目标服务器的文件（目录），则可以将delete 参数设置为false，则不对删除事件进行监控。&lt;localpath watch="/rsync"&gt;： #本地监听目录，源服务器同步目录&lt;remote ip="172.20.20.111" name="liferay1"/&gt;： #服务端IP地址和模块名&lt;auth start="true" users="root" passwordfile="/etc/rsyncd.pass1"/&gt;#服务端模块rsync认证用户名,服务端rsync认证用户的密码存放路径failLog path="/tmp/rsync_fail_log.sh" #脚本运行失败日志路径 4.4.3 创建日志文件1Vi /usr/local/serync/ rsync_fail_log.sh 4.4.4 启动sersync1/usr/local/sersync/sersync2 -d -r -o /usr/local/sersync/confxml.xml 此时客户端可以实施同步目录文件到服务端了。 5 实现双向同步把主从服务器角色互换，从服务器作为服务端，主服务器作为客户端重新部署一次，这样就可以双向实时同步了。 6 附件：=sersync2.5.4_64bit_binary_stable_final.tar.gz]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>rsync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux运维工具]]></title>
    <url>%2F2018%2F09%2F04%2FLinux%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[1 linux磁盘监控工具 1.1. iostat查看存储设备输入和输出状态统计的工具，用来追踪存储设备的性能 问题；包括设备，磁盘，NFS远程磁盘。 1yum install sysstat 123456789101112131415161718192021%user: 在用户级别运行所使用的CPU百分比%nice: 优先进程消耗的CPU时间，占所有CPU百分比%system: 在系统级别运行所使用的CPU百分比%iowait: cpu等待硬件I/O时，所占用的CPU百分比%steal: 管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比%idle: CPU空闲时间的百分比tps: 每秒发送到I/O的请求数KB_read/s: 每秒读取的block数KB_wrtn/s: 每秒写入的block数KB_read: 启动到现在block总数KB_wrtn: 启动到现在写入的block总数 系统systat包里的工具，以kB/s为单位统计,2表示以2秒为频率统计一次：iostat –x –k 2 10000说明rrqm/s：每秒这个设备相关的读取请求有多少被Merge了（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge）；wrqm/s：每秒这个设备相关的写入请求有多少被Merge了。rsec/s：每秒读取的扇区数；wsec/s：每秒写入的扇区数。rKB/s：每秒向设备发出的读取请求数；wKB/s：每秒向设备发出的写入请求数；avgrq-sz： 平均请求扇区的大小avgqu-sz 是平均请求队列的长度。毫无疑问，队列长度越短越好。await：每一个IO请求的处理的平均时间（单位是微秒毫秒）。这里可以理解为IO的响应时间，一般地系统IO响应时间应该低于5ms，如果大于10ms就比较大了。这个时间包括了队列时间和服务时间，也就是说，一般情况下，await大于svctm，它们的差值越小，则说明队列时间越短，反之差值越大，队列时间越长，说明系统出了问题。svctm： 表示平均每次设备I/O操作的服务时间（以毫秒为单位）。如果svctm的值与await很接近，表示几乎没有I/O等待，磁盘性能很好，如果await的值远高于svctm的值，则表示I/O队列等待太长，系统上运行的应用程序将变慢。%util： 在统计时间内所有处理IO时间，除以总共统计时间。例如，如果统计间隔1秒，该设备有0.8秒在处理IO，而0.2秒闲置，那么该设备的 %util = 0.8/1 =80%，所以该参数暗示了设备的繁忙程度。一般地，如果该参数是100%表示设备已经接近满负荷运行了（当然如果是多磁盘，即使%util是100%，因为磁盘的并发能力，所以磁盘使用未必就到了瓶颈）。 1.2 iotop1yum install iotop 监控linux磁盘I/O, 用于查找大量使用磁盘读写进程的时候。python版本需要2.7以上。-h: 查看帮助 用法：iotop -d 1 -o-o：只显示有io操作的进程-b：批量显示，无交互，主要用作记录到文件。-n NUM：显示NUM次，主要用于非交互式模式。-d SEC：间隔SEC秒显示一次。-p PID：监控的进程pid。-u USER：监控的进程用户。常用快捷键 左右箭头：改变排序方式，默认是按IO排序。 r：改变排序顺序。 o：只显示有IO输出的进程。 p：进程/线程的显示方式的切换。 a：显示累积使用量。 q：退出。 1.3 sar - 性能监控和瓶颈检查1yum install sysstat sar –p –d 2 1000说明：tps: 每秒向磁盘设备请求数据的次数，包括读、写请求，为rtps与wtps的和。出于效率考虑，每一次IO下发后并不是立即处理请求，而是将请求合并(merge)，这里tps指请求合并后的请求计数。rtps: 每秒向磁盘设备的读请求次数wtps: 每秒向磁盘设备的写请求次数bread: 每秒从磁盘读的bytes数量bwrtn: 每秒向磁盘写的bytes数量 或者用:sar –b 2 1000 1.4 dstat - 多类型资源统计工具1yum install dstat dstat命令是一个用来替换vmstat、iostat、netstat、nfsstat和ifstat这些命令的工具，是一个全能系统信息统计工具。与sysstat相比，dstat拥有一个彩色的界面，在手动观察性能状况时，数据比较显眼容易观察；而且dstat支持即时刷新，譬如输入dstat 3即每三秒收集一次，但最新的数据都会每秒刷新显示。和sysstat相同的是，dstat也可以收集指定的性能资源，譬如dstat -c即显示CPU的使用情况。常用选项-c：显示CPU系统占用，用户占用，空闲，等待，中断，软件中断等信息。.-C：当有多个CPU时候，此参数可按需分别显示cpu状态，例：-C 0,1 是显示cpu0和cpu1的信息。-d：显示磁盘读写数据大小。-D hda,total：包括hda和总数。-n：显示网络状态。-N eth1,total：有多块网卡时，指定要显示的网卡。-l：显示系统负载情况。-m：显示内存使用情况。-g：显示页面使用情况。-p：显示进程状态。-s：显示交换分区使用情况。-S：类似D/N。-r：I/O请求情况。-y：系统状态。--ipc：显示ipc消息队列，信号等信息。--socket：用来显示tcp udp端口状态。-a：此为默认选项，等同于-cdngy。-v：等同于 -pmgdsc -D total。--output 文件：此选项也比较有用，可以把状态信息以csv的格式重定向到指定的文件中，以便日后查看。例：dstat –output /root/dstat.csv &amp; 此时让程序默默的在后台运行并把结果输出到/root/dstat.csv文件中。 2 Linux进程监控工具2.1 top - 经典的Linux任务管理工具显示所有正在运行而且处于活动状态的实时进程， 而且会定期更新显示结果；它显示了CPU使用率，内存使用率，交换内存使用大小，调整缓存使用大小，缓冲区使用大小，进程PID， 使用的命令等信息。 2.2 Htop - 更加友好的top1yum install htop 使用详解一个非常高级的交互式实时linux进程监控工具，和top相似，但更友好, 还支持鼠标。 sudo apt-get install htop 3 Linux网络监控工具3.1 tcpdump - 洞察网络封包1apt-get install tcpdump 用于捕捉或过滤网络上指定接口上接收或者传输的TCP/IP包。 -i ：网络接口 -c ： 需要输出包数量 3.2 netstat - 显示开放的端口和连接使用详情用于监控进出网络的包和网络接口统计的命令行工具，非常有用，用来监控网络性能，解决网络相关问题。 -h : 查看帮助 -r : 显示路由表 -i : 查看网络接口 3.3 iptraf - 网络性能工具用于采集通过网络接口的IP流量信息，包括tcp标记，icmp信息，TCP，UDP信等。 123$ sudo apt-get install iptraf$ sudo iptraf 3.4 nethogs - 监视每个进程的网络带宽监控每个进程使用的网络带宽 123$ sudo apt-get install nethogs$ sudo nethogs 3.5 iftop - 类似top的网络带宽监控工具监控网络接口的应用网络带宽使用情况 123$ sudo apt-get install iftop$ sudo iftop =&gt;: 表示 流量方向 TX： 发送的流量 RX： 接收的流量 TOTAL： 总流量 Cumm: 运行iftop到目前总流量 peak: 流量峰会 rates: 分别表示 过去2秒，10秒，40秒的平均流量 4 Linux资源监控工具4.1 vmstat - 虚拟内存统计信息系统默认安装工具 一般是通过两个数字参数来完成的，第一个参数是采样时间间隔，单位是秒， 第二个参数是采样的次数 r: 表示运行队列，如果队列过大说明CPU很繁忙，一般会造成CPU使用率高 b: 表示阻塞的进程数 swap: 虚拟内存已使用的大小，如果大于0，说明机器的物理内存不够了 free: 空闲的物理内存大小 buff: 系统占用的缓存大小（写缓存） cache： 直接用来记忆我们打开的文件，给文件做缓冲，读缓存 si: 每秒从磁盘读入虚拟内存大小，如果这个值大于0，表示物理内存不足了 so: 每秒虚拟内存写入磁盘的大小，如果这个值大于0， 表示物理内存不足了 us: 用户cpu时间 sy: 系统CPU时间， 如果值 太高，说明系统调用，例如是IO操作频繁 id: 空闲CPU时间，一般来说 id + us + sy = 100 wt: 等待IO的CPU时间 1.3. lsof - 列表显示打开的文件列出打开的文件；它常用于以列表形式显示所有打开的文件和进程，包括磁盘文件，网络套接字，管道，设备和进程。 主要情形之一就是 无法挂载磁盘和显示正在使用或者打开某个文件的错误时，查看谁正在使用。 1.12. system monitor监控cpu,内存，进程，硬盘的信息；分为进程监控，资源监控，文件监控; 遗憾的是需要图形界面支持。 123sudo apt-get install gnome-system-monitorgnome-system-monitor 2、nmtui配置网卡使用nmtui命令（上一篇博客里有介绍界面） 参考资料https://10.linuxstory.net/command-line-tools-to-monitor-linux-performance/http://os.51cto.com/art/201412/460698_all.htmhttps://zhuanlan.zhihu.com/p/34518047https://segmentfault.com/a/1190000016435032]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>运维工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openvpn基于LDAP认证下进行包过滤（控制访问权限）]]></title>
    <url>%2F2018%2F08%2F31%2FOpenvpn%E5%9F%BA%E4%BA%8ELDAP%E8%AE%A4%E8%AF%81%E4%B8%8B%E8%BF%9B%E8%A1%8C%E5%8C%85%E8%BF%87%E6%BB%A4-%E6%8E%A7%E5%88%B6%E8%AE%BF%E9%97%AE%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[1 说明 方案：采用minimal_pf.so模块和包过滤 此方法同样适用于基本认证。参考资料：http://backreference.org/2010/06/18/openvpns-built-in-packet-filter/ http://732233048.blog.51cto.com/9323668/1713088 https://blog.51cto.com/5766902/2132706?source=dra 2 安装openvpn及设置LDAP认证详细步骤参考本站 openvpn部署之部署基于ad域认证访问内网 3 控制访问权限3.1 创建及编辑minimal_pf.c模块123[root@openvpn ~]# cd /etc/openvpn/[root@openvpn openvpn]# vim minimal_pf.so 键入以下内容（内容固定） 123456789101112131415161718192021222324252627282930313233343536373839404142434445/* minimal_pf.c * ultra-minimal OpenVPN plugin to enable internal packet filter */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; #include "include/openvpn-plugin.h" /* dummy context, as we need no state */struct plugin_context &#123; int dummy;&#125;; /* Initialization function */OPENVPN_EXPORT openvpn_plugin_handle_t openvpn_plugin_open_v1 (unsigned int *type_mask, const char *argv[], const char *envp[]) &#123; struct plugin_context *context; /* Allocate our context */ context = (struct plugin_context *) calloc (1, sizeof (struct plugin_context)); /* Which callbacks to intercept. */ *type_mask = OPENVPN_PLUGIN_MASK (OPENVPN_PLUGIN_ENABLE_PF); return (openvpn_plugin_handle_t) context;&#125; /* Worker function */OPENVPN_EXPORT int openvpn_plugin_func_v2 (openvpn_plugin_handle_t handle, const int type, const char *argv[], const char *envp[], void *per_client_context, struct openvpn_plugin_string_list **return_list) &#123; if (type == OPENVPN_PLUGIN_ENABLE_PF) &#123; return OPENVPN_PLUGIN_FUNC_SUCCESS; &#125; else &#123; /* should not happen! */ return OPENVPN_PLUGIN_FUNC_ERROR; &#125;&#125; /* Cleanup function */OPENVPN_EXPORT void openvpn_plugin_close_v1 (openvpn_plugin_handle_t handle) &#123; struct plugin_context *context = (struct plugin_context *) handle; free (context);&#125; 3.2 构建插件下载并解压OpenVPN源码压缩包（严格来说，只需要openvpn-plugin.h） openvpn源码安装包：openvpn-2.3.11.tar.gz ，解压文件，复制include目录到/etc/openvpn` 并使用以下命令构建插件： 12345INCLUDE="-I/etc/openvpn" # CHANGE THIS!!!!CC_FLAGS="-O2 -Wall -g"NAME=minimal_pfgcc $CC_FLAGS -fPIC -c $INCLUDE $NAME.c &amp;&amp; \gcc $CC_FLAGS -fPIC -shared -Wl,-soname,$NAME.so -o $NAME.so $NAME.o -lc 3.3 创建包过滤文件123mkdir /etc/openvpn/ccdcd /etc/openvpn/ccd 创建以用户名.pf命名的文件，输入以下内容。 1234567891011vi client1.pf #客户client1，只对10.10.1.0网段有权限[CLIENTS ACCEPT][SUBNETS DROP]+10.10.1.0/24[END]vi client.pf #客户client，对所有内网服务器都有权限[CLIENTS ACCEPT][SUBNETS ACCEPT][END] 3.3.1 包过滤文件补充包过滤文件格式： 12345678910111213141516171819202122232425[CLIENTS DROP|ACCEPT]&#123;+|-&#125;common_name1&#123;+|-&#125;common_name2 . . .[SUBNETS DROP|ACCEPT]&#123;+|-&#125;subnet1&#123;+|-&#125;subnet2 . . .[END]过滤文件语法：CLIENTS部分用于定义common name；SUBNETS部分用于定义IP地址、IP网段；DROP|ACCEPT用于设置默认规则，就是没有明确指明的common name，那么他们将会使用；&#123;+|-&#125;用于设置是否允许，如果是“+”，那么表示允许，如果是“-”则表示不允许；[END]表示策略文件的结束cat client10.pf[CLIENTS ACCEPT][SUBNETS ACCEPT]-192.168.9.7+192.168.9.0/24[END] 注意事项： 创建过滤文件时，允许访问的地址写到上面，禁止访问的地址写在后面。如果先禁止访问网段，在允许访问网段IP地址，依然受限。 例如： 3.4 创建客户端连接脚本123cd /etc/openvpnvim client-connect.sh 1234567891011121314#!/bin/sh # /etc/openvpn/client-connect.sh: sample client-connect script using pf rule files # rules template filetemplate="/etc/openvpn/ccd/$&#123;common_name&#125;.pf" # create the file OpenVPN wants with the rules for this clientif [ -f "$template" ] &amp;&amp; [ ! -z "$pf_file" ]; then cp -- "$template" "$pf_file"else # if anything is not as expected, fail exit 1fi 3.5 修改openvpn配置文件1vim /etc/openvpn/server.conf 12345678910111213141516171819202122232425262728293031323334353637383940ca /etc/openvpn/easy-rsa/keys/ca.crtcert /etc/openvpn/easy-rsa/keys/server.crtkey /etc/openvpn/easy-rsa/keys/server.keydh /etc/openvpn/easy-rsa/keys/dh2048.pemserver 10.8.0.0 255.255.255.0ifconfig-pool-persist ipp.txt;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100;push "redirect-gateway def1 bypass-dhcp";push "redirect-gateway";push "dhcp-option DNS 172.20.20.10";push "dhcp-option DNS 202.102.224.68";push "route 0.0.0.0 0.0.0.0"push "route 172.20.20.0 255.255.255.0"push "route 172.20.22.0 255.255.255.0"push "route 172.20.10.0 255.255.255.0"push "route 172.20.19.0 255.255.255.0"push "route 10.224.255.224 255.255.255.224";push "route 172.20.18.0 255.255.255.0";push "route 172.20.17.0 255.255.255.0";push "dhcp-option DNS 172.20.20.10";push "dhcp-option DNS 202.102.224.68"client-to-clientduplicate-cnkeepalive 10 120#tls-auth /etc/openvpn/easy-rsa/ta.key 0comp-lzomax-clients 10persist-keypersist-tunstatus openvpn-status.loglog /var/log/openvpn.loglog-append /var/log/openvpn-append.logverb 3plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so "/etc/openvpn/auth/ldap.conf"client-cert-not-requiredusername-as-common-name # 添加 client-config-dir /etc/openvpn/ccd #添加plugin /etc/openvpn/minimal_pf.so #添加 client-connect /etc/openvpn/client-connect.sh #添加script-security 3 例子：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335################################################## Sample OpenVPN 2.0 config file for ## multi-client server. ## ## This file is for the server side ## of a many-clients &lt;-&gt; one-server ## OpenVPN configuration. ## ## OpenVPN also supports ## single-machine &lt;-&gt; single-machine ## configurations (See the Examples page ## on the web site for more info). ## ## This config should work on Windows ## or Linux/BSD systems. Remember on ## Windows to quote pathnames and use ## double backslashes, e.g.: ## "C:\\Program Files\\OpenVPN\\config\\foo.key" ## ## Comments are preceded with '#' or ';' ################################################### Which local IP address should OpenVPN# listen on? (optional);local a.b.c.d# Which TCP/UDP port should OpenVPN listen on?# If you want to run multiple OpenVPN instances# on the same machine, use a different port# number for each one. You will need to# open up this port on your firewall.port 1194# TCP or UDP server?# ;proto tcp# proto udpproto tcp# "dev tun" will create a routed IP tunnel,# "dev tap" will create an ethernet tunnel.# Use "dev tap0" if you are ethernet bridging# and have precreated a tap0 virtual interface# and bridged it with your ethernet interface.# If you want to control access policies# over the VPN, you must create firewall# rules for the the TUN/TAP interface.# On non-Windows systems, you can give# an explicit unit number, such as tun0.# On Windows, use "dev-node" for this.# On most systems, the VPN will not function# unless you partially or fully disable# the firewall for the TUN/TAP interface.# ;dev tapdev tun# Windows needs the TAP-Win32 adapter name# from the Network Connections panel if you# have more than one. On XP SP2 or higher,# you may need to selectively disable the# Windows firewall for the TAP adapter.# Non-Windows systems usually don't need this.;dev-node MyTap# SSL/TLS root certificate (ca), certificate# (cert), and private key (key). Each client# and the server must have their own cert and# key file. The server and all clients will# use the same ca file.## See the "easy-rsa" directory for a series# of scripts for generating RSA certificates# and private keys. Remember to use# a unique Common Name for the server# and each of the client certificates.## Any X509 key management system can be used.# OpenVPN can also use a PKCS #12 formatted key file# (see "pkcs12" directive in man page).ca /etc/openvpn/server/ca.crt cert /etc/openvpn/server/server.crtkey /etc/openvpn/server/server.key # This file should be kept secret# Diffie hellman parameters.# Generate your own with:# openssl dhparam -out dh2048.pem 2048# dh dh2048.pemdh /etc/openvpn/server/dh.pem# Network topology# Should be subnet (addressing via IP)# unless Windows clients v2.0.9 and lower have to# be supported (then net30, i.e. a /30 per client)# Defaults to net30 (not recommended);topology subnet# Configure server mode and supply a VPN subnet# for OpenVPN to draw client addresses from.# The server will take 10.8.0.1 for itself,# the rest will be made available to clients.# Each client will be able to reach the server# on 10.8.0.1. Comment this line out if you are# ethernet bridging. See the man page for more info.server 10.8.0.0 255.255.255.0# Maintain a record of client &lt;-&gt; virtual IP address# associations in this file. If OpenVPN goes down or# is restarted, reconnecting clients can be assigned# the same virtual IP address from the pool that was# previously assigned.ifconfig-pool-persist logs/ipp.txt# Configure server mode for ethernet bridging.# You must first use your OS's bridging capability# to bridge the TAP interface with the ethernet# NIC interface. Then you must manually set the# IP/netmask on the bridge interface, here we# assume 10.8.0.4/255.255.255.0. Finally we# must set aside an IP range in this subnet# (start=10.8.0.50 end=10.8.0.100) to allocate# to connecting clients. Leave this line commented# out unless you are ethernet bridging.;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100# Configure server mode for ethernet bridging# using a DHCP-proxy, where clients talk# to the OpenVPN server-side DHCP server# to receive their IP address allocation# and DNS server addresses. You must first use# your OS's bridging capability to bridge the TAP# interface with the ethernet NIC interface.# Note: this mode only works on clients (such as# Windows), where the client-side TAP adapter is# bound to a DHCP client.;server-bridge# Push routes to the client to allow it# to reach other private subnets behind# the server. Remember that these# private subnets will also need# to know to route the OpenVPN client# address pool (10.8.0.0/255.255.255.0)# back to the OpenVPN server.;push "route 192.168.10.0 255.255.255.0";push "route 192.168.20.0 255.255.255.0"push "route 192.168.0.0 255.255.255.0"push "route 192.168.1.0 255.255.255.0"# To assign specific IP addresses to specific# clients or if a connecting client has a private# subnet behind it that should also have VPN access,# use the subdirectory "ccd" for client-specific# configuration files (see man page for more info).# EXAMPLE: Suppose the client# having the certificate common name "Thelonious"# also has a small subnet behind his connecting# machine, such as 192.168.40.128/255.255.255.248.# First, uncomment out these lines:;client-config-dir ccd;route 192.168.40.128 255.255.255.248# Then create a file ccd/Thelonious with this line:# iroute 192.168.40.128 255.255.255.248# This will allow Thelonious' private subnet to# access the VPN. This example will only work# if you are routing, not bridging, i.e. you are# using "dev tun" and "server" directives.# EXAMPLE: Suppose you want to give# Thelonious a fixed VPN IP address of 10.9.0.1.# First uncomment out these lines:;client-config-dir ccd;route 10.9.0.0 255.255.255.252# Then add this line to ccd/Thelonious:# ifconfig-push 10.9.0.1 10.9.0.2# Suppose that you want to enable different# firewall access policies for different groups# of clients. There are two methods:# (1) Run multiple OpenVPN daemons, one for each# group, and firewall the TUN/TAP interface# for each group/daemon appropriately.# (2) (Advanced) Create a script to dynamically# modify the firewall in response to access# from different clients. See man# page for more info on learn-address script.;learn-address ./script# If enabled, this directive will configure# all clients to redirect their default# network gateway through the VPN, causing# all IP traffic such as web browsing and# and DNS lookups to go through the VPN# (The OpenVPN server machine may need to NAT# or bridge the TUN/TAP interface to the internet# in order for this to work properly).;push "redirect-gateway def1 bypass-dhcp"# Certain Windows-specific network settings# can be pushed to clients, such as DNS# or WINS server addresses. CAVEAT:# http://openvpn.net/faq.html#dhcpcaveats# The addresses below refer to the public# DNS servers provided by opendns.com.;push "dhcp-option DNS 208.67.222.222";push "dhcp-option DNS 208.67.220.220"push "dhcp-option DNS 114.114.114.114"# Uncomment this directive to allow different# clients to be able to "see" each other.# By default, clients will only see the server.# To force clients to only see the server, you# will also need to appropriately firewall the# server's TUN/TAP interface.client-to-client# Uncomment this directive if multiple clients# might connect with the same certificate/key# files or common names. This is recommended# only for testing purposes. For production use,# each client should have its own certificate/key# pair.## IF YOU HAVE NOT GENERATED INDIVIDUAL# CERTIFICATE/KEY PAIRS FOR EACH CLIENT,# EACH HAVING ITS OWN UNIQUE "COMMON NAME",# UNCOMMENT THIS LINE OUT.;duplicate-cn# The keepalive directive causes ping-like# messages to be sent back and forth over# the link so that each side knows when# the other side has gone down.# Ping every 10 seconds, assume that remote# peer is down if no ping received during# a 120 second time period.keepalive 10 120# For extra security beyond that provided# by SSL/TLS, create an "HMAC firewall"# to help block DoS attacks and UDP port flooding.## Generate with:# openvpn --genkey --secret ta.key## The server and each client must have# a copy of this key.# The second parameter should be '0'# on the server and '1' on the clients.# tls-auth ta.key 0 # This file is secret# Select a cryptographic cipher.# This config item must be copied to# the client config file as well.# Note that v2.4 client/server will automatically# negotiate AES-256-GCM in TLS mode.# See also the ncp-cipher option in the manpagecipher AES-256-CBC# Enable compression on the VPN link and push the# option to the client (v2.4+ only, for earlier# versions see below);compress lz4-v2;push "compress lz4-v2"# For compression compatible with older clients use comp-lzo# If you enable it here, you must also# enable it in the client config file.comp-lzo# The maximum number of concurrently connected# clients we want to allow.;max-clients 100max-clients 30# It's a good idea to reduce the OpenVPN# daemon's privileges after initialization.## You can uncomment this out on# non-Windows systems.;user nobody;group nobody# The persist options will try to avoid# accessing certain resources on restart# that may no longer be accessible because# of the privilege downgrade.persist-keypersist-tun# Output a short status file showing# current connections, truncated# and rewritten every minute.status logs/openvpn-status.log# By default, log messages will go to the syslog (or# on Windows, if running as a service, they will go to# the "\Program Files\OpenVPN\log" directory).# Use log or log-append to override this default.# "log" will truncate the log file on OpenVPN startup,# while "log-append" will append to it. Use one# or the other (but not both).;log openvpn.log;log-append openvpn.loglog logs/openvpn.loglog-append logs/openvpn-append.log# Set the appropriate level of log# file verbosity.## 0 is silent, except for fatal errors# 4 is reasonable for general usage# 5 and 6 can help to debug connection problems# 9 is extremely verboseverb 3# Silence repeating messages. At most 20# sequential messages of the same message# category will be output to the log.;mute 20# Notify the client that when the server restarts so it# can automatically reconnect.# explicit-exit-notify 1### LDAP认证plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so "/etc/openvpn/auth/ldap.conf"# 关闭证书认证;不请求客户的CA证书，使用User/Pass验证client-cert-not-required ### 增加权限配置，包过滤规则(添加如下5项)#添加,否则报错WARNING: External program may not be called unless '--script-security 2' or higher is enabled. See --help text or man page for detailed info.script-security 3#使用客户提供的UserName作为Common Nameusername-as-common-name client-config-dir /etc/openvpn/ccdplugin /etc/openvpn/include/minimal_pf.so client-connect /etc/openvpn/include/client-connect.sh 3.6 重启openvpn服务器1/etc/init.d/openvpn restart 配置完成。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>openvpn</tag>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql命令示例]]></title>
    <url>%2F2018%2F08%2F31%2FMysql%E5%91%BD%E4%BB%A4%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[1 创建数据库 1Mysql&gt;create database 库名; 1.1 创建utf-8格式的数据库1Mysql&gt;CREATE DATABASE `库名` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 2 删除数据库1Mysql&gt;drop database 库名; 3 查询数据库1Mysql&gt;show databases; 4 恢复数据库123Mysql&gt;use 数据库名 //修改要恢复的数据库Mysql&gt;source 数据文件名（可以带路径）； 例如： 1Mysql&gt;source /home/123sql; 5 修改用户密码5.1 修改root密码5.1.1 方法1： 用SET PASSWORD命令首先登录MySQL。 格式：mysql&gt; set password for 用户名@localhost = password(‘新密码’); 例子： 1mysql&gt; set password for root@localhost = password('123'); 5.1.2 方法2：用mysqladmin 格式：mysqladmin -u用户名 -p旧密码 password 新密码 例子： 1mysqladmin -uroot -p123456 password 123 5.1.3 方法3：用UPDATE直接编辑user表首先登录MySQL。 123mysql&gt; use mysql;mysql&gt; update user set password=password('123') where user='root' and host='localhost';mysql&gt; flush privileges; 5.1.4 方法4：在忘记root密码的时候，可以这样以windows为例： 关闭正在运行的MySQL服务。 打开DOS窗口，转到mysql\bin目录。 输入mysqld –skip-grant-tables 回车。–skip-grant-tables 的意思是启动MySQL服务的时候跳过权限表认证。 再开一个DOS窗口（因为刚才那个DOS窗口已经不能动了），转到mysql\bin目录。 输入mysql回车，如果成功，将出现MySQL提示符 &gt;。 连接权限数据库： use mysql; 。 改密码：update user set password=password(“123”) where user=”root”;（别忘了最后加分号） 。 刷新权限（必须步骤）：flush privileges; 。 退出 quit。 注销系统，再进入，使用用户名root和刚才设置的新密码123登录。 6 Mysql远程访问首先确认防火墙开启3306端口,如果my.cnf文件配置有bind-address=127.0.0.1，用#注释掉。 6.1 改表法：可能是你的帐号不允许从远程登陆，只能在localhost。这个时候只要在localhost的那台电脑，登入mysql后，更改 “mysql” 数据库里的 “user” 表里的“host” 项，从“localhost”改称“%” 登录数据库 1234mysql&gt; use mysql;mysql&gt; update user set host = ‘%’ where user = ‘root’;mysql&gt; select host, user from user; //列出host名和user名，查看修改。mysql&gt; flush privileges; //刷新权限列表注：mysql&gt; flush privileges; //使修改生效。 6.2 授权法（推荐）例如，你想myuser使用mypassword从任何主机连接到mysql服务器的话。 1mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'myuser'@'%' IDENTIFIED BY 'mypassword' WITH GRANT OPTION; 如果你想允许用户myuser从ip为192.168.1.3的主机连接到mysql服务器，并使用mypassword作为密码 1mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'myuser'@'192.168.1.3' IDENTIFIED BY 'mypassword’ WITH GRANT OPTION; 下面的语句表示将 discuz 数据库的所有权限授权给 myuser 这个用户，允许 myuser 用户在 123.123.123.123 这个 IP 进行远程登陆，并设置 myuser 用户的密码为 mypassword 。 1mysql&gt; GRANT ALL PRIVILEGES ON discuz.* TO 'myuser'@'192.168.1.3' IDENTIFIED BY 'mypassword’ WITH GRANT OPTION; 注：代表所有数据库，discuz. 代表discuz数据库。 7 Mysqlbinlog用法 注意：用mysqlbinlog前一定要加上数据库安装路径，否则默认情况下调用的是系统默认安装的mysql中的mysqlbinlog 7.1 查看二进制日志查看二进制日志mysql-bin.000014 1/usr/local/mysql/bin/mysqlbinlog /usr/local/mysql/data/mysql-bin.000014 7.2 转换二进制日志生成普通日志转换mysql-bin.000014为f.log,转换到88624位置为止。 1/usr/local/mysql/bin/mysqlbinlog -v --stop-position=88624 /usr/local/mysql/data/mysql-bin.000014 &gt; /home/f.log]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7下Mysql5.5数据同步]]></title>
    <url>%2F2018%2F08%2F30%2FCentos7%E4%B8%8BMysql5-5%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[1 环境 服务器 系统 服务 java mysql ip Liferay-a Centos7 Liferay+keepalive+sersync 1.7.0_80 5.5.42 172.20.20.59 Liferay-b Centos7 Liferay+keepalive+rsync 1.7.0_80 5.5.42 172.20.20.60 虚拟IP:172.20.20.58 2 安装liferay安装liferay官方版本6.2-ce4,方法自行度娘。恢复数据参考《liferay备份还原文档》 3 安装keepalive1Yum install keepalive 3.1 编辑配置文件1vi /etc/keepalived/keepalived.conf Liferay-a: Liferay-b: 启动keepalive，建立虚拟IP，主服务器当机，从服务器获得Ip. 4 mysql主从同步下载mysql-5.5.42-linux2.6-x86_64.tar.gz解压到/usr/local,重命名为mysql 4.1 编辑配置文件1Vi /etc/my.cnf Liferay-a: Liferay-b 4.2 建立mysql主从同步查看liferay-a(主服务器)，查看mysql（主）信息，并建立同步帐号： 1GRANT ALL PRIVILEGES ON bitnami_liferay.* TO 'tongbu'@'%' IDENTIFIED BY 'De123456' WITH GRANT OPTION; 进入liferay-b（从服务器），在mysql中输入命令，建立连接 1CHANGE MASTER TO MASTER_HOST='172.20.20.59', MASTER_USER='tongbu', MASTER_PASSWORD='De123456', MASTER_LOG_FILE='mysql-bin.000022',MASTER_LOG_POS=151424110; 输入命令， 查看备服务器信息 1SHOW SLAVE STATUS\G 4.3 恢复数据库通过mysql命令恢复bitnami_liferay数据库备份到lifreay-a(主服务器)，自动实时同步数据到从服务器。 5 目录同步5.1 安装rsync(liferay-b)只需在liferay-b服务器上安装rsync。 Liferay-b作为rsync服务器，lifray-a作为客户端，实时同步目录数据到liferay-bYum install rsync 5.1.1 编辑配置文件1vi /etc/rsyncd.conf liferay-b: 新建rsyncd.pass1密码文件在/etc/目录 文件格式 帐号:密码 设置文件权限为600（必须） 5.2 安装sersync（liferay-a）Liferay-a安装实时同步工具sersync2.5.4_64bit_binary_stable_final.tar.gz下载sersync2.5.4_64bit_binary_stable_final.tar.gz解压到/usr/local 5.2.1 编辑confxml.xml1vi confxml.xml 5.2.2 在/etc/目录创建密码文件rsyncd.pass1文件格式只填写密码（注意和同步服务器的密码文件内的密码一样） 5.3 启动sersync，开启实时同步1/usr/local/sersync/sersync2 -d -r -o /usr/local/sersync/confxml.xml]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix通过jmx监控tomcat]]></title>
    <url>%2F2018%2F08%2F30%2FZabbix%E9%80%9A%E8%BF%87jmx%E7%9B%91%E6%8E%A7tomcat%2F</url>
    <content type="text"><![CDATA[1 环境 名称 IP zabbix server 172.20.20.22 tomcat server 172.20.20.111 2 服务端安装2.1 安装zabbix_java_Gateway这里直接安装zabbix_java_gateway到zabbix server上面 1rpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-java-gateway-3.0.5-1.el7.x86_64.rpm 2.2 修改配置文件zabbix_java-gateway.修改zabbix_java_gateway.conf 1234LISTEN_IP="0.0.0.0"LISTEN_PORT=10052PID_FILE="/tmp/zabbix_java.pid"START_POLLERS=5 2.3 修改zabbix_server.conf添加如下几行 JavaGateway=127.0.0.1 //zabbix_server与zabbix_java_gateway在一台服务器上，这里指定java_gateway服务器地址为本机；JavaGatewayPort=10052StartJavaPollers=5 2.4 重启zabbix_server1[root@zabbixserver ~]# service zabbix-server restart 3 tomcat客户端配置添加如下代码到tomcat目录/bin/catalina.sh 12345CATALINA_OPTS="-Djava.rmi.server.hostname=172.20.20.111 //tomcat客户端ip地址-Djavax.management.builder.initial=-Dcom.sun.management.jmxremote=true-Dcom.sun.management.jmxremote.ssl=false-Dcom.sun.management.jmxremote.authenticate=false" 3.1 下载catalina-jmx-remote.jar因为tomcat服务器安装的java版本是1.7，所以这里下载的jar包是1.7的版本，不同版本的tomcat对应不同版本的catalina-jmx-remote.jar； 在http://tomcat.apache.org/download-70.cgi 找到以下JMX Remote jar,把这个文件放到tomcat安装目录的lib子目录下 Extras: JMX Remote jar (pgp, md5, sha1) //下载jmx包 Web services jar (pgp, md5, sha1) JULI adapters jar (pgp, md5, sha1) JULI log4j jar (pgp, md5, sha1) 3.2 修改tomcat安装目录conf子目录下的server.xml配置文件添加如下几行 12&lt;Listener className="org.apache.catalina.mbeans.JmxRemoteLifecycleListener" rmiRegistryPortPlatform="12345" rmiServerPortPlatform="12346" /&gt; 完整显示： 12345678910111213141516&lt;Server port="8005" shutdown="SHUTDOWN"&gt; &lt;Listener className="org.apache.catalina.startup.VersionLoggerListener" /&gt; &lt;!-- Security listener. Documentation at /docs/config/listeners.html &lt;Listener className="org.apache.catalina.security.SecurityListener" /&gt; --&gt; &lt;!--APR library loader. Documentation at /docs/apr.html --&gt; &lt;Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" /&gt; &lt;!--Initialize Jasper prior to webapps are loaded. Documentation at /docs/jasper-howto.html --&gt; &lt;Listener className="org.apache.catalina.core.JasperListener" /&gt; &lt;!-- Prevent memory leaks due to use of particular java/javax APIs--&gt; &lt;Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" /&gt; &lt;Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" /&gt; &lt;Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" /&gt; &lt;Listener className="org.apache.catalina.mbeans.JmxRemoteLifecycleListener" rmiRegistryPortPlatform="12345" rmiServerPortPlatform="12346" /&gt;省略... 3.3 重启tomcat.123[root@localhost apache-tomcat-7.0.69]# /opt/tomcat/bin/shutdown.sh [root@localhost apache-tomcat-7.0.69]# /opt/tomcat/bin/startup.sh 测试是否可以获得数据 测试需要cmdline-jmxclient-0.10.3.jar包，下载包 http://dl.bintray.com/typesafe/maven-releases/cmdline-jmxclient/cmdline-jmxclient/0.10.3/cmdline-jmxclient-0.10.3.jar 12345cd /homewget http://dl.bintray.com/typesafe/maven-releases/cmdline-jmxclient/cmdline-jmxclient/0.10.3/cmdline-jmxclient-0.10.3.jarjava -jar /home/cmdline-jmxclient-0.10.3.jar - 127.0.0.1:12345 java.lang:type=Memory NonHeapMemoryUsage 如上图显示，说明可以正常获得数据。 3.4 配置防火墙开放12345/12346端口 到这里配置完成。 参考资料： http://jaychang.iteye.com/blog/2214830 4 问题：按照网上的教程，修改tomcat目录中的catalina.sh ，无法再关闭防火墙的前提下正常获取到数据，是因为即使按照配置设定了端口，但实际中并不是只使用设置的端口。 5 配置例子： （关闭防火墙开启12345端口就无法获取数据） 12345CATALINA_OPTS="-Dcom.sun.management.jmxremote-Dcom.sun.management.jmxremote.authenticate=false-Dcom.sun.management.jmxremote.ssl=false-Dcom.sun.management.jmxremote.port=12345 #定义jmx监听端口-Djava.rmi.server.hostname=客户端IP"]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
        <tag>tomcat</tag>
        <tag>jmx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[workpress编辑器插件UEditor]]></title>
    <url>%2F2018%2F08%2F30%2Fworkpress%E7%BC%96%E8%BE%91%E5%99%A8%E6%8F%92%E4%BB%B6UEditor%2F</url>
    <content type="text"><![CDATA[百度UEditor-KityFormula for wordpress 2.0.2发布 自从UEditor-KityFormula-for-wordpress 2.0.1版本发布以来，用户反应较好，对某些使用者提出的没有百度地图、google地图及iframe,2.0.2版本进行了补充。现已发布，不过谷歌地图由于某些原因(被墙)，国内用不了，需要使用vpn，建议使用百度地图。 更新说明：应网友的要求，添加了百度地图、谷歌地图及iframe,属于小版本升级。 wordpress版本要求wordpress 4.3+ 安装说明：下载 zip版本，直接使用插件-&gt;安装插件-&gt;上传-&gt;启用插件 。 下载地址：A: 插件地址：http://www.yangshengliang.com/kaiyuan-shijie/zuopin/399.html B: 插件地址：http://www.geroro.com/archives/258 解压缩包到wordpress插件文件夹。最终的路径像这样：/wp-content/plugins/UEditor-KityFormula，也可以直接在后台通过插件上传进行安装。 附件： A:全屏左菜单栏遮挡 =UEditor-KityFormula-for-wordpress.zip B:较好，不全屏使用较好。=ueditor1.4.3.3forwordpress4.6.1.zip]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>workpress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openvpn+squid搭建通过http代理访问openvpn服务器]]></title>
    <url>%2F2018%2F08%2F30%2Fopenvpn-squid%E6%90%AD%E5%BB%BA%E9%80%9A%E8%BF%87http%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AEopenvpn%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1 环境： OS : Centos 7.2 x86_64 2 Squid部署1[root@bogon ~]# yum install squid 2.1 编辑配置文件1[root@bogon ~]# vim /etc/squid/squid.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374## Recommended minimum configuration:## Example rule allowing access from your local networks.# Adapt to list your (internal) IP networks from where browsing# should be allowedacl all src all #添加允许所有IP地址访问acl localnet src 10.0.0.0/8 # RFC1918 possible internal networkacl localnet src 172.16.0.0/12 # RFC1918 possible internal networkacl localnet src 192.168.0.0/16 # RFC1918 possible internal networkacl localnet src fc00::/7 # RFC 4193 local private network rangeacl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machinesacl SSL_ports port 443acl Safe_ports port 80 # httpacl Safe_ports port 21 # ftpacl Safe_ports port 443 # httpsacl Safe_ports port 70 # gopheracl Safe_ports port 210 # waisacl Safe_ports port 1025-65535 # unregistered portsacl Safe_ports port 280 # http-mgmtacl Safe_ports port 488 # gss-httpacl Safe_ports port 591 # filemakeracl Safe_ports port 777 # multiling httpacl CONNECT method CONNECT## Recommended minimum Access Permission configuration:## Deny requests to certain unsafe portshttp_access allow !Safe_ports ## Deny CONNECT to other than secure SSL portshttp_access allow CONNECT !SSL_ports #设置允许所有SSL通过(不设置的话无法连接到http服务器)# Only allow cachemgr access from localhosthttp_access allow localhost managerhttp_access allow manageraccess_log /var/log/squid/access.log combinedcache_log /var/log/squid/cache.log# We strongly recommend the following be uncommented to protect innocent# web applications running on the proxy server who think the only# one who can access services on "localhost" is a local user#http_access deny to_localhost## INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS## Example rule allowing access from your local networks.# Adapt localnet in the ACL section to list your (internal) IP networks# from where browsing should be allowed# And finally deny all other access to this proxyhttp_access allow all #修改允许所有人使用该代理.因为这里是代理加速web服务器http_reply_access allow all# Squid normally listens to port 3128#http_port 3128http_port 4346 #修改默认监听端口，即代理端口。# Uncomment and adjust the following to add a disk cache directory.#cache_dir ufs /var/spool/squid 100 16 256# Leave coredumps in the first cache dircoredump_dir /var/spool/squid## Add any of your own refresh_pattern entries above these.#refresh_pattern ^ftp: 1440 20% 10080refresh_pattern ^gopher: 1440 0% 1440refresh_pattern -i (/cgi-bin/|\?) 0 0% 0refresh_pattern . 0 20% 4320 2.2 启动squid及设置开机启动123[root@bogon ~]# systemctl start squid.service[root@bogon ~]# systemctl enable squid.service 2.3 openvpn客户端配置proxy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131############################################### Sample client-side OpenVPN 2.0 config file ## for connecting to multi-client server. ## ## This configuration can be used by multiple ## clients, however each client should have ## its own cert and key files. ## ## On Windows, you might want to rename this ## file so it has a .ovpn extension ################################################ Specify that we are a client and that we# will be pulling certain config file directives# from the server.client# Use the same setting as you are using on# the server.# On most systems, the VPN will not function# unless you partially or fully disable# the firewall for the TUN/TAP interface.;dev tapdev tun# Windows needs the TAP-Win32 adapter name# from the Network Connections panel# if you have more than one. On XP SP2,# you may need to disable the firewall# for the TAP adapter.;dev-node MyTap# Are we connecting to a TCP or# UDP server? Use the same setting as# on the server.proto tcp;proto udp# The hostname/IP and port of the server.# You can have multiple remote entries# to load balance between the servers.;remote 172.20.20.25 1194remote openvpn服务器地址或域名 1194;remote my-server-2 1194;http-proxy vpnproxy.dealeasy.com 4346http-proxy squid服务器地址 4346 #添加http代理# Choose a random host from the remote# list for load-balancing. Otherwise# try hosts in the order specified.;remote-random# Keep trying indefinitely to resolve the# host name of the OpenVPN server. Very useful# on machines which are not permanently connected# to the internet such as laptops.resolv-retry infinite# Most clients don't need to bind to# a specific local port number.nobind# Downgrade privileges after initialization (non-Windows only);user nobody;group nobody# Try to preserve some state across restarts.persist-keypersist-tun# If you are connecting through an# HTTP proxy to reach the actual OpenVPN# server, put the proxy server/IP and# port number here. See the man page# if your proxy server requires# authentication.;http-proxy-retry # retry on connection failures;http-proxy [proxy server] [proxy port #]# Wireless networks often produce a lot# of duplicate packets. Set this flag# to silence duplicate packet warnings.;mute-replay-warnings# SSL/TLS parms.# See the server config file for more# description. It's best to use# a separate .crt/.key file pair# for each client. A single ca# file can be used for all clients.ca ca.crt#cert client.crt#key client.keyauth-user-pass# Verify server certificate by checking that the# certicate has the correct key usage set.# This is an important precaution to protect against# a potential attack discussed here:# http://openvpn.net/howto.html#mitm## To use this feature, you will need to generate# your server certificates with the keyUsage set to# digitalSignature, keyEncipherment# and the extendedKeyUsage to# serverAuth# EasyRSA can do this for you.remote-cert-tls server# If a tls-auth key is used on the server# then every client must also have the key.;tls-auth ta.key 1# Select a cryptographic cipher.# If the cipher option is used on the server# then you must also specify it here.;cipher x# Enable compression on the VPN link.# Don't enable this unless it is also# enabled in the server config file.comp-lzo# Set log file verbosity.verb 3# Silence repeating messages;mute 20 现在可以通过http代理连接VPN服务器。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>openvpn</tag>
        <tag>squid</tag>
        <tag>vpn</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows重置网卡解决无法上网问题]]></title>
    <url>%2F2018%2F08%2F30%2Fwindows%E9%87%8D%E7%BD%AE%E7%BD%91%E5%8D%A1%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E7%BD%91%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1 故障表现： 网卡获取地址正常，无法访问域名，Ping域名无法找到主机，表现为DNS故障。 1.2 解决办法1：管理员打开命令行输入以下命令 netsh int ip reset #重置ip配置 netsh winhttp reset proxy #重置代理 ipconfig /flushdns #重置DNS 1.3 解决办法2：（一般此方法解决问题）管理员打开命令行输入以下命令重置网络连接配置 netsh winsock reset]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>网卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装redis]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E5%AE%89%E8%A3%85redis%2F</url>
    <content type="text"><![CDATA[1 下载redis下载redis-3.2.9.tar.gz 12345wget http://download.redis.io/releases/redis-3.2.9.tar.gztar -zxvf redis-3.2.9.tar.gzmv redis-3.2.9 redis 2 安装redis123[root@bogon home]# cd redis/[root@bogon redis]# make &amp;&amp; make install 2.1 初始化redis123[root@bogon redis]# cd utils/[root@bogon utils]# ./install_server.sh 通过上图，我们可以看出redis初始化后redis配置文件为/etc/redis/6379.conf，日志文件为/var/log/redis_6379.log，数据文件dump.rdb存放到/var/lib/redis/6379目录下，启动脚本为/etc/init.d/redis_6379。 现在我们要使用systemd，所以在 /etc/systems/system 下创建一个单位文件名字为redis_6379.service。 1vim /etc/systemd/system/redis_6379.service 填写以下内容： 12345678[Unit]Description=Redis on port 6379[Service]Type=forkingExecStart=/etc/init.d/redis_6379 startExecStop=/etc/init.d/redis_6379 stop[Install]WantedBy=multi-user.target 2.2 查看redis版本1redis-cli --version 3 参考资料：http://www.cnblogs.com/sandea/p/5782192.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis+Keepalived双机热备]]></title>
    <url>%2F2018%2F08%2F30%2FRedis-Keepalived%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87%2F</url>
    <content type="text"><![CDATA[1 环境 主机 IP OS 主机 IP OS master-server 172.20.22.145 centos7.2 slave-server 172.20.22.53 centos7.2 2 参考资料https://my.oschina.net/guol/blog/182491 http://blog.csdn.net/qguanri/article/details/51120178 http://blog.csdn.net/zgf19930504/article/details/52024724 3 整体思路在keepalived+redis的使用过程中有四种情况： 一种是keepalived挂了，同时redis也挂了，这样的话直接VIP飘走之后，是不需要进行redis数据同步的，因为redis挂了，你也无法去master上同步，不过会损失已经写在master上却还没同步到slave上面的这部分数据。 另一种是keepalived挂了，redis没挂，这时候VIP飘走后，redis的master/slave还是老的对应关系，如果不变化的话会把数据写入redis slave中，从而不会同步到master上去，这就要借助监控脚本反转redis的master/slave关系。这时候就要预留一点时间进行数据同步，然后反转master/slave。 还有一种是keepalived没挂，redis挂了，这时候根据监控脚本会检测到redis挂了，并且降低keepalived master的优先级，同样会导致VIP飘走，情况和第二种一样，也是需要进行数据同步，然后反转当前redis的master/slave关系的。 随后一种是keepalived没挂，redis也没挂，大吉大利啊，什么都不用操作。 本文的实验环境四种情况都适合，第一种是不需要同步数据的，脚本会默认去同步数据，但是其实是不会成功的。脚本主要是用来处理第二和第三种情况的。 4 安装redis参考本站：centos7安装redis 4.1 修改主从redis配置文件1vim /etc/redis/6379.conf 4.1.1 主redis:12#修改bind绑定IP,只有绑定的IP才能访问redis，0.0.0.0标识所有地址都能访问。bind 0.0.0.0 4.1.2 从redis:1234#修改bind绑定IP,只有绑定的IP才能访问redis，0.0.0.0标识所有地址都能访问。bind 0.0.0.0#添加slaveof&amp;nbsp;IP&amp;nbsp;Port,表示作为该IP服务器的备机slaveof 172.20.22.145 6379 4.2 启动并测试1service redis_6379 start 4.2.1 主redis1234567891011[root@bogon ~]# redis-cli -p 6379 #登录redis127.0.0.1:6379&gt; set hao 333 #添加hao值为333 OK127.0.0.1:6379&gt; get hao #查看hao的值"333"127.0.0.1:6379&gt; 4.2.2 从redis1234567[root@bogon scripts]# redis-cli -p 6379127.0.0.1:6379&gt; get hao #查看hao的值，获取到值说明同步成功。"333"127.0.0.1:6379&gt; 5 安装keepalived1yum install keepalived 5.1 修改配置文件1vim /etc/keepalived/keepalived.conf 1234567891011121314151617181920212223242526272829303132333435#MASTER配置文件：! Configuration File for keepalivedvrrp_script chk_redis &#123; script "/etc/keepalived/scripts/chk_redis.sh" #监控脚本 interval 2 #监控间隔时间 秒 timout 2 #响应超时：超过多长时间未响应认为是失败 fall 3 #检测失败几次，认为是redis服务器挂了 weight -60 #自我确定服务器挂了之后,优先级加多少, 也就是宕机之后 priority 加多少weight，这里是减60&#125;vrrp_instance VI_1 &#123; state MASTER #MASTER表示主，BACKUP表示为从 interface eth0 #eth0表示监听的网卡 virtual_router_id 51 priority 100 #优先级，从服务器推选主服务器就是根据这个来比较的，从服务器必须小于主服务器优先级。 advert_int 1 authentication &#123; auth_type PASS #认证凭证，可自定义 auth_pass 1111 &#125; track_script &#123; chk_redis #运行chk_redis模块 &#125; virtual_ipaddress &#123; 172.20.22.199 #VIP地址（虚拟IP） &#125; unicast_src_ip 172.20.22.145 #keepalived 内部通信,本机ip 地址（没加也没影响） unicast_peer &#123; 172.20.22.53 &#125; notify_master /etc/keepalived/scripts/redis-master.sh #keepalived 状态切换master时执行的脚本 notify_backup /etc/keepalived/scripts/redis-backup.sh #keepalived 状态切换为backup时执行的脚本 notify_fault /etc/keepalived/scripts/redis-fault.sh #keepalived 状态为fault时执行的脚本 notify_stop /etc/keepalived/scripts/redis-stop.sh #keepalived 服务停止时执行脚本&#125; 1234567891011121314151617181920212223242526272829303132333435363738# BACKUP配置文件backup配置文件至修改了2处，其它都一样。! Configuration File for keepalivedvrrp_script chk_redis &#123; script "/etc/keepalived/scripts/chk_redis.sh" ###监控脚本 interval 2 ###监控时间 timout 2 #响应超市：超过多长时间未响应认为是失败 fall 3 #检测失败几次，认为是redis服务器挂了 weight -60 #自我确定服务器挂了之后,优先级加多少, 也就是宕机之后 priority + weight&#125;vrrp_instance VI_1 &#123; state BACKUP interface enp2s0virtual_router_id 51 priority 50 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_redis &#125; virtual_ipaddress &#123; 172.20.22.199 &#125; unicast_src_ip 172.20.22.53 unicast_peer &#123; 172.20.22.145 &#125; notify_master /etc/keepalived/scripts/redis-master.sh notify_backup /etc/keepalived/scripts/redis-backup.sh notify_fault /etc/keepalived/scripts/redis-fault.sh notify_stop /etc/keepalived/scripts/redis-stop.sh&#125; 5.2 创建脚本创建scripts目录 /etc/keepalived,在scripts目录内创建redis-master.sh,redis-backup.sh,redis-fault.sh,redis-stop.sh,chk_redis.sh 5.2.1 chk_redis.sh脚本1234567891011121314151617#!/bin/bash #检测redis是否正常运行，根据检测结果返回不同的值 #日志文件位置 logFile=/usr/etc/redis/keepalived/logs/redis-keepalived.log #ping 本机redis服务 pingRS=`/usr/local/bin/redis-cli -a 123456 PING` #如果ping 的结果为PONG,那么返回0 ,否则返回1；PONG代表PING通redis,当返回值为1时，执行降低优先级操作。if [ $pingRS == "PONG" ]; then echo "[`date`] ping is ok !" &gt;&gt;$logFile exit 0 else echo "[`date`] ping is error !" &gt;&gt;$logFile exit 1 fi 5.2.2 redis-master.sh （master主机脚本与slave主机基本雷同，只需修改远程ip地址） 1234567891011#!/bin/bashrediscli="redis-cli"logfile="/var/log/keepalived-redis-state.log"date &gt;&gt; %logfileecho "[master]" &gt;&gt; $logfileecho " 运行作为远程server备机并同步数据命令" &gt;&gt; $logfile$rediscli SLAVEOF 172.20.22.53 6379 &gt;&gt; $logfile #远程redis IP地址，解释为以该IP为主，做数据同步。sleep 10 #等待10秒，再运行下面命令。echo "运行升级为主服务器命令" &gt;&gt; $logfile$rediscli SLAVEOF NO ONE &gt;&gt; $logfileecho "切换master完成" &gt;&gt; $logfile 5.2.3 redis-backup.sh （master主机脚本与slave主机基本雷同，只需修改远程ip地址） 12345678910#!/bin/bashrediscli="redis-cli"logfile="/var/log/keepalived-redis-state.log"date &gt;&gt; $logfileecho "[backup]" &gt;&gt; $logfileecho "等待13秒同步数据后运行下面命令" &gt;&gt; $logfilesleep 13echo "以远方IP为主机，同步数据" &gt;&gt; $logfile$rediscli slaveof 172.20.22.53 6379 &gt;&gt; $logfileecho "切换backup完成" &gt;&gt; $logfile keepalived进入backup/stop/fault时的检测脚本，由于内容都一致，所以只写出redis_backup.sh 主从主机脚本内容一致，只需修改IP地址为对方IP 1$rediscli slaveof 172.20.22.53 6379 #修改为对方IP 6 测试7 注意事项： VRRP脚本(vrrp_script)和VRRP实例(vrrp_instance)属于同一个级别 notify_master 、notify_backup、notify_fault、notify_stop参数 notify_stop keepalived停止运行前运行notify_stop指定的脚本notify_master keepalived切换到master时执行的脚本notify_backup keepalived切换到backup时执行的脚本notify_fault keepalived出现故障时执行的脚本 启动顺序，先启动redis,后启动keepalived 8 说明此方法可以实现完全的自动化主从切换，但同步数据的时间（即脚本中的sleep时间）生产环境中无法完全掌控，实际使用中建议手动切回，或在主服务器上keepalived.conf中添加不抢占主机参数nopreempt（此参数要求配置文件中keepalived状态都为backup，根据优先级选择master） 示例： 123456789101112131415161718192021vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 51i nopreempt #redis或keepalived恢复后不抢占master，依然作为backup运行。 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_redis &#125; virtual_ipaddress &#123; 172.20.22.199 &#125; notify_backup /etc/keepalived/scripts/redis_backup.sh notify_master /etc/keepalived/scripts/redis_master.sh notify_fault /etc/keepalived/scripts/redis_fault.sh&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>keepalived</tag>
        <tag>双机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7通过smtp发送邮件（解决阿里云ECS不能通过25端口发送邮件）]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E9%80%9A%E8%BF%87smtp%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6-%E8%A7%A3%E5%86%B3%E9%98%BF%E9%87%8C%E4%BA%91ECS%E4%B8%8D%E8%83%BD%E9%80%9A%E8%BF%8725%E7%AB%AF%E5%8F%A3%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1 参考资料 https://bbs.aliyun.com/read/316576.html http://blog.csdn.net/qq_25551295/article/details/51803942 2 安装mailx1yum install mailx 2.1 修改配置文件1vim /etc/mail.rc 追加如下内容 123456set smtp="smtps://smtp.mxhichina.com:465"set smtp-auth=loginset smtp-auth-user="sales@vfutai.xxx"set smtp-auth-password="Ni-De-Mi-Ma"set ssl-verify=ignoreset nss-config-dir=/etc/pki/nssdb 2.2 发送测试例子1： 1echo message3 | mail -v -r "sales@vfutai.xxx" -s "This is the subject" dongshan3@foxmail.xxx message3 正文 `dongshan3@foxmail.xxx` 收件人地址，发送多人加逗号添加邮件地址 -r “sales@vfutai.xxx” 发件人地址 -s &quot;This is the subject&quot; 邮件标题 -a /etc/*.txt 附件 列子2： 123456789#!/usr/bin/bash#发送邮件到kxhuanzi@163.comsj=`date +%Y%m%d`echo "备份mysql" &gt;&gt; /opt/mysqldata/$sj.logmysqldump -uroot -ppassword bitnami_wordpress &gt; /opt/mysqldata/$sj.sql &gt;&gt; /opt/mysqldata/$sj.logecho "发送mysql备份电子邮件到k*****@163.com" &gt;&gt; /opt/mysqldate/$sj.logecho MYSQL备份，备份文件名字:$sj.sql,备份日期:`date`.| mail -v -r "32****@qq.com" -a /opt/mysqldata/$sj.sql -s "wordpress备份/$sj" k****@163.com,xh****@gmail.comecho "发送完成" &gt;&gt; /opt/mysqldata/$sj.log~]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>email</tag>
        <tag>smtp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix监控VMware]]></title>
    <url>%2F2018%2F08%2F30%2Fzabbix%E7%9B%91%E6%8E%A7VMware%2F</url>
    <content type="text"><![CDATA[1 环境 Centos 7.2 Zabbix 3.0 VMware 5.5/6.0 2 步骤2.1 修改配置文件 修改zabbix server配置文件 1vim /etc/zabbix/zabbix_server.conf 添加如下内容 12345StartVMwareCollectors=5VMwareFrequency=60VMwareCacheSize=8M StartVMwareCollectors=5 #预启动的VMware数据采集线程数量，值范围：0~250 VMwareFrequency=60 #VMware的数据检测缓存大小，值范围：256K~2G VMwareCacheSize=8M #数据采集的频率，值范围：10~86400 重启zabbix—server服务 1service zabbix-server restart 2.2 添加VM主机监控 添加主机网上很多教程都说端口改为80,但我改成80后无法获取到数据，但使用10050端口可以获取到（默认VMware无需安装agent） 添加模版 添加宏 12345678&#123;$PASSWORD&#125; #连接VMware的用户密码&#123;$URL&#125;#固定格式：`https://IP/sdk ` 网页访问地址错误，`curl -I -k https://192.168.0.19/sdk `测试 没问题&#123;$USERNAME&#125; #连接VMware的用户，默认root或administrator 过一会就可以看到自动检测到的虚拟主机了，自动匹配监控项，**图形需要自行创建**。]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7搭建sock5代理]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E6%90%AD%E5%BB%BAsock5%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[1 环境 OS:centos 7.3 sock5:ss5-3.8.9 2 安装SS52.1 安装必要组件12yum install pam-devel openldap-devel openssl-devel yum install gcc gcc-c++ 2.2 下载ss51wget https://nchc.dl.sourceforge.net/project/ss5/ss5/3.8.9-8/ss5-3.8.9-8.tar.gz 2.2.1 解压安装1234tar -zxvf ss5-3.8.9-8.tar.gzcd ss5-3.8.9/./configuremake&amp;&amp;makeinstall 2.3 修改配置文件1vim /etc/opt/ss5/ss5.conf 修改如下配置： 12345678910111213# ///////////////////////////////////////////////////////////////////////////////////# SHost SPort Authentication##auth 0.0.0.0/0 - -#取消注释，并修改如下，意思是使用用户密码连接，默认是允许所有人连接。auth 0.0.0.0/0 - u# /////////////////////////////////////////////////////////////////////////////////////////////////# Auth SHost SPort DHost DPort Fixup Group Band ExpDate##permit - 0.0.0.0/0 - 0.0.0.0/0 - - - - - #取消注释，并修改如下，意思是使用用户密码连接，默认是允许所有人连接。permit u 0.0.0.0/0 - 0.0.0.0/0 - - - - - 2.4 修改默认端口号1vim /etc/rc.d/init.d/ss5 修改脚本如下 1234#默认端口号为1080，修改为10888daemon /usr/sbin/ss5 -t $SS5_OPTS修改为daemon /usr/sbin/ss5 -t $SS5_OPTS -b 0.0.0.0:10888 2.5 添加帐号密码1vim /etc/opt/ss5/ss5.passwd 格式: 用户+密码 2.6 启动ss5123chmod 700 /etc/rc.d/init.d/ss5 #添加执行权限service ss5 start #启动服务chkconfig ss5 on #开机自启 2.7 检测启动12[root@blog ss5-3.8.9]# netstat -tunlp|grep ss5tcp 0 0 0.0.0.0:10888 0.0.0.0:* LISTEN 22387/ss5 安装完成。 可使用sock5客户端proxifier等进行连接。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>sock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[window下安装压缩版mysql5.6.35]]></title>
    <url>%2F2018%2F08%2F30%2Fwindow%E4%B8%8B%E5%AE%89%E8%A3%85%E5%8E%8B%E7%BC%A9%E7%89%88mysql5-6-35%2F</url>
    <content type="text"><![CDATA[1 环境： OS: windows 7 x64 mysql:mysql-5.6.35-winx64.zip 2 部署2.1 下载及解压解压缩mysql到自定义位置，并重命名为mysql 2.2 添加mysql环境变量编辑path系统变量，在后面追加C:\mysql\bin; 前面用分号“；”隔开。 path完整变量显示如下： 1C:\ProgramData\Oracle\Java\javapath;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;D:\Program Files\MySQL\MySQL Server 5.5\bin;D:\Program Files\php-5.4;D:\Program Files\php-5.4\ext;C:\mysql\bin; 重启生效 2.3 安装mysql使用管理员权限打开命令窗口 12mysqld --install #安装mysql命令mysqld --remove #卸载mysql命令 123net start mysql #启动数据库mysql -uroot -p #登录mysql,默认密码为空。net stop mysql #停止数据库]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>部署</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7缩减home目录扩展根目录（xfs分区格式）]]></title>
    <url>%2F2018%2F08%2F30%2FCentos7%E7%BC%A9%E5%87%8Fhome%E7%9B%AE%E5%BD%95%E6%89%A9%E5%B1%95%E6%A0%B9%E7%9B%AE%E5%BD%95-xfs%E5%88%86%E5%8C%BA%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1 磁盘占用 修改home目录大小，增加根目录容量。 2 注意事项 xfsf分区减小分区无法做到无损数据（必须备份数据） 扩容分区可以保存数据无损 3 xfs分区扩展根目录命令过程把/home内容备份，然后将/home文件系统所在的逻辑卷删除，扩大/root文件系统，新建/home： 12345678910tar cvf /tmp/home.tar /home #备份/homeumount /home #卸载/home，如果无法卸载，先终止使用/home文件系统的进程lvremove /dev/centos/home #删除/home所在的lv（可删除分区，也可使用下行缩减分区）lvreduce -L 180G /dev/centos/home #缩小分区到180G(缩小分区后分区数据丢失，需重新格式化并挂载分区，否则重启故障)lvextend -L +50G /dev/centos/root #扩展/root所在的lv，增加50Gxfs_growfs /dev/centos/root #扩展/root文件系统lvcreate -L 56G -n home centos #重新创建home lvmkfs.xfs /dev/centos/home #创建文件系统及格式化mount /dev/centos/home /home #挂载df -h 4 收缩分区 ext与xfs格式分区收缩分区命令不同，lvextend -L+19.8G /dev/VolGroup00/LogVol00分区命令后，执行相应收缩命令后扩展的容量才能正常使用 xfs格式如果分区格式为xfs，扩展使用 要用xfs_growfs命令而不是resize2fs命令收缩分区， 1[root@rac2 ~]# xfs_growfs /dev/VolGroup00/LogVol00 执行以上命令后，df -h命令查看才会显示扩容成功。 ext2、ext3、ext4格式（补充）Ext2、ext3、ext4等使用resize2fs收缩分区，xfs使用xfs-growfs命令收缩分区。详情找度娘。 例如: 1[root@rac2 ~]# resize2fs -p /dev/VolGroup00/LogVol00]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux调整目录分区大小，linux调整home目录大小，linux调整root目录大小（ext格式/LVM）]]></title>
    <url>%2F2018%2F08%2F30%2Flinux%E8%B0%83%E6%95%B4%E7%9B%AE%E5%BD%95%E5%88%86%E5%8C%BA%E5%A4%A7%E5%B0%8F%EF%BC%8Clinux%E8%B0%83%E6%95%B4home%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F%EF%BC%8Clinux%E8%B0%83%E6%95%B4root%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F-ext%E6%A0%BC%E5%BC%8F-LVM%2F</url>
    <content type="text"><![CDATA[说明：ext格式分区可无损扩大或缩小分区。要先对文件系统进行缩小，然后才能缩小逻辑卷，一层层向下。和扩大正好相反。 注意vg_sql-lv_home其中的sql其实为hostname! resize2fs命令resize2fs命令被用来增大或者收缩未加载的“ext2/ext3”文件系统的大小。如果文件系统是处于mount状态下，那么它只能做到扩容，前提条件是内核支持在线resize。，linux kernel 2.6支持在mount状态下扩容但仅限于ext3文件系统。 来自: http://man.linuxde.net/resize2fs 一、首先df -h查看分区情况（这里我想调整home目录）123456[root@sql ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_sql-lv_root 50G 906M 46G 2% /tmpfs 935M 0 935M 0% /dev/shm/dev/sda1 477M 30M 422M 7% /boot/dev/mapper/vg_sql-lv_home 341G 67M 323G 1% /home 二、卸载home目录umount /home123456[root@sql ~]# umount /home[root@sql ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_sql-lv_root 50G 706M 46G 2% /tmpfs 935M 0 935M 0% /dev/shm/dev/sda1 477M 30M 422M 7% /boot 三、重新指定/home目录大小缩小文件系统 12345678910111213141516[root@sql ~]# e2fsck -f /dev/mapper/vg_sql-lv_homee2fsck 1.41.12 (17-May-2010)Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary information/dev/mapper/vg_sql-lv_home: 11/22650880 files (0.0% non-contiguous), 1471409/90597376 blocks[root@sql ~]# resize2fs -p /dev/mapper/vg_sql-lv_home 30Gresize2fs 1.41.12 (17-May-2010)Resizing the filesystem on /dev/mapper/vg_sql-lv_home to 7864320 (4k) blocks.Begin pass 2 (max = 32768)Relocating blocks XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXBegin pass 3 (max = 2765)Scanning inode table XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXThe filesystem on /dev/mapper/vg_sql-lv_home is now 7864320 blocks long. 四、挂载/home，然后查看调整后的大小1234567[root@sql ~]# mount /home[root@sql ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_sql-lv_root 50G 706M 46G 2% /tmpfs 935M 0 935M 0% /dev/shm/dev/sda1 477M 30M 422M 7% /boot/dev/mapper/vg_sql-lv_home 30G 44M 28G 1% /home 五、用lvreduce命令把目标分区(/home)减小至30G缩小逻辑卷 123456[root@sql ~]# lvreduce -L 30G /dev/mapper/vg_sql-lv_homeWARNING: Reducing active and open logical volume to 30.00 GiBTHIS MAY DESTROY YOUR DATA (filesystem etc.)Do you really want to reduce lv_home? [y/n]: y Size of logical volume vg_sql/lv_home changed from 345.60 GiB (88474 extents) to 30.00 GiB (7680 extents). Logical volume lv_home successfully resized 六、用vgdisplay命令查看多余的空间，可以看到多出约320G的空间123456789101112131415161718192021[root@sql ~]# vgdisplay --- Volume group --- VG Name vg_sql System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 5 VG Access read/write VG Status resizable MAX LV 0 Cur LV 3 Open LV 3 Max PV 0 Cur PV 1 Act PV 1 VG Size 399.51 GiB PE Size 4.00 MiB Total PE 102274 Alloc PE / Size 21480 / 83.91 GiB Free PE / Size 80794 / 315.60 GiB VG UUID L9OUKR-6alh-ms7H-yimo-ypYm-lLYa-DqkpMC 七、用lvextend命令将多余的约320G空间挂载到/目录下扩大逻辑卷 注：在设定lv_root的大小时，不要把Free PE / Size的空间全部都用上，这很可能会出现FreePE空间不足的现象，建议保留一点Free PE的空间。 另：我这里搞上完没有出错，其实没有出错，查看空闲大小，显示Free PE / Size 0 / 0 1234[root@sql ~]# lvextend -L +315.60G /dev/mapper/vg_sql-lv_root Rounding size to boundary between physical extents: 315.60 GiB Size of logical volume vg_sql/lv_root changed from 50.00 GiB (12800 extents) to 365.60 GiB (93594 extents). Logical volume lv_root successfully resized 八、激活目录大小（扩展后的/目录）扩大文件系统 注：执行这个命令后，会进入漫长的等待，这里我是机械硬盘，且调整分区约320G，耗时较长 123456[root@sql ~]# resize2fs -p /dev/mapper/vg_sql-lv_rootresize2fs 1.41.12 (17-May-2010)Filesystem at /dev/mapper/vg_sql-lv_root is mounted on /; on-line resizing requiredold desc_blocks = 4, new_desc_blocks = 23Performing an on-line resize of /dev/mapper/vg_sql-lv_root to 95840256 (4k) blocks.The filesystem on /dev/mapper/vg_sql-lv_root is now 95840256 blocks long. 九、df -h查看修改成功后的分区情况123456[root@sql ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_sql-lv_root 360G 720M 341G 1% /tmpfs 935M 0 935M 0% /dev/shm/dev/sda1 477M 30M 422M 7% /boot/dev/mapper/vg_sql-lv_home 30G 44M 28G 1% /home 转载：https://www.cplusplus.me/2316.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux调整目录分区大小，linux调整home目录大小，linux调整root目录大小(xfs格式/LVM)]]></title>
    <url>%2F2018%2F08%2F30%2Flinux%E8%B0%83%E6%95%B4%E7%9B%AE%E5%BD%95%E5%88%86%E5%8C%BA%E5%A4%A7%E5%B0%8F%EF%BC%8Clinux%E8%B0%83%E6%95%B4home%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F%EF%BC%8Clinux%E8%B0%83%E6%95%B4root%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F-xfs%E6%A0%BC%E5%BC%8F-LVM%2F</url>
    <content type="text"><![CDATA[说明：xfs格式分区无法无损缩减分区，可无损扩大分区。 一：环境概览：/1571817332831.png)/1571817343568.png) 二、操作步骤：12345678910111213141516171819# 1.终止占用 /home 进程 fuser -m -v -i -k /home # 2.备份/home cp -r /home/ homebak/ # 3.卸载 /home umount /home # 4.删除/home所在的lv lvremove /dev/mapper/centos-home # 5.扩展/root所在的lv，增加100G lvextend -L +100G /dev/mapper/centos-root # 6.扩展/root文件系统 xfs_growfs /dev/mapper/centos-root # 7.重新创建home lv lvcreate -L 40G -n home centos # 8.创建文件系统 mkfs.xfs /dev/centos/home # 9.挂载 mount /dev/centos/home /home # 10.还原 /home 相关文件以及对应目录权限 三、执行结果： /1571817356365.png) 四、实测成功：依旧教程扩容xfs，测试成功！ 转载：https://www.cplusplus.me/2717.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7进入单用户模式修改root密码]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E8%BF%9B%E5%85%A5%E5%8D%95%E7%94%A8%E6%88%B7%E6%A8%A1%E5%BC%8F%E4%BF%AE%E6%94%B9root%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[init方法 1、centos7的grub2界面会有两个入口，正常系统入口和救援模式； 2、修改grub2引导 在正常系统入口上按下”e“，会进入edit模式，搜寻ro那一行，以linux16开头的； 把ro更改成rw；（把只读更改成可写） 把rhgb quiet删除；（quiet模式没有代码行唰唰的走，可以删除） 增加init=/bin/sh；（或init=/bin/bash,指定shell环境) 按下ctrl+x来启动系统。 3、修改root密码 #passwd #修改密码 #touch /.autorelabel #据说是selinux在重启后更新label #exec /sbin/init #正常启动init进程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7如何进入救援模式_resue]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E5%A6%82%E4%BD%95%E8%BF%9B%E5%85%A5%E6%95%91%E6%8F%B4%E6%A8%A1%E5%BC%8F-resue%2F</url>
    <content type="text"><![CDATA[进入系统开机引导界面，按↓键，按e键，找到linux16开头的行，在后面添加systemd.unit=rescue.target,然后按ctrl+x来进入系统，就能够进入rescue的环境了，输入帐号及密码，就可以进行相应操作。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[widowns配额管理]]></title>
    <url>%2F2018%2F08%2F30%2Fwidowns%E9%85%8D%E9%A2%9D%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[1 简介 磁盘配额就是管理员可以为用户所能使用的磁盘空间进行配额限制，每一用户只能使用最大配额范围内的磁盘空间。 2 案例公共共享文件夹限制用户上传文件占用空间； 3 操作3.1 第1步：在“我的电脑”窗口中右键单击共享文件夹所在的磁盘分区，选择“属性”快捷命令，打开磁盘属性对话框。然后切换到“配额”选项卡，保持“启用配额管理”和“拒绝将磁盘空间给超过配额限制的用户”复选框的选中状态。另外建议选中“用户超出配额限制时记录事件”和“用户超过警告等级时记录事件”两个复选框，以便将配额告警记录到日志中。接着单击“配额项”按钮。 3.2 第2步：打开“本地磁盘的配额项”窗口，依次单击“配额”→“新建配额项”菜单命令，在打开的“选择用户”对话框中查找并选中目标用户并单击“确定”按钮。 3.3 第3步:在打开的“添加新配额项”对话框中选中“将磁盘空间限制为”单选框，并设置空间大小为100MB。接着在“将警告等级设置为”编辑框中设置空间大小为95MB。最后单击“确定”按钮使设置生效。 3.4 第4步:返回“本地磁盘的配额项”窗口，重复上述步骤针对其他用户新建配额项，设置完毕关闭该窗口，返回“本地磁盘 属性”对话框后单击“确定”按钮。设置配额项的用户只能使用规定容量以内的磁盘空间。]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker基础命令]]></title>
    <url>%2F2018%2F08%2F28%2FDocker%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1 获取镜像 从 Docker Registry 获取镜像的命令是 docker pull。其命令格式为： 1docker pull [选项] [Docker Registry地址]&lt;仓库名&gt;:&lt;标签&gt; 例子： 1docker pull ubuntu:14.04 2 运行镜像docker run 1234567891011121314$ docker run -it --rm ubuntu:14.04 bashroot@e7009c6ce357:/# cat /etc/os-releaseNAME="Ubuntu"VERSION="14.04.5 LTS, Trusty Tahr"ID=ubuntuID_LIKE=debianPRETTY_NAME="Ubuntu 14.04.5 LTS"VERSION_ID="14.04"HOME_URL="http://www.ubuntu.com/"SUPPORT_URL="http://help.ubuntu.com/"BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"root@e7009c6ce357:/# exitexit$ docker run 就是运行容器的命令，具体格式我们会在后面的章节讲解，我们这里简要的说明一下上面用到的参数。 -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用--rm 可以避免浪费空间。 ubuntu:14.04：这是指用 ubuntu:14.04 镜像为基础来启动容器。 bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是bash。 进入容器后，我们可以在 Shell 下操作，执行任何所需的命令。这里，我们执行了 cat /etc/os-release，这是 Linux 常用的查看当前系统版本的命令，从返回的结果可以看到容器内是 Ubuntu 14.04.5 LTS 系统。 最后我们通过exit退出了这个容器。 3 列出镜像docker images 123456789$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MBmongo 3.2 fe9198c04d62 5 days ago 342 MB&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MBubuntu 16.04 f753707788c5 4 weeks ago 127 MBubuntu latest f753707788c5 4 weeks ago 127 MBubuntu 14.04 1e0c3dd64ccd 4 weeks ago 188 MB 列表包含了仓库名、标签、镜像 ID、创建时间以及所占用的空间。 其中仓库名、标签在之前的基础概念章节已经介绍过了。镜像 ID 则是镜像的唯一标识，一个镜像可以对应多个标签。因此，在上面的例子中，我们可以看到ubuntu:16.04 和ubuntu:latest拥有相同的 ID，因为它们对应的是同一个镜像。 4 虚悬镜像上面的镜像列表中，还可以看到一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 &lt;none&gt;。： 1&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像： 123$ docker images -f dangling=trueREPOSITORY TAG IMAGE ID CREATED SIZE&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 5 删除镜像一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。 1$ docker rmi $(docker images -q -f dangling=true) 6 中间层镜像为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像。所以在使用一段时间后，可能会看到一些依赖的中间层镜像。默认的 docker images 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加-a参数。 1$ docker images -a 这样会看到很多无标签的镜像，与之前的虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。实际上，这些镜像也没必要删除，因为之前说过，相同的层只会存一遍，而这些镜像是别的镜像的依赖，因此并不会因为它们被列出来而多存了一份，无论如何你也会需要它们。只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除。 7 列出部分镜像不加任何参数的情况下，docker images会列出所有顶级镜像，但是有时候我们只希望列出部分镜像。docker images有好几个参数可以帮助做到这个事情。 根据仓库名列出镜像 12345$ docker images ubuntuREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 16.04 f753707788c5 4 weeks ago 127 MBubuntu latest f753707788c5 4 weeks ago 127 MBubuntu 14.04 1e0c3dd64ccd 4 weeks ago 188 MB 列出特定的某个镜像，也就是说指定仓库名和标签 123$ docker images ubuntu:16.04REPOSITORY TAG IMAGE ID CREATED SIZEubuntu 16.04 f753707788c5 4 weeks ago 127 MB 除此以外，docker images还支持强大的过滤器参数 --filter，或者简写-f。之前我们已经看到了使用过滤器来列出虚悬镜像的用法，它还有更多的用法。比如，我们希望看到在 mongo:3.2之后建立的镜像，可以用下面的命令： 1234$ docker images -f since=mongo:3.2REPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MB 想查看某个位置之前的镜像也可以，只需要把 since换成 before 即可。 此外，如果镜像构建时，定义了 LABEL，还可以通过 LABEL 来过滤。 12$ docker images -f label=com.example.version=0.1... 8 以特定格式显示默认情况下，docker images 会输出一个完整的表格，但是我们并非所有时候都会需要这些内容。比如，刚才删除虚悬镜像的时候，我们需要利用 docker images 把所有的虚悬镜像的 ID 列出来，然后才可以交给docker rmi 命令作为参数来删除指定的这些镜像，这个时候就用到了-q参数。 12345678$ docker images -q5f515359c7f805a60462f8bafe9198c04d6200285df0df87f753707788c5f753707788c51e0c3dd64ccd --filter 配合-q产生出指定范围的 ID 列表，然后送给另一个 docker 命令作为参数，从而针对这组实体成批的进行某种操作的做法在 Docker命令行使用过程中非常常见，不仅仅是镜像，将来我们会在各个命令中看到这类搭配以完成很强大的功能。因此每次在文档看到过滤器后，可以多注意一下它们的用法。 另外一些时候，我们可能只是对表格的结构不满意，希望自己组织列；或者不希望有标题，这样方便其它程序解析结果等，这就用到了 Go 的模板语法。 比如，下面的命令会直接列出镜像结果，并且只包含镜像ID和仓库名： 12345678$ docker images --format "&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;"5f515359c7f8: redis05a60462f8ba: nginxfe9198c04d62: mongo00285df0df87: &lt;none&gt;f753707788c5: ubuntuf753707788c5: ubuntu1e0c3dd64ccd: ubuntu 或者打算以表格等距显示，并且有标题行，和默认一样，不过自己定义列： 123456789$ docker images --format "table &#123;&#123;.ID&#125;&#125;\t&#123;&#123;.Repository&#125;&#125;\t&#123;&#123;.Tag&#125;&#125;"IMAGE ID REPOSITORY TAG5f515359c7f8 redis latest05a60462f8ba nginx latestfe9198c04d62 mongo 3.200285df0df87 &lt;none&gt; &lt;none&gt;f753707788c5 ubuntu 16.04f753707788c5 ubuntu latest1e0c3dd64ccd ubuntu 14.04 9 启动容器1docker run --name webserver -d -p 80:80 nginx 这条命令会用nginx镜像启动一个容器，命名为 webserver，并且映射了 80 端口，这样我们可以用浏览器去访问这个 nginx 服务器。 9.1 运行容器容器创建后二次启动及关闭使用： 1234[root@mini ~]# docker start webweb[root@mini ~]# docker stop webweb 10 进入容器现在，假设我们非常不喜欢这个欢迎页面，我们希望改成欢迎 Docker 的文字，我们可以使用 docker exec命令进入容器，修改其内容。 1234$ docker exec -it webserver bashroot@3729b97e8226:/# echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.htmlroot@3729b97e8226:/# exitexit 我们以交互式终端方式进入 webserver 容器，并执行了 bash 命令，也就是获得一个可操作的 Shell。 然后，我们用&lt;h1&gt;Hello, Docker!&lt;/h1&gt; 覆盖了/usr/share/nginx/html/index.html 的内容。 现在我们再刷新浏览器的话，会发现内容被改变了。 我们修改了容器的文件，也就是改动了容器的存储层。我们可以通过 docker diff 命令看到具体的改动。 1234567891011121314151617$ docker diff webserverC /rootA /root/.bash_historyC /runC /usrC /usr/shareC /usr/share/nginxC /usr/share/nginx/htmlC /usr/share/nginx/html/index.htmlC /varC /var/cacheC /var/cache/nginxA /var/cache/nginx/client_tempA /var/cache/nginx/fastcgi_tempA /var/cache/nginx/proxy_tempA /var/cache/nginx/scgi_tempA /var/cache/nginx/uwsgi_temp 11 利用commit定制镜像现在我们定制好了变化，我们希望能将其保存下来形成镜像。 要知道，当我们运行一个容器的时候（如果不使用卷的话），我们做的任何文件修改都会被记录于容器存储层里。而 Docker 提供了一个docker commit 命令，可以将容器的存储层保存下来成为镜像。换句话说，就是在原有镜像的基础上，再叠加上容器的存储层，并构成新的镜像。以后我们运行这个新镜像的时候，就会拥有原有容器最后的文件变化。 docker commit的语法格式为： 1docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]] 我们可以用下面的命令将容器保存为镜像： 123456$ docker commit \ --author "Tao Wang &lt;twang2218@gmail.com&gt;" \ --message "修改了默认网页" \ webserver \ nginx:v2sha256:07e33465974800ce65751acc279adc6ed2dc5ed4e0838f8b86f0c87aa1795214 其中--author 是指定修改的作者，而 --message则是记录本次修改的内容。这点和git 版本控制相似，不过这里这些信息可以省略留空。 我们可以在docker images中看到这个新定制的镜像： 12345$ docker images nginxREPOSITORY TAG IMAGE ID CREATED SIZEnginx v2 07e334659748 9 seconds ago 181.5 MBnginx 1.11 05a60462f8ba 12 days ago 181.5 MBnginx latest e43d811ce2f4 4 weeks ago 181.5 MB 我们还可以用 docker history具体查看镜像内的历史记录，如果比较nginx:latest 的历史记录，我们会发现新增了我们刚刚提交的这一层。 1234567891011$ docker history nginx:v2IMAGE CREATED CREATED BY SIZE COMMENT07e334659748 54 seconds ago nginx -g daemon off; 95 B 修改了默认网页e43d811ce2f4 4 weeks ago /bin/sh -c #(nop) CMD ["nginx" "-g" "daemon 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) EXPOSE 443/tcp 80/tcp 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c ln -sf /dev/stdout /var/log/nginx/ 22 B&lt;missing&gt; 4 weeks ago /bin/sh -c apt-key adv --keyserver hkp://pgp. 58.46 MB&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) ENV NGINX_VERSION=1.11.5-1 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) MAINTAINER NGINX Docker Ma 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) CMD ["/bin/bash"] 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) ADD file:23aa4f893e3288698c 123 MB 新的镜像定制好后，我们可以来运行这个镜像。 1docker run --name web2 -d -p 81:80 nginx:v2 这里我们命名为新的服务为 web2，并且映射到 81 端口。如果是Docker for Mac/Windows 或Linux 桌面的话，我们就可以直接访问http://localhost:81 看到结果，其内容应该和之前修改后的 webserver 一样。 至此，我们第一次完成了定制镜像，使用的是 docker commit 命令，手动操作给旧的镜像添加了新的一层，形成新的镜像，对镜像多层存储应该有了更直观的感觉。 11.1 慎用 docker commit使用docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念，但是实际环境中并不会这样使用。 首先，如果仔细观察之前的 docker diff webserver 的结果，你会发现除了真正想要修改的/usr/share/nginx/html/index.html 文件外，由于命令的执行，还有很多文件被改动或添加了。这还仅仅是最简单的操作，如果是安装软件包、编译构建，那会有大量的无关内容被添加进来，如果不小心清理，将会导致镜像极为臃肿。 此外，使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。而且，即使是这个制作镜像的人，过一段时间后也无法记清具体在操作的。虽然 docker diff 或许可以告诉得到一些线索，但是远远不到可以确保生成一致镜像的地步。这种黑箱镜像的维护工作是非常痛苦的。 而且，回顾之前提及的镜像所使用的分层存储的概念，除当前层外，之前的每一层都是不会发生改变的，换句话说，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。如果使用 docker commit制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到™。这会让镜像更加臃肿。 docker commit 命令除了学习之外，还有一些特殊的应用场合，比如被入侵后保存现场等。但是，不要使用 docker commit 定制镜像，定制行为应该使用 Dockerfile 来完成。下面的章节我们就来讲述一下如何使用 Dockerfile 定制镜像。 12 使用 Dockerfile 定制镜像从刚才的 docker commit 的学习中，我们可以了解到，镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 还以之前定制 nginx镜像为例，这次我们使用 Dockerfile 来定制。 在一个空白目录中，建立一个文本文件，并命名为 Dockerfile： 123$ mkdir mynginx$ cd mynginx$ touch Dockerfile 其内容为： 12FROM nginxRUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html 这个Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN。 12.1 FROM 指定基础镜像所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定基础镜像，因此一个Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 在 Docker Hub1 上有非常多的高质量的官方镜像， 有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat等； 也有一些方便开发、构建、运行各种语言应用的镜像，如node、openjdk、python、ruby、golang 等。 可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。 如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine等，这些操作系统的软件库为我们提供了更广阔的扩展空间。 除了选择现有镜像为基础镜像外，Docker还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。 12FROM scratch... 如果你以scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。 不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见，比如 warm、coreos/etcd。对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch会让镜像体积更加小巧。使用 Go 语言 开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 Go 是特别适合容器微服务架构的语言的原因之一。 13 RUN 执行命令RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式：RUN &lt;命令&gt;，就像直接在命令行中输入的命令一样。刚才写的Dockrfile中的 RUN 指令就是这种格式。 1RUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html exec 格式：RUN [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;]，这更像是函数调用中的格式。 123456789FROM debian:jessieRUN apt-get updateRUN apt-get install -y gcc libc6-dev makeRUN wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz"RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 之前说过，Dockerfile 中每一个指令都会建立一层，RUN也不例外。每一个 RUN的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit这一层的修改，构成新的镜像。 而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。 Union FS是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 上面的 Dockerfile 正确的写法应该是这样： 1234567891011121314FROM debian:jessieRUN buildDeps='gcc libc6-dev make' \ &amp;&amp; apt-get update \ &amp;&amp; apt-get install -y $buildDeps \ &amp;&amp; wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz" \ &amp;&amp; mkdir -p /usr/src/redis \ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \ &amp;&amp; make -C /usr/src/redis \ &amp;&amp; make -C /usr/src/redis install \ &amp;&amp; rm -rf /var/lib/apt/lists/* \ &amp;&amp; rm redis.tar.gz \ &amp;&amp; rm -r /usr/src/redis \ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个RUN 对一一对应不同的命令，而是仅仅使用一个 RUN指令，并使用 &amp;&amp; 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 并且，这里为了格式化还进行了换行。Dockerfile 支持Shell 类的行尾添加\的命令换行方式，以及行首#进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。 此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 很多人初学Docker制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 14 构建镜像好了，让我们再回到之前定制的 nginx 镜像的 Dockerfile 来。现在我们明白了这个 Dockerfile 的内容，那么让我们来构建这个镜像吧。 在Dockerfile文件所在目录执行： 123456789$ docker build -t nginx:v3 .Sending build context to Docker daemon 2.048 kBStep 1 : FROM nginx ---&gt; e43d811ce2f4Step 2 : RUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html ---&gt; Running in 9cdc27646c7b ---&gt; 44aa4490ce2cRemoving intermediate container 9cdc27646c7bSuccessfully built 44aa4490ce2c 从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2 中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。 这里我们使用了 docker build命令进行镜像构建。其格式为： docker build [选项] &lt;上下文路径/URL/-&gt;在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，我们可以像之前运行 nginx:v2 那样来运行这个镜像，其结果会和 nginx:v2 一样。 15 镜像构建上下文（Context）如果注意，会看到 docker build 命令最后有一个.。.表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定Dockerfile所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定上下文路径。那么什么是上下文呢？ 首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种C/S设计，让我们操作远程服务器的 Docker引擎变得轻而易举。 当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过COPY指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是Docker 引擎中构建的。那么在这种客户端/服务端的架构中，如何才能让服务端获得本地文件呢？ 这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。 如果在Dockerfile中这么写： 1COPY ./package.json /app/ 这并不是要复制执行docker build 命令所在的目录下的 package.json，也不是复制Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的package.json。 因此，COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 COPY ../package.json /app 或者COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。 现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 Docker引擎以帮助构建镜像。 如果观察 docker build 输出，我们其实已经看到了这个发送上下文的过程： 123$ docker build -t nginx:v3 .Sending build context to Docker daemon 2.048 kB... 理解构建上下文对于镜像构建是很重要的，避免犯一些不应该的错误。比如有些初学者在发现 COPY /opt/xxxx /app 不工作后，于是干脆将 Dockerfile 放到了硬盘根目录去构建，结果发现 docker build执行后，在发送一个几十 GB 的东西，极为缓慢而且很容易构建失败。那是因为这种做法是在让 docker build 打包整个硬盘，这显然是使用错误。 一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 Docker引擎的。 那么为什么会有人误以为 . 是指定Dockerfile所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为Dockerfile。 这只是默认行为，实际上Dockerfile 的文件名并不要求必须为 Dockerfile，而且并不要求必须位于上下文目录中，比如可以用 -f ../Dockerfile.php 参数指定某个文件作为Dockerfile。 当然，一般大家习惯性的会使用默认的文件名 Dockerfile，以及会将其置于镜像构建上下文目录中。 16 其它 docker build 的用法16.1 直接用 Git repo 进行构建或许你已经注意到了，docker build 还支持从 URL 构建，比如可以直接从 Git repo 中构建： 12345678$ docker build https://github.com/twang2218/gitlab-ce-zh.git#:8.14docker build https://github.com/twang2218/gitlab-ce-zh.git\#:8.14Sending build context to Docker daemon 2.048 kBStep 1 : FROM gitlab/gitlab-ce:8.14.0-ce.08.14.0-ce.0: Pulling from gitlab/gitlab-ceaed15891ba52: Already exists773ae8583d14: Already exists... 这行命令指定了构建所需的Git repo，并且指定默认的 master 分支，构建目录为 /8.14/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。 16.2 用给定的 tar 压缩包构建1$ docker build http://server/context.tar.gz 如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。 16.3 从标准输入中读取 Dockerfile 进行构建1docker build - &lt; Dockerfile 或 1cat Dockerfile | docker build - 如果标准输入传入的是文本文件，则将其视为Dockerfile，并开始构建。这种形式由于直接从标准输入中读取 Dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件COPY 进镜像之类的事情。 16.4 从标准输入中读取上下文压缩包进行构建1$ docker build - &lt; context.tar.gz 如果发现标准输入的文件格式是 gzip、bzip2 以及xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos搭建私有仓库]]></title>
    <url>%2F2018%2F08%2F28%2FCentos%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1 本地YUM源 1.1 安装createrepo1yum -y install createrepo 1.2 创建目录1mkdir -p /yum/yum-custom/packages 1.3 下载rpm包及依赖到packages rpm包放在/yum-custom/ “``”可以是自定义文件夹，不一定要是packages 1yum install httpd --downloadonly --downloaddir=/yum/yum-custom/packages 1.4 创建repo12345678910111213$ createrepo /yum/yum-custom/Spawning worker 0 with 1 pkgsSpawning worker 1 with 0 pkgsWorkers FinishedSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs complete$ ll /yum/yum-custom/total 4drwxr-xr-x. 2 root root 41 Dec 20 07:03 packagesdrwxr-xr-x. 2 root root 4096 Dec 20 07:08 repodata 1.5 配置自定义repo1.5.1 删除备份repo文件123456[root@localhost /]# cd /etc/yum.repos.d/[root@localhost yum.repos.d]# lsCentOS-Base.repo CentOS-CR.repo CentOS-fasttrack.repo CentOS-Media.repo.bak CentOS-QEMU-EV.repo CentOS-Vault.repoCentOS-Ceph-Hammer.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-OpenStack-mitaka.repo CentOS-Sources.repo repo.bak.tar[root@localhost yum.repos.d]# tar zcvf *.repo[root@localhost yum.repos.d]# rm -f *.repo 1.5.2 创建CentOS-Media.repo123456789# vi /etc/yum.repos.d/CentOS-Media.repo# 填入如下内容[c7-media]name=CentOS-$releasever - Mediabaseurl=file:///yum/yum-custom/gpgcheck=0 enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 1.6 重建YUM缓存12345678910111213[root@linuxidc.com ~]# yum clean all #删除缓存Loaded plugins: fastestmirrorCleaning repos: c7-mediaCleaning up everythingCleaning up list of fastest mirrors[root@linuxidc.com ~]# yum makecache #建立缓存Loaded plugins: fastestmirrorc7-media | 3.0 kB 00:00:00 (1/3): c7-media/filelists_db | 880 B 00:00:00 (2/3): c7-media/primary_db | 1.8 kB 00:00:00 (3/3): c7-media/other_db | 1.3 kB 00:00:00 Determining fastest mirrorsMetadata Cache Created 1.7 使用本地yum1yum install httpd 2 局域网YUM源（vsftpd）2.1 安装createrepo1yum -y install createrepo 2.2 安装vsftpd1yum -y install vsftpd 2.2.1 配置vsftpd.conf1234# vi /etc/vsftpd/vsftpd.conf#并增加匿名用户root目录（默认已经启用匿名访问）anon_root=/yum/ 2.2.2 启动vsftpd12systemctl start vsftpd #启动服务systemctl enable vsftpd #开机启动 2.3 创建目录1mkdir -p /yum/yum-custom/packages 2.4 下载rpm包及依赖到packages rpm包放在/yum-custom/ “”可以是自定义文件夹，不一定要是packages 1yum install httpd --downloadonly --downloaddir=/yum/yum-custom/packages 2.5 创建repo12345678910111213# createrepo /yum/yum-custom/Spawning worker 0 with 1 pkgsSpawning worker 1 with 0 pkgsWorkers FinishedSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs complete# ll /yum/yum-custom/total 4drwxr-xr-x. 2 root root 41 Dec 20 07:03 packagesdrwxr-xr-x. 2 root root 4096 Dec 20 07:08 repodata 以下步骤局域网机器操作！ 2.5.1 删除备份repo文件123456[root@localhost /]# cd /etc/yum.repos.d/[root@localhost yum.repos.d]# lsCentOS-Base.repo CentOS-CR.repo CentOS-fasttrack.repo CentOS-Media.repo.bak CentOS-QEMU-EV.repo CentOS-Vault.repoCentOS-Ceph-Hammer.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-OpenStack-mitaka.repo CentOS-Sources.repo repo.bak.tar[root@localhost yum.repos.d]# tar zcvf *.repo[root@localhost yum.repos.d]# rm -f *.repo 2.5.2 修改自定义repo如下 修改局域网服务器的repo，哪个服务器使用，修改哪个服务器文件。 12345678910[root@localhost /]# vim /etc/yum.repos.d/CentOS-Media.repo# 内容如下[c7-media]name=CentOS-$releasever - Mediabaseurl=ftp://192.168.118.133/yum-custom #注意是ftp，不是filegpgcheck=0enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7# 其中192.168.118.133为上面vsftp服务器地址 2.6 重建YUM缓存12345678910111213[root@linuxidc.com ~]# yum clean all #删除缓存Loaded plugins: fastestmirrorCleaning repos: c7-mediaCleaning up everythingCleaning up list of fastest mirrors[root@linuxidc.com ~]# yum makecache #建立缓存Loaded plugins: fastestmirrorc7-media | 3.0 kB 00:00:00 (1/3): c7-media/filelists_db | 880 B 00:00:00 (2/3): c7-media/primary_db | 1.8 kB 00:00:00 (3/3): c7-media/other_db | 1.3 kB 00:00:00 Determining fastest mirrorsMetadata Cache Created 2.7 使用局域网yum1yum install httpd 3 局域网YUM源（httpd）3.1 安装createrepo1yum -y install createrepo 3.2 安装httpd1yum -y install httpd 3.2.1 配置httpd.conf1234567891011121314151617181920212223242526272829303132333435363738394041$ vim /etc/httpd/conf/httpd.conf# 修改http根目录DocumentRoot "/yum" #修改## Relax access to content within /var/www.#&lt;Directory "/var/www/html"&gt; AllowOverride None # Allow open access: Require all granted&lt;/Directory&gt;# Further relax access to the default document root:&lt;Directory "/yum"&gt; #修改 # # Possible values for the Options directive are "None", "All", # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that "MultiViews" must be named *explicitly* --- "Options All" # doesn't give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be "All", "None", or any combination of the keywords: # Options FileInfo AuthConfig Limit # AllowOverride None # # Controls who can get stuff from this server. # Require all granted&lt;/Directory&gt; 3.2.2 启动httpd12systemctl start httpd #启动服务systemctl enable httpd #开机启动 3.3 创建目录1mkdir -p /yum/yum-custom/packages 3.4 下载rpm包及依赖到packages rpm包放在/yum-custom/ “”可以是自定义文件夹，不一定要是packages 1yum install httpd --downloadonly --downloaddir=/yum/yum-custom/packages 3.5 创建repo12345678910111213#createrepo /yum/yum-custom/ Spawning worker 0 with 1 pkgsSpawning worker 1 with 0 pkgsWorkers FinishedSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs complete#ll /yum/yum-custom/total 4drwxr-xr-x. 2 root root 41 Dec 20 07:03 packagesdrwxr-xr-x. 2 root root 4096 Dec 20 07:08 repodata 以下步骤局域网机器操作！ 3.5.1 删除备份repo文件123456[root@localhost /]# cd /etc/yum.repos.d/[root@localhost yum.repos.d]# lsCentOS-Base.repo CentOS-CR.repo CentOS-fasttrack.repo CentOS-Media.repo.bak CentOS-QEMU-EV.repo CentOS-Vault.repoCentOS-Ceph-Hammer.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-OpenStack-mitaka.repo CentOS-Sources.repo repo.bak.tar[root@localhost yum.repos.d]# tar zcvf *.repo[root@localhost yum.repos.d]# rm -f *.repo 3.5.2 修改自定义repo如下 修改局域网服务器的repo，哪个服务器使用，修改哪个服务器文件。 12345678910[root@localhost /]# vim /etc/yum.repos.d/CentOS-Media.repo#内容如下[c7-media]name=CentOS-$releasever - Mediabaseurl=http://192.168.118.133/yum-custom #注意是http，不是file、ftpgpgcheck=0enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#其中192.168.118.133为上面vsftp服务器地址 3.6重建YUM缓存12345678910111213[root@linuxidc.com ~]# yum clean all #删除缓存Loaded plugins: fastestmirrorCleaning repos: c7-mediaCleaning up everythingCleaning up list of fastest mirrors[root@linuxidc.com ~]# yum makecache #建立缓存Loaded plugins: fastestmirrorc7-media | 3.0 kB 00:00:00 (1/3): c7-media/filelists_db | 880 B 00:00:00 (2/3): c7-media/primary_db | 1.8 kB 00:00:00 (3/3): c7-media/other_db | 1.3 kB 00:00:00 Determining fastest mirrorsMetadata Cache Created 3.7 使用局域网yum1yum install httpd 4 注意事项：1、YUM源服务器每次添加rpm包到yum-custom目录，都需要重新运行#createrepo /yum/yum-custom/命令（YUM源服务器运行），同时局域网电脑重新建立缓存#yum clean all 和#yum makecache，否则无法发现新加安装包。 2、YUM源在/yum/yum-custom下可创建自定义目录，可根据rpm包进行分类，便于本地YUM源管理，同上，有任何改变，必须重新运行事项1命令。 3、局域网主机安装rpm的时候出现报错 原因：使用 1createrepo -u -d /yum/yum-custom/ 创建repo， 解决办法：使用命令 1createrepo /yum/yum-custom/ 不添加任何参数。 4、selinux必须关闭，否则局域网机器无法从仓库建立缓存。 资料http://www.linuxidc.com/Linux/2017-03/141391.htm]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-mitaka调整实例大小报错]]></title>
    <url>%2F2018%2F08%2F28%2Fopenstack-mitaka%E8%B0%83%E6%95%B4%E5%AE%9E%E4%BE%8B%E5%A4%A7%E5%B0%8F%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[1 问题1 在openstack图形界面对实例进行迁移操作时，出现如下错误 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748492017-09-18 11:00:21.828 1386 INFO nova.compute.resource_tracker [req-4fe2d206-e9bc-4a61-9389-a01007335c61 - - - - -] Total usable vcpus: 12, total allocated vcpus: 52017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py", line 127, in _do_dispatch2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher result = func(ctxt, **new_args)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/exception.py", line 114, in wrapped2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher payload)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.force_reraise()2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/exception.py", line 89, in wrapped2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return f(self, context, *args, **kw)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 359, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher LOG.warning(msg, e, instance=instance)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.force_reraise()2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 328, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return function(self, context, *args, **kwargs)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 409, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return function(self, context, *args, **kwargs)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 316, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher migration.instance_uuid, exc_info=True)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.force_reraise()2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 293, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return function(self, context, *args, **kwargs)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 387, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher kwargs['instance'], e, sys.exc_info())2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.force_reraise()2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 375, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return function(self, context, *args, **kwargs)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3933, in resize_instance2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.instance_events.clear_events_for_instance(instance)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib64/python2.7/contextlib.py", line 35, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.gen.throw(type, value, traceback)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 6643, in _error_out_instance_on_exception2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher raise error.inner_exception2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher ResizeError: Resize error: not able to execute ssh command: Unexpected error while running command.2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher Command: ssh -o BatchMode=yes 192.168.1.116 mkdir -p /var/lib/nova/instances/724a9fc0-3786-4ba4-8a43-744ec19a08812017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher Exit code: 2552017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher Stdout: u''2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher Stderr: u'Host key verification failed.\r\n 建立各计算节点、控制节点之间的SSH免密登录，建立私钥 解决方案 开启nova用户登录权限 1usermod -s /bin/bash nova 切换到nova用户 1su nova 生成密钥（各个计算节点执行，控制节点也执行） 12$ ssh-keygen -t rsa#默认路径，直接回车 所有计算节点均配置 依然在nova用户下操作 123456789$cat &lt;&lt; EOF &gt; ~/.ssh/config&gt; Host *&gt; StrictHostKeyChecking no&gt; UserKnownHostsFile=/dev/null&gt; EOF 发送公钥到控制节点 compute1 1scp id_rsa.pub 10.20.0.2:/var/lib/nova/.ssh/id_rsa.pub2 compute2 1scp id_rsa.pub 10.20.0.2:/var/lib/nova/.ssh/id_rsa.pub3 contrloller(10.20.0.2) 123# cat id_dsa.pub id_dsa.pub2 id_rsa.pub id_rsa.pub2 id_rsa.pub3 id_dsa.pub3 &gt; authorized_keys# scp authorized_keys computer1:/var/lib/nova/.ssh# scp authorized_keys computer2:/var/lib/nova/.ssh 修改权限 1# chown nova:nova /var/lib/nova/.ssh/id_rsa /var/lib/nova/.ssh/authorized_keys 登录测试 1ssh nova@computer 在迁移实例，不再报错，并成功。 实例迁移根据使用存储不同，有不同差异，详细介绍参考openstack官方文档]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack调整实例大小_冷迁移]]></title>
    <url>%2F2018%2F08%2F28%2FOpenstack%E8%B0%83%E6%95%B4%E5%AE%9E%E4%BE%8B%E5%A4%A7%E5%B0%8F-%E5%86%B7%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[有时虚拟机创建后发现虚拟机规格太小，满足不了业务需求。于是需要在线拉伸虚拟机的规格。1、用admin用户登录dashboard，创建满足需求的虚拟机规格 2、输入适当的参数 3、修改controller和各个computer节点的nova.cnf文件，打开下面两个参数 12allow_resize_to_same_host=True scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter 4、重启控制节点nova服务 1# systemctl restart openstack-nova-api.service openstack-nova-conductor.service openstack-nova-scheduler.service openstack-nova-cert.service openstack-nova-consoleauth.service openstack-nova-compute.service openstack-nova-novncproxy.service 5、重启计算节点nova服务 1# service openstack-nova-compute restart 参考资料：http://www.cnblogs.com/goodcook/p/6509808.html]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack的临时(Ephemeral)存储和块(Block)存储]]></title>
    <url>%2F2018%2F08%2F28%2FOpenstack%E7%9A%84%E4%B8%B4%E6%97%B6(Ephemeral)%E5%AD%98%E5%82%A8%E5%92%8C%E5%9D%97(Block)%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[转载：http://www.aboutyun.com/thread-8114-1-1.html 1 Openstack的临时(Ephemeral)存储和块(Block)存储1.2 问题导读：1、OpenStack中什么是临时存储，有什么好处？2、什么是块存储，和临时的有什么不同？ 1.3 背景Openstack不管是Ephemeral Storage还是Block Storage, 其实从接口上看，其实都是块服务。那么为什么要搞两个不同的类型呢，本文从这两种不同类型块存储的实现上来分析下其中的原因。 1.4 临时存储Openstack临时存储是由Nova提供的，主要是利用主机的本地存储给虚拟机提供卷服务。如果虚拟机被删除了，挂在这个虚拟机上的任何临时存储自动释放。这样的实现方式决定了：使用Ephemeral Storage的虚拟机不能支持迁移，以及和虚拟机迁移相关的特性，包括 1) HA 2) 动态调度 等等。存放在Ephemeral Storage上的数据是高度不可靠的，任何虚拟机和主机的故障都可能会导致数据丢失。 1.5 块存储目前Openstack的块存储由Cinder提供，其后端支持很多类型的存储设备，比如多个厂商不同型号的阵列设备，或者是Ceph, Glusterfs, Sheepdog之类的分布式存储系统。基于块存储，可以为用户提供：高可靠的存储（基于阵列的RAID, 或者是分布式存储的多副本机制；甚至还可以充分利用设备的备份，远程复制能力） 共享存储 （意味着可以支持HA, 虚拟机迁移等等） 1.6 临时存储的妙用这么看来，临时存储岂不是几乎没什么作用了，那为什么还需要提供这个服务呢？其实原因非常简单： 这个服务便宜，而且便宜到令人发指的地步，比如AWS的Ephemeral Storage, 就是免费的。用户可以用它来做不少有意思的事情，比如：无状态虚拟机，为系统提供Cache服务为虚拟机操作系统提供交换分区，或者用来存放其它类型的临时文件改进EBS的性能，比如买4个EBS盘，再配置2个免费的Ephermal盘，组建一个RAID 10系统 1.7 总结对于云服务提供商，不管采用什么样的后端技术，为用户提供7个9甚至更高可靠性的EBS服务，成本是巨大的，如果使用阵列，其价格本来就昂贵；如果使用分布式存储，起码要3个副本，再考虑到定期备份，快照，跨地域容灾，成本一样很高。现在的SATA, SAS盘便宜而且量又足，很容易造成在本地主机上空闲，所以干脆直接送给用户，由他们去玩，而且对于玩的好的用户，还真能对业务有不少帮助。最后再附上Openstack官方文档对几种存储的对比：]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack热迁移和冷迁移]]></title>
    <url>%2F2018%2F08%2F28%2FOpenstack%E7%83%AD%E8%BF%81%E7%A7%BB%E5%92%8C%E5%86%B7%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[1 迁移类型： *非在线迁移 (有时也称之为‘迁移’)。也就是在迁移到另外的计算节点时的这段时间虚拟机实例是处于宕机状态的。在此情况下，实例需要重启才能工作。 *在线迁移 (或 ‘真正的在线迁移‘)。实例几乎没有宕机时间。用于当实例需要在迁移时保持运行。在线迁移有下面几种类型： 基于共享存储的在线迁移。所有的Hypervisor都可以访问共享存储。 块在线迁移。无须共享存储。但诸如CD-ROM之类的只读设备是无法实现的。 基于卷的在线迁移。实例都是基于卷的而不是临时的磁盘，无须共享存储，也支持迁移(目前仅支持基于libvirt的hypervisor)。 1.1 什么是热迁移热迁移（Live Migration，又叫动态迁移、实时迁移），即虚拟机保存/恢复(Save/Restore)：将整个虚拟机的运行状态完整保存下来，同时可以快速的恢复到原有硬件平台甚至是不同硬件平台上。恢复以后，虚拟机仍旧平滑运行，用户不会察觉到任何差异。 1.2 Openstack热迁移OpenStack有两种在线迁移类型：live migration和block migration。Livemigration需要实例保存在NFS共享存储中，这种迁移主要是实例的内存状态的迁移，速度应该会很快。Block migration除了实例内存状态要迁移外，还得迁移磁盘文件，速度会慢些，但是它不要求实例存储在共享文件系统中。NFS允许一个系统在网络上与他人共享目录和文件。通过使用NFS，用户和程序可以像访问本地文件一样访问远端系统上的文件。 1.2.1 迁移步骤 迁移前的条件检查动态迁移要成功执行，一些条件必须满足，所以在执行迁移前必须做一些条件检查。 权限检查，执行迁移的用户是否有足够的权限执行动态迁移。 参数检查，传递给 API 的参数是否足够和正确，如是否指定了 block-migrate 参数。 检查目标物理主机是否存在。 检查被迁移的虚拟机是否是 running 状态。 检查源和目的物理主机上的 nova-compute service 是否正常运行。 检查目的物理主机和源物理主机是否是同一台机器。 检查目的物理主机是否有足够的内存(memory)。 检查目的和源物理主机器 hypervisor 和 hypervisor 的版本是否相同。 迁移前的预处理在真正执行迁移前，必须做一下热身，做一些准备工作。 在目的物理主机上获得和准备虚拟机挂载的块设备(volume)。 在目的物理主机上设置虚拟机的网络(networks)。 目的物理主机上设置虚拟机的防火墙(fireware)。 迁移条件满足并且做完了预处理工作后，就可以执行动态迁移了。主要步骤如下： 调用 libvirt python 接口 migrateToURI，来把源主机迁移到目的主机。dom.migrateToURI(CONF.live_migration_uri % dest,logical_sum,None,CONF.live_migration_bandwidth)live_migration_uri：这个 URI 就是在 3.2.2 里介绍的 libvirtd 进程定义的。live_migration_bandwidth：这个参数定义了迁移过程中所使用的最大的带宽。 以一定的时间间隔（0.5）循环调用 wait_for_live_migration 方法，来检测虚拟机迁移 的状态，一直到虚拟机成功迁移为止。 迁移后的处理当虚拟机迁移完成后，要做一些善后工作。 在源物理主机上 detach volume。 在源物理主机上释放 security group ingress rule。 在目的物理主机上更新数据库里虚拟机的状态。 在源物理主机上删除虚拟机。上面四步正常完成后，虚拟机就成功的从源物理主机成功地迁移到了目的物理主机了 1.2.2 Live Migration 的实现热迁移条件： 计算节点之间可以通过主机名互相访问 计算节点和控制节点的nova uid和gid保持一致 vncserver_proxyclient_address和vncserver_listen 监听的是本地IP 必须有共享存储，实例存放在共享存储中，且每个计算节点都可以访问共享存储。否则只能使用块迁移 1.2.3 配置 添加live_migration_flag修改nova的配置文件，在[libvirt] 段下 添加如下字段 1live_migration_flag="VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED" 配置配置versh免密码连接，修改/etc/libvirt/libvirtd.conf添加如下配置 123456789listen_tls = 0listen_tcp = 1tcp_port = "16509"listen_addr = "172.16.201.8" #根据自己的计算节点IP改写auth_tcp = "none" 修改/etc/sysconfig/libvirtd 添加如下参数 123LIBVIRTD_CONFIG=/etc/libvirt/libvirtd.confLIBVIRTD_ARGS="--listen" 重启libvirt 1systemctl restart libvirtd.service 查看监听端口： 123[root@compute1 ~]# netstat -lnpt | grep libvirtdtcp 0 0 172.16.206.6:16509 0.0.0.0:* LISTEN 9852/libvirtd 测试： 1234567在compute1节点上：virsh -c qemu+tcp://compute2/system在compute2节点上virsh -c qemu+tcp://compute1/system 如果能无密码连接上去，表示配置没问题 1.2.4动态迁移 查看所有实例nova list 查看需要迁移虚拟机实例nova show f3d749ba-98e1-4624-9782-6da729ad164c 查看可用的计算节点nova-manage service list 查看目标节点资源nova-manage service describe_resource computer1 开始迁移，正常无任何回显nova live-migration 8da00f69-05f6-4425-9a8a-df56b79a474f computer1 也可以通过dashboard 节点迁移用节点迁移需要使用admin管理员用户执 1.3 冷迁移配置 冷迁移需要启动nova账户，并配置ssh 免密码认证 123456789usermod -s /bin/bash novasu - novassh-keygen -t rsa# 生成密钥cp -fa id_rsa.pub authorized_keys 将密钥复制到所有计算节点的/var/lib/nova/.ssh下，并设置权限为nova用户 编辑/etc/nova/nova.conf的配置文件，修改下面参数 123allow_resize_to_same_host=True scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter 在计算节点重启nova服务 1systemctl restart openstack-nova-compute 在controller节点重启nova 相关服务 1systemctl restart openstack-nova-api.service openstack-nova-scheduler.service]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dokcer常用命令]]></title>
    <url>%2F2018%2F08%2F28%2FDokcer%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1 Docker常用命令 1.1 镜像管理12345678910# 列出本地所有镜像docker images# 查找imagedocker search &lt;IMAGE_ID/NAME&gt;# 下载imagedocker pull &lt;IMAGE_ID&gt;# 上传imagedocker push &lt;IMAGE_ID&gt;# 删除imagedocker rmi &lt;IMAGE_ID&gt; 1.2 容器管理123456789101112131415161718192021222324252627282930313233343536373839404142434445docker run -i -t &lt;IMAGE_ID&gt; /bin/bash： -i：标准输入给容器 -t：分配一个虚拟终端 /bin/bash：执行bash脚本-d：以守护进程方式运行（后台）-P：默认匹配docker容器的5000端口号到宿主机的49153 to 65535端口-p &lt;HOT_PORT&gt;:&lt;CONTAINER_PORT&gt;：指定端口号--name： 指定容器的名称--rm：退出时删除容器# 停止containerdocker stop &lt;CONTAINER_ID&gt;#重新启动containerdocker start &lt;CONTAINER_ID&gt;#显示运行的容器docker ps-l：显示最后启动的容器-a：同时显示停止的容器，默认只显示启动状态# 连接到启动的容器docker attach &lt;CONTAINER_ID&gt;#输出容器日志docker logs &lt;CONTAINER_ID&gt;-f：实时输出# 复制容器内的文件到宿主机目录上docker cp &lt;CONTAINER_ID&gt;:路径 宿主机路径# 删除containerdocker rm &lt;CONTAINER_ID&gt;# 删除所有容器docker rm `docker ps -a -q`docker kill `docker ps -q`docker rmi `docker images -q -a`docker wait &lt;CONTAINER_ID&gt;：阻塞对容器的其他调用方法，直到容器停止后退出# 查看容器中运行的进程docker top &lt;CONTAINER_ID&gt;# 查看容器中的变化docker diff &lt;CONTAINER_ID&gt;# 查看容器详细信息（输出为Json）docker inspect &lt;CONTAINER_ID&gt;-f：查找特定信息，如docker inspect -f '&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;'docker commit -m "comment" -a "author" &lt;CONTAINER_ID&gt; ouruser/imagename:tagdocker extc -it &lt;CONTAINER&gt; &lt;COMMAND&gt;：在容器里执行命令，并输出结果 1.3 网络管理1234567891011121314# 随机分配端口号docker run -P# 绑定特定端口号（主机的所有网络接口的5000端口均绑定容器的5000端口）docker run -p 5000:5000# 绑定主机的特定接口的端口号docker run -p 127.0.0.1:5000:5000# 绑定udp端口号docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py# 查看容器的5000端口对应本地机器的IP和端口号docker port &lt;CONTAINER_ID&gt; 5000# 使用Docker Linking连接容器：# Docker为源容器和接收容器创建一个安全的通道，容器之间不需要暴露端口，接收的容器可以访问源容器的数据docker run -d -P --name &lt;CONTAINER_NAME&gt; --link &lt;CONTAINER_NAME_TO_LINK&gt;:&lt;ALIAS&gt; 1.4 数据管理1234567891011121314151617181920212223# Data Volumes：volume是在一个或多个容器里指定的特殊目录# 数据卷可以在容器间共享和重复使用# 可以直接修改容器卷的数据# 容器卷里的数据不会被包含到镜像中# 容器卷保持到没有容器再使用它# 可以在容器启动的时候添加-v参数指定容器卷，也可以在Dockerfile里用VOLUMN命令添加docker run -d -P --name web -v /webapp training/webapp python app.py# 也可以将容器卷挂载到宿主机目录或宿主机的文件上，&lt;容器目录或文件&gt;的内容会被替换为&lt;宿主机目录或文件&gt;的内容，默认容器对这个目录有可读写权限docker run -d -P --name web -v &lt;宿主机目录&gt;:&lt;容器目录&gt; training/webapp python app.py# 可以通过指定ro，将权限改为只读docker run -d -P --name web -v &lt;宿主机目录&gt;:&lt;容器目录&gt;:ro training/webapp python app.py# 在一个容器创建容器卷后，其他容器便可以通过--volumes-from共享这个容器卷数据，如下：docker run -d -v /dbdata --name db1 training/postgres echo Data-only container for postgres# 首先启动了一个容器，并为这个容器增加一个数据卷/dbdata，然后启动另一个容器，共享这个数据卷docker run -d --volumes-from db1 --name db2 training/postgres# 此时db2使用了db1的容器卷，当容器db1被删除时，容器卷也不会被删除，只有所有容器不再使用此容器卷时，才会被删除docker rm -v：删除容器卷# 除了共享数据外，容器卷另一个作用是用来备份、恢复和迁移数据docker run --volumes-from db1 -v /home/backup:/backup ubuntu tar cvf /backup/backup.tar /dbdata# 启动一个容器数据卷使用db1容器的数据卷，同时新建立一个数据卷指向宿主机目录/home/backup，将/dbdata目录的数据压缩为/backup/backup.tardocker run -v /dbdata --name dbdata2 ubuntu /bin/bashdocker run --volumes-from dbdata2 -v /home/backup:/backup busybox tar xvf /backup/backup.tar# 启动一个容器，同时把backup.tar的内容解压到容器的backup 1.5 仓库管理docker login：登录 1.6 镜像打包及恢复1.6.1 保存镜像为文件1234# centos为镜像，centos为打包名，centos后可以指定标签docker save -o centos.tar centosordocker save -o centos.tar centos:7.2 1.6.2 从文件载入镜像1234# home/centos.tar为镜像文件路径docker load --input /home/centos.tarordocker load &lt; /home/centos.tar]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu部署本地源仓库]]></title>
    <url>%2F2018%2F08%2F27%2FUbuntu%E9%83%A8%E7%BD%B2%E6%9C%AC%E5%9C%B0%E6%BA%90%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1 本地源的制作 1.1 安装所需软件包1sudo apt-get install dpkg-dev 1.2 打包deb软件包12#加上-d参数，只下载安装包，不安装及解压。sudo apt-get install -d nginx 将/var/cache/apt/archives/下的所有deb文件拷到/home/packages/下的Natty目录中：/home/packages/Natty，拷贝前建议执行一下: 12#autoclean - 删除已下载的旧包文件sudo apt-get autoclean 1.3 进入指定目录上一级目录拷完后在终端中进入刚才新建的目录Natty所在的上一级目录，也就是： 1cd /home/packages 生成软件包依赖信息文件 1sudo dpkg-scanpackages Natty/ | gzip &gt;Natty/Packages.gz 至此本地源的软件包已经准备完毕；下面接着介绍如何使用。 2 本地源的使用2.1 本机源服务器的搭建将地址加入更新源列表文件 123$ sudo vim /etc/apt/sources.list# 添加以下路径，其它deb信息使用#号注释掉deb file:///home/packages/ Natty/ #注意Natty后面有一个斜杠,前面还要有空格(这是书写方式) 2.2 更新源信息12#更新信息，生成数据缓存$ sudo apt-get update 之后即可正常安装所需软件。 3 局域网源服务器3.1 安装apache21sudo apt-get install apache2 启动服务 注意：配置apache2的时候注意端口，不要配置成可能被其他网络应用使用的端口就可以。 3.2 配置服务器上的Ubuntu源12#在apache2发布目录/var/www/html位置创建到源目录的软链接sudo ln -s /home/packages/Natty/ /var/www/html/ubuntu-local 3.3 配置局域网客户机sources.list1234$ sudo vim /etc/apt/sources.list#添加如下信息,其它deb信息使用#号注释掉deb http://192.168.1.224 ubuntu-local/ #注意书写方式，ip地址后空格，目录地址/ 3.4 客户机更新源信息12#更新信息，生成数据缓存sudo apt-get update 之后即可正常安装所需软件。 4 参考资料https://www.iyunv.com/thread-384273-1-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu同步源仓库到本地]]></title>
    <url>%2F2018%2F08%2F27%2FUbuntu%E5%90%8C%E6%AD%A5%E6%BA%90%E4%BB%93%E5%BA%93%E5%88%B0%E6%9C%AC%E5%9C%B0%2F</url>
    <content type="text"><![CDATA[1 步骤 1.1 安装apt-mirror1sudo apt-get install apt-mirro 1.2 创建下载目录12345#路径自定义.../ubuntu.../ubuntu/mirror.../ubuntu/skel.../ubuntu/var 1.3 修改配置文件1sudo vi /etc/apt/mirror.list 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263############# config ###################set base_path /home/packages/ubuntu#set mirror_path $base_path/mirrorset skel_path $base_path/skelset var_path $base_path/varset cleanscript $var_path/clean.shset nthreads 20set _tilde 0############## end config ###############deb http://archive.ubuntu.com/ubuntu trusty main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-security main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-updates main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-proposed main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-backports main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-security main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-updates main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-proposed main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-backports main restricted universe multiverse#clean http://archive.ubuntu.com/ubuntu#自定义#参考以下配置文件：#清空原有的配置文件，直接使用以下配置文件即可############# config ################### 以下注释的内容都是默认配置，如果需要自定义，取消注释修改即可#set base_path /var/spool/apt-mirror## 镜像文件下载地址# set mirror_path $base_path/mirror# 临时索引下载文件目录，也就是存放软件仓库的dists目录下的文件（默认即可）# set skel_path $base_path/skel# 配置日志（默认即可）# set var_path $base_path/var# clean脚本位置# set cleanscript $var_path/clean.sh# 架构配置，i386/amd64，默认的话会下载跟本机相同的架构的源#set defaultarch amd64# set postmirror_script $var_path/postmirror.sh# set run_postmirror 0# 下载线程数#set nthreads 20#set _tilde 0############## end config ############### Ali yun（这里没有添加deb-src的源）#我们把常用的软件同步过来就够用了deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse#当某些软件包在服务器端进行了升级，或者服务器端不再需要这些软件包时，我们使用了 apt-mirror与服务器同步后，会在本地的$var_path/下生成一个clean.sh的脚本，列出了遗留在本地的旧版本和无用的软件包，你可 以手动运行这个脚本来删除遗留在本地的且不需要用的软件包clean http://mirrors.aliyun.com/ubuntu 如果用amd64位架构下的包，可以加上deb-amd64的标记如果什么都不加，直接使用deb http…..这种格式，则在同步时，只同步当前系统所使用的架构下的软件包。比如一个64位系统，直接debhttp….只同步64位的软件 包。如果还嫌麻烦，直接去改set defaultarch 这个参数就好，比如改成set defaultarch i386，这样你使用debhttp…..这种格式，则在同步时，只同步i386的软件包了。 如果你还想要源码，可以把源码也加到mirror.list里面同步过来，比如加上deb-src这样的标记。想要其他的东西也可以追加相应的标记来完成。 1.4 同步源配置好后我们就可以和指定的镜像进行同步了 1sudo apt-mirror 1.5 建立本地源参考：Ubuntu部署本地源仓库 2 参考资料http://www.linuxidc.com/Linux/2014-08/105415.htm https://www.iyunv.com/thread-384273-1-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群-Docker_Swarm]]></title>
    <url>%2F2018%2F08%2F27%2FDocker%E9%9B%86%E7%BE%A4-Docker-Swarm%2F</url>
    <content type="text"><![CDATA[1 安装 Swarm 1.1 下载镜像1$ docker pull swarm 可以使用下面的命令来查看 Swarm 版本，验证是否成功下载 Swarm 镜像。 12$ docker run --rm swarm -vswarm version 1.2.2 (34e3da3) 1.2 配置节点Docker 主机在加入 Swarm 集群前，需要进行一些简单配置，添加 Docker daemon 的网络监听。 例如，在启动 Docker daemon 的时候通过 -H 参数： 1$ sudo docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 注：Docker 1.8.0 版本之前不支持 daemon 命令，可以用 -d 代替。 如果是通过服务方式启动，则需要修改服务的配置文件。 以 Ubuntu 14.04 为例，配置文件为 /etc/default/docker（其他版本的 Linux 上略有不同）。 在文件的最后添加： 1DOCKER_OPTS="$DOCKER_OPTS -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock" 1.3 启动集群Docker 集群管理需要使用服务发现（Service Discover）功能，Swarm 支持以下的几种方式：DockerHub、本地文件、etcd、consul、zookeeper 和手动指定节点 IP 地址信息等。 除了手动指定外，这些方法原理上都是通过维护一套数据库机制，来管理集群中注册节点的 Docker daemon 的访问信息。 本地配置集群推荐使用 consul 作为服务发现后端。利用社区提供的 Docker 镜像，整个过程只需要三步即可完成。 1.3.1 启动 Consul 服务后端启动 consul 服务容器，映射到主机的 8500 端口。 1$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap 获取到本地主机的地址作为 consul 的服务地址：&lt;consul_ip&gt;:8500。 1.3.2 启动管理节点首先，启动一个主管理节点，映射到主机的 4000 端口，并获取所在主机地址为 &lt;manager0_ip&gt;。其中 4000 端口是 Swarm 管理器的默认监听端口，用户也可以指定映射为其它端口。 1$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &lt;manager0_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500 为了提高高可用性，用户也可以启动从管理节点。假定获取所在主机地址为 &lt;manager1_ip&gt;。 1$ docker run -d swarm manage -H :4000 --replication --advertise &lt;manager1_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500 1.3.3 启动工作节点需要在每个工作节点上启动 agent 服务。 获取节点的主机地址为 &lt;node_ip&gt;，并指定前面获取到的 consul 服务地址。 1$ docker run -d swarm join --advertise=&lt;node_ip&gt;:2375 consul://&lt;consul_ip&gt;:8500 节点启动后，用户可以指定 Docker 服务地址为 &lt;manager0_ip&gt;:4000&gt; 来测试各种 Docker 命令，可以看到整个 Swarm 集群就像一个虚拟的 Docker 主机一样正常工作。 由于 Swarm 实际上是通过 agent 调用了本地的 Docker daemon 来运行容器，当 Swarm 集群服务出现故障时，无法接受新的请求，但已经运行起来的容器将不会受到影响。 2 使用 Swarm前面演示了基于 consul 服务发现后端来配置一个本地 Swarm 集群。其中，consul 也可以被替换为 etcd、zookeeper 等。 另外一个更方便的方式是直接使用 DockerHub 提供的免费服务发现后端。 下面使用这种方式来演示 Swarm 的主要操作，包括： create：创建一个集群； list：列出集群中的节点； manage：管理一个集群； join：让节点加入到某个集群。 注意，使用 DockerHub 的服务发现后端，需要各个节点能通过公网访问到 DockerHub 的服务接口。 2.1 创建集群 id在任意一台安装了 Swarm 的机器上执行 swarm create 命令来在 DockerHub 服务上进行注册。 Swarm 会通过服务发现后端（此处为 DockerHub 提供）来获取一个唯一的由数字和字母组成的 token，用来标识要管理的集群。 12$ docker run --rm swarm create946d65606f7c2f49766e4dddac5b4365 注意返回的字符串，这是集群的唯一 id，加入集群的各个节点将需要这个信息。 2.2 配置集群节点在所有要加入集群的普通节点上面执行 swarm join 命令，表示把这台机器加入指定集群当中。 例如某台机器 IP 地址为 192.168.0.2，将其加入我们刚创建的 946d65606f7c2f49766e4dddac5b4365 集群，则可以通过： 12$ docker run --rm swarm join --addr=192.168.0.2:2375 token://946d65606f7c2f49766e4dddac5b4365time="2015-12-09T08:59:43Z" level=info msg="Registering on the discovery service every 20s..." addr="192.168.0.2:2375" discovery="token://946d65606f7c2f49766e4dddac5b4365" 注：其中 –addr 指定的 IP 地址信息将被发送给服务发现后端，用以区分集群不同的节点。manager服务必须要通过这个地址可以访问到该节点。 通过控制台可以看到，上述命令执行后，默认每隔 20 秒（可以通过 –heartbeat 选项指定），会输出一条心跳信息。对于发现服务后端来说，默认如果超过 60 秒（可以通过 –ttl 选项指定）没有收到心跳信息，则将节点从列表中删除。 如果不希望看到输出日志信息，则可以用 -d 选项替换 –rm 选项，让服务后台执行。 执行 swarm join 命令实际上是通过 agent 把自己的信息注册到发现服务上，因此，此时对于后端的发现服务来说，已经可以看到有若干节点注册上来了。那么，如何管理和使用这些节点呢，这就得需要 Swarm 的 manager 服务了。 2.3 配置管理节点配置管理节点需要通过 swarm manage 命令，该命令将启动 manager 服务，默认监听到 2375 端口，所有对集群的管理可以通过该服务接口进行。 读者可能注意到，manager 服务默认监听的端口跟 Docker 服务监听端口是一样的，这是为了兼容其它基于 Docker 的服务，可以无缝地切换到 Swarm 平台上来。 仍然在节点 192.168.0.2 进行操作。由于我们是采用 Docker 容器形式启动 manager 服务，本地的 2375端口已经被 Docker Daemon 占用。我们将 manager 服务监听端口映射到本地一个空闲的 12375 端口。 12$ docker run -d -p 12375:2375 swarm manage token://946d65606f7c2f49766e4dddac5b43651e1ca8c4117b6b7271efc693f9685b4e907d8dc95324350392b21e94b3cffd18 可以通过 docker ps 命令来查看启动的 swarm manager 服务容器。 123$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1e1ca8c4117b swarm "/swarm manage token:" 11 seconds ago Up 10 seconds 0.0.0.0:12375-&gt;2375/tcp jovial_rosalind 命令如果执行成功会返回刚启动的 Swarm 容器的 ID，此时一个简单的 Swarm 集群就已经搭建起来了，包括一个普通节点和一个管理节点。 2.4 查看集群节点列表集群启动成功以后，用户可以在任何一台节点上使用 swarm list 命令查看集群中的节点列表。例如 12$ docker run --rm swarm list token://946d65606f7c2f49766e4dddac5b4365192.168.0.2:2375 显示正是之前用 swarm join 命令加入集群的节点的地址。 我们在另外一台节点 192.168.0.3 上同样使用 swarm join 命令新加入一个节点： 123$ docker run --rm swarm join --addr=192.168.0.3:2375 token://946d65606f7c2f49766e4dddac5b4365time="2015-12-10T02:05:34Z" level=info msg="Registering on the discovery service every 20s..." addr="192.168.0.3:2375" discovery="token://946d65606f7c2f49766e4dddac5b4365"... 再次使用 swarm list 命令查看集群中的节点列表信息，可以看到新加入的节点： 123$ docker run --rm swarm list token://946d65606f7c2f49766e4dddac5b4365192.168.0.3:2375192.168.0.2:2375 2.5 使用集群服务那么，怎么使用 Swarm 提供的服务呢？ 实际上，所有 Docker 客户端可以继续使用，只要指定使用 Swarm manager 服务的监听地址即可。 例如，manager 服务监听的地址为 192.168.0.2:12375，则可以通过指定 -H 192.168.0.2:12375 选项来继续使用 Docker 客户端，执行任意 Docker 命令，例如 ps、info、run 等等。 在任意节点上使用 docker run 来启动若干容器，例如 12$docker -H 192.168.0.2:12375:12375 run -d ubuntu ping 127.0.0.14c9bccbf86fb6e2243da58c1b15e9378fac362783a663426bbe7058eea84de46 使用 ps 命令查看集群中正在运行的容器。 12345$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4c9bccbf86fb ubuntu "ping 127.0.0.1" About a minute ago Up About a minute clever_wright730061a3801a registry:latest "docker-registry" 2 minutes ago Up 2 minutes 192.168.0.2:5000-&gt;5000/tcp Host-1/registry_registry_172d99f24a06f redis:3.0 "/entrypoint.sh redis" 2 minutes ago Up 2 minutes 6379/tcp Host-1/registry_redis_1,Host-1/registry_registry_1/redis,Host-1/registry_registry_1/redis_1,Host-1/registry_registry_1/registry_redis_1 输出结果中显示目前集群中正在运行的容器（注意不包括 Swarm manager 服务容器），可以在不同节点上使用 docker ps 查看本地容器，发现这些容器实际上可能运行在集群中多个节点上（被 Swarm 调度策略进行分配）。 使用 info 查看所有节点的信息。 1234567891011121314151617181920$ docker -H 192.168.0.2:12375 infoContainers: 18Images: 36Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 2 Host-1: 192.168.0.2:2375 └ Containers: 15 └ Reserved CPUs: 0 / 4 └ Reserved Memory: 1 GiB / 4.053 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-43-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs Host-2: 192.168.0.3:2375 └ Containers: 3 └ Reserved CPUs: 0 / 8 └ Reserved Memory: 0 B / 16.46 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufsCPUs: 12Total Memory: 20.51 GiBName: 1e1ca8c4117b 结果输出显示这个集群目前只有两个节点，地址分别是 192.168.0.2 和 192.168.0.3。 类似的，也可以通过 Compose 模板来启动多个服务。不过请注意，要想让服务分布到多个 Swarm 节点上，需要采用版本 2 的写法。 2.6 使用网络Swarm 为了支持跨主机的网络，默认采用了 overlay 网络类型，实现上通过 vxlan 来构建联通整个 Swarm 集群的网络。 首先，在集群中所有节点上，添加配置 Docker daemon 选项： 1--cluster-store=&lt;DISCOVERY_HOST:PORT&gt; --cluster-advertise=&lt;DOCKER_DAEMON_HOST:PORT&gt; 以 consul 服务为例，可能类似： 1--cluster-store=consul://&lt;consul 服务地址&gt;:8500 --cluster-advertise=192.168.0.3:2375 之后重启 Docker 服务。 首先，创建一个网络。 1$ docker -H 192.168.0.2:12375 network create swarm_network 查看网络，将看到一个 overlay 类型的网络。 123$ docker -H 192.168.0.2:12375 network lsNETWORK ID NAME DRIVER6edf2d16ec97 swarm_network overlay 此时，所有添加到这个网络上的容器将自动被分配到集群中的节点上，并且彼此联通。 3 使用其它服务发现后端Swarm 目前可以支持多种服务发现后端，这些后端功能上都是一致的，即维护属于某个集群的节点的信息。不同方案并无优劣之分，在实际使用时候，可以结合自身需求和环境限制进行选择，甚至自己定制其它方案。 使用中可以通过不同的路径来选择特定的服务发现后端机制。 -token://&lt;token&gt;：使用 DockerHub 提供的服务，适用于可以访问公网情况； file://path/to/file：使用本地文件，需要手动管理； consul://&lt;ip&gt;/&lt;path&gt;：使用 consul服务，私有环境推荐； etcd://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;：使用 etcd 服务，私有环境推荐； zk://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;：使用 zookeeper 服务，私有环境推荐； [nodes://]&lt;ip1&gt;,&lt;ip2&gt;：手动指定集群中节点的地址，方便进行服务测试。 3.1 使用文件使用本地文件的方式十分简单，就是讲所有属于某个集群的节点的 Docker daemon 信息写入一个文件中，然后让 manager 从这个文件中直接读取相关信息。 首先，在 Swarm 管理节点（192.168.0.2）上新建一个文件，把要加入集群的机器的 Docker daemon 信息写入文件： 1234$ tee /tmp/cluster_info &lt;&lt;-'EOF'192.168.0.2:2375192.168.0.3:2375EOF 然后，本地执行 swarm manage 命令，并指定服务发现机制为本地文件，注意因为是容器方式运行 manager，需要将本地文件挂载到容器内。 1$ docker run -d -p 12375:2375 -v /tmp/cluster_info:/tmp/cluster_info swarm manage file:///tmp/cluster_info 接下来就可以通过使用 Swarm 服务来进行管理了，例如使用 info 查看所有节点的信息。 1234567891011121314151617181920$ docker -H 192.168.0.2:12375 infoContainers: 18Images: 36Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 2 Host-1: 192.168.0.2:2375 └ Containers: 15 └ Reserved CPUs: 0 / 4 └ Reserved Memory: 1 GiB / 4.053 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-43-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs Host-2: 192.168.0.3:2375 └ Containers: 3 └ Reserved CPUs: 0 / 8 └ Reserved Memory: 0 B / 16.46 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufsCPUs: 12Total Memory: 20.51 GiBName: e71eb5f1d48b 3.2 其它发现服务后端其它服务发现后端的使用方法，也是大同小异，不同之处在于使用 Swarm 命令时指定的路径格式不同。 例如，对于前面介绍的 consul 服务后端来说。 快速部署一个 consul 服务的命令为： 1$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap 之后创建 Swarm 的管理服务，指定使用 consul 服务，管理端口监听在本地的 4000 端口。 1$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &lt;manager_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500 Swarm 节点注册时候命令格式类似于： 1$ swarm join --advertise=&lt;node_ip:2375&gt; consul://&lt;consul_addr&gt;/&lt;optional path prefix&gt; 对于 etcd 服务后端来说，节点注册时候命令格式类似于： 1$ swarm join --addr=&lt;node_addr:2375&gt; etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt; 启动管理服务时候，格式类似于： 1$ swarm manage -H tcp://&lt;manager_ip&gt;:4000 etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt; 3.3 地址和端口的范围匹配对于基于文件，以及手动指定节点信息两种服务发现后端机制来说，其中地址和端口域可以支持指定一个范围，以一次性指定多个地址。 例如： 192.168.0.[2:10]:2375 代表 192.168.0.2:2375 – 192.168.0.10:2375 一共 9 个地址； 192.168.0.2:[2:9]375 代表 192.168.0.2:2375 – 192.168.0.2:9375 一共 8 个地址。 4 Swarm 中的调度器调度是集群十分重要的功能，Swarm 目前支持三种调度策略：spread、binpack 和 random。 在执行swarm manage命令启动管理服务的时候，可以通过--strategy 参数指定调度策略，默认的是 spread。 简单来说，这三种调度策略的优化目标如下： spread：如果节点配置相同，选择一个正在运行的容器数量最少的那个节点，即尽量平摊容器到各个节点； binpack：跟 spread 相反，尽可能的把所有的容器放在一台节点上面运行，即尽量少用节点，避免容器碎片化。 random：直接随机分配，不考虑集群中节点的状态，方便进行测试使用。 4.1 spread 调度策略仍然以之前创建好的集群为例，来演示下 spread 策略的行为。 在 192.168.0.2 节点启动管理服务，管理 token://946d65606f7c2f49766e4dddac5b4365 的集群。 12$ docker run -d -p 12375:2375 swarm manage --strategy "spread" token://946d65606f7c2f49766e4dddac5b4365c6f25e6e6abbe45c8bcf75ac674f2b64d5f31a5c6070d64ba954a0309b197930 列出集群中节点。 123$ docker run --rm swarm list token://946d65606f7c2f49766e4dddac5b4365192.168.0.3:2375192.168.0.2:2375 此时，两个节点上除了 swarm 外都没有运行其它容器。 启动一个 ubuntu 容器。 12$ docker -H 192.168.0.2:12375 run -d ubuntu:14.04 ping 127.0.0.1bac3dfda5306181140fc959969d738549d607bc598390f57bdd432d86f16f069 查看发现它实际上被调度到了 192.168.0.3 节点（当节点配置相同时候，初始节点随机选择）。 再次启动一个 ubuntu 容器。 12$ docker -H 192.168.0.2:12375 run -d ubuntu:14.04 ping 127.0.0.18247067ba3a31e0cb692a8373405f95920a10389ce3c2a07091408281695281c 查看它的位置，发现被调度到了另外一个节点：192.168.0.2 节点。 1234$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8247067ba3a3 ubuntu:14.04 "ping 127.0.0.1" 1 minutes ago Up 1 minutes Host-2/sick_galileobac3dfda5306 ubuntu:14.04 "ping 127.0.0.1" 2 minutes ago Up 2 minutes Host-3/compassionate_ritchie 当节点配置不同的时候，spread会更愿意分配到配置较高的节点上。 4.2 binpack 调度策略现在来看看 binpack 策略下的情况。 直接启动若干 ubuntu 容器，并查看它们的位置。 1234567$ docker -H 192.168.0.2:12375 run -d ubuntu:14.04 ping 127.0.0.1$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4c4f45eba866 ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/hopeful_brown5e650541233c ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/pensive_wright99c5a092530a ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/naughty_engelbart4ab392c26eb2 ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/thirsty_mclean 可以看到，所有的容器都是分布在同一个节点（192.168.0.3）上运行的。 5 Swarm 中的过滤器Swarm 的调度器可以按照指定调度策略自动分配容器到节点。但有些时候希望能对这些分配加以干预。比如说，让 IO 敏感的容器分配到安装了 SSD 的节点上；让计算敏感的容器分配到 CPU 核数多的机器上；让网络敏感的容器分配到高带宽的机房；让某些容器尽量放同一个节点……。 这可以通过过滤器（filter）来实现，目前支持 Constraint、Affinity、Port、Dependency、Health等五种过滤器。 5.1 Constraint 过滤器Constraint 过滤器是绑定到节点的键值对，相当于给节点添加标签。 可在启动 Docker 服务的时候指定，例如指定某个节点颜色为 red。 1$ sudo docker daemon --label color=red -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 同样的，可以写在 Docker 服务的配置文件里面（以 Ubuntu 14.04 为例，是 /etc/default/docker）。 1DOCKER_OPTS="--label color=red -H 0.0.0.0:2375 -H unix:///var/run/docker.sock" 使用 Swarm 启动容器的时候，采用 -e constarint:key=value 的形式，可以过滤选择出匹配条件的节点。 例如，我们将 192.168.0.2 节点打上红色标签，192.168.0.3 节点打上绿色标签。 然后，分别启动两个容器，指定使用过滤器分别为红色和绿色。 1234$ docker -H 192.168.0.2:12375 run -d -e constraint:color==red ubuntu:14.04 ping 127.0.0.1252ffb48e64e9858c72241f5eedf6a3e4571b1ad926faf091db3e26672370f64$ docker -H 192.168.0.2:12375 run -d -e constraint:color==green ubuntu:14.04 ping 127.0.0.13d6f8d7af8583416b17061d038545240c9e5c3be7067935d3ef2fbddce4b8136 注：指定标签中间是两个等号 查看它们将被分配到指定节点上。 1234$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES252ffb48e64e ubuntu:14.04 "ping 127.0.0.1" 1 minutes ago Up 1 minutes Host-2/sick_galileo3d6f8d7af858 ubuntu:14.04 "ping 127.0.0.1" 2 minutes ago Up 2 minutes Host-3/compassionate_ritchie 另外，Docker 内置了一些常见的过滤器，包括 node、storagedriver、executiondriver、kernelversion、operatingsystem 等。这些值可以通过 docker info 命令查看。 例如，目前集群中各个节点的信息为： 1234567891011121314151617181920$ docker -H 192.168.0.2:12375 infoContainers: 5Images: 39Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 2 Host-2: 192.168.0.2:2375 └ Containers: 4 └ Reserved CPUs: 0 / 4 └ Reserved Memory: 1 GiB / 4.053 GiB └ Labels: color=red, executiondriver=native-0.2, kernelversion=3.16.0-43-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs Host-3: 192.168.0.3:2375 └ Containers: 1 └ Reserved CPUs: 0 / 8 └ Reserved Memory: 0 B / 16.46 GiB └ Labels: color=green, executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufsCPUs: 12Total Memory: 20.51 GiBName: 946d65606f7c 5.2 Affinity 过滤器Affinity 过滤器允许用户在启动一个容器的时候，让它分配到某个已有容器的节点上。 例如，下面我们将启动一个 nginx 容器，让它分配到已经运行某个 ubuntu 容器的节点上。 在 Constraint 过滤器的示例中，我们分别启动了两个 ubuntu 容器 sick_galileo 和 compassionate_ritchie，分别在 Host-2 和 Host-3 上。 现在启动一个 nginx 容器，让它跟容器 sick_galileo 放在一起，都放到 Host-2 节点上。可以通过 -e affinity:container==&lt;name or id&gt; 参数来实现。 1$ docker -H 192.168.0.2:12375 run -d -e affinity:container==sick_galileo nginx 然后启动一个 redis 容器，让它跟容器 compassionate_ritchie 放在一起，都放到 Host-3 节点上。 1$ docker -H 192.168.0.2:12375 run -d -e affinity:container==compassionate_ritchie redis 查看所有容器运行情况。 123456$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0a32f15aa8ee redis "/entrypoint.sh redis" 2 seconds ago Up 1 seconds 6379/tcp Host-3/awesome_darwind2b9a53e67d5 nginx "nginx -g 'daemon off" 29 seconds ago Up 28 seconds 80/tcp, 443/tcp Host-2/fervent_wilson252ffb48e64e ubuntu:14.04 "ping 127.0.0.1" 2 minutes ago Up 2 minutes Host-2/sick_galileo3d6f8d7af858 ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/compassionate_ritchie 5.3 其它过滤器其它过滤器的使用方法也是大同小异，例如通过 affinity:image==&lt;name or id&gt; 来选择拥有指定镜像的节点；通过 -e affinity:label_name==value 来选择拥有指定标签的容器所允许的节点。 此外，当容器端口需要映射到宿主机指定端口号的时候，Swarm 也会自动分配容器到指定宿主机端口可用的节点。 当不同容器之间存在数据卷或链接依赖的时候，Swarm 会分配这些容器到同一个节点上。 6 本章小结本章笔者介绍了 Docker Swarm 的安装、使用和主要功能。 通过使用 Swarm，用户可以将若干 Docker 主机节点组成的集群当作一个大的虚拟 Docker 主机使用。并且，原先基于单机的 Docker 应用，可以无缝的迁移到 Swarm 上来。 实现这些功能的前提是服务自动发现能力。在现代分布式系统中，服务的自动发现、注册、更新等能力将成为系统的基本保障和重要基础。 在生产环境中，Swarm 的管理节点和发现服务后端要采用高可用性上的保护，可以采用集群模式。 值得一提的是，Swarm V2 功能已经被无缝嵌入到了 Docker 1.12+ 版本中，用户今后可以直接使用 Docker 命令来完成相关功能的配置，这将使得集群功能的管理更加简便。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>swarm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack云镜像制作-Centos7篇]]></title>
    <url>%2F2018%2F08%2F27%2FOpenstack%E4%BA%91%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C-Centos7%E7%AF%87%2F</url>
    <content type="text"><![CDATA[一、制作步骤 1、安装kvm参考centos 7系统安装配置kvm软件步骤 1、创建虚拟硬盘大小10G 名称：centos7-dis.qcow2 2、安装系统 注意一：分区，分区的时候只给”/“ 根目录分一个区即可，其他都不要。格式ext4注意二：网络设置方面，确保你的网卡eth0是DHCP状态的，而且请务必勾上”auto connect”的对勾 2、进入虚拟机系统操作关于CentOS镜像制作需要注意以下几点： (1) 修改网络信息 /etc/sysconfig/network-scripts/ifcfg-eth0 （删掉mac信息)，如下： 12345TYPE=Ethernet DEVICE=eth0 ONBOOT=yes BOOTPROTO=dhcp NM_CONTROLLED=no (2) 删除已生成的网络设备规则，否则制作的镜像不能上网1$ rm -rf /etc/udev/rules.d/70-persistent-net.rules (3)增加一行到/etc/sysconfig/network 1NOZERCONF=yes (4)安装cloud-init（可选），cloud-init可以在开机时进行密钥注入以及修改hostname等，关于cloud-init，陈沙克的一篇博文有介绍：http://www.chenshake.com/about-openstack-centos-mirror/ 1$ yum install -y cloud-utils cloud-init parted 修改配置文件/etc/cloud/cloud.cfg ，在cloud_init_modules 下面增加: 1- resolv-conf (5)设置系统能自动获取openstack指定的hostname和ssh-key（可选）编辑/etc/rc.local文件，该文件在开机后会执行，加入以下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950if [ ! -d /root/.ssh ]; thenmkdir -p /root/.sshchmod 700 /root/.sshfi# Fetch public key using HTTPATTEMPTS=30FAILED=0 while [ ! -f /root/.ssh/authorized_keys ]; docurl -f http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key &gt; /tmp/metadata-key 2&gt;/dev/nullif [ $? -eq 0 ]; thencat /tmp/metadata-key &gt;&gt; /root/.ssh/authorized_keyschmod 0600 /root/.ssh/authorized_keysrestorecon /root/.ssh/authorized_keysrm -f /tmp/metadata-keyecho “Successfully retrieved public key from instance metadata”echo “*****************”echo “AUTHORIZED KEYS”echo “*****************”cat /root/.ssh/authorized_keysecho “*****************”curl -f http://169.254.169.254/latest/meta-data/hostname &gt; /tmp/metadata-hostname 2&gt;/dev/nullif [ $? -eq 0 ]; thenTEMP_HOST=`cat /tmp/metadata-hostname`sed -i “s/^HOSTNAME=.*$/HOSTNAME=$TEMP_HOST/g” /etc/sysconfig/network/bin/hostname $TEMP_HOSTecho “Successfully retrieved hostname from instance metadata”echo “*****************”echo “HOSTNAME CONFIG”echo “*****************”cat /etc/sysconfig/networkecho “*****************”elseecho “Failed to retrieve hostname from instance metadata. This is a soft error so we’ll continue”firm -f /tmp/metadata-hostnameelseFAILED=$(($FAILED + 1))if [ $FAILED -ge $ATTEMPTS ]; thenecho “Failed to retrieve public key from instance metadata after $FAILED attempts, quitting”breakfiecho “Could not retrieve public key from instance metadata (attempt #$FAILED/$ATTEMPTS), retrying in 5 seconds…”sleep 5fidone 或者 1234567891011121314151617181920212223# set a random pass on first bootif [ -f /root/firstrun ]; then dd if=/dev/urandom count=50|md5sum|passwd --stdin root passwd -l root rm /root/firstrunfiif [ ! -d /root/.ssh ]; then mkdir -m 0700 -p /root/.ssh restorecon /root/.sshfi# Get the root ssh key setup# Get the root ssh key setupReTry=0while [ ! -f /root/.ssh/authorized_keys ] &amp;&amp; [ $ReTry -lt 10 ]; do sleep 2 curl -f http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key &gt; /root/.ssh/pubkey if [ 0 -eq 0 ]; then mv /root/.ssh/pubkey /root/.ssh/authorized_keys fi ReTry=$[Retry+1]donechmod 600 /root/.ssh/authorized_keys &amp;&amp; restorecon /root/.ssh/authorized_keys 主要目的就是获取hostname和公钥 (6)其他 route命令查看一下路由表 查看/etc/ssh/sshd_conf中PermitRootLogin是不是为yes 清除操作记录 123456789101112131415161718192021222324252627282930清除登陆系统成功的记录[root@localhost root]# echo &gt; /var/log/wtmp //此文件默认打开时乱码，可查到ip等信息[root@localhost root]# last //此时即查不到用户登录信息清除登陆系统失败的记录[root@localhost root]# echo &gt; /var/log/btmp //此文件默认打开时乱码，可查到登陆失败信息[root@localhost root]# lastb //查不到登陆失败信息 清除历史执行命令[root@localhost root]# history -c //清空历史执行命令[root@localhost root]# echo &gt; ./.bash_history //或清空用户目录下的这个文件即可 导入空历史记录[root@localhost root]# vi /root/history //新建记录文件[root@localhost root]# history -c //清除记录 [root@localhost root]# history -r /root/history.txt //导入记录 [root@localhost root]# history //查询导入结果example [root@localhost root]# vi /root/history[root@localhost root]# history -c [root@localhost root]# history -r /root/history.txt [root@localhost root]# history [root@localhost root]# echo &gt; /var/log/wtmp [root@localhost root]# last[root@localhost root]# echo &gt; /var/log/btmp[root@localhost root]# lastb [root@localhost root]# history -c [root@localhost root]# echo &gt; ./.bash_history[root@localhost root]# history 关闭虚拟机 3、宿主机操作资料：KVM镜像管理利器-guestfish使用详解 1）安装guestfish套件安装 1$ yum install libguestfs-tools 2）压缩镜像文件 1$ virt-sparsify --compress centos7-dis.qcow2 centos7-dis-cloud.qcow2 镜像制作完成 上传openstack 二、参考文档：penStack镜像制作-CentOS openstack镜像制作思路、指导及问题总结 openstack制作centos6.5镜像 制作OpenStack上使用的CentOS系统镜像 KVM镜像管理利器-guestfish使用详解 CentOS清除用户登录记录和命令历史方法]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware共享文件夹设置方法]]></title>
    <url>%2F2018%2F08%2F27%2FVMware%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%E5%A4%B9%E8%AE%BE%E7%BD%AE%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、安装包依赖： 12$ yum -y install kernel-devel-$(uname -r) $ yum -y install net-tools perl gcc gcc-c++ 二、安装vmtool123456$ mount /dev/cdrom /home/tmp$ cp /home/tmp/VMwareTools-9.6.0-1294478.tar.gz /tmp$ cd /tmp$ tar -zxvf VMwareTools-9.6.0-1294478.tar.gz$ cd vmware-tools-distrib$ ./vmware-install.pl 按提示操作即可。 三、问题有/mnt/hgfs但没有共享文件的解决方法： 12$ mount -t vmhgfs .host:/ /mnt/hgfsError: cannot mount filesystem: No such device 这时不能用mount工具挂载，而是得用vmhgfs-fuse，需要安装工具包 123$ yum install open-vm-tools-devel -y有的源的名字并不一定为open-vm-tools-devel(centos) ，而是open-vm-dkms(unbuntu)执行：vmhgfs-fuse .host:/ /mnt/hgfs 此时进入/mnt/hgfs就能看到你设置的共享文件夹了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSL证书制作]]></title>
    <url>%2F2018%2F08%2F27%2FSSL%E8%AF%81%E4%B9%A6%E5%88%B6%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一、环境 OS:centos 7.3 二、步骤1、安装openssl1$ yum install opensll 2、制作CA证书12$ openssl genrsa -des3 -out my-ca.key 2048$ openssl req -new -x509 -days 3650 -key my-ca.key -out my-ca.crt 3、生成服务器证书123$ openssl genrsa -des3 -out mars-server.key 1024$ openssl req -new -key mars-server.key -out mars-server.csr$ openssl x509 -req -in mars-server.csr -out mars-server.crt -sha1 -CA my-ca.crt -CAkey my-ca.key -CAcreateserial -days 3650 4、生成无密码密钥1$ openssl rsa -in mars-server.key -out mars-server.key.insecure]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ssl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[proxy_pass反向代理配置中url后面加不加/的说明]]></title>
    <url>%2F2018%2F08%2F23%2Fproxy-pass%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE%E4%B8%ADurl%E5%90%8E%E9%9D%A2%E5%8A%A0%E4%B8%8D%E5%8A%A0-%E7%9A%84%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[1 环境 OS:centos7 nginx _proxy服务器：192.168.1.23 web服务器：192.168.1.5 2 情况说明2.1 path路径后面加”/”2.1.1 情况一NGINX配置 12345678910111213[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090/; &#125;&#125; 这样，访问http://192.168.1.23/proxy/就会被代理到http://192.168.1.5:8090/。匹配的proxy目录不需要存在根目录/var/www/html里面 注意，终端里如果访问http://192.168.1.23/proxy（即后面不带”/”），则会访问失败！因为proxy_pass配置的url后面加了”/” 访问结果如下 12345678910[root@localhost conf.d]# curl http://192.168.1.23/proxy/this is 192.168.1.5[root@localhost conf.d]# curl http://192.168.1.23/proxy&lt;html&gt;&lt;head&gt;&lt;title&gt;301 Moved Permanently&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor="white"&gt;&lt;center&gt;&lt;h1&gt;301 Moved Permanently&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.10.3&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 页面访问http://103.110.186.23/proxy的时候，会自动加上”/”（同理是由于proxy_pass配置的url后面加了”/”），并反代到http://103.110.186.5:8090的结果。 2.1.2 情况二123456789101112131415161718[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service## 那么访问http://192.168.1.23/proxy或http://192.168.1.23/proxy/，都会失败！## 这样配置后，访问http://192.168.1.23/proxy/就会被反向代理到http://192.168.1.5:8090/proxy/ 2.1.3 情况三1234567891011121314151617[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090/haha/;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service[root@localhost conf.d]# curl http://192.168.1.23/proxy/192.168.1.5 haha-index.html 这样配置的话，访问http://103.110.186.23/proxy代理到http://192.168.1.5:8090/haha/ 2.1.4 情况四相对于第三种配置的url不加”/” 1234567891011121314151617181920212223[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090/haha;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service[root@localhost conf.d]# curl http://192.168.1.23/proxy/index.html192.168.1.5 hahaindex.html##################################### 上面配置后，访问http://192.168.1.23/proxy/index.html就会被代理到http://192.168.1.5:8090/hahaindex.html同理，访问http://192.168.1.23/proxy/test.html就会被代理到http://192.168.1.5:8090/hahatest.html[root@localhost conf.d]# curl http://192.168.1.23/proxy/index.html192.168.1.5 hahaindex.html 注意，这种情况下，不能直接访问http://192.168.1.23/proxy/，后面就算是默认的index.html文件也要跟上，否则访问失败！ ———————————————————————————————————————————上面四种方式都是匹配的path路径后面加”/”，下面说下path路径后面不带”/”的情况： 2.2 path路径后面不加”/”2.2.1 情况一proxy_pass后面url带”/”： 123456789101112131415[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090/;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service 2.2.2 情况二，proxy_pass后面url不带”/” 12345678910111213141516[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service[root@localhost conf.d]# 这样配置的话，访问http://103.110.186.23/proxy会自动加上”/”（即变成http://103.110.186.23/proxy/），代理到192.168.1.5:8090/proxy/ 2.2.3 情况三123456789101112131415[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090/haha/;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service 这样配置的话，访问http://103.110.186.23/proxy会自动加上”/”（即变成http://103.110.186.23/proxy/），代理到http://192.168.1.5:8090/haha/ 2.2.4 情况四相对于第三种配置的url不加”/” 123456789101112131415[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090/haha;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service 这样配置的话，访问http://103.110.186.23/proxy，和第三种结果一样，同样被代理到http://192.168.1.5:8090/haha/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xmanager远程连接CentOS7]]></title>
    <url>%2F2018%2F08%2F23%2FXmanager%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5CentOS7%2F</url>
    <content type="text"><![CDATA[1 安装epel源 1yum install -y epel-release 2 安装lightdm和xfce12yum install -y lightdm yum groupinstall -y xfce 2.1 修改配置文件1vim /etc/lightdm/lightdm.conf 内容如下 123[XDMCPServer]enabled=trueport=177 2.2 将Display Manager切换为lightdm1systemctl disable gdm &amp;&amp; systemctl enable lightdm 2.3 启动lightdm1systemctl start lightdm 2.4 关闭防火墙1systemctl stop firewalld.service 3 登录打开Xmanger客户端，选择XDMCP并输入服务器的ip，回车运行即可。输入账号密码然后就出现下图：（如果正常跳过这步）或者出现黑屏提示无法建立连接这是因为刚开始安装的是Gnome，所以系统默认使用它，现在要改成Xfce，最简单的方法就是把xfce.desktopz之外的文件都干掉。 1234cd /usr/share/xsessions/mkdir bakmv gnome* baksystemctl restart lightdm 重新连接一切正常操作之后就成功连接了。然后就可以快速便捷的工作了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>运维工具</tag>
        <tag>xmanager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编辑rc.local启动命令执行不成功处理]]></title>
    <url>%2F2018%2F08%2F23%2F%E7%BC%96%E8%BE%91rc-local%E5%90%AF%E5%8A%A8%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E4%B8%8D%E6%88%90%E5%8A%9F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[1 问题举例： rc.local内容如下 123456789101112131415#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure# that this script will be executed during boot.touch /var/lock/subsys/localmount -t cifs -o username=backup,password=xxxxxxx //192.168.1.170/192.168.1.11/ /mnt/backup_data//usr/local/tomcat/bin/startup.sh 挂载命令执行了，但tomcat没有启动 2 问题原因及处理方法2.1 原因因java使用的是解压缩版，在/etc/profile内添加java的环境变量，系统启动时先执行的是rc.local,因此tomcat启动失败 2.2 解决办法2.2.1 方法1在rc.local内添加java的环境变量命令（必须放在tomcat启动命令前） 12export JAVA_HOME=/usr/local/java/jdk1.6.0_18export JRE_HOME=/usr/local/java/jdk1.6.0_18/jre 2.2.2 方法2在tomcat的启动脚本内添加java路径分别在tomcat_home/bin目录内的catalina.sh ，setclasspath.sh脚本前面指定JAVA_HOME路径 12export JAVA_HOME=/usr/local/java/jdk1.6.0_18export JRE_HOME=/usr/local/java/jdk1.6.0_18/jre]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>运维开发</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker私有仓库搭建]]></title>
    <url>%2F2018%2F08%2F23%2Fdocker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1 搭建仓库 安装docker-ce,过程省略…….；运行以下命令启动仓库 1$ docker run --name registry --restart always -d -p 5000:5000 -v /.registry:/var/lib/registry registry 2 修改配置修改所有需要使用私有仓库的docker服务器（包括仓库服务器）的docker配置，在/etc/docker目录创建daemon.json文件内容如下：123456# 下面这句表示表示开启5000端口的非安全模式，也就是http模式，否则在push或pull时会报https错误&#123; "insecure-registries":["192.168.1.11:5000"] &#125;# 下面这句是使用阿里云镜像加速，提高外网官方仓库的下载速度，这里一起列出了，不是必须要添加的，和私有仓库没有关系。&#123; "registry-mirrors": ["https://cz0az3lb.mirror.aliyuncs.com"]&#125; 可以同时设置，写法如下 1234&#123;"insecure-registries":["192.168.1.118:5000"],"registry-mirrors": ["http://192.168.1.118:5001"]&#125; 重启docker服务，配置生效 3 查询仓库镜像列表12345678910[root@zabbix-11 docker]# curl -XGET http://192.168.1.11:5000/v2/_catalog&#123;"repositories":["nginx"]&#125;# 显示镜像nginx# 查询镜像版本``` elixir[root@zabbix-11 docker]# curl -XGET http://192.168.1.11:5000/v2/nginx/tags/list&#123;"name":"nginx","tags":["1","1.13.7"]&#125;#查询nginx镜像的版本号，有1/1.13.7版本 以上查询也可以在web页面查询，复制命令后方http链接地址就行。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>私有仓库</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix3.4.7监控日志]]></title>
    <url>%2F2018%2F08%2F23%2Fzabbix3-4-7%E7%9B%91%E6%8E%A7%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[一、日志item介绍 下面介绍zabbix另一个“重量级”的功能——日志文件监控，它最主要的是监控日志文件中有没有某个字符串的表达式，对应日志轮转与否，zabbix都支持。 在配置Item的时候，Type选择Zabbix agent (active)，这里主要需要配置的是Key。下面是监控日志的两种key——log和logtr。 log[/path/to/some/file,,,,,] logtr[/path/to/some/filename_format,,,,,] ◆ regexp：要匹配内容的正则表达式，或者直接写你要检索的内容也可以，例如我想检索带ERROR关键词的记录 ◆ encoding：编码相关，留空即可 ◆maxlines：一次性最多提交多少行，这个参数覆盖配置文件zabbxi_agentd.conf中的’MaxLinesPerSecond’，我们也可以留空 ◆ mode：默认是all，也可以是skip，skip会跳过老数据 ◆ output：输出给zabbixserver的数据。可以是\1、\2一直\9，\1表示第一个正则表达式匹配出得内容，\2表示第二个正则表达式匹配错的内容。 如果仔细看可以发现，第一个参数不一样，logrt的第一个参数可以使用正则表达式。针对日志回滚用得，例如我们每天都切割nginx日志，日志名位www.a.com_2015-01-01.log、www.a.com_2015-01-02.log等等，使用log肯定不合适，如果文件名使用正则，那么新增的日志文件会立即加入监控。 备注：不管新日志、老日志，只要他们有变更，zabbix都会监控。 只要配置了，Zabbix会根据的正则表达式来匹配日志中的内容。注意，一定要保证Zabbix用户对日志文件有可读权限，否则这个Item的状态会变成“unsupported”。 二、监控原理及注意事项1、Zabbix Server和Zabbix Agent会追踪日志文件的大小和最后修改时间，并且分别记录在字节计数器和最新的时间计数器中。 2、Agent会从上次读取日志的地方开始读取日志。 3、字节计数器和最新时间计数器的数据会被记录在Zabbix数据库，并且发送给Agent，这样能够保证Agent从上次停止的地方开始读取日志。 4、当日志文件大小小于字节计数器中的数字时，字节计数器会变为0，从头开始读取文件。 5、所有符合配置的文件，都会被监控。 6、一个目录下的多个文件如果修改时间相同，会按照字母顺序来读取。 7、到每个Update interval的时间时，Agent会检查一次目录下的文件。 8、Zabbix Agent每秒发送日志量，有一个日志行数上限，防止网络和CPU负载过高，这个数字在zabbix_agentd.conf中的MaxLinePerSecond。 9、在logtr中，正则表达式只对文件名有效，对文件目录无效。 三、日志监控配置请确保Agent有如下两项配置 1、Hostname设定为Server创建主机是填写的Host name，必须一致 2、ServerActive设定为Server的IP Host&gt;&gt;目标主机&gt;&gt;监控项&gt;&gt;创建监控项，如下： 1、log方式 2、logrt方式 说明： type必须选择zabbix agent（active），因为数据是zabbix被监控的主动提交给server key：log[/var/log/message,error]，我们这里是监控的系统日志，打印出带有error的行，大家也可以去监控其他的日志，mysql、nginx等等都是可以的。 key: logrt[“/mnt/backup/log/[0-9]+_[0-9]+.log”,END],监控备份日志，例如20180305_113523.log,正则表达式[0-9]+_[0-9]+.log，具体可以百度学习正则表达式写法；“”双引号在使用表达式时 候最好用上，否则在保存时可能报语法错误； log time format：MMpddphh:mm:ss，对应日志的行头Sep 14 07:32:38，y表示年、M表示月、d表示日、p和:一个占位符，h表示小时，m表示分钟，s表示秒。 四、结果查看切换到最新数据里面，找到相应数据，如下是我的监控截图 五、触发器设置创建触发器，在周期24h内，写入日志则备份正常，否则告警；这里如下简单设置]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins邮件通知]]></title>
    <url>%2F2018%2F08%2F23%2Fjenkins%E9%82%AE%E4%BB%B6%E9%80%9A%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[1 简介 jenkins集成mail通知服务不再说明，功能简单，这里简单说下常用mail插件Email Extension Plugin 2 安装Email Extension Plugin 2.1 配置Email系统管理&gt;系统设置&gt;Extended E-mail Notification 下面是我的配置，改动不大，邮件通知帐号在系统集成email处配置，这里主要提下插件通知模版 2.2 配置默认模版Default Content Type配置为html 2.2.1 标题1构建通知：$PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS! 2.2.2 内容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;$&#123;ENV, var="JOB_NAME"&#125;-第$&#123;BUILD_NUMBER&#125;次构建日志&lt;/title&gt;&lt;/head&gt;&lt;body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0"&gt; &lt;table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif"&gt; &lt;tr&gt; &lt;td&gt; &lt;h2&gt; &lt;font&gt;来自Mr.Jenkins的邮件通知&lt;/font&gt; &lt;/h2&gt; &lt;font&gt;(本邮件是程序自动下发的，请勿回复！)&lt;/font&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;br /&gt; &lt;b&gt;&lt;font color="#0B610B"&gt;构建信息&lt;/font&gt;&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;ul&gt; &lt;li&gt;项目名称：$&#123;PROJECT_NAME&#125;&lt;/li&gt; &lt;li&gt;构建编号：$&#123;BUILD_NUMBER&#125;&lt;/li&gt; &lt;li&gt;构建状态：$&#123;BUILD_STATUS&#125;&lt;/li&gt; &lt;li&gt;触发原因：$&#123;CAUSE&#125;&lt;/li&gt; &lt;li&gt;构建地址：$&#123;BUILD_URL&#125;&lt;/li&gt; &lt;li&gt;构建日志：&lt;a href="$&#123;BUILD_URL&#125;console"&gt;$&#123;BUILD_URL&#125;console&lt;/a&gt;&lt;/li&gt; &lt;li&gt;单元测试报告：&lt;a href="$&#123;BUILD_URL&#125;testReport/"&gt;$&#123;BUILD_URL&#125;testReport/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;工作目录：&lt;a href="$&#123;PROJECT_URL&#125;ws"&gt;$&#123;PROJECT_URL&#125;ws&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;&lt;font color="#0B610B"&gt;构建日志:&lt;/font&gt;&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;textarea cols="80" rows="30" readonly="readonly" style="font-family: Courier New"&gt;$&#123;BUILD_LOG&#125;&lt;/textarea&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 3 添加项目构建 4 通知展示]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群WEB管理工具Shipyard]]></title>
    <url>%2F2018%2F08%2F22%2FDocker%E9%9B%86%E7%BE%A4WEB%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7Shipyard%2F</url>
    <content type="text"><![CDATA[一、 说明 Shipyard部署总体较为简单，有一键部署脚本，只需执行相应命令就可以实现部署，但在部署中还是出现很多问题，主要是shipyard官方网站无法访问，无法使用官方一键脚本部署，使用网上找到的修改版，里面有部分内容有所缺失，现已修改。 二、环境centos:7.4 master: 192.168.1.44 node1: 192.168.1.12 node2: 192.168.1.18 docker version: docker-ce-18.03.0-ce 三、安装步骤安装之前需要部署好docker环境 1、master节点执行一件部署命令12345678910111213141516171819202122232425# 正常直接从官方拉取脚本执行就行了，命令如下$ curl -s https://shipyard-project.com/deploy | bash -s[root@master ~]# curl -s https://shipyard-project.com/deploy | bash -sDeploying Shipyard -&gt; Starting DatabaseUnable to find image 'rethinkdb:latest' locallyTrying to pull repository xxx.mirror.aliyuncs.com/rethinkdb ...Pulling repository xxx.mirror.aliyuncs.com/rethinkdbTrying to pull repository docker.io/library/rethinkdb ...latest: Pulling from docker.io/library/rethinkdbDigest: sha256:29640c7d5015832c40305ad5dcc5d0996ce79b87f7e32d2fd99c9d65ad9414d4 -&gt; Starting Discovery -&gt; Starting Cert Volume -&gt; Starting Proxy -&gt; Starting Swarm Manager -&gt; Starting Swarm Agent -&gt; Starting ControllerWaiting for Shipyard on 192.168.1.44:8080 Shipyard available at http://192.168.1.44:8080Username: admin Password: shipyard# 或者下载脚本后本地执行，命令如下$ sh deploy 执行脚本的时候会自动下载相关镜像，也可以事先下载镜像文件 123456[root@master ~]# docker pull alpine[root@master ~]# docker pull library/rethinkdb[root@master ~]# docker pull microbox/etcd[root@master ~]# docker pull shipyard/docker-proxy[root@master ~]# docker pull swarm[root@master ~]# docker pull shipyard/shipyard 通过访问http://IP：8080登录shipyard,默认帐号密码：admin shipyard 2、分别在node节点执行如下命令，添加节点到shipyard1234# 部署机就是master节点$ curl -sSL https://shipyard-project.com/deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s# 或者通过本地脚本执行命令$ cat deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s 四、问题总结注意项目1：—————————————————————————————————————上面安装shipyard的脚本是英文版的，其实还有中文版的脚本，下面两种都可以使用：（两个地址都失效） 1）安装shipyard 12$ curl -sSL http://dockerclub.net/public/script/deploy | bash -s ==&gt; 中文版$ curl -sSL https://shipyard-project.com/deploy | bash -s ==&gt; 英文版 2）添加node节点 12$ curl -sSL http://dockerclub.net/public/script/deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s ==&gt; 中文版$ curl -sSL https://shipyard-project.com/deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s ==&gt; 英文版 3）删除shipyard（在节点机上执行，就会将节点从shipyard管理里踢出） 12$ curl http://dockerclub.net/public/script/deploy | ACTION=remove bash -s ==&gt; 中文版$ curl -sSL https://shipyard-project.com/deploy | ACTION=remove bash -s ==&gt; 英文版 ————————————————————————————————————— 问题项目2：————————————————————————————————————— 1）安装shipyard前不需要部署swarm集群，一键包已包含集群建设 2）部署后无法发现节点，容器页面报错，意思是到节点IP:3375端口无法建立连接，500错误；根据排查，发现所有节点swarm_manager容器都没有开启3375端口，于是在脚本里面修改 容器启动命令，添加开启3375端口，具体看下方脚本； 3）在shipyard web页面，master自身发现较慢，不知道什么原因 4）中文版shipard创建容器的时候，设置端口映射但不生效，使用英文版没有问题，未发现问题原因 5）shipyard只适合较小规模docker集群，功能上已跟不上现阶段的docker集群需求。 ————————————————————————————————————— deploy脚本这个是中文版的脚本，和官方的区别就是修改了shipyard镜像文件，下方标注部分说明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375#!/bin/bashif [ "$1" != "" ] &amp;&amp; [ "$1" = "-h" ]; then echo "Shipyard Deploy uses the following environment variables:" echo " ACTION: this is the action to use (deploy, upgrade, node, remove)" echo " DISCOVERY: discovery system used by Swarm (only if using 'node' action)" echo " IMAGE: this overrides the default Shipyard image" echo " PREFIX: prefix for container names" echo " SHIPYARD_ARGS: these are passed to the Shipyard controller container as controller args" echo " TLS_CERT_PATH: path to certs to enable TLS for Shipyard" echo " PORT: specify the listen port for the controller (default: 8080)" echo " IP: specify the address at which the controller or node will be available (default: eth0 ip)" echo " PROXY_PORT: port to run docker proxy (default: 2375)" exit 1fiif [ -z "`which docker`" ]; then echo "You must have the Docker CLI installed on your \$PATH" echo " See http://docs.docker.com for details" exit 1fiACTION=$&#123;ACTION:-deploy&#125;#官方IMAGE=$&#123;IMAGE:-shipyard/shipyard:latest&#125;#备注官方shipyard:latest标签镜像似乎有问题，实际使用shipyard:master标签镜像IMAGE=$&#123;IMAGE:-dockerclub/shipyard:latest&#125;PREFIX=$&#123;PREFIX:-shipyard&#125;SHIPYARD_ARGS=$&#123;SHIPYARD_ARGS:-""&#125;TLS_CERT_PATH=$&#123;TLS_CERT_PATH:-&#125;CERT_PATH="/etc/shipyard"PROXY_PORT=$&#123;PROXY_PORT:-2375&#125;SWARM_PORT=3375SHIPYARD_PROTOCOL=httpSHIPYARD_PORT=$&#123;PORT:-8080&#125;SHIPYARD_IP=$&#123;IP&#125;DISCOVERY_BACKEND=etcdDISCOVERY_PORT=4001DISCOVERY_PEER_PORT=7001ENABLE_TLS=0CERT_FINGERPRINT=""LOCAL_CA_CERT=""LOCAL_SSL_CERT=""LOCAL_SSL_KEY=""LOCAL_SSL_CLIENT_CERT=""LOCAL_SSL_CLIENT_KEY=""SSL_CA_CERT=""SSL_CERT=""SSL_KEY=""SSL_CLIENT_CERT=""SSL_CLIENT_KEY=""show_cert_help() &#123; echo "To use TLS in Shipyard, you must have existing certificates." echo "The certs must be named ca.pem, server.pem, server-key.pem, cert.pem and key.pem" echo "If you need to generate certificates, see https://github.com/ehazlett/certm for examples."&#125;check_certs() &#123; if [ -z "$TLS_CERT_PATH" ]; then return fi if [ ! -e $TLS_CERT_PATH ]; then echo "Error: unable to find certificates in $TLS_CERT_PATH" show_cert_help exit 1 fi if [ "$PROXY_PORT" = "2375" ]; then PROXY_PORT=2376 fi SWARM_PORT=3376 SHIPYARD_PROTOCOL=https LOCAL_SSL_CA_CERT="$TLS_CERT_PATH/ca.pem" LOCAL_SSL_CERT="$TLS_CERT_PATH/server.pem" LOCAL_SSL_KEY="$TLS_CERT_PATH/server-key.pem" LOCAL_SSL_CLIENT_CERT="$TLS_CERT_PATH/cert.pem" LOCAL_SSL_CLIENT_KEY="$TLS_CERT_PATH/key.pem" SSL_CA_CERT="$CERT_PATH/ca.pem" SSL_CERT="$CERT_PATH/server.pem" SSL_KEY="$CERT_PATH/server-key.pem" SSL_CLIENT_CERT="$CERT_PATH/cert.pem" SSL_CLIENT_KEY="$CERT_PATH/key.pem" CERT_FINGERPRINT=$(openssl x509 -noout -in $LOCAL_SSL_CERT -fingerprint -sha256 | awk -F= '&#123;print $2;&#125;') if [ ! -e $LOCAL_SSL_CA_CERT ] || [ ! -e $LOCAL_SSL_CERT ] || [ ! -e $LOCAL_SSL_KEY ] || [ ! -e $LOCAL_SSL_CLIENT_CERT ] || [ ! -e $LOCAL_SSL_CLIENT_KEY ]; then echo "Error: unable to find certificates" show_cert_help exit 1 fi ENABLE_TLS=1&#125;# container functionsstart_certs() &#123; ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-certs \ -v $CERT_PATH \ alpine \ sh) if [ $ENABLE_TLS = 1 ]; then docker cp $LOCAL_SSL_CA_CERT $PREFIX-certs:$SSL_CA_CERT docker cp $LOCAL_SSL_CERT $PREFIX-certs:$SSL_CERT docker cp $LOCAL_SSL_KEY $PREFIX-certs:$SSL_KEY docker cp $LOCAL_SSL_CLIENT_CERT $PREFIX-certs:$SSL_CLIENT_CERT docker cp $LOCAL_SSL_CLIENT_KEY $PREFIX-certs:$SSL_CLIENT_KEY fi&#125;remove_certs() &#123; docker rm -fv $PREFIX-certs &gt; /dev/null 2&gt;&amp;1&#125;get_ip() &#123; if [ -z "$SHIPYARD_IP" ]; then SHIPYARD_IP=`docker run --rm --net=host alpine ip route get 8.8.8.8 | awk '&#123; print $7; &#125;'` fi&#125;start_discovery() &#123; get_ip ID=$(docker run \ -ti \ -d \ -p 4001:4001 \ -p 7001:7001 \ --restart=always \ --name $PREFIX-discovery \ microbox/etcd:latest -addr $SHIPYARD_IP:$DISCOVERY_PORT -peer-addr $SHIPYARD_IP:$DISCOVERY_PEER_PORT)&#125;remove_discovery() &#123; docker rm -fv $PREFIX-discovery &gt; /dev/null 2&gt;&amp;1&#125;start_rethinkdb() &#123; ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-rethinkdb \ rethinkdb)&#125;remove_rethinkdb() &#123; docker rm -fv $PREFIX-rethinkdb &gt; /dev/null 2&gt;&amp;1&#125;start_proxy() &#123; TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="-e SSL_CA=$SSL_CA_CERT -e SSL_CERT=$SSL_CERT -e SSL_KEY=$SSL_KEY -e SSL_SKIP_VERIFY=1" fi # Note: we add SSL_SKIP_VERIFY=1 to skip verification of the client # certificate in the proxy image. this will pass it to swarm that # does verify. this helps with performance and avoids certificate issues # when running through the proxy. ultimately if the cert is invalid # swarm will fail to return. ID=$(docker run \ -ti \ -d \ -p $PROXY_PORT:$PROXY_PORT \ --hostname=$HOSTNAME \ --restart=always \ --name $PREFIX-proxy \ -v /var/run/docker.sock:/var/run/docker.sock \ -e PORT=$PROXY_PORT \ --volumes-from=$PREFIX-certs $TLS_OPTS\ shipyard/docker-proxy:latest)&#125;remove_proxy() &#123; docker rm -fv $PREFIX-proxy &gt; /dev/null 2&gt;&amp;1&#125;start_swarm_manager() &#123; get_ip TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="--tlsverify --tlscacert=$SSL_CA_CERT --tlscert=$SSL_CERT --tlskey=$SSL_KEY" fi EXTRA_RUN_OPTS="" if [ -z "$DISCOVERY" ]; then DISCOVERY="$DISCOVERY_BACKEND://discovery:$DISCOVERY_PORT" EXTRA_RUN_OPTS="--link $PREFIX-discovery:discovery" fi ID=$(docker run \ -ti \ -d \#下面3375端口是我出现无法连接节点后自己添加的，问题是启动容器没有开放3375端口 -p 3375:3375 \ --restart=always \ --name $PREFIX-swarm-manager \ --volumes-from=$PREFIX-certs $EXTRA_RUN_OPTS \ swarm:latest \ m --replication --addr $SHIPYARD_IP:$SWARM_PORT --host tcp://0.0.0.0:$SWARM_PORT $TLS_OPTS $DISCOVERY)&#125;remove_swarm_manager() &#123; docker rm -fv $PREFIX-swarm-manager &gt; /dev/null 2&gt;&amp;1&#125;start_swarm_agent() &#123; get_ip if [ -z "$DISCOVERY" ]; then DISCOVERY="$DISCOVERY_BACKEND://discovery:$DISCOVERY_PORT" EXTRA_RUN_OPTS="--link $PREFIX-discovery:discovery" fi ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-swarm-agent $EXTRA_RUN_OPTS \ swarm:latest \ j --addr $SHIPYARD_IP:$PROXY_PORT $DISCOVERY)&#125;remove_swarm_agent() &#123; docker rm -fv $PREFIX-swarm-agent &gt; /dev/null 2&gt;&amp;1&#125;start_controller() &#123; #-v $CERT_PATH:/etc/docker:ro \ TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="--tls-ca-cert $SSL_CA_CERT --tls-cert=$SSL_CERT --tls-key=$SSL_KEY --shipyard-tls-ca-cert=$SSL_CA_CERT --shipyard-tls-cert=$SSL_CERT --shipyard-tls-key=$SSL_KEY" fi ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-controller \ --link $PREFIX-rethinkdb:rethinkdb \ --link $PREFIX-swarm-manager:swarm \ -p $SHIPYARD_PORT:$SHIPYARD_PORT \ --volumes-from=$PREFIX-certs \ $IMAGE \ --debug \ server \ --listen :$SHIPYARD_PORT \ -d tcp://swarm:$SWARM_PORT $TLS_OPTS $SHIPYARD_ARGS)&#125;wait_for_available() &#123; set +e IP=$1 PORT=$2 echo Waiting for Shipyard on $IP:$PORT docker pull ehazlett/curl &gt; /dev/null 2&gt;&amp;1 TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="-k" fi until $(docker run --rm ehazlett/curl --output /dev/null --connect-timeout 1 --silent --head --fail $TLS_OPTS $SHIPYARD_PROTOCOL://$IP:$PORT/ &gt; /dev/null 2&gt;&amp;1); do printf '.' sleep 1 done printf '\n'&#125;remove_controller() &#123; docker rm -fv $PREFIX-controller &gt; /dev/null 2&gt;&amp;1&#125;if [ "$ACTION" = "deploy" ]; then set -e check_certs get_ip echo "Deploying Shipyard" echo " -&gt; Starting Database" start_rethinkdb echo " -&gt; Starting Discovery" start_discovery echo " -&gt; Starting Cert Volume" start_certs echo " -&gt; Starting Proxy" start_proxy echo " -&gt; Starting Swarm Manager" start_swarm_manager echo " -&gt; Starting Swarm Agent" start_swarm_agent echo " -&gt; Starting Controller" start_controller wait_for_available $SHIPYARD_IP $SHIPYARD_PORT echo "Shipyard available at $SHIPYARD_PROTOCOL://$SHIPYARD_IP:$SHIPYARD_PORT" if [ $ENABLE_TLS = 1 ] &amp;&amp; [ ! -z "$CERT_FINGERPRINT" ]; then echo "SSL SHA-256 Fingerprint: $CERT_FINGERPRINT" fi echo "Username: admin Password: shipyard"elif [ "$ACTION" = "node" ]; then set -e if [ -z "$DISCOVERY" ]; then echo "You must set the DISCOVERY environment variable" echo "with the discovery system used with Swarm" exit 1 fi check_certs echo "Adding Node" echo " -&gt; Starting Cert Volume" start_certs echo " -&gt; Starting Proxy" start_proxy echo " -&gt; Starting Swarm Manager" start_swarm_manager $DISCOVERY echo " -&gt; Starting Swarm Agent" start_swarm_agent echo "Node added to Swarm: $SHIPYARD_IP" elif [ "$ACTION" = "upgrade" ]; then set -e check_certs get_ip echo "Upgrading Shipyard" echo " -&gt; Pulling $IMAGE" docker pull $IMAGE echo " -&gt; Upgrading Controller" remove_controller start_controller wait_for_available $SHIPYARD_IP $SHIPYARD_PORT echo "Shipyard controller updated"elif [ "$ACTION" = "remove" ]; then # ignore errors set +e echo "Removing Shipyard" echo " -&gt; Removing Database" remove_rethinkdb echo " -&gt; Removing Discovery" remove_discovery echo " -&gt; Removing Cert Volume" remove_certs echo " -&gt; Removing Proxy" remove_proxy echo " -&gt; Removing Swarm Agent" remove_swarm_agent echo " -&gt; Removing Swarm Manager" remove_swarm_manager echo " -&gt; Removing Controller" remove_controller echo "Done"else echo "Unknown action $ACTION" exit 1fi 参考资料https://www.cnblogs.com/kevingrace/p/6867820.html https://www.fcwys.cc/index.php/archives/145.html]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>shipyard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle-11g-R2安装教程]]></title>
    <url>%2F2018%2F08%2F20%2FOracle-11g-R2%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1 安装Oracle前准备 1.1 创建运行oracle数据库的系统用户和用户组1234567891011121314[sonny@localhost ~]$ su root #切换到rootPassword: [root@localhost sonny]# groupadd oinstall #创建用户组oinstall[root@localhost sonny]# groupadd dba #创建用户组dba[root@localhost sonny]# useradd -g oinstall -g dba -m oracle #创建oracle用户，并加入到oinstall和dba用户组[root@localhost sonny]# passwd oracle #设置用户oracle的登陆密码，不设置密码，在CentOS的图形登陆界面没法登陆Changing password for user oracle.New password: # 密码BAD PASSWORD: The password is shorter than 8 charactersRetype new password: # 确认密码passwd: all authentication tokens updated successfully.[root@localhost sonny]# id oracle # 查看新建的oracle用户uid=1001(oracle) gid=1002(dba) groups=1002(dba)[root@localhost sonny]# 1.2 创建oracle数据库安装目录123456789101112[sonny@localhost ~]$ su rootPassword: [root@localhost sonny]# mkdir -p /data/oracle #oracle数据库安装目录[root@localhost sonny]# mkdir -p /data/oraInventory #oracle数据库配置文件目录[root@localhost sonny]# mkdir -p /data/database #oracle数据库软件包解压目录[root@localhost sonny]# cd /data[root@localhost data]# ls #创建完毕检查一下（强迫症）database oracle oraInventory[root@localhost data]# chown -R oracle:oinstall /data/oracle #设置目录所有者为oinstall用户组的oracle用户[root@localhost data]# chown -R oracle:oinstall /data/oraInventory[root@localhost data]# chown -R oracle:oinstall /data/database[root@localhost data]# 1.3 修改OS系统标识oracle默认不支持CentOS系统安装，Oracle Database 11g Release 2 的 OS要求参考： https://docs.oracle.com/cd/E11882_01/install.112/e47689/pre_install.htm#LADBI1106 我安装是64位数据库，On Linux x86-64：Red Hat Enterprise Linux 7 （RHEL 7） 另外，CentOS7.0.1511 基于 RHEL7.2 参考：http://www.linuxidc.com/Linux/2015-12/126283.htm 修改文件 /etc/RedHat-release 12345678910[sonny@localhost data]$ su rootPassword: [root@localhost data]# cat /proc/version Linux version 3.10.0-327.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) #1 SMP Thu Nov 19 22:10:57 UTC 2015[root@localhost data]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@localhost data]# vi /etc/redhat-release[root@localhost data]# cat /etc/redhat-release redhat-7 [root@localhost data]# 1.4 安装oracle数据库所需要的软件包重复一遍，我安装时Oracle Database 11g Release 2 64位数据库。 Oracle Database Package Requirements for Linux x86-64 如下：（参考：https://docs.oracle.com/cd/E11882_01/install.112/e47689/pre_install.htm#BABCFJFG） 1234操作系统:Oracle Linux 7 and Red Hat Enterprise Linux 7The following packages (or later versions) must be installed:$ yum install binutils compat-libstdc++ compat-libstdc++-33 elfutils-libelf-devel gcc gcc-c++ glibc-devel glibc-headers ksh libaio-devel libstdc++-devel make sysstat unixODBC-devel binutils-* compat-libstdc++* elfutils-libelf* glibc* gcc-* libaio* libgcc* libstdc++* make* sysstat* unixODBC* wget unzip 1.5 关闭防火墙 CentOS 7.2默认使用的是firewall作为防火墙12$ systemctl disable firewalld$ systemctl stop firewalld 1.6 关闭selinux（需重启生效）1234567891011121314[root@localhost /]# vi /etc/selinux/config[root@localhost /]# cat /etc/selinux/config# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled #此处修改为disabled# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection.SELINUXTYPE=targeted 1.7 修改内核参数1234567891011121314151617181920212223242526[sonny@localhost /]$ su rootPassword: [root@localhost /]# vi /etc/sysctl.conf [root@localhost /]# cat /etc/sysct.confcat: /etc/sysct.conf: No such file or directory[root@localhost /]# cat /etc/sysctl.conf # System default settings live in /usr/lib/sysctl.d/00-system.conf.# To override those settings, enter new settings here, or in an /etc/sysctl.d/&lt;name&gt;.conf file## For more information, see sysctl.conf(5) and sysctl.d(5).net.ipv4.icmp_echo_ignore_broadcasts = 1net.ipv4.conf.all.rp_filter = 1fs.file-max = 6815744 #设置最大打开文件数fs.aio-max-nr = 1048576kernel.shmall = 2097152 #共享内存的总量，8G内存设置：2097152*4k/1024/1024kernel.shmmax = 2147483648 #最大共享内存的段大小kernel.shmmni = 4096 #整个系统共享内存端的最大数kernel.sem = 250 32000 100 128net.ipv4.ip_local_port_range = 9000 65500 #可使用的IPv4端口范围net.core.rmem_default = 262144net.core.rmem_max= 4194304net.core.wmem_default= 262144net.core.wmem_max= 1048576[root@localhost /]# 1234567891011121314151617181920212223使配置参数生效[root@localhost /]# sysctl -pnet.ipv4.icmp_echo_ignore_broadcasts = 1net.ipv4.conf.all.rp_filter = 1sysctl: setting key "fs.file-max": Invalid argumentfs.file-max = 6815744 #设置最大打开文件数fs.aio-max-nr = 1048576sysctl: setting key "kernel.shmall": Invalid argumentkernel.shmall = 2097152 #共享内存的总量，8G内存设置：2097152*4k/1024/1024sysctl: setting key "kernel.shmmax": Invalid argumentkernel.shmmax = 2147483648 #最大共享内存的段大小sysctl: setting key "kernel.shmmni": Invalid argumentkernel.shmmni = 4096 #整个系统共享内存端的最大数kernel.sem = 250 32000 100 128sysctl: setting key "net.ipv4.ip_local_port_range": Invalid argumentnet.ipv4.ip_local_port_range = 9000 65500 #可使用的IPv4端口范围net.core.rmem_default = 262144net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048576[root@localhost /]# 1.8 解压安装包1234567891011[oracle@localhost /]$ cd /usr/local/src #进入/usr/local/src目录[oracle@localhost src]$ lslinux.x64_11gR2_database_1of2.zip linux.x64_11gR2_database_2of2.zip[oracle@localhost src]$ unzip linux.x64_11gR2_database_1of2.zip -d /data/database/ #解压(省略...)[oracle@localhost src]$ unzip linux.x64_11gR2_database_2of2.zip -d /data/database/ #解压(省略...)[oracle@localhost src]$ su rootPassword: [root@localhost src]# chown -R oracle:oinstall /data/database/database/[root@localhost src]# 2 oracle安装1.图形界面登陆oracle用户：警报root用户切换到oracle用户无法安装报错，必须使用oracle直接登录，否则报如下错误 Exception in thread “main” java.lang.NoClassDefFoundError 2.启动oralce安装，到/data/database/database/目录下，执行runInstaller3.去掉勾，懒得填，个人使用环境不需要自动接收Oracle的安全更新。4.下一步，只安装数据库软件，个人用不要那些玩意~~5.选择单例安装，前面的所有配置均为单例安装。6.添加语言7.默认安装版本企业版-Enterprise Edition8.确定数据软件的安装路径，自动读取前面Oracle环境变量中配置的值。9.理论上要创建Database Operation（OSOPER）Group:oper ,个人用，懒得建，就使用dba用户组10.安装检查，按照提示信息一个一个解决。11.一个一个检查package，在准备阶段中漏掉的，此处再安装，有些系统报错是因为现有的包的版本比检测要高，最后忽略即可。（点击Check_Again 多检查几次）12.准备完毕，fuck “Finish”开始安装。14.提示安装成功。安装日志懒得看，再说。 3 配置监听listener警告创建监听和实例前，一定要用root用户运行/data/oracle/product/11.2.0/db_1/root.sh 脚本,否则无法生成/etc/oratab文件 及造成实例无法注册等异常现象，无法设置自动等 3.1 执行netca 报错 12345678910111213141516171819[@localhost ~]$ netcaOracle Net Services Configuration:## An unexpected error has been detected by HotSpot Virtual Machine:## SIGSEGV (0xb) at pc=0x00007f69a69fcb9d, pid=8033, tid=140092892297024## Java VM: Java HotSpot(TM) 64-Bit Server VM (1.5.0_17-b03 mixed mode)# Problematic frame:# C [libclntsh.so.11.1+0x62ab9d] snlinGetAddrInfo+0x1b1## An error report file with more information is saved as hs_err_pid8033.log## If you would like to submit a bug report, please visit:# http://java.sun.com/webapps/bugreport/crash.jsp#/data/oracle/product/11.2.0/db_1/bin/netca: line 178: 8033 Aborted (core dumped) $JRE $JRE_OPTIONS -classpath $CLASSPATH oracle.net.ca.NetCA $*[oracle@localhost ~]$ 错误原因：安装操作系统是默认主机名localhost造成错误 解决办法： 12345678910111213141516[racle]# cat /etc/sysconfig/network# Created by anaconda[root@localhost oracle]# vi /etc/sysconfig/network #增加HOSTNAME[root@localhost oracle]# cat /etc/sysconfig/network# Created by anacondaHOSTNAME=odb-sonny[root@localhost oracle]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6[root@localhost oracle]# vi /etc/hosts #增加HOSTNAME&lt;/strong&gt;[root@localhost oracle]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 odb-sonny::1 localhost localhost.localdomain localhost6 localhost6.localdomain6[root@localhost oracle]# hostname odb-sonny #执行[root@localhost oracle]# 最后注销当前oracle用户，重新登陆即可！！这次发现打开配置界面正常，安装windows下面配置即可。 4 创建Oracle数据实例Orcl执行dbca命令，启动oracle实例安装界面，剩下的与Windows上安装一样，不废话了： 注意：必须先创建监听，并且监听是启动中，否则报错。 5 配置oracle自启参考：https://blog.51cto.com/meiling/2071260 5.1 编辑文件/etc/oratab1234567dbca建库时都会自动创建/etc/oratab文件#vi /etc/oratab将“orcl:/u01/app/oracle/product/12.1.0/dbhome_1:N”，改为“orcl:/u01/app/oracle/product/12.1.0/dbhome_1:Y”。修改完成后，保存退出--&lt;N|Y&gt;选项代表开机是否自启动 说明:orcl为实例名；/u01/app/oracle/product/12.1.0/dbhome1为oracle安装目录；会因安装的情况不同而有所不同。 5.2 编辑/etc/rc.d/rc.local启动文件，添加数据库启动脚本dbstart12345678910#vi /etc/rc.d/rc.local末尾添加：su oracle -lc "/u01/app/oracle/product/12.1.0/dbhome_1/bin/lsnrctl start"su oracle -lc /u01/app/oracle/product/12.1.0/dbhome_1/bin/dbstart或启动指定实例su - oracle -lc "/u01/app/oracle/product/11.2.0/dbhome_1/bin/lsnrctl start ORCL"su - oracle -lc /u01/app/oracle/product/11.2.0/dbhome_1/bin/dbstart--命令中-c代表执行脚本，脚本dbstart中指定启动的实例，脚本lsnrctl中启动配置的监听 说明:/u01/app/oracle/product/12.1.0/dbhome1为oracle的安装目录，要根据实际情况进行修改。 5.3 修改DB启动配置文件1234567# vim /u01/app/oracle/product/11.2.0/dbhome_1/bin/dbstart找到ORACLE_HOME_LISTNER这行# First argument is used to bring up Oracle Net ListenerORACLE_HOME_LISTNER=$1将$1修改为如下(你的路径)：ORACLE_HOME_LISTNER=/u01/app/oracle/product/11.2.0/dbhome_1 5.4 重启主机，查看数据库和监听是自启动开机自启动多个监听及多个实例 12345# vim /etc/rc.d/rc.localsu - oracle -lc "/u01/app/oracle/product/11.2.0/dbhome_1/bin/lsnrctl start orcl1"su - oracle -lc "/u01/app/oracle/product/11.2.0/dbhome_1/bin/lsnrctl start orcl2"su - oracle -lc /u01/app/oracle/product/11.2.0/dbhome_1/bin/dbstart监听写多个启动，实例用一个dbstart命令去启动，它会去读oratab文件，读到Y就会把对应的实例开机自启动 6 小结linux下设置实例自启动有关脚本： oratab：实例是否自启动的注册信息 dbstart：开机启动脚本文件会读取oratab信息 rc.local：开机后立即要做的文件 “启动监听”lsnrctl start “启动数据库实例”dbstart “关闭数据库实例”dbshut “关闭监听”lsnrctl stop 7 问题总结7.1 问题一：问题一iroot用户切换到oracle用户无法安装报错，必须使用oracle直接登录，否则报如下错误 Exception in thread “main” java.lang.NoClassDefFoundError 7.2 问题二：问题二Oracle 安装报错 [INS-06101] IP address of localhost could not be determined 解决方法 出现这种错误是因为主机名和/etc/hosts 文件不一致，只需要把主机名和其IP 写入/etc/hosts 文件，就ok了。 7.3 问题三：问题三在ORACLE11G R2 安装ORACLE时出现以下错误： [INS-08109] Unexpected error occurred while validating inputs at state ‘getOCMDetails’. 经GOOGLE看到 http://www.linkedin.com/groups/I-try-clone-oracle-grid-77941.S.38808726 说是使用了 LD_LIBRARY_PATH 环境参数，经查看.bash_profile ，有如下设置： LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib; export LD_LIBRARY_PATH 注释掉后，问题解决。 7.4 问题四：问题四Error in invoking target ‘install’ of makefile ‘/data/oracle/product/11.2.0/db_1/ctx/lib/ins_ctx.mk解决方法：打开一个新的终端，使用root身份登入， #vi ORACLE_HOME/ctx/lib/ins_ctx.mk找到ctxhx: $(CTXHXOBJ)$(LINK_CTXHX) $(CTXHXOBJ) $(INSO_LINK)修改为(添加红色部分): ctxhx: $(CTXHXOBJ)-static $(LINK_CTXHX) $(CTXHXOBJ) $(INSO_LINK) /usr/lib64/libc.a 7.5 问题五：warningError in invoking target ‘agent nmhs’ of makefile ‘/home/oracle_11/app/oracle/product/11.2.0/db_1/sysman/lib/ins_emagent.mk解决方法：打开一个新的终端，使用root身份登入， #vi $ORACLE_HOME/sysman/lib/ins_emagent.mk找到$(MK_EMAGENT_NMECTL)修改为(添加红色部分)：$(MK_EMAGENT_NMECTL) -lnnz11完成后在错误提示框上retry既可 7.6 问题六问题六弹出窗口乱码原因：系统默认语言中文，没有相应的中文字体包解决办法： 修改默认系统系统语言为英文 7.7 问题七问题七创建实例报错 java可以使用非oracle自带的java安装java]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm私库部署-cnpmjs]]></title>
    <url>%2F2018%2F08%2F16%2Fnpm%E7%A7%81%E5%BA%93%E9%83%A8%E7%BD%B2-cnpmjs%2F</url>
    <content type="text"><![CDATA[一、部署cnpm 1、获取代码1git clone git://github.com/fengmk2/cnpmjs.org.git 2、创建mysql库Default 123create database cnpmjs;use cnpmjs;source docs/db.sql【db.sql位于cnpmjs.org/docs/db.sql】 3、安装依赖安装依赖其实就是一个 npm install，不过 CNPM 把该指令已经写到 Makefile 里面了，所以直接执行下面的命令就好了。12$ cd cnpmjs.org $ npm install 当然万一你是 Windows 用户或者不会 make，那么还是要用 npm install。1$ npm install --build-from-source --registry=https://registry.npm.taobao.org --disturl=https://npm.taobao.org/mirrors/node 4、修改配置文件修改配置 1vim /cnpmjs.org/config/index.js cnpm提供两个端口：7001和7002，其中7001用于NPM的注册服务，7002用于Web访问。 bindingHost为安装cnpm的服务器ip地址，也就是在浏览器中只能通过访问http://192.168.48.57来访问cnpm以及获取npm的注册服务。 按照之前创建的数据库来进行配置 这里将会列举一些常用的配置项，其余的一些配置项请自行参考 config/index.js 文件。 配置字段参考https://segmentfault.com/a/1190000005946580 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748enableCluster：是否启用 cluster-worker 模式启动服务，默认 false，生产环节推荐为 true;registryPort：API 专用的 registry 服务端口，默认 7001；webPort：Web 服务端口，默认 7002；bindingHost：监听绑定的 Host，默认为 127.0.0.1，如果外面架了一层本地的 Nginx 反向代理或者 Apache 反向代理的话推荐不用改；sessionSecret：session 用的盐；logdir：日志目录；uploadDir：临时上传文件目录；viewCache：视图模板缓存是否开启，默认为 false；enableCompress：是否开启 gzip 压缩，默认为 false；admins：管理员们，这是一个 JSON Object，对应各键名为各管理员的用户名，键值为其邮箱，默认为 &#123; fengmk2: 'fengmk2@gmail.com', admin: 'admin@cnpmjs.org', dead_horse: 'dead_horse@qq.com' &#125;；logoURL：Logo 地址，不过对于我这个已经把 CNPM 前端改得面目全非的人来说已经忽略了这个配置了；adBanner：广告 Banner 的地址；customReadmeFile：实际上我们看到的 cnpmjs.org 首页中间一大堆冗长的介绍是一个 Markdown 文件转化而成的，你可以设置该项来自行替换这个文件；customFooter：自定义页脚模板；npmClientName：默认为 cnpm，如果你有自己开发或者 fork 的 npm 客户端的话请改成自己的 CLI 命令，这个应该会在一些页面的说明处替换成你所写的；backupFilePrefix：备份目录；database：数据库相关配置，为一个对象，默认如果不配置将会是一个 ~/.cnpmjs.org/data.sqlite 的 SQLite；db：数据的库名；username：数据库用户名；password：数据库密码；dialect：数据库适配器，可选 "mysql"、"sqlite"、"postgres"、"mariadb"，默认为 "sqlite"；hsot：数据库地址；port：数据库端口；pool：数据库连接池相关配置，为一个对象；maxConnections：最大连接数，默认为 10；minConnections：最小连接数，默认为 0；maxIdleTime：单条链接最大空闲时间，默认为 30000 毫秒；storege：仅对 SQLite 配置有效，数据库地址，默认为 ~/.cnpmjs/data.sqlite；nfs：包文件系统处理对象，为一个 Node.js 对象，默认是 [fs-cnpm]() 这个包，并且配置在 ~/.cnpmjs/nfs 目录下，也就是说默认所有同步的包都会被放在这个目录下；开发者可以使用别的一些文件系统插件（如上传到又拍云等）,又或者自己去按接口开发一个逻辑层，这些都是后话了；registryHost：暂时还未试过，我猜是用于 Web 页面显示用的，默认为 r.cnpmjs.org；enablePrivate：是否开启私有模式，默认为 false；如果是私有模式则只有管理员能发布包，其它人只能从源站同步包；如果是非私有模式则所有登录用户都能发布包；scopes：非管理员发布包的时候只能用以 scopes 里面列举的命名空间为前缀来发布，如果没设置则无法发布，也就是说这是一个必填项，默认为 [ '@cnpm', '@cnpmtest', '@cnpm-test' ]，据苏千大大解释是为了便于管理以及让公司的员工自觉按需发布；更多关于 NPM scope 的说明请参见 npm-scope；privatePackages：就如该配置项的注释所述，出于历史包袱的原因，有些已经存在的私有包（可能之前是用 Git 的方式安装的）并没有以命名空间的形式来命名，而这种包本来是无法上传到 CNPM 的，这个配置项数组就是用来加这些例外白名单的，默认为一个空数组；sourceNpmRegistry：更新源 NPM 的 registry 地址，默认为 https://registry.npm.taobao.org；sourceNpmRegistryIsCNpm：源 registry 是否为 CNPM，默认为 true，如果你使用的源是官方 NPM 源，请将其设为 false；syncByInstall：如果安装包的时候发现包不存在，则尝试从更新源同步，默认为 true；syncModel：更新模式（不过我觉得是个 typo），有下面几种模式可以选择，默认为 "none";"none"：永不同步，只管理私有用户上传的包，其它源包会直接从源站获取；"exist"：定时同步已经存在于数据库的包；"all"：定时同步所有源站的包；syncInterval：同步间隔，默认为 "10m" 即十分钟；syncDevDependencies：是否同步每个包里面的 devDependencies 包们，默认为 false；badgeSubject：包的 badge 显示的名字，默认为 cnpm；userService：用户验证接口，默认为 null，即无用户相关功能也就是无法有用户去上传包，该部分需要自己实现接口功能并配置，如与公司的 Gitlab 相对接，这也是后话了；alwaysAuth：是否始终需要用户验证，即便是 $ cnpm install 等命令；httpProxy：代理地址设置，用于你在墙内源站在墙外的情况。 下面是index.js配置实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275'use strict';var mkdirp = require('mkdirp');var copy = require('copy-to');var path = require('path');var fs = require('fs');var os = require('os');var version = require('../package.json').version;var root = path.dirname(__dirname);var dataDir = path.join(process.env.HOME || root, '.cnpmjs.org');var config = &#123; version: version, dataDir: dataDir, /** * Cluster mode */ enableCluster: false, numCPUs: os.cpus().length, /* * server configure */ //cnpm提供两个端口：7001和7002，其中7001用于NPM的注册服务，7002用于Web访问。 registryPort: 7001, webPort: 7002, //bindingHost为安装cnpm的服务器ip地址，也就是在浏览器中只能通过访问http://192.168.48.57来访问cnpm以及获取npm的注册服务。 bindingHost: '192.168.1.12', // only binding on 127.0.0.1 for local access // debug mode // if in debug mode, some middleware like limit wont load // logger module will print to stdout debug: process.env.NODE_ENV === 'development', // page mode, enable on development env pagemock: process.env.NODE_ENV === 'development', // session secret sessionSecret: 'cnpmjs.org test session secret', // max request json body size jsonLimit: '10mb', // log dir name logdir: path.join(dataDir, 'logs'), // update file template dir uploadDir: path.join(dataDir, 'downloads'), // web page viewCache viewCache: false, // config for koa-limit middleware // for limit download rates limit: &#123; enable: false, token: 'koa-limit:download', limit: 1000, interval: 1000 * 60 * 60 * 24, whiteList: [], blackList: [], message: 'request frequency limited, any question, please contact fengmk2@gmail.com', &#125;, enableCompress: false, // enable gzip response or not // default system admins admins: &#123; // name: email fengmk2: 'fengmk2@gmail.com', admin: 'admin@cnpmjs.org', dead_horse: 'dead_horse@qq.com', &#125;, // email notification for errors // check https://github.com/andris9/Nodemailer for more informations mail: &#123; enable: false, appname: 'cnpmjs.org', from: 'cnpmjs.org mail sender &lt;adderss@gmail.com&gt;', service: 'gmail', auth: &#123; user: 'address@gmail.com', pass: 'your password' &#125; &#125;, logoURL: 'https://os.alipayobjects.com/rmsportal/oygxuIUkkrRccUz.jpg', // cnpm logo image url adBanner: '', customReadmeFile: '', // you can use your custom readme file instead the cnpm one customFooter: '', // you can add copyright and site total script html here npmClientName: 'cnpm', // use `$&#123;name&#125; install package` packagePageContributorSearch: true, // package page contributor link to search, default is true // max handle number of package.json `dependencies` property maxDependencies: 200, // backup filepath prefix backupFilePrefix: '/cnpm/backup/', /** * database config */ //配置数据库连接信息 database: &#123; db: 'cnpm', username: 'root', password: '123456', // the sql dialect of the database // - currently supported: 'mysql', 'sqlite', 'postgres', 'mariadb' dialect: 'mysql', // custom host; default: 127.0.0.1 host: '192.168.1.12', // custom port; default: 3306 port: 3306, // use pooling in order to reduce db connection overload and to increase speed // currently only for mysql and postgresql (since v1.5.0) pool: &#123; maxConnections: 10, minConnections: 0, maxIdleTime: 30000 &#125;, // the storage engine for 'sqlite' // default store into ~/.cnpmjs.org/data.sqlite storage: path.join(dataDir, 'data.sqlite'), logging: !!process.env.SQL_DEBUG, &#125;, // package tarball store in local filesystem by default nfs: require('fs-cnpm')(&#123; dir: path.join(dataDir, 'nfs') &#125;), // if set true, will 302 redirect to `nfs.url(dist.key)` downloadRedirectToNFS: false, // registry url name registryHost: 'r.cnpmjs.org', /** * registry mode config */ // enable private mode or not // private mode: only admins can publish, other users just can sync package from source npm // public mode: all users can publish enablePrivate: false, // registry scopes, if don't set, means do not support scopes scopes: [ '@cnpm', '@cnpmtest', '@cnpm-test' ], // some registry already have some private packages in global scope // but we want to treat them as scoped private packages, // so you can use this white list. privatePackages: [], /** * sync configs */ // the official npm registry // cnpm wont directly sync from this one // but sometimes will request it for some package infomations // please don't change it if not necessary officialNpmRegistry: 'https://registry.npmjs.com', officialNpmReplicate: 'https://replicate.npmjs.com', // sync source, upstream registry // If you want to directly sync from official npm's registry // please drop them an email first sourceNpmRegistry: 'https://registry.npm.taobao.org', // upstream registry is base on cnpm/cnpmjs.org or not // if your upstream is official npm registry, please turn it off sourceNpmRegistryIsCNpm: true, // if install return 404, try to sync from source registry syncByInstall: true, // sync mode select // none: do not sync any module, proxy all public modules from sourceNpmRegistry // exist: only sync exist modules // all: sync all modules //配置从仓库同步到本地（重要） syncModel: 'exist', // 'none', 'all', 'exist' syncConcurrency: 1, // sync interval, default is 10 minutes syncInterval: '10m', // sync polular modules, default to false // because cnpm can't auto sync tag change for now // so we want to sync popular modules to ensure their tags syncPopular: false, syncPopularInterval: '1h', // top 100 topPopular: 100, // sync devDependencies or not, default is false syncDevDependencies: false, // try to remove all deleted versions from original registry syncDeletedVersions: true, // changes streaming sync syncChangesStream: false, handleSyncRegistry: 'http://127.0.0.1:7001', // badge subject on http://shields.io/ badgePrefixURL: 'https://img.shields.io/badge', badgeSubject: 'cnpm', // custom user service, @see https://github.com/cnpm/cnpmjs.org/wiki/Use-Your-Own-User-Authorization // when you not intend to ingegrate with your company's user system, then use null, it would // use the default cnpm user system userService: null, // always-auth https://docs.npmjs.com/misc/config#always-auth // Force npm to always require authentication when accessing the registry, even for GET requests. alwaysAuth: false, // if you're behind firewall, need to request through http proxy, please set this // e.g.: `httpProxy: 'http://proxy.mycompany.com:8080'` httpProxy: null, // snyk.io root url snykUrl: 'https://snyk.io', // https://github.com/cnpm/cnpmjs.org/issues/1149 // if enable this option, must create module_abbreviated and package_readme table in database //enableAbbreviatedMetadata: false, //配置enableAbbreviatedMetadata为true（默认false）,解决不能同步。重要！ enableAbbreviatedMetadata: true, // global hook function: function* (envelope) &#123;&#125; // envelope format please see https://github.com/npm/registry/blob/master/docs/hooks/hooks-payload.md#payload globalHook: null, opensearch: &#123; host: '', &#125;,&#125;;if (process.env.NODE_ENV === 'test') &#123; config.enableAbbreviatedMetadata = true;&#125;if (process.env.NODE_ENV !== 'test') &#123; var customConfig; if (process.env.NODE_ENV === 'development') &#123; customConfig = path.join(root, 'config', 'config.js'); &#125; else &#123; // 1. try to load `$dataDir/config.json` first, not exists then goto 2. // 2. load config/config.js, everything in config.js will cover the same key in index.js customConfig = path.join(dataDir, 'config.json'); if (!fs.existsSync(customConfig)) &#123; customConfig = path.join(root, 'config', 'config.js'); &#125; &#125; if (fs.existsSync(customConfig)) &#123; copy(require(customConfig)).override(config); &#125;&#125;mkdirp.sync(config.logdir);mkdirp.sync(config.uploadDir);module.exports = config;config.loadConfig = function (customConfig) &#123; if (!customConfig) &#123; return; &#125; copy(customConfig).override(config);&#125;; 5、启动服务搞好配置之后就可以直接启动服务了。1$ node dispatch.js 官方脚本启动官方的其它一些指令，比如你可以用 NPM 的 script 来运行。 1$ npm run start 在 CNPM 里面，npm script 还有下面几种指令 12345npm run dev：调试模式启动；npm run test：跑测试；npm run start：启动 CNPM；npm run status：查看 CNPM 启动状态；npm run stop：停止 CNPM。 6、访问cnpmhttp://192.168.1.12:7002/ 二、问题总结问题1：安装cnmp依赖问题npm 默认使用官方源（国外），速度慢,已报错；可以安装cnpm，使用cnpm install安装（淘宝源） Default 1234//安装cnpm $ npm install -g cnpm --registry=https://registry.npm.taobao.org//使用cnpm执行安装依赖命令 $ cnpm install 问题2：设置同步后报错设置同步后，syncModel设置为exist或all,通过仓库下载模块报如下错误 1[c#0] [error] [connect] sync error: TypeError: Cannot read property ‘findAll’ of null 解决办法： 在index.js文件内设置enableAbbreviatedMetadata: true，问题解决。 参考：https://github.com/cnpm/cnpmjs.org/issues/1236]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>cnpmjs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos同步yum源到本地]]></title>
    <url>%2F2018%2F08%2F16%2Fcentos%E5%90%8C%E6%AD%A5yum%E6%BA%90%E5%88%B0%E6%9C%AC%E5%9C%B0%2F</url>
    <content type="text"><![CDATA[一、环境 os:centos 7.3 1611 应用：yum-utils 互联网源：阿里云 二、步骤删除/etc/yum.repos.d下所有源文件 1、下载源repo到本地1$ wget -O /etc/yum.repos.d/aliyun.repo https://mirrors.aliyun.com/repo/Centos-7.repo 2、安装yum-utils提供reporsync服务1$ yum install yum-utils -y 3、查看yum源仓库标识1234567891011[root@localhost yum.repos.d]# yum repolist已加载插件：fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.com源标识 源名称 状态base/7/x86_64 CentOS-7 - Base - mirrors.aliyun.com 9,591extras/7/x86_64 CentOS-7 - Extras - mirrors.aliyun.com 196updates/7/x86_64 CentOS-7 - Updates - mirrors.aliyun.com 657repolist: 10,444 4、根据源标识同步源到本地目录1[root@localhost ~]# reposync -r base -p /var/www/html/ #这里同步base目录到本地 注意： 部分互联网yum源不支持同步 参考资料http://www.cnblogs.com/chengd/articles/6912938.html https://www.2cto.com/net/201512/455901.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7安装独立显卡驱动]]></title>
    <url>%2F2018%2F08%2F16%2FCentos7%E5%AE%89%E8%A3%85%E7%8B%AC%E7%AB%8B%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[1 Centos7 安装独立显卡驱动 1.1 参考：https://blog.csdn.net/u013378306/article/details/69229919 1.2 安装基础依赖环境1$ Yum install gcc kernel-delve -y 注意事项，保证内核版本和源码版本一样，否则，安装报错误6： 查看内核版本：1$ ls /boot | grep vmlinu 查看源码包版本 1rpm -aq | grep kernel-devel 从上面的输出中可以看出内核版本号和内核源码版本。为了解决这个错误，需要从FC官方网站上下载与内核版本对应的源码包进行安装。可以在以下网站下载并安装：http://rpmfind.net/linux/rpm2html/search.php?query=kernel-devel 1.3 源码安装1.3.1 在英伟达官网下载相应驱动搜索出相应的驱动后，不要直接点，而是右健，Save Link as… 否则，会出现下载半天没动静的情况。 存放的路径上最好不要有中文。 我存放的路径是 ~/Downloads/NVIDIA-Linux-x86_64-346.47.run 1.3.2 屏蔽默认带有的nouveau使用su命令切换到root用户下: su root 打开/lib/modprobe.d/dist-blacklist.conf 12345# 将nvidiafb注释掉。# blacklist nvidiafb 然后添加以下语句：blacklist nouveauoptions nouveau modeset=0 1.3.3 重建initramfs image步骤1234567891011121314$ mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak$ dracut /boot/initramfs-$(uname -r).img $(uname -r)``` ### 1.3.4 修改运行级别为文本模式``` bash$ systemctl set-default multi-user.target``` ### 1.3.5 重新启动, 使用root用户登陆``` bash$ reboot 1.3.6 查看nouveau是否已经禁用1ls mod | grep nouveau 如果没有显示相关的内容，说明已禁用。 1.3.7 进入下载的驱动所在目录123$ chmod +x NVIDIA-Linux-x86_64-346.47.run$ ./NVIDIA-Linux-x86_64-346.47.run 安装过程中，选择accept 如果提示要修改xorg.conf，选择yes 1.3.8 修改运行级别回图形模式1systemctl set-default graphical.target 1.3.9 重新启动，OK在Applications–Other可以看见NVIDIA X Server Settings菜单。 1.4 问题：错误1： 1ERROR: The Nouveau kernel driver is currently in use by your system. This driver is incompatible with the NVIDIA driver, and must be disabled before proceeding. Please consult the NVIDIA driver README and your Linux distribution's documentation for details on how to correctly disable the Nouveau kernel driver. 解释：如果没有执行屏蔽nouveau操作，报以上错误。 错误2： 1unable to find the development too 'cc' in you path; please make sure that you have the package 'gcc 解决办法： 1$ yum install gcc 错误3： 123ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the 'kernel-source' or 'kernel-devel' RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the '--kernel-source-path' command line option. 解决办法：1$ yum install kernel-delve 错误5： 1ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the 'kernel-source' or 'kernel-devel' RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the '--kernel-source-path' command line option. 解决方法： 1$ ./NVIDIA-Linux-x86_64-390.67.run --kernel-source-path=/usr/src/kernels/3.10.0-862.3.2.el7.x86_64/ 错误6： 1ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most frequently when this kernel module was built against the wrong or improperly configured kernel sources, with a version of gcc that differs from the one used to build the target kernel, or if another driver, such as nouveau, is present and prevents the NVIDIA kernel module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA GPU installed in this system is supported by this NVIDIA Linux graphics driver release. Please see the log entries &apos;Kernel module load error&apos; and &apos;Kernel messages&apos; at the end of the file &apos;/var/log/nvidia-installer.log&apos; for more information. 解决办法： 可以通过以下方式查看内核版本和源码包版本：ls /boot | grep vmlinuz如果上面的命令输出中有多个内核，则按grub.conf中指定的文件为准。rpm -aq | grep kernel-develkernel-devel-2.6.35.13-92.fc14.i686从上面的输出中可以看出内核版本号和内核源码版本。为了解决这个错误，需要从FC官方网站上下载与内核版本对应的源码包进行安装。 可以在以下网站下载并安装： http://rpmfind.net/linux/rpm2html/search.php?query=kernel-devel 备注：执行更新内核操作好需要重新执行屏蔽nouveau，及重建initramfs image步骤。 警告： 123456**WARNING: nvidia-installer was forced to guess the X library path '/usr/lib64' and X module path '/usr/lib64/xorg/modules'; these paths were not queryable from the system. If X fails to find the NVIDIA X driver module, please install the `pkg-config` utility and the X.Org SDK/development package for your distribution and reinstall the driver. 字符模式安装警告信息，可忽略。 2 安装cuda参考：https://blog.csdn.net/claroja/article/details/81034147 错误： 123Installing the CUDA Toolkit in /usr/local/cuda-6.5 ...Missing recommended library: libGLU.soMissing recommended library: libXmu.so 解决：安装第三方软件 123$ yum install freeglut-devel libX11-devel libXi-devel libXmu-devel \$ make mesa-libGLU-devel 2.1 测试CUDA1234567[root@fengyun6 ~]# find / -name deviceQuery/root/NVIDIA_CUDA-9.0_Samples/1_Utilities/deviceQuery/usr/local/cuda-9.0/extras/demo_suite/deviceQuery/usr/local/cuda-9.0/samples/1_Utilities/deviceQuery 若出现以下信息，则表示安装成功 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[root@fengyun6 ~]# /usr/local/cuda-9.0/extras/demo_suite/deviceQuery/usr/local/cuda-9.0/extras/demo_suite/deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: "GeForce GTX 1080 Ti" CUDA Driver Version / Runtime Version 9.1 / 9.0 CUDA Capability Major/Minor version number: 6.1 Total amount of global memory: 11178 MBytes (11721113600 bytes) (28) Multiprocessors, (128) CUDA Cores/MP: 3584 CUDA Cores GPU Max Clock rate: 1645 MHz (1.64 GHz) Memory Clock rate: 5505 Mhz Memory Bus Width: 352-bit L2 Cache Size: 2883584 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 2 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 9.0, NumDevs = 1, Device0 = GeForce GTX 1080 TiResult = PASS 安装cudnn参考：https://www.cnblogs.com/mar-q/p/7482720.html 下载：https://developer.nvidia.com/rdp/cudnn-archive 3 安装cudnn1$ tar -xvf cudnn-8.0-linux-x64-v6.0.tgz -C /usr/local/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>显卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins各种触发方式介绍]]></title>
    <url>%2F2018%2F08%2F16%2Fjenkins%E5%90%84%E7%A7%8D%E8%A7%A6%E5%8F%91%E6%96%B9%E5%BC%8F%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[触发远程构建使用svn存储库hooks的post-commit,调用jenkins的api触发job。（存储库更新即触发构建，不能针对某个分支目录更新触发）Build after other projects are built某个projects触发构建后执行构建 Build periodicallyBuild periodically：周期进行项目构建（它不care源码是否发生变化），我的配置如下： 0 2 * （每天2:00 必须build一次源码） Poll SCMPoll SCM：定时检查源码变更（根据SCM软件的版本号），如果有更新就checkout最新code下来，然后执行构建动作。我的配置如下： /5 * （每5分钟检查一次源码变化）可针对某个分支目录更新触发构建]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx反向代理报504超时错误]]></title>
    <url>%2F2018%2F08%2F16%2FNginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E6%8A%A5504%E8%B6%85%E6%97%B6%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[一、nginx+tomcat 后端为tomcat,nginx代理报504超时错误。 问题描述： #错误 121.198.17.123 - - [06/Jul/2018:01:48:57 +0000] "POST /mapbj3/getticket HTTP/1.1" 504 537 "https://XXXXXXXXXX.com/walkcode3/index.html?openId=oB6UW0cF3Z_dnYXnz4tG4OFt7Rt0" "Mozilla/5.0 (Linux; Android 8.1; PACM00 Build/O11019; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.143 Crosswalk/24.53.595.0 XWEB/155 MMWEBSDK/19 Mobile Safari/537.36 MicroMessenger/6.6.6.1300(0x26060638) NetType/WIFI Language/zh_CN MicroMessenger/6.6.6.1300(0x26060638) NetType/WIFI Language/zh_CN miniProgram" "-"2018/07/06 01:48:57 [error] 6#6: *2573 upstream timed out (110: Connection timed out) while connecting to upstream, client: 1.198.17.123, server: , request: "POST /mapbj3/getticket HTTP/1.1", upstream: "http://123.149.236.180:8022/mapbj3//getticket", host: "XXXXXXXX.com", referrer: "https://XXXXXXX.com/walkcode3/index.html?openId=oB6UW0cF3Z_dnYXnz4tG4OFt7Rt0" 1、项目本地访问没问题，通过nginx访问报504错误； 2、重启nginx后正常，反复发生，其它项目代理没有问题； 3、搜索了一大推”NGINX 504 Gateway Time-out tomcat”,都是与php有关的,而默认优化的就是php配置的; 问题处理：修改/etc/nginx/nginx.conf,添加如下信息 1234567891011121314151617181920212223242526272829303132333435363738394041# cat /etc/nginx/nginx.confuser nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; #用于tomcat反向代理,解决nginx 504错误 proxy_connect_timeout 300; proxy_send_timeout 300; proxy_read_timeout 300; proxy_buffer_size 16k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; proxy_temp_file_write_size 128k; # ps:以timeout结尾配置项时间要配置大点&#125; 二、nginx+php(未验证)问题如上，问题处理添加如下内容 1234567891011121314151617181920212223242526272829303132333435363738394041user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; #用于php反向代理,解决nginx 504错误 #以fastcgi_*配置项是php用的 fastcgi_connect_timeout 1000; fastcgi_send_timeout 1000; fastcgi_read_timeout 1000; fastcgi_buffer_size 64k; fastcgi_buffers 8 128k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; fastcgi_intercept_errors on;&#125; 参考文档https://blog.csdn.net/lcj_star/article/details/76672748https://www.iyunv.com/thread-319236-1-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yapi部署]]></title>
    <url>%2F2018%2F08%2F16%2Fyapi%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、在线安装 1、安装nodejs下载压缩包，设置环境变量，这里不祥述。 2、安装mongodb12345678910111213141516171819202122232425262728293031323334353637383940# 添加yum源$ vim /etc/yum.repos.d/mongodb-3.4.repo #添加下面的内容，wq保存。 [mongodb-org-3.4]name=MongoDB Repositorybaseurl=https://repo.mongodb.org/yum/RedHat/$releasever/mongodb-org/3.4/x86_64/gpgcheck= 0enabled=1# 安装mongodbyum install -y mongodb-org# 配置mongod#启动:$ service mongod start[root@CENTSVR247 vendors]# mongo&amp;gt; use admin #切换到admin数据库switched to db admin#创建dba用户&amp;gt; db.createUser(... ... &#123;... ... user: "dba",... ... pwd: "dba",... ... roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ]... ... &#125;... ... )# 创建yapi数据库use yapiswitched to db yapi给yapi数据库添加test1用户,权限为读写db.createUser(... ... &#123;... ... user: "test1",... ... pwd: "test1",... ... roles: [... ... &#123; role: "readWrite", db: "yapi" &#125; ... ... ]... ... &#125;... ... ) 3、安装Yapi官方说明：https://yapi.ymfe.org/devops/index.html 方式一：可视化部署12$ npm install -g yapi-cli --registry https://registry.npm.taobao.org$ yapi server 根据提示，浏览器访问 http://部署YApi服务器的IP:9090。 填写完信息后，点击“开始部署”。（大概等待1分钟）退出当前状态 CTRL + C 修改配置 这里我们不急着根据提示进行启动，有些参数我们可以通过修改配置达到。 123456789101112131415161718192021222324#修改config.json$ vim /root/my-yapi/config.json 修改下面的内容（邮箱可以不用163的），wq保存。&#123; "port": "80", "adminAccount": "yizitadmin@yizit.cn", "db": &#123; "servername": "127.0.0.1", "DATABASE": "yapi", "port": "27017" &#125;, "mail": &#123; "enable": true, "host": "smtp.163.com", "port": 465, "from": "可用于发送邮件的163邮箱", "auth": &#123; "user": "163邮箱", "pass": "163邮箱对应的密码或授权码" &#125; &#125;&#125; 启动 切换到部署目录下 cd /root/my-yapi启动服务 1$ node vendors/server/app.js 由于修改了配置，所以直接访问 http://部署YApi服务器的IP/login。 访问http://部署YApi服务器的IP:3000/login 默认用户密码：admin@admin.com ymfe.org 方式二：命令行部署安装yapi 123456$ mkdir yapi$ cd yapi$ git clone https://github.com/YMFE/yapi.git vendors //或者下载 zip 包解压到 vendors 目录$ cp vendors/config_example.json ./config.json //复制完成后请修改相关配置$ cd vendors$ npm install --production --registry https://registry.npm.taobao.org 安装pm2 12$ cd vendors$ npm install -S pm2 初始化及启动yapi 12$ npm run install-server //安装程序会初始化数据库索引和管理员账号，管理员账号名可在 config.json 配置$ node server/app.js //启动服务器后，请访问 127.0.0.1:&#123;config.json配置的端口&#125;，初次运行会有个编译的过程，请耐心等候 使用pm2启动方式 1234# 启动$ npx pm2 start ./server/app.js# 停止$ npx pm2 stop all 二、离线安装 离线安装只能采用命令行部署方式 node安装不再详述。 内网安装mongodb解压mongodb-linux-x86_64-3.6.4.tgz并放入mongodb文件夹中 12$ tar -zxvf mongodb-linux-x86_64-3.6.4.tgz$ mv mongodb-linux-x86_64-3.6.4 mongodb 把mongodb放入环境变量中, 修改~/.bashrc, 加入以下内容 1export PATH=&amp;lt;mongodb文件夹的路径&amp;gt;/bin:$PATH 验证安装 12$ source ~/.bashrc$ mongo --version 创建dbdata/db文件夹和dblog文件夹(请自行确保这些文件夹的读写权限) 12$ mkdir -p dbdata/db$ mkdir dblog 启动mongodb服务 1$ sudo ./mongodb/bin/mongod --fork --dbpath ./dbdata --logpath ./dblog/log 配置 参考上文mongodb配置。 离线安装yapi在一台连接互联网的pc上安装node环境 在外网机器获取yapi源码并安装依赖使用git获取yapi源码, 如果没有git命令请按照对应平台的安装方法安装git. 创建一个新文件夹yapi, 使用clone将yapi源码放入vendors中: 123456$ mkdir yapi$ cd yapi$ git clone https://github.com/YMFE/yapi.git vendors$ cp vendors/config_example.json ./config.json$ cd vendors$ npm install --production 我这里还安装了pm2 1$ npm n install -S pm2 将创建的yapi文件夹打成压缩包得到yapi.tar.gz(其目录下有config.json和vendors) 1$ tar -czf yapi.tar.gz yapi 至此, 所有需要外部网络的操作已经完成, 可以进行内网部署. 启动yapi解压yapi.tar.gz 1$ tar -zxvf yapi.tar.gz 按需要修改yapi/config.json中的相关配置(例如管理员账号等) 初始化数据库: 12$ cd ./yapi/vendors$ npm run install-server 使用pm2启动 1$ npx n pm2 start ./server/app.js 启动完成后即可尝试访问yapi看是否成功, 具体地址要根据内网机器的ip和在config.json中配置的端口号 如果要关闭yapi服务, 可以使用 1$ npx n pm2 stop all 问题总结：两种方式安装yapi,按照正常方式安装都是无法安装的，有如下错误 方式1图形界面，yapi server 启动9090服务后，页面无法打开，会报错误，原因是无网络。方式2命令行安装，npm install –production 回报git错误，因需要联网git操作，原因无网络，npm使用私库代理也不行。 参考资料：https://yapi.ymfe.org/devops/index.html http://stlighter.github.io/2018/04/19/yapi%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2/ https://www.linuxidc.com/Linux/2018-01/150513.htm https://blog.csdn.net/luwei42768/article/details/78919073]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>yapi</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vsftp部署]]></title>
    <url>%2F2018%2F08%2F16%2Fvsftp%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、安装前的准备1、关闭防火墙或者开端口权限。一般是firewalld或者iptables。 12$ systemctl stop firewalld$ systemctl disable firewalld 防火墙配置 1234567891011121314151617181920212223242526272829303132333435363738如果开启防火墙，请开放21端口，被动模式下设置最大和最小端口范围，并在防火墙开放端口范围。修改vsftpd.conf文件，添加pasv_min_port=8022 #最小端口pasv_max_port=8030 #最大端口pasv_promiscuous=YES #开启pasv（被动模式）放开21端口：firewall-cmd –zone=public –add-port=21/tcp –permanent放开8022-8030端口：firewall-cmd –zone=public –add-port=8022-8030/tcp –permanent重新加载防火墙：firewall-cmd –reload可能用到的命令：永久开放 ftp 服務：firewall-cmd –add-service=ftp –permanent (关闭ftp服务：firewall-cmd –remove-service=ftp –permanent) （验证不起作用）systemctl start firewalld 启动防火墙服务firewall-cmd –add-service=ftp 暂时开放ftp服务firewall-cmd –add-service=ftp –permanent永久开放ftp服務firewall-cmd –remove-service=ftp –permanent永久关闭ftp服務systemctl restart firewalld 重启firewalld服务firewall-cmd –reload 重载配置文件firewall-cmd –query-service ftp查看服务的启动状态firewall-cmd –list-all 显示防火墙应用列表firewall-cmd –add-port=8001/tcp 添加自定义的开放端口iptables -L -n | grep 21 查看设定是否生效firewall-cmd –state 检测防火墙状态 2、关闭sellinux12345678910# 临时关闭$ setenforce 0# 永久关闭$ vi /etc/selinux/config修改SELINUX=disabled# 查看是否关闭$ getenforce 二、安装vsftpd12345$ yum install -y vsftpd# 启动$ systemctl start vsftpd# 自启$ systemctl enable vsftpd 三、配置vsftpd创建vsftpd使用的系统用户，主目录为/home/vsftpd，禁止ssh登录。创建之后所有虚拟用户使用这个系统用户访问文件。useradd vsftpd -d /home/vsftpd -s /bin/false 方式一、虚拟用户配置1、创建虚拟用户主目录，比如虚拟用户叫ftp1，执行下面的命令。1$ mkdir -p /home/vsftpd/ftp1 2、创建这个虚拟用户123456$ vi /etc/vsftpd/loginusers.conf增加ftp1123456# 这样就创建了ftp1这个虚拟用户，密码为123456 3、根据这个文件创建数据库文件12$ db_load -T -t hash -f /etc/vsftpd/loginusers.conf /etc/vsftpd/loginusers.db$ chmod 600 /etc/vsftpd/loginusers.db 4、启用这个数据库文件12345$ vi /etc/pam.d/vsftpd注释掉所有内容后，增加下面的内容auth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/loginusersaccount sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/loginusers 5、创建虚拟用户配置文件123456789101112$ mkdir /etc/vsftpd/userconf这里的文件名称必须与虚拟用户名一致$ vi /etc/vsftpd/userconf/ftp1增加下面的内容local_root=/home/vsftpd/ftp1/ # 设定主目录为/home/vsftpd/ftp1；虚拟用户的根目录(根据实际修改)write_enable=YES # 开启写权限anon_umask=022 # 设定上传后文件的权限掩码。anon_world_readable_only=NO anon_upload_enable=YES anon_mkdir_write_enable=YESanon_other_write_enable=YES 6、最后修改主配置文件1234567891011121314151617$ vi /etc/vsftpd/vsftpd.conf# 更改anonymous_enable=NO #设定不允许匿名访问local_enable=YES #设定本地用户可以访问。注：如使用虚拟宿主用户，在该项目设定为NO的情况下所有虚拟用户将无法访问。# 去掉注释chroot_local_user=YES # 禁止本地用户访问除主目录以外的目录chroot_list_enable=YES #使用户不能离开主目录ascii_upload_enable=YES #允许使用ASCII模式上传ascii_download_enable=YES #设定支持ASCII模式的上传和下载功能。xferlog_file=/var/log/vsftpd.log #设定vsftpd的服务日志保存路径。注意，该文件默认不存在。必须要手动touch出来# 增加guest_enable=YES # 设定启用虚拟用户功能。guest_username=vsftpd # 指定虚拟用户的宿主用户。centos 里面已经有内置的ftp用户了（注：此用户在chroot_list_file=/etc/vsftpd/chroot_list文件里所指定的用户）-RHEL/CentOS中已经有内置的ftp用户了user_config_dir=/etc/vsftpd/userconf # 设定虚拟用户个人vsftp的RHEL/CentOS FTP服务文件存放路径。存放虚拟用户个性的CentOS FTP服务文件(配置文件名=虚拟用户名)allow_writeable_chroot=YES # 最新版的vsftpd为了安全必须使用用户主目录（也就是/home/vsftpd/ftp1）没有写权限，才能登录，或者使用allow_writeable_chroot=YES 配置介绍： 12345678910anonymous_enable=NO 禁止匿名用户登录chroot_local_user=YES # 禁止用户访问除主目录以外的目录ascii_upload_enable=YES ascii_download_enable=YES 设定支持ASCII模式的上传和下载功能guest_enable=YES 启动虚拟用户guest_username=vsftpd 虚拟用户使用的系统用户名user_config_dir=/etc/vsftpd/userconf 虚拟用户使用的配置文件目录allow_writeable_chroot=YES 最新版的vsftpd为了安全必须用户主目录（也就是/home/vsftpd/ftp1）没有写权限，才能登录，或者使用allow_writeable_chroot=YES最后重启服务使配置生效systemctl restart vsftpd备注：设置ftp1目录权限为vsftpd, chown -R vsftpd:vsftpd /home/vsftpd/ftp1,否则没有权限创建目录等写的权限，设置777权限也不行。 方式二、本地用户配置配置 FTP 权限 1、了解 VSFTP 配置vsftpd 的配置目录为 /etc/vsftpd，包含下列的配置文件： vsftpd.conf 为主要配置文件 ftpusers 配置禁止访问 FTP 服务器的用户列表 user_list 配置用户访问控制——这里的用户默认情况（即在/etc/vsftpd/vsftpd.conf中设置了userlist_deny=YES）下也不能访问FTP服务器 2、阻止匿名访问和切换根目录匿名访问和切换根目录都会给服务器带来安全风险，我们把这两个功能关闭。编辑 /etc/vsftpd/vsftpd.conf，找到下面两处配置并修改： 12345# 禁用匿名用户访问 YES 改为NO anonymous_enable=NO# 禁止本地用户登出自己的FTP主目录。chroot_local_user=YES 3、修改默认根目录修改ftp的根目录只要修改/etc/vsftpd/vsftpd.conf文件即可： 加入如下几行： 123local_root=/var/www/htmlchroot_local_user=YESanon_root=/var/www/html 注：local_root 针对系统用户；anon_root 针对匿名用户。 编辑完成后保存配置，重新启动 FTP 服务 service vsftpd restart 其它配置项说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051anonymous_enable=YES #允许匿名登陆local_enable=YES # 开启本地用户访问write_enable=YES # ftp写的权限local_umask=022 # 设定上传后文件的权限掩码。dirmessage_enable=YES #连接打印的消息connect_from_port_20=YES # 连接端口20端口xferlog_std_format=YESidle_session_timeout=600data_connection_timeout=300accept_timeout=60connect_timeout=60ascii_upload_enable=YES #上传ascii_download_enable=YES #下载chroot_local_user=NO #是否限制用户在主目录活动chroot_list_enable=YES #启动限制用户的列表chroot_list_file=/etc/vsftpd/chroot_list #每行一个用户名allow_writeable_chroot=YES #允许写listen=NOlisten_ipv6=YESpasv_min_port=50000 允许ftp工具访问的端口起止端口pasv_max_port=60000pam_service_name=vsftpd #配置虚拟用户需要的userlist_enable=NO #配置yes之后，user_list的用户不能访问ftptcp_wrappers=YESchroot_list 文件需要自己建,内容一行一个用户名字anon_root=/data/ftp/public #修改匿名用户的访问路径 4、 创建 FTP 用户新建一个不能登录系统用户. 只用来登录ftp服务 ,这里如果没设置用户目录。默认是在home下： 1$ useradd ftpuser -d /home/vsftpd -s /bin/false 为ftpuser用户设置密码：passwd ftpuser 可能用到： 1234567设置用户的主目录：usermod -d /data/ftp ftpuser彻底删除用户：#userdel -rf Fuser //强制删除用户及相关目录文件变更用户属性：#usermod -s /sbin/nologinftpuser (/bin/bash：可以登录shell，/bin/false：禁止登录shell )查看当前服务：#netstat -lntp 备注：需要设置根目录权限为777 ，否则会出现无法写入的问题，chmod 777 /var/www/html 四、访问FTP通过 FTP 客户端工具访问 FTP 客户端工具众多，下面推荐两个常用的： WinSCP– Windows 下的 FTP 和 SFTP 连接客户端 FileZilla – 跨平台的 FTP 客户端，支持 Windows 和 Mac 本人测试时使用的是Xftp 打开Xftp软件，新建一个会话，输入对应的信息，点击确定(查看ip地址：ip addr) 选中我们新建的会话，点击连接 连接成功后就可以使用Xftp上传文件了 五、要使用Xshell连接，则需要安装openssh-service查看是否安装ssh安装包，CentOS是被访问者，所以需要安装ssh-server安装包（如果没任何输出显示表示没有安装 openssh-server，可以通过输入 yum install openssh-serve进行安装），查看命令为：rpm -qa | grep ssh，如下图所示，已经安装： 找到/etc/ssh目录下的sshd_config文件，修改一些参数。去掉端口和监听地址的注释；然后允许远程登录；再开启使用用户名密码作为连接验证开启sshd服务，service sshd start检查sshd是否开启，ps -e|grep sshd或者查看22端口是否被监听，netstat -an | grep 22使用Xshell进行连接，打开Xshell软件，新建一个会话，输入对应的信息，点击确定(查看ip地址：ip addr)选中我们新建的会话，点击连接连接成功后就可以使用Xshell执行命令了 参考：https://blog.csdn.net/will0532/article/details/79175478 https://blog.csdn.net/qq_32786873/article/details/78730303 https://www.cnblogs.com/huangye-dream/p/3454595.html https://blog.csdn.net/yifansj/article/details/72855484]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes创建资源对象yaml文件例子--pod]]></title>
    <url>%2F2018%2F08%2F16%2Fkubernetes%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1yaml%E6%96%87%E4%BB%B6%E4%BE%8B%E5%AD%90-pod%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172apiVersion: v1 #指定api版本，此值必须在kubectl apiversion中,通过kubectl api-versions命令查询 kind: Pod #指定创建资源的角色/类型 metadata: #资源的元数据/属性 name: web04-pod #资源的名字，在同一个namespace中必须唯一 labels: #设定资源的标签，详情请见http://blog.csdn.net/liyingke112/article/details/77482384 k8s-app: apache version: v1 kubernetes.io/cluster-service: "true" annotations: #自定义注解列表 - name: String #自定义注解名字 spec:#specification of the resource content 指定该资源的内容 restartPolicy: Always #表明该容器一直运行，默认k8s的策略，在此容器退出后，会立即创建一个相同的容器 nodeSelector: #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1 zone: node1 containers: - name: web04-pod #容器的名字 image: web:apache #容器使用的镜像地址 imagePullPolicy: Never #三个选择Always、Never、IfNotPresent，每次启动时检查和更新（从registery）images的策略， # Always，每次都检查 # Never，每次都不检查（不管本地是否有） # IfNotPresent，如果本地有就不检查，如果没有就拉取 command: ['sh'] #启动容器的运行命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT args: ["$(str)"] #启动容器的命令参数，对应Dockerfile中CMD参数 env: #指定容器中的环境变量 - name: str #变量的名字 value: "/etc/run.sh" #变量的值 resources: #资源管理，请求请见http://blog.csdn.net/liyingke112/article/details/77452630 requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行 cpu: 0.1 #CPU资源（核数），两种方式，浮点数或者是整数+m，0.1=100m，最少值为0.001核（1m） memory: 32Mi #内存使用量 limits: #资源限制 cpu: 0.5 memory: 32Mi ports: - containerPort: 80 #容器开发对外的端口 name: httpd #名称 protocol: TCP livenessProbe: #pod内容器健康检查的设置，详情请见http://blog.csdn.net/liyingke112/article/details/77531584 httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常 path: / #URI地址 port: 80 #host: 127.0.0.1 #主机地址 scheme: HTTP initialDelaySeconds: 180 #表明第一次检测在容器启动后多长时间后开始 timeoutSeconds: 5 #检测的超时时间 periodSeconds: 15 #检查间隔时间 #也可以用这种方法 #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常 # command: # - cat # - /tmp/health #也可以用这种方法 #tcpSocket: //通过tcpSocket检查健康 # port: number lifecycle: #生命周期管理 postStart: #容器运行之前运行的任务 exec: command: - 'sh' - 'yum upgrade -y' preStop:#容器关闭之前运行的任务 exec: command: ['service httpd stop'] volumeMounts: #详情请见http://blog.csdn.net/liyingke112/article/details/76577520 - name: volume #挂载设备的名字，与volumes[*].name 需要对应 mountPath: /data #挂载到容器的某个路径下 readOnly: True volumes: #定义一组挂载设备 - name: volume #定义一个挂载设备的名字 #meptyDir: &#123;&#125; hostPath: path: /opt #挂载设备类型为hostPath，路径为宿主机下的/opt,这里设备类型支持很多种 转载：https://blog.csdn.net/liyingke112/article/details/76155428]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>yaml</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes创建资源对象yaml文件例子--rc]]></title>
    <url>%2F2018%2F08%2F16%2Fkubernetes%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1yaml%E6%96%87%E4%BB%B6%E4%BE%8B%E5%AD%90-rc%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990apiVersion: v1 #指定api版本，此值必须在kubectl apiversion中kind: ReplicationController #指定创建资源的角色/类型metadata: #资源的元数据/属性 name: test-rc #资源的名字，在同一个namespace中必须唯一 labels: #设定资源的标签，详情请见http://blog.csdn.net/liyingke112/article/details/77482384 k8s-app: apache software: apache project: test app: test-rc version: v1 annotations: #自定义注解列表 - name: String #自定义注解名字spec: replicas: 2 #副本数量2 selector: #RC通过spec.selector来筛选要控制的Pod software: apache project: test app: test-rc version: v1 name: test-rc template: #这里Pod的定义 metadata: labels: #Pod的label，可以看到这个label与spec.selector相同 software: apache project: test app: test-rc version: v1 name: test-rc spec:#specification of the resource content 指定该资源的内容 restartPolicy: Always #表明该容器一直运行，默认k8s的策略，在此容器退出后，会立即创建一个相同的容器 nodeSelector: #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1 zone: node1 containers: - name: web04-pod #容器的名字 image: web:apache #容器使用的镜像地址 imagePullPolicy: Never #三个选择Always、Never、IfNotPresent，每次启动时检查和更新（从registery）images的策略， # Always，每次都检查 # Never，每次都不检查（不管本地是否有） # IfNotPresent，如果本地有就不检查，如果没有就拉取 command: ['sh'] #启动容器的运行命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT args: ["$(str)"] #启动容器的命令参数，对应Dockerfile中CMD参数 env: #指定容器中的环境变量 - name: str #变量的名字 value: "/etc/run.sh" #变量的值 resources: #资源管理，请求请见http://blog.csdn.net/liyingke112/article/details/77452630 requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行 cpu: 0.1 #CPU资源（核数），两种方式，浮点数或者是整数+m，0.1=100m，最少值为0.001核（1m） memory: 32Mi #内存使用量 limits: #资源限制 cpu: 0.5 memory: 32Mi ports: - containerPort: 80 #容器开发对外的端口 name: httpd #名称 protocol: TCP livenessProbe: #pod内容器健康检查的设置，详情请见http://blog.csdn.net/liyingke112/article/details/77531584 httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常 path: / #URI地址 port: 80 #host: 127.0.0.1 #主机地址 scheme: HTTP initialDelaySeconds: 180 #表明第一次检测在容器启动后多长时间后开始 timeoutSeconds: 5 #检测的超时时间 periodSeconds: 15 #检查间隔时间 #也可以用这种方法 #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常 # command: # - cat # - /tmp/health #也可以用这种方法 #tcpSocket: //通过tcpSocket检查健康 # port: number lifecycle: #生命周期管理 postStart: #容器运行之前运行的任务 exec: command: - 'sh' - 'yum upgrade -y' preStop:#容器关闭之前运行的任务 exec: command: ['service httpd stop'] volumeMounts: #详情请见http://blog.csdn.net/liyingke112/article/details/76577520 - name: volume #挂载设备的名字，与volumes[*].name 需要对应 mountPath: /data #挂载到容器的某个路径下 readOnly: True volumes: #定义一组挂载设备 - name: volume #定义一个挂载设备的名字 #meptyDir: &#123;&#125; hostPath: path: /opt #挂载设备类型为hostPath，路径为宿主机下的/opt,这里设备类型支持很多种]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>yaml</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批处理文件小技巧]]></title>
    <url>%2F2018%2F08%2F16%2F%E6%89%B9%E5%A4%84%E7%90%86%E6%96%87%E4%BB%B6%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1 批处理|多服务多窗口 123@echo off start cmd /k "d:&amp;&amp;cd d:\123&amp;&amp;echo 这是一个窗口&amp;&amp;pause&amp;&amp;ping 192.168.1.3&amp;&amp;ping 172.168.1.10" start cmd /k "d:&amp;&amp;cd d:\321&amp;&amp;echo 这是另一个窗口&amp;&amp;pause&amp;&amp;ping 192.168.1.20" #参数说明1、start 用来启动一个应用2、cmd /k 表示cmd后面的命令执行完后不关闭窗口。如果要在执行完成后关闭窗口可以用/c 。详细请使用cmd/?查看3、”命令1&amp;&amp;命令2&amp;&amp;..” 将要执行的多条命令使用引号全部包起来，并且在命令间用&amp;&amp;分隔。如果只有一条命令则不用引号也可以。 2 批处理|choice的使用示例示例1：12345678910111213141516171819@echo off ::设置CMD窗口字体颜色为0a 在CMD中输入命令 color /? 可查看颜色列表color 0a::设置CMD窗口显示模式为100列宽 20行高MODE con: COLS=100 LINES=20echo -------------------echo choice 命令示例echo -------------------echo.echo.:: /c按键列表 /m提示内容 /d默认选择 /t等待秒数 /d 必须和 /t同时出现choice /c abcde /m "请输入" /d e /t 5 ::用户选择的结果会按项目序号数字（从1开始）返回在errorlevel变量中if %errorlevel%==1 echo 你选择了aif %errorlevel%==2 echo 你选择了bif %errorlevel%==3 echo 你选择了cif %errorlevel%==4 echo 你选择了dif %errorlevel%==5 echo 你选择了e 示例2： 12345678910111213141516171819202122232425262728293031323334353637383940@echo offecho ***安装并启动mysql请输入1echo ***启动Tomcat请输入2echo *******************************************echo 备注：echo 1.通过关闭tomcat运行窗口关闭Tomcat###echo 2.以下为mysql启动及关闭操作###echo *******************************************echo ***启动mysql请输入3echo ***关闭mysql请输入4choice /C 1234 /m ""if %errorlevel%==1 goto installmysqlif %errorlevel%==2 goto starttomcatif %errorlevel%==3 goto startmysqlif %errorlevel%==4 goto stopmysql:installmysqlecho "设置Mysql环境变量"setx PATH "%PATH%;D:\iwhereEarth\mysql-5.6.39-winx64\bin" /mecho "设置Mysql环境变量成功"echo "安装MYSQL"pushd D:\iwhereEarth\mysql-5.6.39-winx64\binmysqld installnet start mysqlecho "Mysql安装并启动成功"goto end:starttomcatecho "启动Tomcat"start cmd /k "d:&amp;&amp;cd D:\iwhereEarth\apache-tomcat-7.0.63\bin&amp;&amp;echo Tomcat运行窗口&amp;&amp;catalina.bat run"goto end:startmysqlpushd D:\iwhereEarth\mysql-5.6.39-winx64\binnet start mysqlgoto end:stopmysqlpushd D:\iwhereEarth\mysql-5.6.39-winx64\binnet stop mysqlgoto end:endecho.&amp;pause 3 批处理|脚本设置环境变量12345678910111213::set system environment variable::set ant environment variablesetx ANT_HOME E:\tools\apache-ant-1.9.0 /msetx PATH "%PATH%;%ANT_HOME%\BIN" /m::set android environment variableSETX ANDROID_HOME E:\android\android-sdk-windows /mSETX PATH "%PATH;%ANDROID_HOME%\platform-tools" /mecho "设置成功"pauseexit 4 批处理|局域网备份4.1 环境windows server 2000 (理论上可以用于所有windows) 4.2 问题说明创建以下批处理bat文件，拷贝文件及移动文件到指定位置，Z盘为网络映射盘符。 添加计划任务，定时执行脚本。任务执行时，显示执行完成，但bat文件中脚本命令并没有执行。根据网上方法另存为ANSI编码文件；添加执行用户及密码，都不行。最后在一篇文章中找到方法。 12345echo ****#####start备份#####**** &gt;&gt;F:\shell\day1.logxcopy d:\usr\sap\*.* z:\sap\D /E /H /R /Y /I /d &gt;&gt;F:\shell\day1.logecho %date%."success backup d sap" &gt;&gt;F:\shell\day1.logmove /Y H:\backup\*.* z:\sap\database &gt;&gt;F:\shell\day1.logecho %date%."success move databackup files" &gt;&gt;F:\shell\day1.log 4.3 问题解决4.3.1 参考资料http://blog.csdn.net/tzysf/article/details/51302039 https://social.microsoft.com/Forums/zh-CN/cc080642-9368-467a-b781-d108f1d6c214/windows-server-2003-scheduled-taskbat?forum=windowsxpzhchs 4.3.2 处理方法在脚本开头添加如下命令 1NET USE Z: \\XXX.XXX.XXX.XXX\D$\XXXX "Password" /User:"Administrator" 例子： NET USE Z: \\172.0.0.22\backup &quot;Password&quot; /User:&quot;Administrator&quot; Z: #网络映射启动器盘符 172.0.0.22 #网络映射远程主机的ip地址 Password 和Administrator #连接远程网络驱动器的用户名、密码（远程主机的授权账户密码） 123456NET USE Z: \\172.0.0.22\backup "Password" /User:"Administrator"echo ****#####start备份#####**** &gt;&gt;F:\shell\day1.logxcopy d:\usr\sap\*.* z:\sap\D /E /H /R /Y /I /d &gt;&gt;F:\shell\day1.logecho %date%."success backup d sap" &gt;&gt;F:\shell\day1.logmove /Y H:\backup\*.* z:\sap\database &gt;&gt;F:\shell\day1.logecho %date%."success move databackup files" &gt;&gt;F:\shell\day1.log 就是在脚本开始，添加连接到驱动器的命令，脚本执行时不知道为什么没有默认确定连接账户密码。 5 在cmd/bat脚本中获取当前脚本文件所在目录在xp、2000、2003等系统中都可以正常双击运行。在win7/Win10系统中双击运行时，会以普通用户身份运行，此时所获取的文件路径的确是当前路径，而不是C:\Windows\System32。但是运行到安装netpay_Service.exe -install 的系统服务时，普通用户显然权限是不够的。 于是在InstllSvc_En.cmd右键选择“以管理员身份运行”，此时又会出问题，win7/win10可能出于安全问题考虑，此时获得的目录是C:\Windows\System32，于是提示netpay_Service.exe命令无效或程序文件不存在，执行出错。 此时在脚本开始尝试加入命令cd %cd%，来获取当前路径，实验得知，这行语句在xp等系统中有效，但是在win7/win10中依然无效。得到的目录依然是C:\Windows\System32。 百度一下才知道要使用cd /d %~dp0命令来获取脚本所在的目录。在脚本最开始添加cd /d %~dp0即可，如下： 123cd /d %~dp0netpay_Service.exe -installnetpay_Monitor.exe -install 在Windows XP~Windows 10系统上运行此脚本，确认都没有问题。]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>小知识</tag>
        <tag>基础运维</tag>
        <tag>bat</tag>
        <tag>运维开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux小知识]]></title>
    <url>%2F2018%2F08%2F16%2FLinux%E5%B0%8F%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1、centos -Tab键命令补全 12345$ yum install -y bash-completion#执行脚本，使其生效或重新登录生效$ source /usr/share/bash-completion/bash_completion 2、自定义命令补全1234567# 自定义生成kubectl命令补全source &lt;(kubectl completion bash)# 将命令添加入bashrc文件，每次登录当前用户执行，使命令补全生效，也可添加入其它开机执行的脚本echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc 3、ubuntu-Tab键命令补全编辑/etc/bash.bashrc 里面有这几行语句，去掉#注释 123456789101112131415#enable bash completion in interactive shellsif ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi 4、Linux设置环境变量在 linux 里设置环境变量的方法 （ export PATH ） 一般来说，配置交叉编译工具链的时候需要指定编译工具的路径，此时就需要设置环境变量。例如我的mips-linux-gcc编译器在“/opt/au1200_rm /build_tools/bin”目录下，build_tools就是我的编译工具，则有如下三种方法来设置环境变量： 4.1、直接用export命令：立即生效，重启丢失。 1export PATH=$PATH:/opt/au1200_rm/build_tools/bin 查看是否已经设好，可用命令export查看： 12345678#exportdeclare -x BASH_ENV=&quot;/root/.bashrc&quot;declare -x G_BROKEN_FILENAMES=&quot;1&quot;declare -x HISTSIZE=&quot;1000&quot;PATH里面已经有了我要加的编译器的路径。 4.2、修改profile文件：1234567$ vi /etc/profile在里面加入:export PATH=”$PATH:/opt/au1200_rm/build_tools/bin”$ . /etc/profile #执行命令使配置生效 4.3. 修改.bashrc文件：1234$ vi /root/.bashrc在里面加入：export PATH=”$PATH:/opt/au1200_rm/build_tools/bin” 后两种方法一般需要重新注销系统才能生效，最后可以通过echo命令测试一下： 12$ echo $PATH #输出变量看看输出里面是不是已经有了 /my_new_path这个路径了。 5、实时查看日志tail -f /var/log/messages 6、客户端(Xshell、SecureCRT)拖拉文件到服务器yum install lrzsz 7、YUM下载rpm包及依赖包#只下载bash-completion包到home目录，不进行安装yum install –downloadonly –downloaddir=/home bash-completion 8、删除多少前天备份#删除目录/mnt/backup_data下30天前后缀为.sql的文件 find /mnt/backup_data/ -name “*.sql” -type f -mtime +30 -exec rm -f {} \; 9、nmon显示系统性能显示工具123yum install nmonnmon 参考https://linux.cn/article-6467-1.html 10、Htop进程浏览器yum install htop 11、查看磁盘i/o工具12345678910111213$ yum install sysstat#每2秒更新一次，-m 以MB显示，-k以kb显示iostat -d -m 2#oriostat -d -k 2#oriostat -d -m /dev/sda1 12345yum install iotop#c查看哪个进程占用i/oiotop 使用详解参考：http://man.linuxde.net/iotop 12、禁止用户登录系统1234567#禁止usermod -s /bin/false ftpuser#开启usermod -s /bin/base ftpuser]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>小知识</tag>
        <tag>基础运维</tag>
        <tag>运维工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker小技巧]]></title>
    <url>%2F2018%2F08%2F15%2Fdocker%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1 dockers开启privileged解决centos7容器无法使用systemctl命令的问题1docker run -d -e "container=docker" --privileged=true -v /sys/fs/cgroup:/sys/fs/cgroup --name centos7 centos:7.5 /usr/sbin/init]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>小知识</tag>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
