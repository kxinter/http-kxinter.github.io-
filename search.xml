<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MyISAM与InnoDB两者之间区别与选择]]></title>
    <url>%2F2019%2F04%2F28%2FMyISAM%E4%B8%8EInnoDB%E4%B8%A4%E8%80%85%E4%B9%8B%E9%97%B4%E5%8C%BA%E5%88%AB%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[1、MyISAM：默认表类型，它是基于传统的ISAM类型，ISAM是Indexed Sequential Access Method (有索引的顺序访问方法) 的缩写，它是存储记录和文件的标准方法。不是事务安全的，而且不支持外键，如果执行大量的select，insert MyISAM比较适合。 2、InnoDB：支持事务安全的引擎，支持外键、行锁、事务是他的最大特点。如果有大量的update和insert，建议使用InnoDB，特别是针对多个并发和QPS较高的情况。 一、表锁差异MyISAM:myisam只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。也可以通过lock table命令来锁表，这样操作主要是可以模仿事务，但是消耗非常大，一般只在实验演示中使用。 InnoDB ：Innodb支持事务和行级锁，是innodb的最大特色。 事务的ACID属性：atomicity,consistent,isolation,durable。 并发事务带来的几个问题：更新丢失，脏读，不可重复读，幻读。 事务隔离级别：未提交读(Read uncommitted)，已提交读(Read committed)，可重复读(Repeatable read)，可序列化(Serializable) 四种隔离级别的比较查看mysql的默认事务隔离级别“show global variables like ‘tx_isolation’;” Innodb的行锁模式有以下几种：共享锁，排他锁，意向共享锁(表锁)，意向排他锁(表锁)，间隙锁。 注意：当语句没有使用索引，innodb不能确定操作的行，这个时候就使用的意向锁，也就是表锁 关于死锁：什么是死锁？当两个事务都需要获得对方持有的排他锁才能完成事务，这样就导致了循环锁等待，也就是常见的死锁类型。 解决死锁的方法： 1、 数据库参数 2、 应用中尽量约定程序读取表的顺序一样 3、 应用中处理一个表时，尽量对处理的顺序排序 4、 调整事务隔离级别（避免两个事务同时操作一行不存在的数据，容易发生死锁） 二、数据库文件差异MyISAM ：myisam属于堆表 myisam在磁盘存储上有三个文件，每个文件名以表名开头，扩展名指出文件类型。 .frm用于存储表的定义 .MYD用于存放数据 .MYI 用于存放表索引 myisam表还支持三种不同的存储格式： 静态表(默认，但是注意数据末尾不能有空格，会被去掉) 动态表 压缩表 InnoDB ：innodb属于索引组织表 innodb有两种存储方式，共享表空间存储和多表空间存储 两种存储方式的表结构和myisam一样，以表名开头，扩展名是.frm。 如果使用共享表空间，那么所有表的数据文件和索引文件都保存在一个表空间里，一个表空间可以有多个文件，通过innodb_data_file_path和innodb_data_home_dir参数设置共享表空间的位置和名字，一般共享表空间的名字叫ibdata1-n。 如果使用多表空间，那么每个表都有一个表空间文件用于存储每个表的数据和索引，文件名以表名开头，以.ibd为扩展名。 三、索引差异1、关于自动增长myisam引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 innodb引擎的自动增长咧必须是索引，如果是组合索引也必须是组合索引的第一列。 2、关于主键myisam允许没有任何索引和主键的表存在， myisam的索引都是保存行的地址。 innodb引擎如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见) innodb的数据是主索引的一部分，附加索引保存的是主索引的值。 3、关于count()函数myisam保存有表的总行数，如果select count(*) from table;会直接取出出该值 innodb没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了wehre 条件后，myisam和innodb处理的方式都一样。 4、全文索引myisam支持FULLTEXT类型的全文索引 innodb不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。（sphinx 是一个开源软件，提供多种语言的API接口，可以优化mysql的各种查询） 5、delete from table使用这条命令时，innodb不会从新建立表，而是一条一条的删除数据，在innodb上如果要清空保存有大量数据的表，最 好不要使用这个命令。(推荐使用truncate table，不过需要用户有drop此表的权限) 6、索引保存位置myisam的索引以表名+.MYI文件分别保存。 innodb的索引和数据一起保存在表空间里。 四、开发的注意事项 可以用show create table tablename 命令看表的引擎类型。 对不支持事务的表做start/commit操作没有任何效果，在执行commit前已经提交。 可以执行以下命令来切换非事务表到事务（数据不会丢失），innodb表比myisam表更安全：alter table tablename type=innodb;或者使用 alter table tablename engine = innodb; 默认innodb是开启自动提交的，如果你按照myisam的使用方法来编写代码页不会存在错误，只是性能会很低。如何在编写代码时候提高数据库性能呢？ a、尽量将多个语句绑到一个事务中，进行提交，避免多次提交导致的数据库开销。 b、在一个事务获得排他锁或者意向排他锁以后，如果后面还有需要处理的sql语句，在这两条或者多条sql语句之间程序应尽量少的进行逻辑运算和处理，减少锁的时间。 c、尽量避免死锁 d、sql语句如果有where子句一定要使用索引，尽量避免获取意向排他锁。 f、针对我们自己的数据库环境，日志系统是直插入，不修改的，所以我们使用混合引擎方式，ZION_LOG_DB照旧使用myisam存储引擎，只有ZION_GAME_DB，ZION_LOGIN_DB，DAUM_BILLING使用Innodb引擎。 五、究竟该怎么选择下面先让我们回答一些问题： ◆ 你的数据库有外键吗？ ◆ 你需要事务支持吗？ ◆ 你需要全文索引吗？ ◆ 你经常使用什么样的查询模式？ ◆ 你的数据有多大？ myisam只有索引缓存innodb不分索引文件数据文件innodb buffermyisam只能管理索引，在索引数据大于分配的资源时，会由操作系统来cache；数据文件依赖于操作系统的cache。innodb不管是索引还是数据，都是自己来管理 思考上面这些问题可以让你找到合适的方向，但那并不是绝对的。如果你需要事务处理或是外键，那么InnoDB可能是比较好的方式。如果你需要全文索引，那么通常来说 MyISAM是好的选择，因为这是系统内建的，然而，我们其实并不会经常地去测试两百万行记录。所以，就算是慢一点，我们可以通过使用Sphinx从InnoDB中获得全文索引。 数据的大小，是一个影响你选择什么样存储引擎的重要因素，大尺寸的数据集趋向于选择InnoDB方式，因为其支持事务处理和故障恢复。数据库的在小决定了故障恢复的时间长短，InnoDB可以利用事务日志进行数据恢复，这会比较快。而MyISAM可能会需要几个小时甚至几天来干这些事，InnoDB只需要几分钟。 操作数据库表的习惯可能也会是一个对性能影响很大的因素。比如： COUNT() 在 MyISAM表中会非常快，而在InnoDB表下可能会很痛苦。而主键查询则在InnoDB下会相当相当的快，但需要小心的是如果我们的主键太长了也会导致性能问题。大批的inserts语句在 MyISAM下会快一些，但是updates 在InnoDB下会更快一些——尤其在并发量大的时候。 所以，到底你检使用哪一个呢？根据经验来看，如果是一些小型的应用或项目，那么MyISAM 也许会更适合。当然，在大型的环境下使用MyISAM 也会有很大成功的时候，但却不总是这样的。如果你正在计划使用一个超大数据量的项目，而且需要事务处理或外键支持，那么你真的应该直接使用InnoDB方式。但需要记住InnoDB 的表需要更多的内存和存储，转换100GB 的MyISAM表到InnoDB 表可能会让你有非常坏的体验。 对于支持事务的InnoDB类型的表，影响速度的主要原因是AUTOCOMMIT默认设置是打开的，而且程序没有显式调用BEGIN 开始事务，导致每插入一条都自动Commit，严重影响了速度。可以在执行sql前调用begin，多条sql形成一个事务（即使autocommit打开也可以），将大大提高性能。 InnoDBInnoDB 给 MySQL 提供了具有事务(commit)、回滚(rollback)和崩溃修复能力 (crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。InnoDB提供了行锁(locking on row level)，提供与 Oracle 类型一致的不加锁读取(non- locking read in SELECTs)。这些特性均提高了多用户并发操作的性能表现。在InnoDB表中不需要扩大锁定 (lock escalation)，因为InnoDB的列锁定(row level locks)适宜非常小的空间。 InnoDB 是 MySQL 上第一个提供外键约束(FOREIGN KEY constraints)的表引擎。 InnoDB 的设计目标是处理大容量数据库系统，它的 CPU 利用率是其它基于磁盘的关系数据库引擎所不能比的。在技术上，InnoDB 是一套放在 MySQL 后台的完整数据库系统，InnoDB 在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 InnoDB 把数据和索引存放在表空间里，可能包含多个文件，这与其它的不一样，举例来说，在 MyISAM 中，表被存放在单独的文件中。InnoDB 表的大小只受限于操作系统的文件大小，一般为 2 GB。InnoDB所有的表都保存在同一个数据文件ibdata1 中（也可能是多个文件，或者是独立的表空间文件）,相对来说比较不好备份，免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump。 MyISAMMyISAM 是MySQL缺省存贮引擎 .每张MyISAM表被存放在三个文件 。frm 文件存放表格定义。 数据文件是MYD (MYData)。 索引文件是 MYI (MYIndex) 引伸。因为MyISAM相对简单所以在效率上要优于InnoDB..小型应用使用MyISAM是不错的选择.MyISAM表是保存成文件的形式,在跨平台的数据转移中使用MyISAM存储会省去不少的麻烦 以下是一些细节和具体实现的差别： InnoDB不支持FULLTEXT类型的索引。 InnoDB中不保存表的具体行数，也就是说，执行select count() from table时，InnoDB要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。注意的是，当count()语句包含 where条件时，两种表的操作是一样的。 对于AUTO_INCREMENT类型的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中，可以和其他字段一起建立联合索引。 DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除。 LOAD TABLE FROM MASTER操作对InnoDB是不起作用的，解决方法是首先把InnoDB表改成MyISAM表，导入数据后再改成InnoDB表，但是对于使用的额外的InnoDB特性（例如外键）的表不适用。 另外，InnoDB表的行锁也不是绝对的，如果在执行一个SQL语句时MySQL不能确定要扫描的范围，InnoDB表同样会锁全表，例如update table set num=1 where name like &quot;%aaa%&quot;; 任何一种表都不是万能的，只用恰当的针对业务类型来选择合适的表类型，才能最大的发挥MySQL的性能优势。 六、重复地总结一遍 MyISAM不支持事务，InnoDB是事务类型的存储引擎，当我们的表需要用到事务支持的时候，那肯定是不能选择MyISAM了。 MyISAM只支持表级锁，BDB支持页级锁和表级锁默认为页级锁，而InnoDB支持行级锁和表级锁默认为行级锁 表级锁：直接锁定整张表，在锁定期间，其他进程无法对该表进行写操作，如果设置的是写锁，那么其他进程读也不允许 MyISAM是表级锁定的存储引擎，它不会出现死锁问题 对于write，表锁定原理如下： 如果表上没有锁，在其上面放置一个写锁，否则，把锁定请求放在写锁队列中。 对于read，表锁定原理如下 ： 如果表上没有写锁定，那么把一个读锁放在其上面，否则把锁请求放在读锁定队列中 当一个锁定被释放时，表可被写锁定队列中的线程得到，然后才是读锁定队列中的线程。这意味着，如果你在一个表上有许多更新，那么你的SELECT语句将等到所有的写锁定线程执行完。 行级锁：只对指定的行进行锁定，其他进程还是可以对表中的其他行进行操作的。 行级锁是Mysql粒度最小的一种锁，它能大大的减少数据库操作的冲突，但是粒度越小实现成本也越大。 行级锁可能会导致“死锁”，那到底是怎么导致的呢，分析原因：Mysql行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，那么Mysql就会锁定这个主键索引，如果sql语句操作的是非主键索引，那么Mysql会先锁定这个非主键索引，再去锁定主键索引。 在UPDATE 和 DELETE操作时Mysql不仅会锁定所有WHERE 条件扫描过得索引，还会锁定相邻的键值。 “死锁”举例分析： 表Test：（ID,STATE,TIME） 主键索引：ID 非主键索引：STATE 当执行”UPDATE STATE =1011 WHERE STATE=1000“ 语句的时候会锁定STATE索引，由于STATE 是非主键索引,所以Mysql还会去请求锁定ID索引 当另一个SQL语句与语句1几乎同时执行时：“UPDATE STATE=1010 WHERE ID=1” 对于语句2 Mysql会先锁定ID索引，由于语句2操作了STATE字段，所以Mysql还会请求锁定STATE索引。这时。彼此锁定着对方需要的索引，又都在等待对方释放锁定。所以出现了”死锁”的情况。 行级锁的优点： 有许多线程访问不同的行时，只存在少量的冲突。 回滚时只有少量的更改 可以长时间锁定单一的行 行级锁缺点： 相对于页级锁和表级锁来说占用了更多的内存 当表的大部分行在使用时，比页级锁和表级锁慢，因为你必须获得更多的锁 当在大部分数据上经常使用GROUP BY操作，肯定会比表级锁和页级锁慢。 页级锁：表级锁速度快，但是冲突多；行级锁速度慢，但冲突少；页级锁就是他俩折中的，一次锁定相邻的一组记录。 MyISAM引擎不支持外键，InnoDB支持外键 MyISAM引擎的表在大量高并发的读写下会经常出现表损坏的情况 我们以前做的项目就遇到这个问题，表的INSERT 和 UPDATE操作很频繁，原来用的MyISAM引擎，导致表隔三差五就损坏，后来更换成了InnoDB引擎。 其他容易导致表损坏原因： 服务器突然断电导致数据文件损坏，强制关机（mysqld未关闭情况下）导致表损坏 mysqld进程在写入操作的时候被杀掉 磁盘故障 表损坏常见症状： 查询表不能返回数据或返回部分数据 打开表失败： Can’t open file: ‘×××.MYI’ (errno: 145) 。 Error: Table ‘p’ is marked as crashed and should be repaired 。 Incorrect key file for table: ‘…’. Try to repair it Mysql表的恢复： 对于MyISAM表的恢复： 可以使用Mysql自带的myisamchk工具： myisamchk -r tablename 或者 myisamchk -o tablename（比前面的更保险） 对表进行修复 对于count()查询来说MyISAM更有优势 因为MyISAM存储了表中的行数记录，执行SELECT COUNT() 的时候可以直接获取到结果，而InnoDB需要扫描全部数据后得到结果。 但是注意一点：对于带有WHERE 条件的SELECT COUNT()语句两种引擎的表执行过程是一样的，都需要扫描全部数据后得到结果 InnoDB是为处理巨大数据量时的最大性能设计，它的CPU效率可能是任何其它基于磁盘的关系数据库引擎所不能匹敌的。 MyISAM支持全文索引（FULLTEXT），InnoDB不支持 MyISAM引擎的表的查询、更新、插入的效率要比InnoDB高 网上截取了前辈们测试结论： 测试方法：连续提交10个query， 表记录总数：38万 ， 时间单位 s 12345678910111213引擎类型 MyISAM InnoDB 性能相差 count 0.0008357 3.0163 3609 查询主键 0.005708 0.1574 27.57 查询非主键 24.01 80.37 3.348 更新主键 0.008124 0.8183 100.7 更新非主键 0.004141 0.02625 6.338 插入 0.004188 0.3694 88.21 (1) 加了索引以后，对于MyISAM查询可以加快：4 206.09733倍，对InnoDB查询加快510.72921倍，同时对MyISAM更新速度减慢为原来的1/2，InnoDB的更新速度减慢为原来的1/30。要看情况决定是否要加索引，比如不查询的log表，不要做任何的索引。 (2) 如果你的数据量是百万级别的，并且没有任何的事务处理，那么用MyISAM是性能最好的选择。 (3）InnoDB表的大小更加的大，用MyISAM可省很多的硬盘空间。 123456789101112在我们测试的这个38w的表中，表占用空间的情况如下： 引擎类型 MyISAM InnoDB 数据 53,924 KB 58,976 KB 索引 13,640 KB 21,072 KB 占用总空间 67,564 KB 80,048 KB 另外一个176W万记录的表， 表占用空间的情况如下： 引擎类型 MyIsam InnorDB 数据 56,166 KB 90,736 KB 索引 67,103 KB 88,848 KB 占用总空间 123,269 KB 179,584 KB 七、性能对比测试的版本是mysql Ver 14.14 Distrib 5.1.49, for debian-linux-gnu (i686)，使用的是Innodb plugin 1.0.8（官方称比built-in版本性能更好）和默认的MyISAM。 测试机器是笔记本，配置如下：Intel 酷睿2双核 P8600，4G DDR3 1066内存，320G硬盘5400转。 测试一：数据插入性能测试，这里我分别对innodb_flush_log_at_trx_commit参数打开和关闭都测了了一下，每次测试都是运行40s，表中数字都是实际插入条数。 12345678910 MyISAM Innodb (打开) Innodb(关闭)单线程，逐个插入 120000 60000 600004线程，逐个插入 40000*4 20000*4 40000*4单线程，批量100条/次插入 3600*100 800*100 3000*100单线程，批量200条/次插入 1800*200 400*200 1600*200 可以发现批量插入的性能远高于单条插入，但是一次批量的大小对性能影响不大。每条记录是否都刷新日志的参数对innodb性能的影响巨大。总体上来说，MyISAM性能更优一点。这里有一点需要注意，在插入测试过程中，我对系统资源进行了监控，发现MyISAM对系统资源占用很低，但是Innodb对磁盘占用却很高，应该是对事务控制多了很多需要记录的日志。 测试二：数据读取性能测试。每次随机读取1000条记录，反复进行读取。 123456 MyISAM Innodb单线程，200次读取 5.7s 16.7s4线程，200次读取 12s 40.8s 可以看出MyISAM的读取性能非常恐怖，性能差距在3倍的样子。 以上两个测试发现MyISAM在无事务的需求下几乎完胜，但是要知道它是表锁，Innodb是行锁，那么在并发读写同时存在的情况下，那结果会是怎么样呢？！ 测试三：两个线程并发写入，2个线程并发读取。 123456 MyISAM Innodb逐个插入 写入40s：10000*2 读取200次*2：14s 写入40s：60000*2 读取200次*2：50s批量100条/次插入 写入40s：1000*100*2 读取200次*2：10s 写入40s：1500*100*2 读取200次*2：50s 这下立刻显示出Innodb在并发情况下强劲的性能，几乎没有什么性能衰减。而MyISAM单条插入速度变得非常慢，批量插入也下降了40%性能。 总结一下，在写多读少的应用中还是Innodb插入性能更稳定，在并发情况下也能基本，如果是对读取速度要求比较快的应用还是选MyISAM。 转自于：http://blog.csdn.net/wjtlht928/article/details/46641865]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL自带的性能压力测试工具mysqlslap]]></title>
    <url>%2F2019%2F04%2F28%2FMySQL%E8%87%AA%E5%B8%A6%E7%9A%84%E6%80%A7%E8%83%BD%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7mysqlslap%2F</url>
    <content type="text"><![CDATA[1 mysqlslap介绍 mysqlslap是从MySQL5.1.4版就开始官方提供的压力测试工具。 通过模拟多个并发客户端并发访问MySQL来执行压力测试，同时提供了较详细的SQL执行数据性能报告，并且能很好的对比多个存储引擎（MyISAM，InnoDB等）在相同环境下的相同并发压力下的性能差别。 mysqlslap 官方介绍：http://dev.mysql.com/doc/refman/5.6/en/mysqlslap.html 1.1 常用参数 [options] 详解--host=host_name: -h host_name 连接到的MySQL服务器的主机名（或IP地址），默认为本机localhost; --user=user_name: -u user_name 连接MySQL服务时用的用户名; --password[=password]: -p[password] 连接MySQL服务时用的密码; --create-schema: 代表自定义的测试库名称，测试的schema，MySQL中schema也就是database;(没指定使用哪个数据库时，可能会遇到错误mysqlslap: Error when connecting to server: 1049 Unknown database ‘mysqlslap’) `–query=name`: -q 使用自定义脚本执行测试（可以是SQL字符串或脚本），例如可以调用自定义的一个存储过程或者sql语句来执行测试。 --create: 创建表所需的SQL（可以是SQL字符串或脚本） --concurrency=N: -c N 表示并发量，也就是模拟多少个客户端同时执行query。可指定多个值，以逗号或者–delimiter参数指定的值做为分隔符。例如：–concurrency=100,200,500（分别执行100、200、500个并发）。 --iterations=N: -i N测试执行的迭代次数，代表要在不同的并发环境中，各自运行测试多少次；多次运行以便让结果更加准确。 --number-of-queries=N: 总的测试查询次数(并发客户数×每客户查询次数) --engine=engine_name: -e engine_name 代表要测试的引擎，可以有多个，用分隔符隔开。例如：–engines=myisam,innodb,memory。 --auto-generate-sq: -a 自动生成测试表和数据，表示用mysqlslap工具自己生成的SQL脚本来测试并发压力。 --auto-generate-sql-load-type=type: 测试语句的类型。代表要测试的环境是读操作还是写操作还是两者混合的。取值包括：read (scan tables), write (insert into tables), key (read primary keys), update (update primary keys), or mixed (half inserts, half scanning selects). 默认值是：mixed. --auto-generate-sql-add-auto-increment: 代表对生成的表自动添加auto_increment列，从5.1.18版本开始支持。 --number-char-cols=N: -x N 自动生成的测试表中包含多少个字符类型的列，默认1 --number-int-cols=N: -y N自动生成的测试表中包含多少个数字类型的列，默认1 --commint=N: 多少条DML后提交一次。 --compress: -C 如果服务器和客户端支持都压缩，则压缩信息传递。 --only-print: 只打印测试语句而不实际执行。 --detach=N: 执行N条语句后断开重连。 --debug-info: -T 打印内存和CPU的相关信息。 1.2 测试范例：1mysqlslap -uroot -p --socket /tmp/mysql3306.sock --concurrency=1 --iterations=1 --create-schema='test' --query='SELECT id,unionid,current_num,total_num FROM invite_join WHERE unionid="Cmo" AND active_id="3" AND is_deleted =0 ORDER BY id DESC LIMIT 1;' --number-of-queries=1000000 各种测试参数实例（-p后面跟的是mysql的root密码）： 单线程测试。测试做了什么。 1# mysqlslap -a -uroot -p123456 多线程测试。使用–concurrency来模拟并发连接。 1# mysqlslap -a -c 100 -uroot -p123456 迭代测试。用于需要多次执行测试得到平均值。 1# mysqlslap -a -i 10 -uroot -p123456 1234567# mysqlslap ---auto-generate-sql-add-autoincrement -a -uroot -p123456# mysqlslap -a --auto-generate-sql-load-type=read -uroot -p123456# mysqlslap -a --auto-generate-secondary-indexes=3 -uroot -p123456# mysqlslap -a --auto-generate-sql-write-number=1000 -uroot -p123456# mysqlslap --create-schema world -q "select count(*) from City" -uroot -p123456# mysqlslap -a -e innodb -uroot -p123456# mysqlslap -a --number-of-queries=10 -uroot -p123456 测试同时不同的存储引擎的性能进行对比： 1# mysqlslap -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 --engine=myisam,innodb --debug-info -uroot -p123456 执行一次测试，分别50和100个并发，执行1000次总查询： 1# mysqlslap -a --concurrency=50,100 --number-of-queries 1000 --debug-info -uroot -p123456 50和100个并发分别得到一次测试结果(Benchmark)，并发数越多，执行完所有查询的时间越长。为了准确起见，可以多迭代测试几次: 1# mysqlslap -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 --debug-info -uroot -p123456]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[note样式]]></title>
    <url>%2F2019%2F04%2F25%2Fnote%E6%A0%B7%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1 default123&#123;%note default %&#125;**default**info&#123;%endnote%&#125; defaultinfo 2 info123&#123;%note info %&#125;**info**info&#123;%endnote%&#125; infoinfo 3 primary123&#123;%note primary %&#125;**primary**primary&#123;%endnote%&#125; primaryprimary 4 success123&#123;%note success %&#125;**success**success&#123;%endnote%&#125; successsuccess 5 warning123&#123;%note warning %&#125;**warning**warning&#123;%endnote%&#125; warningwarning 6 danger123&#123;%note danger %&#125;**danger**danger&#123;%endnote%&#125; dangerdanger]]></content>
  </entry>
  <entry>
    <title><![CDATA[mysql优化]]></title>
    <url>%2F2019%2F04%2F25%2Fmysql%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1 缓存优化之QueryCache 从 MySQL4开始，出现了QueryCache查询缓存，如果使用了QueryCache，当查询接收到一个和之前同样的查询，服务器将会从查询缓存种检索结果，而不是再次分析和执行上次的查询。这样就大大提高了性能，节省时间，非常有用。 打开查询缓存，是通过几个步骤来设置的，例如：虽然你设置Mysql允许查询缓存，但是如果你设置的查询缓存大小为了0，这和没有允许没什么区别。所以必须是几个步骤的设置才能真正打开查询缓存这个功能。 1.1 查询开启状态 一般，我们会把 query_cache_type 设置为 ON,默认情况下应该是ON(其实5.6默认是OFF) 1mysql&gt; select @@query_cache_type; # 查询开启状态 +——————–+| @@query_cache_type |+——————–+| ON |+——————–+这样 当我们执行 select id,name from tableName; 这样就会用到查询缓存。在 query_cache_type 打开的情况下，如果你不想使用缓存，需要指明select sql_no_cache id,name from tableName;当然也可以禁用查询缓存：mysql&gt; set session uery_cache_type=off;这里我们不讨论这个，我们演示常用的设置。 介绍 query_cache_size: 主要用来缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集，所以仅仅只能针对select语句。当我们打开了 Query Cache功能，MySQL在接受到一条select语句的请求后，如果该语句满足Query Cache的要求(未显式说明不允许使用Query Cache，或者已经显式申明需要使用Query Cache)，MySQL会直接根据预先设定好的HASH算法将接受到的select语句以字符串方式进行hash，然后到Query Cache中直接查找是否已经缓存。也就是说，如果已经在缓存中，该select请求就会直接将数据返回，从而省略了后面所有的步骤(如SQL语句的解析，优化器优化以及向存储引擎请求数据等)，极大的提高性能。根据MySQL用户手册，使用查询缓冲最多可以达到238%的效率。 当然，Query Cache也有一个致命的缺陷，那就是当某个表的数据有任何任何变化，都会导致所有引用了该表的select语句在Query Cache中的缓存数据失效。所以，当我们的数据变化非常频繁的情况下，使用Query Cache可能会得不偿失 Query Cache的使用需要多个参数配合，其中最为关键的是query_cache_size和query_cache_type，前者设置用于缓存 ResultSet的内存大小，后者设置在何场景下使用Query Cache。在以往的经验来看，如果不是用来缓存基本不变的数据的MySQL数据库，query_cache_size一般256MB是一个比较合适的大小。当然，这可以通过计算Query Cache的命中率(Qcache_hits/(Qcache_hits+Qcache_inserts)*100))来进行调整. query_cache_type可以设置为0(OFF)，1(ON)或者2(DEMOND)，分别表示完全不使用query cache，除显式要求不使用query cache(使用sql_no_cache)之外的所有的select都使用query cache，只有显示要求才使用query cache(使用sql_cache)。 如果Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲. 如果Qcache_hits的值也非常大，则表明查询缓冲使用非常频繁，此时需要增加缓冲大小； 根据命中率(Qcache_hits/(Qcache_hits+Qcache_inserts)*100))进行调整，一般不建议太大，256MB可能已经差不多了，大型的配置型静态数据可适当调大. 可以通过命令：show status like &#39;Qcache_%&#39;;查看目前系统Query catch使用大小 | Qcache_hits | 1892463 | | Qcache_inserts | 35627 命中率98.17%=1892463/(1892463 +35627 )*100 1.2 系统变量 have_query_cache 设置查询缓存是否可用1mysql&gt; show variables like 'have_query_cache'; +——————+——-+| Variable_name | Value |+——————+——-+| have_query_cache | YES |+——————+——-+上面的显示，表示设置查询缓存是可用的。 1.3 系统变量 query_cache_size表示查询缓存大小，也就是分配内存大小给查询缓存，如果你分配大小为0，那么 第一步 和 第二步 起不到作用，还是没有任何效果。 1mysql&gt; select @@global.query_cache_size; +—————————+| @@global.query_cache_size |+—————————+| 16777216 |+—————————+上面是 mysql6.0设置默认的，之前的版本好像默认是0的，那么就要自己设置下。设置set @@global.query_cache_size=1000000; 这里是设置1M左右，900多K。再次查看下 select @@global.query_cache_size;+—————————+| @@global.query_cache_size |+—————————+| 999424 |+—————————+显示我们设置新的大小，表示设置成功。 1.4 query_cache_limit 控制缓存查询结果的最大值例如： 如果查询结果很大， 也缓存？？？？这个明显是不可能的。MySql 可以设置一个最大的缓存值，当你查询缓存数结果数据超过这个值就不会进行缓存。缺省为1M，也就是超过了1M查询结果就不会缓存。 1mysql&gt; select @@global.query_cache_limit; +—————————-+| @@global.query_cache_limit |+—————————-+| 1048576 |+—————————-+这个是默认的数值，如果需要修改，就像设置缓存大小一样设置，使用set重新指定大小。好了，通过4个步骤就可以 打开了查询缓存，具体值的大小和查询的方式 这个因不同的情况来指定了。 1.5 缓存合理性，优化1.5.1 query_cache_size 优化通过调节以下几个参数可以知道query_cache_size设置得是否合理 Qcache_insertsQcache_hitsQcache_lowmem_prunesQcache_free_blocks 如果Qcache_lowmem_prunes 的值非常大，则表明经常出现缓存不够的情况，如果Qcache_hits的值非常大，则表明查询缓存冲使用非常频繁，如果该值较小反而影响效率，那么可以考虑不用查询缓存； Qcache_free_blocks 值非常大，则表明缓存区中的碎片很多，可能需要需找合适的机会进行整理 Qcache_hits 表示多少次命中，通过这个参数我们可以查看到 Query Cache的基本效果； Qcache_inserts 表示多少次未命中然后插入，通过Qcache_hits 和 Qcache_inserts 两个参数可以算出Query Cache的命中率 Query Cache命中率 = Qcache_hits / (Qcache_hits + Qcache_inserts) Qcache_lowmem_prunes 表示多少条Query 因为内存不足而被清除出Query Cache，通过Qcache_lowmem_prunes和Qcache_free_memory相互结合，能够更清楚地了解到系统中Query Cache的内存大小是否真的足够，是否频繁出现因为内存不足而有Query被换出的情况 1.5.2 query_cache_min_res_unit 优化开启了数据库缓存后用 1show status like 'qcache%'; ## 查看缓存`query_cache_min_res_unit` 默认是4k 发现 Qcache_free_blocks 数目大 说明可能有碎片。Qcache_free_blocks: 表示查询缓存中目前还有多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内存碎片过多了，可能在一定的时间进行整理。 减少碎片： 合适的query_cache_min_res_unit可以减少碎片，这个参数最合适的大小和应用程序查询结果的平均大小直接相关. 可以通过内存实际消耗（ query_cache_size - Qcache_free_memory ）除以 Qcache_queries_in_cache 计算平均缓存大小。 其中 Qcache_free_memory 和 Qcache_queries_in_cache 在上面已将有了 分别是119423544和 13205query_cache_size 是自己设置的可以通过 1SHOW VARIABLES LIKE '%query_cache%'; # 查看 （ query_cache_size - Qcache_free_memory ）除以 Qcache_queries_in_cache就是 （134217728 - 119423544）/13205=1120.34所以在设置的的时候 query_cache_min_res_unit可以设置成2k修改my.cnf,配置如下： query_cache_min_res_unit= 2k 然后在查看他的Qcache_free_blocks运行一段时间有没有减少 2 缓存优化之 Innodb当使用innoDB存储引擎的时候，innodb_buffer_pool_size参数可能是影响性能的最为关键的一个参数了，用来设置用于缓存innoDB索引及数据块的内存区域大小，更像是Oracle数据库的db_cache_size。简单来说，当操作一个InnoDB表的时候，返回的所有数据或者查询过程中用到的任何一个索引块，都会在这个内存区域中区查询一遍。 和key_buffer_size 对于MyISAM引擎一样，innodb_buffer_pool_size设置了InnoDB存储引擎需求最大的一块内存区域的大小，直接关系到InnoDB存储引擎的性能，所有如果有足够的内存，尽可将该参数设置到足够大，将尽可能多的InnoDB的索引及数据都放入到该缓存区域中，直至全部。 可以通过(innodb_buffer_pool_read_requests - innodb_buffer_pool_reads) / innodb_buffer_pool_read_requests * 100%计算缓存命中率，并根据命中率来调整innodb_buffer_pool_size参数大小进行优化。 3 缓存优化之table_cache另外，table_cache是一个非常重要的MySQL性能参数，主要用于设置table高速缓存的**数量**，由于每个用户端链接都会至少访问一个表，因此该参数是与max_connections有关。当某一连接访问一个表时，MySQL会检查当前已缓存表的数量。如果该表已经在缓存中打开，则会直接访问缓存中的表以加快查询速度； 如果该表未被缓存，则会将当前的表添加进缓存并进行查询。在执行缓存操作前，table_cache参数用于限制缓存表的最大数目；如果当前已经缓存的表未达到table_cache数目，则会将新表添加进去，若已经达到此值，MySQL将根据缓存表的最后查询时间，查询率，等规则释放之前的缓存。 1show global status like 'open%_tables'; 来查看这两个参数的值。其中Open_tables是当前正在打开表的数量，Opened_tables是所有已经打开表的数量。 参考地址：https://blog.csdn.net/qq_40460909/article/details/81236624https://blog.csdn.net/z13615480737/article/details/82621116https://www.cnblogs.com/onlysun/p/4513029.html]]></content>
      <categories>
        <category>Linux</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下run包制作]]></title>
    <url>%2F2019%2F03%2F28%2FLinux%E4%B8%8Brun%E5%8C%85%E5%88%B6%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1 Run软件包介绍 run 程序安装包实质上是一个安装脚本加要安装的程序，如下所示： |—————–——|| || 安装脚本 || ||—————–——|| || 程序 || ||—————–——| 这样整个 run 安装包结构就一目了然了，实际上因为实际需要结构多少有点变动但这个无关紧要，只需要明白原理就行了。安装文件的优点： （1）只有一个包文件； （2）可以直接运行在 Linux上，因为它是 sh（它的前半部分是sh）； （3）在 sh 中可以包含需要用户接收的协议信息，而且提示用户接收，如果用户不接收，安装退出。 2 制作run安装包2.1 压缩环境包1tar -zcvf app.tar.gz app/ 2.2 制作安装脚本install.sh 12345678910111213#! /bin/bashlines=13 #这个值是指这个脚本的行数加 1，这个脚本共有 12 行tail -n +$lines $0 &gt; /tmp/app.tar.gz # $0 表示脚本本身，这个命令用来把从 $lines 开始的内容写入一个 /tmp 目录的 scan.tar.gz 文件里。tar zxvf /tmp/app.tar.gzcp -pR app /opt/rm -rf app/echo "********************************************************************************************************************"echo "*****Install Success **********************************************************************************************"echo "*****Installation path: /opt/app **********************************************************************************"echo "*****Explain: Modify the start.sh configuration parameter, and then execute the start.sh script startup program.****"echo "********************************************************************************************************************"exit 0 3 生成run文件1cat install.sh app.tar.gz &gt; app.run 这样就得到了 app.run 文件，它的结构如下：|—————–———| 第1行| || install.sh || | 第12行|—————–———|| app.tar.gz | 第13行| ||—————–———| 结尾 可通过vi/vim app.run 查看脚本内容 在运行 apprun 时，运行到第 12 行的 exit 0 ，脚本就会自动退出了，不会去运行第 13 行以下的二进制数据（即 app.tar.gz 文件），这样 shell 就不会因为识别不了二进制数据而出错了。这里我们巧妙地使用了 tail 命令，把第 12 行以下的数据重新生成了一个app.tar.gz文件，然后再执行安装。运行超级简单，使用 sh app.run 或赋予可执行权限然后直接执行 ./app.run 就可以安装了。 run 安装包制作较小的程序包是很好的选择，但是它也有缺点，做逻辑比较复杂的安装包，写的安装脚本将会很麻烦，因此此时还是用其他的安装包更好。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>run</tag>
        <tag>打包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Postgresql_Postgis解压版安装]]></title>
    <url>%2F2019%2F03%2F28%2FPostgresql-Postgis%E8%A7%A3%E5%8E%8B%E7%89%88%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1．软件下载 postgresql-9.6.1-1-windows-x64-binaries.zip https://www.postgresql.org/download/windows/ postgis-bundle-pg96-2.3.1x64.zip http://download.osgeo.org/postgis/windows/pg96/ 2. 将postgresql.zip解压解压postgresql-9.6.1-1-windows-x64-binaries.zip到你想要的安装目录（D:\GreenSoftware\PostgreSQL961），主要最好不要有中文或者空格， 3. 创建数据存放目录D:\GreenSoftware\PostgreSQL961\data 4. 初始化数据库D:\GreenSoftware\PostgreSQL961\bin\ initdb.exe -D D:\GreenSoftware\PostgreSQL961\data -E UTF8 --locale=Chinese 5. 启动数据库，有两种方式5.1 第一种方式：注册为windows服务方式5.1.1 注册服务D:\GreenSoftware\PostgreSQL961\bin\ pg_ctl.exe register -D D:\GreenSoftware\PostgreSQL961\data -Npgsql备注：-N表示windows服务名称为pgsql； 5.1.2 启动服务1net start pgsql 如果你的安装没有错误，现在就应该可以起来了。 5.1.3 关闭服务1net stop pgsql 5.1.4 卸载服务D:\GreenSoftware\PostgreSQL961\bin\ pg_ctl.exe unregister -D D:\GreenSoftware\PostgreSQL961\data –Npgsql 5.2 第二种方式：直接启动方式5.2.1 启动D:\GreenSoftware\PostgreSQL961\bin\ pg_ctl.exe start -w -D D:\GreenSoftware\PostgreSQL961\data 5.2.2 关闭D:\GreenSoftware\PostgreSQL961\bin\ pg_ctl.exe stop -W -D D:\GreenSoftware\PostgreSQL961\data 6 创建数据库D:\GreenSoftware\PostgreSQL961\bin\ createdb.exe -E UTF8 geodb D:\GreenSoftware\PostgreSQL961\bin\ dropdb.exe geodb 7 创建用户D:\GreenSoftware\PostgreSQL961\bin\ createuser.exe -s -r postgres 会有是否创建superuser的选项，创建一个名为postgres的超级用户； 使用方法: createuser [选项]... [用户名] 选项: -c, –connection-limit=N 角色的连接限制(缺省: 没有限制) -d, –createdb 此角色可以创建新数据库 -D, –no-createdb 此角色不可以创建新数据库 -e, –echo 显示发送到服务端的命令 -E, –encrypted 口令加密存储 -i, –inherit 角色能够继承它所属角色的权限 （这是缺省情况) -I, –no-inherit 角色不继承权限 -l, –login 角色能够登录(这是缺省情况) -L, –no-login 角色不能登录 -N, –unencrypted 口令不加密存储 -P, –pwprompt 给新角色指定口令 -r, –createrole 这个角色可以创建新的角色 -R, –no-createrole 这个角色没有创建其它角色的权限 -s, –superuser 角色将是超级用户 -S, –no-superuser 角色不能是超级用户 –help 显示此帮助信息, 然后退出 –version 输出版本信息, 然后退出 联接选项: -h, –host=HOSTNAM 数据库服务器所在机器的主机名或套接字目录 -p, –port=PORT 数据库服务器端口号 -U, –username=USERNAME 联接用户 (不是要创建的用户名) -w, -no-password 永远不提示输入口令 -W, –password 强制提示输入口令 如果 -d, -D, -r, -R, -s, -S 和 ROLENAME 一个都没有指定,将使用交互式提示 你. 臭虫报告至 &#x70;&#x67;&#x73;&#x71;&#x6c;&#x2d;&#98;&#x75;&#x67;&#115;&#64;&#112;&#111;&#x73;&#x74;&#x67;&#114;&#x65;&#x73;&#113;&#108;&#46;&#111;&#114;&#103;. 例子1:&gt;createuser -P -d -U postgres dan 解释:-P(大写)说的是为新用户指定口令;-d说的该角色是否可以创建数据库;-U(大写)当前的操作是哪个用户发出的;最后的dan是新用户的名字。 补充： 查看系统中的所用用户：select * from pg_user; 删除一个用户：drop user dan;其中dan为用户名 D:\GreenSoftware\PostgreSQL961\bin\dropuser.exe postgres 7.1 修改用户密码7.1.1第一种方式：应用psql命令D:\GreenSoftware\PostgreSQL961\bin\ psql.exe postgres 123postgres=# alter user postgres with password 'xxx';postgres-# \q 7.1.2第二种方式：为使用pgAdmin修改用pgAdmin连接到服务器，可以直接修改密码； 8 将postgis-bundle-pg96-2.3.1x64.zip解压解压postgis-bundle-pg96-2.3.1x64.zip到没有中文或者空格的目录。 9 修改makepostgisdb_using_extensions.bat文件 10 将空间数据导入PostGIS中 11 显示PostGIS中空间数据 12处理外网访问1. 修改D:\GreenSoftware\PostgreSQL961\data\pg_hba.conf文件加入如下的文字： host all all 192.168.1.0/24 md5 2.修改D:\GreenSoftware\PostgreSQL961\data\postgresql.conf文件加入如下的文字： 将 #listen_addresses = &#39;127.0.0.1&#39; 改为： listen_addresses = &#39;*&#39;]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgresql部署]]></title>
    <url>%2F2019%2F03%2F01%2Fpostgresql%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1 Postgres部署手册1.1 安装 1.1.1安装软件库12yum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpmyum install epel-release #安装postgis时提示部分依赖无法安装 1.1.2 安装软件1yum install postgresql10 &amp;&amp; yum install postgresql10-server 1.1.3 初始化及开机启动123/usr/pgsql-10/bin/postgresql-10-setup initdbsystemctl enable postgresql-10systemctl start postgresql-10 1.1.4 设置postgres密码12Sudo –u postgres psqlpostgres=# ALTER USER postgres WITH PASSWORD 'zhjx123' 1.1.5 开启远程访问12345678vim /var/lib/pgsql/10/data/postgresql.conf # 修改#listen_addresses = 'localhost' 为 listen_addresses='*' 当然，此处‘*’也可以改为任何你想开放的服务器IPvim /var/lib/pgsql/10/data/pg_hba.conf 修改如下内容，信任指定服务器连接# IPv4 local connections:host all all 192.168.0.0/23（需要连接的服务器IP） md5 1.2 安装postgis插件（补充）1yum install postgis25_10 2 greenplum部署手册2.1修改Linux内核参数123456789101112131415161718# vi /etc/sysctl.confnet.ipv4.ip_forward = 0net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 1kernel.core_uses_pid = 1net.ipv4.tcp_syncookies = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.sem = 250 64000 100 512kernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_max_syn_backlog = 4096net.core.netdev_max_backlog = 10000vm.overcommit_memory = 2net.ipv4.conf.all.arp_filter = 1 2.2 修改Linux最大限制1234567# vi /etc/security/limits.conf#greenplum configs* soft nofile 65536* hard nofile 65536* soft nproc 131072 * hard nproc 131072 2.3 关闭selinux12345# vim /etc/selinux/conf修改如下：SELINUX=disabled# setenforce 0 # 临时关闭，重启恢复。 2.4 greenplum安装2.4.1 创建数据库用户123# groupadd -g 530 gpadmin# useradd -g 530 -u530 -m -d /home/gpadmin -s /bin/bash gpadmin# passwd gpadmin 2.4.2 修改hosts设置集群解析 12# vim /etc/hosts192.168.0.174 mdw sdw 2.4.3 下载安装包官网https://network.pivotal.io/products/pivotal-gpdb#/releases/1683 2.4.3 赋权及安装1234# unzip greenplum-db-5.9.0-rhel7-x86_64.zip# chmod +x greenplum-db-5.9.0-rhel7-x86_64.bin# ./ greenplum-db-5.9.0-rhel7-x86_64.bin一路yes,安装完成 默认目录/usr/loca/greenplum-db 2.4.4 设置gpadmin用户环境12345678910# cd /home/gpadmin# vi .bashrc# vi .bash_profile.bashrc和.bash_profile最后都添加下面两行source /usr/local/greenplum-db/greenplum_path.shexport MASTER_DATA_DIRECTORY=/home/gpadmin/gpdata/gpmaster/gpseg-1export PGPORT=5433 # 应为本机有postsql，端口占用，所以使用5433端口#export PGDATABASE=testDB 2.4.4 准备节点服务器信息文件后面的批量安装会用到这两个文件，如果all_host和all_segment内容一样，可以只创建一个文件 123456[root@mdw ~]# cd /home/gp[root@mdw~ ]# touch all_host[root@mdw ~]# touch all_segmentall_host和all_segment内容：mdwsdw 2.4.5 建立节点服务器间的信任1gpssh-exkeys -f /opt/gpinit/all_host 2.4.6 批量安装1gpseginstall -f /home/gpadmin/all_host -u gpadmin -p gpadmin 2.4.7 检查批量安装情况1gpssh -f /usr/local/greenplum-db/all_host -e ls -l $GPHOME #检查安装情况 2.4.8 创建存储目录123456789101112131415161718192021222324252627282930313233343536373839404142[gpadmin@mdw conf]$ gpssh -f /home/gpadmin/all_host=&gt; mkdir gpdata[sdw3][ mdw][sdw2][sdw1]=&gt; cd gpdata[sdw3][ mdw][sdw2][sdw1]=&gt; mkdir gpmaster gpdatap1 gpdatap2 gpdatam1 gpdatam2[sdw3][ mdw][sdw2][sdw1]=&gt; ll[sdw3] 总用量 20[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam1[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam2[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap1[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap2[sdw3] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpmaster[ mdw] 总用量 20[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam1[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam2[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap1[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap2[ mdw] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpmaster[sdw2] 总用量 20[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam1[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam2[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap1[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap2[sdw2] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpmaster[sdw1] 总用量 20[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam1[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatam2[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap1[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpdatap2[sdw1] drwxrwxr-x 2 gpadmin gpadmin 4096 7月 18 19:46 gpmaster=&gt; exit 2.4.9 配置.bash_profile环境变量1234567[gpadmin@mdw ~]$ vim /home/gpadmin/.bash_profilesource /opt/greenplum/greenplum-db/greenplum_path.shexport MASTER_DATA_DIRECTORY=/home/gpadmin/gpdata/gpmaster/gpseg-1export PGPORT=5433# export PGDATABASE=testDB[gpadmin@mdw ~]$ source .bash_profile(让环境变量生效) 2.4.10 创建初始化配置文件1234567891011121314[gpadmin@mdw ~]$ vim /home/gpadmin/gpinitsystem_configARRAY_NAME="Greenplum"SEG_PREFIX=gpsegPORT_BASE=33000declare -a DATA_DIRECTORY=(/home/gpadmin/gpdata/gpdatap1 /home/gpadmin/gpdata/gpdatap2)MASTER_HOSTNAME=mdwMASTER_DIRECTORY=/home/gpadmin/gpdata/gpmasterMASTER_PORT=5433TRUSTED_SHELL=/usr/bin/sshMIRROR_PORT_BASE=43000REPLICATION_PORT_BASE=34000MIRROR_REPLICATION_PORT_BASE=44000declare -a MIRROR_DATA_DIRECTORY=(/home/gpadmin/gpdata/gpdatam1 /home/gpadmin/gpdata/gpdatam2)MACHINE_LIST_FILE=/home/gpadmin/seg_hosts 2.4.11 初始化数据库1[gpadmin@mdw ~]$ gpinitsystem -c /home/gpadmin/gpinitsystem_config -s mdw 2.4.12 启动/关闭/状态1Gpadmin用户执行命令gpstart/gpstop/gpstate 3 安装postgis插件1gppkg -i postgis-2.1.5+pivotal.1-gp5-rhel7-x86_64.gppkg]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vsftp示例1]]></title>
    <url>%2F2019%2F01%2F21%2Fvsftp%E7%A4%BA%E4%BE%8B1%2F</url>
    <content type="text"><![CDATA[1 部署要求在阿里云服务器上部署FTP.服务要求：FTP用户A、B可互相访问，A对B有写权限，B对A只有读权限。 2 步骤2.1 安装vsftpd1yum install -y vsftpd 2.2 修改配置文件1234567891011121314vim /etc/vsftpd/vsftpd.conf配置如下：anonymous_enable=NOchroot_local_user=YESchroot_list_enable=NOchroot_list_file=/etc/vsftpd/chroot_listlocal_root=/home/vsftpdallow_writeable_chroot=YESpasv_enable=YESpasv_min_port=8022pasv_max_port=8030pasv_promiscuous=YES 2.3 创建FTP根目录1mkdir -p /home/vsftpd 2.4 创建系统用户12useradd ftpsg -d /home/vsftpd/ftpsguseradd ftpxc -d /home/vsftpd/ftpxc -g ftpsg 警告：1、创建用户必须赋予登陆权限，不可使用useradd vsftpd -d /home/vsftpd -s /bin/false 禁用登陆；2、两个用户在一个用户组里面，后面分配权限更安全。 2.5 修改目录权限12chmod 770 /home/ftpsgchmod 750 /home/ftpxc 信息：通过设置目录权限，达到限制用户权限的目的。ftpxc用户可读写ftpsg目录，ftpsg用户可读不可写ftpxc目录，其他用户无法访问2个目录。备注：同时设置r-x权限，才可以访问目录。 3 FTP客户端访问使用FileZilla客户端访问FTP服务器，出现如下问题 状态: 连接建立，等待欢迎消息…状态: 已登录状态: 读取目录列表…状态: 服务器发回了不可路由的地址。使用服务器地址代替 编辑-设置注释：因阿里云服务器事vps环境，访问FTP服务器，服务器返回的是内网地址，客户端无法连接；使用主动模式，可以直接通过互联网地址访问FTP。 4 参考资料重要!!!讲解一个重要配置组合。vsftp 实现不同用户不同权限配置]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat优化]]></title>
    <url>%2F2018%2F12%2F21%2Ftomcat%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1 常用配置详解 1.1 目录结构 /bin：脚本文件目录。 /common/lib：存放所有web项目都可以访问的公共jar包（使用Common类加载器加载）。 /conf：存放配置文件，最重要的是server.xml。 /logs：存放日志文件。 /server/webapps：来管理Tomcat-web服务用的。仅对TOMCAT可见，对所有的WEB APP都不可见（使用Catalina类加载器加载）。 /shared/lib：仅对所有WEB APP可见，对TOMCAT不可见（使用Shared类加载器加载）。 /temp：Tomcat运行时候存放临时文件用的。 /webapps：web应用发布目录。 /work：Tomcat把各种由jsp生成的servlet文件放在这个目录下。删除后，启动时会自动创建。 1.2 配置文件 server.xml：主要的配置文件。 web.xml：缺省的web app配置，WEB-INF/web.xml会覆盖该配置。 context.xml：不清楚跟server.xml里面的context是否有关系。 server.xml配置 ==server标签== port：指定一个端口，这个端口负责监听关闭tomcat的请求。 shutdown：指定向端口发送的命令字符串。 ==service标签== name：指定service的名字。 ==Connector(表示客户端和service之间的连接)标签== port：指定服务器端要创建的端口号，并在这个端口监听来自客户端的请求。 minProcessors：服务器启动时创建的处理请求的线程数。 maxProcessors：最大可以创建的处理请求的线程数。 enableLookups：如果为true，则可以通过调用request.getRemoteHost()进行DNS查询来得到远程客户端的实际主机名，若为false则不进行DNS查询，而是返回其ip地址。 redirectPort：指定服务器正在处理http请求时收到了一个SSL传输请求后重定向的端口号。 acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。 connectionTimeout：指定超时的时间数(以毫秒为单位)。 ==Engine(表示指定service中的请求处理机，接收和处理来自Connector的请求)标签== defaultHost：指定缺省的处理请求的主机名，它至少与其中的一个host元素的name属性值是一样的。 ==Context(表示一个web应用程序，通常为WAR文件，关于WAR的具体信息见servlet规范)标签== docBase：该web应用的文档基准目录（Document Base，也称为Context Root），或者是WAR文件的路径。可以使用绝对路径，也可以使用相对于context所属的Host的appBase路径。 path：表示此web应用程序的url的前缀，这样请求的url为http://localhost:8080/path/****。 reloadable：这个属性非常重要，如果为true，则tomcat会自动检测应用程序的/WEB-INF/lib和/WEB-INF/classes目录的变化，自动装载新的应用程序，我们可以在不重起tomcat的情况下改变应用程序。 useNaming：如果希望Catalina为该web应用使能一个JNDI InitialContext对象，设为true。该InitialialContext符合J2EE平台的约定，缺省值为true。 workDir：Context提供的临时目录的路径，用于servlet的临时读/写。利用javax.servlet.context.tempdir属性，servlet可以访问该目录。如果没有指定，使用$CATALINA_HOME/work下一个合适的目录。 swallowOutput：如果该值为true，System.out和System.err的输出被重定向到web应用的logger。如果没有指定，缺省值为false debug：与这个Engine关联的Logger记录的调试信息的详细程度。数字越大，输出越详细。如果没有指定，缺省为0。 ==host(表示一个虚拟主机)标签== name：指定主机名。 appBase：应用程序基本目录，即存放应用程序的目录。 unpackWARs：如果为true，则tomcat会自动将WAR文件解压，否则不解压，直接从WAR文件中运行应用程序。 ==Logger(表示日志，调试和错误信息)标签== className：指定logger使用的类名，此类必须实现org.apache.catalina.Logger接口。 prefix：指定log文件的前缀。 suffix：指定log文件的后缀。 timestamp：如果为true，则log文件名中要加入时间，如下例:localhost_log.2001-10-04.txt。 ==Realm(表示存放用户名，密码及role的数据库)标签== className：指定Realm使用的类名，此类必须实现org.apache.catalina.Realm接口。 ==Valve(功能与Logger差不多，其prefix和suffix属性解释和Logger 中的一样)标签== className：指定Valve使用的类名，如用org.apache.catalina.valves.AccessLogValve类可以记录应用程序的访问信息。directory：指定log文件存放的位置。 pattern：有两个值，common方式记录远程主机名或ip地址，用户名，日期，第一行请求的字符串，HTTP响应代码，发送的字节数。combined方式比common方式记录的值更多。 1.3 配置虚拟目录 直接部署到webapps目录下面访问。 修改conf/server.xml文件。在&lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; xmlValidation=&quot;false&quot; xmlNamespaceAware=&quot;false&quot;&gt;&lt;/host&gt;中加入&lt;Context path=&quot;/test&quot; docBase=&quot;webdemo&quot; debug=&quot;0&quot; reloadable=&quot;true&quot; /&gt;。docBase目录默认使用appBase=”webapps”这个目录。也可以是绝对路径。配置主目录，可以将path=””。 当项目没有放在webapps目录下时，可以在conf/Catalina/localhost新建一个XXX.XML文件。里面加入&lt;Context docBase=&quot;E:webdemo&quot; debug=&quot;0&quot; reloadable=&quot;true&quot; /&gt;。 注意： 这里的path属性不需要设置，设置了也不会起作用的。 也可以使用该方法建立主目录指向另一个目录，例如：&lt;Context docBase=&quot;E:webdemo&quot; debug=&quot;0&quot; reloadable=&quot;true&quot; /&gt;命名为ROOT.xml，这样默认访问的主目录就被修改过了。 配置连接数maxThreads：Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。minSpareThreads：Tomcat初始化时创建的线程数。maxSpareThreads：一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket线程。enableLookups：是否反查域名，取值为：true或false。为了提高处理能力，应设置为falseconnectionTimeout：网络连接超时，单位：毫秒。设置为0表示永不超时，这样设置有隐患的。默认可设置为20000毫秒。 web server允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。 配置内存大小修改bin/catalina.bat中的set CATALINA_OPTS=-Xms64m -Xmx128m。Xms指最小内存，Xmx指最大内存。 安全配置1）将&lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt;SHUTDOWN修改为其他一些字符串。否则就容易被人给停止掉了。 2）对应tomcat3.1中，屏蔽目录文件自动列出修改conf/web.xml中的 12345678910111213&lt;servlet&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.catalina.servlets.DefaultServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;debug&lt;/param-name&gt; &lt;param-value&gt;0&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;listings&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt;&lt;!-- 改成false --&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; 3）访问日志设置 在server.xml中加入 123&lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log." suffix=".txt" pattern="common" resolveHosts="false"/&gt; 这样访问日志会记录到Logs中。 4）修改用户名、密码 conf/tomcat-users.xml 5）屏蔽后台管理入口 方法一：从控制用户和权限着手。废掉要管理权限的用户就可以了。 方法二：将conf/Catalina/localhost/manager.xml改名。 6）配置403,404,500错误页面 默认情况下，报出HTTP错误的时候会暴露tomcat版本号。如果不想暴露的话，就需要重新定义错误跳转页面。 123456789101112&lt;error-page&gt; &lt;error-code&gt;401&lt;/error-code&gt; &lt;location&gt;/401.jsp&lt;/location&gt;&lt;/error-page&gt;&lt;error-page&gt; &lt;error-code&gt;404&lt;/error-code&gt; &lt;location&gt;/404.jsp&lt;/location&gt;&lt;/error-page&gt;&lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/500.jsp&lt;/location&gt;&lt;/error-page&gt; 注意： 在测试的时候碰到一个奇怪的现象，平时项目里面的时候测试正常的。可是今天在tomcat目录里面新建一个测试目录测试并不能跳转到指定错误页面。暂时不知道为什么。 配置Log4j日志记录项目中抛出的异常，抛到tomcat中的异常会被tomcat记录下来，存放至logs/localhost.yyyy-MM-dd.log文件中。平时我们在项目中使用的log4j记录日志跟tomcat是没有任何关系的，是独立的一个程序，记录的文件是自定义的。我们可以在tomcat中定义一个log4j的公共日志处理方式，这样在项目中就不需要在定义log4j的配置了。1）将log4j-1.2.15.jar加入到commonlib目录。2）将log4j.properties加入到commonclasses目录。内容例如： Output pattern : date [thread] priority category - messagelog4j.rootLogger=DEUBG, stdout, logfilelog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d [%t] %-5p [%c] -%m%nlog4j.appender.logfile=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.logfile.File=${catalina.home}/logs/tomcat_app.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d [%t] %-5p [%c] -%m%n #3rd party library levellog4j.logger.org.hibernate.cache=ERROR 注意： 我们项目中使用e.printStackTrace();输出的异常会在控制台输出来，但是，不会记录到tomcat日志中。 而且，也不会记录到log4j的日志中。要想记录到log4j日志中，必须使用log4j输出来。 所以，实际上web项目中进行异常处理应该将e.printStackTrace();写写法多改成log4j的形式才对！ 但是，实际项目中很多项目多偷懒使用了e.printStackTrace();方式输出异常。当出现异常的时候在控制台上查看一下就可以了，也不考虑实际运行时候的维护。假如有人不小心关了控制台，那么，你不就看不到异常了吗？ 个人介意使用log4j的形式记入web异常！ Tomcat5乱码问题Tomcat5跟Tomcat4对参数处理是不一样的，在Tomcat4中get与post的编码是一样的，所以只要在过滤器中通过request.setCharacterEncoding()设定一次就可以解决get与set的问题。然而，在Tomcat5中，get与post的处理是分开的，对get请求使用URIEncoding进行处理，对post使用request.setCharacterEncoding()处理。Tomcat5中，在server.xml的Connector元素增加了以下配置参数：URIEncoding：用来设定通过URI传递的 2 tomcat优化配置参数2.1 内存优化优化内存，主要是在bin/catalina.bat/sh 配置文件中进行。linux上，在catalina.sh中添加： 1JAVA_OPTS="-server -Xms1G -Xmx2G -Xss256K -Djava.awt.headless=true -Dfile.encoding=utf-8 -XX:MaxPermSize=256m -XX:PermSize=128M -XX:MaxPermSize=256M" 其中： • -serve：启用jdk的server版本。• -Xms：虚拟机初始化时的最小堆内存。• -Xmx：虚拟机可使用的最大堆内存。 #-Xms与-Xmx设成一样的值，避免JVM因为频繁的GC导致性能大起大落• -XX:PermSize：设置非堆内存初始值,默认是物理内存的1/64。• -XX:MaxNewSize：新生代占整个堆内存的最大值。• -XX:MaxPermSize：Perm（俗称方法区）占整个堆内存的最大值，也称内存最大永久保留区域。 2.1.1 错误提示：java.lang.OutOfMemoryError:Java heap spaceTomcat默认可以使用的内存为128MB，在较大型的应用项目中，这点内存是不够的，有可能导致系统无法运行。常见的问题是报Tomcat内存溢出错误，Outof Memory(系统内存不足)的异常，从而导致客户端显示500错误，一般调整Tomcat的-Xms和-Xmx即可解决问题，通常将-Xms和-Xmx设置成一样，堆的最大值设置为物理可用内存的最大值的80%。 1set JAVA_OPTS=-Xms512m-Xmx512m 2.1.2 错误提示：java.lang.OutOfMemoryError: PermGenspacePermGenspace的全称是Permanent Generationspace,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGenspace中，它和存放类实例(Instance)的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGenspace进行清理，所以如果你的应用中有很CLASS的话,就很可能出现PermGen space错误，这种错误常见在web服务器对JSP进行precompile的时候。如果你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。解决方法： 1setJAVA_OPTS=-XX:PermSize=128M 2.1.3 垃圾回收机制在使用-Xms和-Xmx调整tomcat的堆大小时，还需要考虑垃圾回收机制。如果系统花费很多的时间收集垃圾，请减小堆大小。一次完全的垃圾收集应该不超过3-5 秒。如果垃圾收集成为瓶颈，那么需要指定代的大小，检查垃圾收集的详细输出，研究垃圾收集参数对性能的影响。一般说来，你应该使用物理内存的 80% 作为堆大小。当增加处理器时，记得增加内存，因为分配可以并行进行，而垃圾收集不是并行的。 2.2 连接数优化：#优化连接数，主要是在conf/server.xml配置文件中进行修改。 2.2.1 优化线程数找到Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;，增加maxThreads和acceptCount属性（使acceptCount大于等于maxThreads），如下： 1&lt;Connector port="8080" protocol="HTTP/1.1"connectionTimeout="20000" redirectPort="8443"acceptCount="500" maxThreads="400" /&gt; 其中： • maxThreads：tomcat可用于请求处理的最大线程数，默认是200• minSpareThreads：tomcat初始线程数，即最小空闲线程数• maxSpareThreads：tomcat最大空闲线程数，超过的会被关闭• acceptCount：当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理.默认100 2.2.2 使用线程池在server.xml中增加executor节点，然后配置connector的executor属性，如下： 12&lt;Executor name="tomcatThreadPool" namePrefix="req-exec-"maxThreads="1000" minSpareThreads="50"maxIdleTime="60000"/&gt;&lt;Connector port="8080" protocol="HTTP/1.1"executor="tomcatThreadPool"/&gt; 其中： •namePrefix：线程池中线程的命名前缀• maxThreads：线程池的最大线程数• minSpareThreads：线程池的最小空闲线程数• maxIdleTime：超过最小空闲线程数时，多的线程会等待这个时间长度，然后关闭• threadPriority：线程优先级注：当tomcat并发用户量大的时候，单个jvm进程确实可能打开过多的文件句柄，这时会报java.net.SocketException:Too many open files错误。可使用下面步骤检查：•ps -ef |grep tomcat查看tomcat的进程ID，记录ID号，假设进程ID为10001• lsof -p 10001|wc -l 查看当前进程id为10001的 文件操作数• 使用命令：ulimit -a 查看每个用户允许打开的最大文件数 2.3 Tomcat Connector三种运行模式（BIO, NIO, APR）2.3.1 三种模式比较：1）BIO：一个线程处理一个请求。缺点：并发量高时，线程数较多，浪费资源。Tomcat7或以下在Linux系统中默认使用这种方式。 2）NIO：利用Java的异步IO处理，可以通过少量的线程处理大量的请求。Tomcat8在Linux系统中默认使用这种方式。Tomcat7必须修改Connector配置来启动（conf/server.xml配置文件）： 1&lt;Connector port="8080"protocol="org.apache.coyote.http11.Http11NioProtocol" connectionTimeout="20000"redirectPort="8443"/&gt; 3）APR(Apache Portable Runtime)：从操作系统层面解决io阻塞问题。Linux如果安装了apr和native，Tomcat直接启动就支持apr。 2.3.2 apr模式安装apr以及tomcat-native 1yum -y install apr apr-devel 进入tomcat/bin目录，比如： 12345cd /opt/local/tomcat/bin/tar xzfv tomcat-native.tar.gzcd tomcat-native-1.1.32-src/jni/native./configure --with-apr=/usr/bin/apr-1-configmake &amp;&amp; make install 注意： #最新版本的tomcat自带tomcat-native.war.gz，不过其版本相对于yum安装的apr过高，configure的时候会报错。 解决：yum remove apr apr-devel –y,卸载yum安装的apr和apr-devel,下载最新版本的apr源码包，编译安装;或者下载低版本的tomcat-native编译安装 安装成功后还需要对tomcat设置环境变量，方法是在catalina.sh文件中增加1行： 1CATALINA_OPTS="-Djava.library.path=/usr/local/apr/lib" #apr下载地址：http://apr.apache.org/download.cgi #tomcat-native下载地址：http://tomcat.apache.org/download-native.cgi 修改8080端对应的conf/server.xml 123456789protocol="org.apache.coyote.http11.Http11AprProtocol"&lt;Connector executor="tomcatThreadPool"port="8080"protocol="org.apache.coyote.http11.Http11AprProtocol"connectionTimeout="20000"enableLookups="false"redirectPort="8443"URIEncoding="UTF-8" /&gt; PS:启动以后查看日志 显示如下表示开启 apr 模式 12Sep 19, 2016 3:46:21 PM org.apache.coyote.AbstractProtocol startINFO: Starting ProtocolHandler ["http-apr-8081"] 3 tomcat常见面试题3.1 Tomcat的缺省是多少，怎么修改Tomcat的缺省端口号是8080.修改Tomcat端口号： 找到Tomcat目录下的conf文件夹 进入conf文件夹里面找到server.xml文件 打开server.xml文件 在server.xml文件里面找到下列信息 1maxThreads=”150″ minSpareThreads=”25″ maxSpareThreads=”75″ enableLookups=”false” redirectPort=”8443″ acceptCount=”100″ connectionTimeout=”20000″ disableUploadTimeout=”true” /&gt; 5.把port=”8080″改成port=”8888″，并且保存 6.启动Tomcat，并且在IE浏览器里面的地址栏输入http://127.0.0.1:8888/ 7、tomcat默认采用的BIO模型，在几百并发下性能会有很严重的下降。tomcat自带还有NIO的模型，另外也可以调用APR的库来实现操作系统级别控制。NIO模型是内置的，调用很方便，只需要将上面配置文件中protocol修改成 org.apache.coyote.http11.Http11NioProtocol，重启即可生效。如下面的参数配置，默认的是HTTP/1.1。 123&lt;Connector port=”8080″ protocol=”org.apache.coyote.http11.Http11NioProtocol” connectionTimeout=”20000″ redirectPort=”8443″ maxThreads=”500″ minSpareThreads=”20″ acceptCount=”100″ disableUploadTimeout=”true”enableLookups=”false” URIEncoding=”UTF-8″ /&gt; 3.2 tomcat 如何优化？3.2.1优化连接配置这里以tomcat7的参数配置为例，需要修改conf/server.xml文件，修改连接数，关闭客户端dns查询。 参数解释： URIEncoding=”UTF-8″ :使得tomcat可以解析含有中文名的文件的url，真方便，不像apache里还有搞个mod_encoding，还要手工编译 maxSpareThreads : 如果空闲状态的线程数多于设置的数目，则将这些线程中止，减少这个池中的线程总数。 minSpareThreads : 最小备用线程数，tomcat启动时的初始化的线程数。 enableLookups : 这个功效和Apache中的HostnameLookups一样，设为关闭。 connectionTimeout : connectionTimeout为网络连接超时时间毫秒数。 maxThreads: maxThreads Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数，即最大并发数。 acceptCount: acceptCount是当线程数达到maxThreads后，后续请求会被放入一个等待队列，这个acceptCount是这个队列的大小，如果这个队列也满了，就直接refuse connection maxProcessors与minProcessors : 在 Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。多线程帮助程序员写出CPU最 大利用率的高效程序，使空闲时间保持最低，从而接受更多的请求。 通常Windows是1000个左右，Linux是2000个左右。 useURIValidationHack: 我们来看一下tomcat中的一段源码： 1234567891011121314151617181920212223【security】if (connector.getUseURIValidationHack()) &#123;String uri = validate(request.getRequestURI());if (uri == null) &#123;res.setStatus(400);res.setMessage(“Invalid URI”);throw new IOException(“Invalid URI”);&#125; else &#123;req.requestURI().setString(uri);// Redoing the URI decodingreq.decodedURI().duplicate(req.requestURI());req.getURLDecoder().convert(req.decodedURI(), true); 可以看到如果把useURIValidationHack设成”false”，可以减少它对一些url的不必要的检查从而减省开销。 enableLookups=”false” ： 为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。 disableUploadTimeout ：类似于Apache中的keeyalive一样 给Tomcat配置gzip压缩(HTTP压缩)功能 1compression=”on” compressionMinSize=”2048″ compressableMimeType=”text/html,text/xml,text/javascript,text/css,text/plain” HTTP 压缩可以大大提高浏览网站的速度，它的原理是，在客户端请求网页后，从服务器端将网页文件压缩，再下载到客户端，由客户端的浏览器负责解压缩并浏览。相对于普通的浏览过程HTML,CSS,Javascript , Text ，它可以节省40%左右的流量。更为重要的是，它可以对动态生成的，包括CGI、PHP , JSP , ASP , Servlet,SHTML等输出的网页也能进行压缩，压缩效率惊人。 1)compression=”on” 打开压缩功能 2)compressionMinSize=”2048″ 启用压缩的输出内容大小，这里面默认为2KB 3)noCompressionUserAgents=”gozilla, traviata” 对于以下的浏览器，不启用压缩 4)compressableMimeType=”text/html,text/xml” 压缩类型 最后不要忘了把8443端口的地方也加上同样的配置，因为如果我们走https协议的话，我们将会用到8443端口这个段的配置，对吧？｛ tomcat设置https端口时,8443和443区别: 8443端口在访问时需要加端口号,相当于http的8080,不可通过域名直接访问,需要加上端口号;https://yuming.com:8443。 443端口在访问时不需要加端口号,相当于http的80,可通过域名直接访问;例:https://yuming.com。 *问:https使用域名访问网站,而不显示端口号? 答:将端口号设置为443,即可通过域名直接访问网站 1234567｝&lt;!–enable tomcat ssl–&gt;&lt;Connector port=”8443″ protocol=”HTTP/1.1″ URIEncoding=”UTF-8″ minSpareThreads=”25″ maxSpareThreads=”75″ enableLookups=”false” disableUploadTimeout=”true” connectionTimeout=”20000″ acceptCount=”300″ maxThreads=”300″ maxProcessors=”1000″ minProcessors=”5″ useURIValidationHack=”false” compression=”on” compressionMinSize=”2048″ compressableMimeType=”text/html,text/xml,text/javascript,text/css,text/plain”SSLEnabled=”true”scheme=”https” secure=”true”clientAuth=”false” sslProtocol=”TLS”keystoreFile=”d:/tomcat2/conf/shnlap93.jks” keystorePass=”aaaaaa”/&gt; 好了，所有的Tomcat优化的地方都加上了。 3.2.2优化JDKTomcat默认可以使用的内存为128MB,Windows下,在文件{tomcat_home}/bin/catalina.bat，Unix下，在文件$CATALINA_HOME/bin/catalina.sh的前面，增加如下设置： 1JAVA_OPTS=”‘$JAVA_OPTS” -Xms[初始化内存大小] -Xmx[可以使用的最大内存] 或 设置环境变量：export JAVA_OPTS=””$JAVA_OPTS” -Xms[初始化内存大小] -Xmx[可以使用的最大内存]” 一般说来，你应该使用物理内存的 80% 作为堆大小。如果本机上有Apache服务器，可以先折算Apache需要的内存，然后修改堆大小。建议设置为70％；建议设置[[初始化内存大小]等于[可以使用的最大内存]，这样可以减少平凡分配堆而降低性能。 本例使用加入环境变量的方式： 1# vi /etc/profile 加入：export JAVA_OPTS=””$JAVA_OPTS” -Xms700 —Xmx700 1# source /etc/profile 【参数说明】 -Xms 是指设定程序启动时占用内存大小。一般来讲，大点，程序会启动的 快一点，但是也可能会导致机器暂时间变慢。 -Xmx 是指设定程序运行期间最大可占用的内存大小。如果程序运行需要占 用更多的内存，超出了这个设置值，就会抛出OutOfMemory 异常。 -Xss 是指设定每个线程的堆栈大小。这个就要依据你的程序，看一个线程 大约需要占用多少内存，可能会有多少线程同时运行等。 -XX:PermSize设置非堆内存初始值，默认是物理内存的1/64 。 -XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。 3.3 tomcat 有那几种Connector 运行模式？tomcat的运行模式有3种.修改他们的运行模式.3种模式的运行是否成功,可以看他的启动控制台,或者启动日志.或者登录他们的默认页面http://localhost:8080/查看其中的服务器状态。 1)bio 默认的模式,性能非常低下,没有经过任何优化处理和支持. 2)nio 利用java的异步io护理技术,no blocking IO技术. 想运行在该模式下，直接修改server.xml里的Connector节点,修改protocol为 &lt;Connector port=”80″ protocol=”org.apache.coyote.http11.Http11NioProtocol” connectionTimeout=”20000″ URIEncoding=”UTF-8″ useBodyEncodingForURI=”true” enableLookups=”false” redirectPort=”8443″ /&gt;启动后,就可以生效。 3)apr 安装起来最困难,但是从操作系统级别来解决异步的IO问题,大幅度的提高性能. 必须要安装apr和native，直接启动就支持apr。下面的修改纯属多余，仅供大家扩充知识,但仍然需要安装apr和native。如nio修改模式,修改protocol为org.apache.coyote.http11.Http11AprProtocol 3.4 tomcat调优 JVM参数调优-Xms&lt;size&gt; 表示JVM初始化堆的大小，-Xmx&lt;size&gt;表示JVM堆的最大值。这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。在catalina.bat中，设置JAVA_OPTS=&#39;-Xms256m -Xmx512m&#39;，表示初始化内存为256MB，可以使用的最大内存为512MB。 禁用DNS查询 当web应用程序想要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。DNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值：Tomcat4 1&lt;Connector className="org.apache.coyote.tomcat4.CoyoteConnector" port="80" minProcessors="5" maxProcessors="75" enableLookups="false" redirectPort="8443" acceptCount="100" debug="0" connectionTimeout="20000" useURIValidationHack="false" disableUploadTimeout="true" /&gt; Tomcat5 1&lt;Connector port="80" maxThreads="150" minSpareThreads="25" maxSpareThreads="75" enableLookups="false" redirectPort="8443" acceptCount="100" debug="0" connectionTimeout="20000" disableUploadTimeout="true"/&gt; 调整线程数通过应用程序的连接器（Connector）进行性能控制的的参数是创建的处理请求的线程数。Tomcat使用线程池加速响应速度来处理请求。在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接受更多的请求。Tomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。minProcessors服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。maxProcessors的值也是有上限的，应防止流量不可控制（或者恶意的服务攻击），从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。web server允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。在Tomcat5对这些参数进行了调整，请看下面属性：maxThreads Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。acceptCount 指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。connnectionTimeout 网络连接超时，单位：毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。minSpareThreads Tomcat初始化时创建的线程数。maxSpareThreads 一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket线程。最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。 Tomcat作为Web服务器，它的处理性能直接关系到用户体验，下面是几种常见的优化措施： 一、掉对web.xml的监视，把jsp提前编辑成Servlet。有富余物理内存的情况，加大tomcat使用的jvm的内存 二、服务器资源 服务器所能提供CPU、内存、硬盘的性能对处理能力有决定性影响。 (1) 对于高并发情况下会有大量的运算，那么CPU的速度会直接影响到处理速度。 (2) 内存在大量数据处理的情况下，将会有较大的内存容量需求，可以用-Xmx -Xms -XX:MaxPermSize等参数对内存不同功能块进行划分。我们之前就遇到过内存分配不足，导致虚拟机一直处于full GC，从而导致处理能力严重下降。 (3) 硬盘主要问题就是读写性能，当大量文件进行读写时，磁盘极容易成为性能瓶颈。最好的办法还是利用下面提到的缓存。 三、利用缓存和压缩 对于静态页面最好是能够缓存起来，这样就不必每次从磁盘上读。这里我们采用了Nginx作为缓存服务器，将图片、css、js文件都进行了缓存，有效的减少了后端tomcat的访问。 另外，为了能加快网络传输速度，开启gzip压缩也是必不可少的。但考虑到tomcat已经需要处理很多东西了，所以把这个压缩的工作就交给前端的Nginx来完成。 除了文本可以用gzip压缩，其实很多图片也可以用图像处理工具预先进行压缩，找到一个平衡点可以让画质损失很小而文件可以减小很多。曾经我就见过一个图片从300多kb压缩到几十kb，自己几乎看不出来区别。 四、采用集群 单个服务器性能总是有限的，最好的办法自然是实现横向扩展，那么组建tomcat集群是有效提升性能的手段。我们还是采用了Nginx来作为请求分流的服务器，后端多个tomcat共享session来协同工作。 五、 优化tomcat参数 这里以tomcat7的参数配置为例，需要修改conf/server.xml文件，主要是优化连接配置，关闭客户端dns查询。 12345678910&lt;Connector port="8080" protocol="org.apache.coyote.http11.Http11NioProtocol" connectionTimeout="20000" redirectPort="8443" maxThreads="500" minSpareThreads="20" acceptCount="100" disableUploadTimeout="true" enableLookups="false" URIEncoding="UTF-8" /&gt; 3.5 Tomcat 部署项目的三种方法目录 1、下载 Tomcat 服务器2、启动并部署 Tomcat 服务器3、Tomcat 的目录结构4、部署项目的第一种方法（项目直接放入 webapps 目录中）5、部署项目的第二种方法（修改 conf/server.xml 文件 ）6、部署项目的第三种方法（apache-tomcat-7.0.52\conf\Catalina\localhost ） 3.5.1 下载 Tomcat 服务器 ①、官网下载地址：http://tomcat.apache.org/ ②、tomcat 8.0 64位百度云下载地址：http://pan.baidu.com/s/1slbKPsx 密码：ewui ③、tomcat 8.0 32位百度云下载地址：http://pan.baidu.com/s/1o8G28rS 密码：k11n 3.5.2 启动并部署 Tomcat 服务器 ①、解压 tomcat 安装包到一个非中文目录下 ②、配置环境变量。JAVA_HOME(指向 JDK 安装的根目录) ③、双击 apache-tomcat-6.0.16\bin 目录下的 startup.bat，启动服务器(如果一闪而过，那就是没有配置 JAVA_HOME 的环境变量) ④、在浏览器中输入 http://localhost:8080 注意：Tomcat 启动不了的时候注意配置 JAVA_HOME:C:\Program Files\Java\jdk1.6.0_43这是安装 JDK的根目录 3.5.3 Tomcat 的目录结构 3.5.4 部署项目的第一种方法（项目直接放入 webapps 目录中）1、将编写并编译好的web项目(注意要是编译好的，如果是 eclipse，可以将项目打成 war 包放入)，放入到 webapps 中2、启动tomcat服务器（双击 apache-tomcat-6.0.16\bin 目录下的 startup.bat，启动服务器）3、在浏览器输入：http://localhost:8080/项目名/访问的文件名 3.5.5 部署项目的第二种方法（修改 conf/server.xml 文件 ）①、打开tomcat下conf/server.xml，在 标签之间输入项目配置信息 1&lt;Context path="/WebProject" docBase="D:/WebProject" reloadable="true" /&gt; path:浏览器访问时的路径名 docBase:web项目的WebRoot所在的路径，注意是WebRoot的路径，不是项目的路径。其实也就是编译后的项目 reloadble:设定项目有改动时，tomcat是否重新加载该项目 ②、双击 startup.bat，启动 tomcat 服务器，然后在浏览器输入访问的项目名称路径注意：如果你配置的 path=”/xx”,那么访问的时候就是这样： 3.5.6 部署项目的第三种方法（apache-tomcat-7.0.52\conf\Catalina\localhost ）①、进入到 apache-tomcat-7.0.52\conf\Catalina\localhost 目录，新建一个 项目名.xml 文件②、在 那个新建的 xml 文件中，增加下面配置语句（和上面的是一样的,但是不需要 path 配置，加上也没什么用） 1&lt;Context docBase="D:/WebProject" reloadable="true" /&gt; ③、在浏览器输入路径：localhost:8080/xml文件名/访问的文件名总结： ①、第一种方法比较普通，但是我们需要将编译好的项目重新 copy 到 webapps 目录下，多出了两步操作 ②、第二种方法直接在 server.xml 文件中配置，但是从 tomcat5.0版本开始后，server.xml 文件作为 tomcat 启动的主要配置文件，一旦 tomcat 启动后，便不会再读取这个文件，因此无法再 tomcat 服务启动后发布 web 项目 ③、第三种方法是最好的，每个项目分开配置，tomcat 将以\conf\Catalina\localhost 目录下的 xml 文件的文件名作为 web 应用的上下文路径，而不再理会 中配置的 path 路径，因此在配置的时候，可以不写 path。 通常我们使用第三种方法]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux环境SVN钉钉通知]]></title>
    <url>%2F2018%2F12%2F11%2FLinux%E7%8E%AF%E5%A2%83SVN%E9%92%89%E9%92%89%E9%80%9A%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[1 参考资料 https://www.cnblogs.com/jianxuanbing/p/6835765.html 2 步骤 2.1 修改pre-commit1234567891011#"!/bin/shREPOS="$1"TXN="$2"SVNLOOK=/usr/bin/svnlook # 同pre-commit.tmpl文件中的SVNLOOKLOGMSG=`$SVNLOOK log -t "$TXN" "$REPOS" | grep "[a-zA-Z0-9]" | wc -c`if [ "$LOGMSG" -lt 10 ];then echo "提交失败： 注释不能低于10个字符" 1&gt;&amp;2 exit 1fi 2.2 修改post-commit12345678910111213141516#!/bin/sh# 设置默认字符集，否则post信息到钉钉时中文乱码export LANG=en_US.UTF-8# svn中变量1为仓库路径，2为提交版本号REPOS="$1"REV="$2"time=$(date +%F/%T)# 下方svnlook命令获取相应的结果AUTHOR=$(/usr/bin/svnlook author -r $&#123;REV&#125; $&#123;REPOS&#125;)CHANGEDDIRS=$(/usr/bin/svnlook dirs-changed $REPOS)MESSAGE=$(/usr/bin/svnlook log -r $REV $REPOS)CONTENT=提交时间：$&#123;time&#125;\\n提交版本：$&#123;REV&#125;\\n作者：$&#123;AUTHOR&#125;\\n提交备注：$&#123;MESSAGE&#125;\\n修改目录：$CHANGEDDIRScd /home//usr/local/jdk1.8.0_152/bin/java Request 92bfa50db10658ee1c37f9ab6b7a000e14ce94a901002170762e472af3754fbf $CONTENT 备注： 脚本需要设置LANG变量为UTF8,否则中文乱码 CONTENT变量赋值种的:为中文书写方式，输出的消息存在空格的效果。 Request为java请求文件编译后的class文件，下面讲解 效果对比： 3 配置Java请求文件由于钉钉提供的接口是https协议，curl需要支持https，因此通过java代码发起Post请求，打包成可运行的class文件，然后用post-commit调用，传入信息即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.wolf.util;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.OutputStream;import java.net.URL;import javax.net.ssl.HttpsURLConnection;public class Request &#123; public static void main(String[] args) throws Exception &#123; String token = args[0]; String content = args[1]; content = "&#123;\"msgtype\": \"text\",\"text\": &#123;\"content\": \""+content+"\"&#125;&#125;"; httpsRequest("https://oapi.dingtalk.com/robot/send?access_token="+token, "POST", content); System.out.println("OK"); System.exit(0); &#125; /** * 发送https请求 */ public static String httpsRequest(String requestUrl, String requestMethod, String outputStr) throws Exception &#123; HttpsURLConnection conn = null; BufferedReader bufferedReader = null; try &#123; URL url = new URL(requestUrl); conn = (HttpsURLConnection) url.openConnection(); conn.setDoOutput(true); conn.setDoInput(true); conn.setUseCaches(false); conn.setRequestMethod(requestMethod); conn.setRequestProperty("content-type", "application/json"); if (null != outputStr) &#123; OutputStream outputStream = conn.getOutputStream(); outputStream.write(outputStr.getBytes("utf-8")); outputStream.close(); &#125; bufferedReader = new BufferedReader(new InputStreamReader(conn.getInputStream(), "utf-8")); String str = null; StringBuffer buffer = new StringBuffer(); while ((str = bufferedReader.readLine()) != null) &#123; buffer.append(str); &#125; return buffer.toString(); &#125; catch (Exception e) &#123; throw e; &#125; finally &#123; if (conn != null) &#123; conn.disconnect(); &#125; if (bufferedReader != null) &#123; try &#123; bufferedReader.close(); &#125; catch (IOException e) &#123; &#125; &#125; &#125; &#125;&#125; 备注：将上面代码使用eclipse工具导出为java文件，然后javac Request.java命令生成class文件，java Request命令执行class文件。 3 问题汇总3.1 乱码问题参考上面讲述，及解决方法 3.2 消息：后空格问题参考上面代码：使用中文写法。 4 附件Request.classRequest.java]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>钉钉</tag>
        <tag>svn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mount挂载共享文件夹报错]]></title>
    <url>%2F2018%2F12%2F05%2Fmount%E6%8C%82%E8%BD%BD%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%E5%A4%B9%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[mount挂载报错 1 失败: 关键字已过期 原因：账号密码过期，更换账号或设置密码永不过期。]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>小知识</tag>
        <tag>基础运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nohup输出重定向]]></title>
    <url>%2F2018%2F09%2F20%2Fnohup%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%2F</url>
    <content type="text"><![CDATA[Linux shell中有三种输入输出，分别为标准输入，标准输出，错误输出，分别对应0，1，2。我们可以直接通过输出重定向&gt;（或&gt;&gt;，表示追加）将某种输出重定向到其他地方，如设备，文件，比如:123ls &gt; ls.log #标准输出重定向ls 2&gt; ls.log #标准错误重定向ls &gt; /dev/null #重定向到null设备，相当于直接忽略输出 但是，有时候，我们想把标准输出以及错误输出一起重定向某个文件，这是可以通过 2&gt;&amp;1 实现，也可以通过两个同时重定向到某个文件 12ls &gt;ls.log 2&gt;&amp;1 //标准输出重定向到ls.log,而错误又重定向到标准输出，这两个位置不可换ls 2&gt;&gt;ls.log 1&gt;&gt;ls.log 转载：http://mblog.sigma.me/2011/08/15/linux-output-redirect.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux输出记录到文件]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E8%BE%93%E5%87%BA%E8%AE%B0%E5%BD%95%E5%88%B0%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1 如何把命令运行的结果保存到文件当中 这个问题太简单了，大家都知道，用&gt; 把输出转向就可以了 例子: 1234567 [lhd@hongdi ~]$ ls &gt; ls.txt [lhd@hongdi ~]$ cat ls.txt 1.gtkrc-2.0 2009 a amsn_received a.tar.gz 说明: &gt; 是把输出转向到指定的文件，如文件已存在的话也会重新写入，文件原内容不会保留 &gt;&gt;是把输出附向到文件的后面，文件原内容会保留下来 2 如何能在输出信息的同时把信息记录到文件中 我们在上面的例子中可以看到，我们使用输出转向，命令在终端上的输出转向到了文件中，但如果我希望能同时在终端上看到输出信息怎么办？ 我们可以使用这个命令: tee解释一下tee的作用:read from standard input and write to standard output and files它从标准输入读取内容并将其写到标准输出和文件中 看例子: 123456789101112 [lhd@hongdi ~]$ ls | tee ls_tee.txt 1.gtkrc-2.0 2009 a amsn_received a.tar.gz [lhd@hongdi ~]$ cat ls_tee.txt 1.gtkrc-2.0 2009 a amsn_received a.tar.gz 备注：使用tee时,如果想保留目标文件原有的内容怎么办？ 可以使用-a参数-a, --appendappend to the given FILEs, do not overwrite附加至给出的文件，而不是覆盖它 3 多个命令的输出都需要记录，可以用script script这个命令很强大，可以记录终端的所有输出到相应的文件中 看例子: 12345678910111213141516 [lhd@hongdi ~]$ script Script. started, file is typescript [lhd@hongdi ~]$ ls 1.gtkrc-2.0 c.tar kmess-2.0alpha2.tar.gz secpanel-0.5.3-1.noarch.rpm 2009 DownZipAction.php kmesslog secpanel-0.5.4-2.noarch.rpm [lhd@hongdi ~]$ exit exit Script. done, file is typescript [lhd@hongdi ~]$ cat typescript Script. started on 2009年02月08日 星期日 18时56分52秒 [lhd@hongdi ~]$ ls 1.gtkrc-2.0 c.tar kmess-2.0alpha2.tar.gz secpanel-0.5.3-1.noarch.rpm 2009 DownZipAction.php kmesslog secpanel-0.5.4-2.noarch.rpm [lhd@hongdi ~]$ exit exit Script. done on 2009年02月08日 星期日 18时57分00秒 说明: 1. 我们在启动script时没有指定文件名，它会自动记录到当前目录下一个名为 typescript的文件中。也可以用-a参数 指定文件名 例子: 12 [lhd@hongdi ~]$ script. -a example.txt Script. started, file is example.txt 此时终端的输出内容被记录到 example.txt这个文件中 2. 退出script时，用exit 感到奇怪吗？事实上script就是启动了一个shell 看一下ps auxfww的信息就知道了 12345 lhd 17738 0.1 3.2 152028 33328 ? Sl 18:30 0:03 /usr/bin/konsole lhd 17740 0.0 0.1 6372 1720 pts/1 Ss 18:30 0:00 \_ /bin/bash lhd 17900 0.0 0.0 5344 628 pts/1 S 19:01 0:00 | \_ script lhd 17901 0.0 0.0 5348 464 pts/1 S 19:01 0:00 | \_ script lhd 17902 0.5 0.1 6372 1688 pts/2 Ss 19:01 0:00 | \_ bash -i 3. 查看typescript的内容，可以看到它同时记录下了script的启动和结束时间 4 用script录制并播放session的内容 我们可以用 script把整个终端会话的所有操作和输出录制下来，然后再用scriptreplay进行播放。 如果录制时记录下来了操作时的时间数据，那么播放时和操作时的使用时间完全相同。 **这个很有用吧，比如：我们可以把安装软件时编译的过程记录下来，然后给别人进行演示** **看例子:** undefined 说明: -t 2&gt;example.time -t是把时间数据输出到标准错误(standard error)，所以我们使用 2&gt;example.time 把数据转向到 example.time这个文件当中如何播放所记录的内容? 4.1 安装scriptreplay下载 1wget linux/utils/util-linux/util-linux-2.12r.tar.bz2"&gt;ftp://ftp.kernel.org/pub/linux/utils/util-linux/util-linux-2.12r.tar.bz2 解压 1tar -jxvf util-linux-2.12r.tar.bz2 之后复制文件到系统的命令目录中即可 12 [root@hongdi 下载]# cp util-linux-2.12r/misc-utils/scriptreplay.pl /usr/bin/scriptreplay [root@hongdi 下载]# chmod 755 /usr/bin/scriptreplay 备注: fedora 10的util-linux-ng-2.14.1-3.2.fc10.i386.rpm此包中已包含 scriptreplay,已无需另行安装 4.2 播放所录制的session内容1234567 [lhd@hongdi ~]$ scriptreplay example1.time example1.txt [lhd@hongdi ~]$ ls 1.gtkrc-2.0 c.tar jeffray_lee@hotmail.com pass [lhd@hongdi ~]$ abcd bash: abcd: command not found [lhd@hongdi ~]$ exit 转载：http://www.eetop.cn/blog/html/03/6503-25123.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用系统性能诊断工具]]></title>
    <url>%2F2018%2F09%2F14%2FLinux%E5%B8%B8%E7%94%A8%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E8%AF%8A%E6%96%AD%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[简介：Linux下的系统性能诊断工具特别的多，例如：top、iotop、tiptop、slabtop、iostat、mpstat、vmstat、dstat、pidstat、netstat、iptraf、nicstat、ss、sar、free、strace、ltrace、ftrace、dtrace、blktrace、perf、stap、ktap、ebpf、lttng等等，下面一张图宏观的标注出来，更加的方便记忆！]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>运维工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins报错]]></title>
    <url>%2F2018%2F09%2F13%2Fjenkins%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[1 ERROR: Exception when publishing, exception message [Exec timed out or was interrupted after XXX ms] 我在使用Jenkins进行远程部署时,执行的命令突然中断，包如下错误：ERROR: Exception when publishing, exception message [Exec timed out or was interrupted after 12,001 ms] 解决办法： 参考资料：https://blog.csdn.net/u013066244/article/details/52788407]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker自动登陆脚本]]></title>
    <url>%2F2018%2F09%2F13%2FDocker%E8%87%AA%E5%8A%A8%E7%99%BB%E9%99%86%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[安装expect命令 1yum install expect -y 示例脚本 docker自动登陆脚本12345678#！/usr/bin/expectset timeout 2 # 超时时间2sspawn docker login 192.168.0.200:5000 # 要执行的命令expect "Username" # 匹配字符Usernamesend "user\r" # 键入用户user \r 回车expect "password" # 匹配passwordsend "pass\r" # 键入密码pass \r 回车interact]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker清除日志脚本]]></title>
    <url>%2F2018%2F09%2F11%2FDocker%E6%B8%85%E9%99%A4%E6%97%A5%E5%BF%97%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[清除日志脚本Docker12345678910111213#!/bin/shecho "==================== start clean docker containers logs ==========================" logs=$(find /var/lib/docker/containers/ -name *-json.log) for log in $logs do echo "clean logs : $log" cat /dev/null &gt; $log done echo "==================== end clean docker containers logs =========================="]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache搭建环境经验与问题-WinX86版]]></title>
    <url>%2F2018%2F09%2F11%2FApache%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83%E7%BB%8F%E9%AA%8C%E4%B8%8E%E9%97%AE%E9%A2%98-WinX86%E7%89%88%2F</url>
    <content type="text"><![CDATA[1 搭建环境： OS: server 2008 Enterprise sp1（X86） , VMware-workstation-full-9.0.2-1031769下搭建 2 软件版本(X86)：1) VC11(VSU_4\vcredist_x86.exe) http://www.microsoft.com/en-us/download/details.aspx?id=30679 2) apache 2.4.10 (httpd-2.4.10-win32-VC11) http://www.apachelounge.com/download/ apache版本并不是越新越好，不同版本apache对系统要求不一样(旧版本支持XP 、2003，新版本不支持XP、2003) 3) PHP5.6.12(php-5.6.12-win32-VC11-X86) Thread safe版本 http://windows.php.net/download/ 搭建PHP与apache使用安全线程（Thread safe）搭建PHP与iis使用非安全线程版（NO Thread safe） 4) mysql5.6.26(mysql-5.6.26-win32.zip”免安装版”) http://dev.mysql.com/downloads/mysql/ 3 apache安装在安装中，apache修改httpd.conf文件正常，未有出错，注意”\”要改成“/”,结尾新增加语句注意有空格，按教程走没有任何错误问题。 3.1 出现问题1：在CMD窗口启动安装apache（如下图启动路径内的安装执行文件），出现 1234567Installing the Apache2. 2 serviceThe Apache2. 2 service is successfully installed testing httpd.conf..... Errors reported here must be corrected before the service can be started. Errors reported here must be corrected before the service can be started结尾，并不是错误，而是提示：如果这行下边出现错误提示则解决错误后再启动！（当时以为是出错了） 服务安装成功后在系统服务里面会有apache服务，如图 此时服务是未启动状态，可以右键启动，也可以去目录找到ApacheMonitor.exe打开执行 双击ApacheMonitor.exe 点击start出现弹窗错误，the requested operation has failed无法启动apache服务，后来是通过在cmd命令窗口执行netsh winsock reset命令重启后恢复正常。方法具体内容见结尾重启后打开apache,点击start按钮正常开启，如下图，图标变绿# 4 问题解决Apache无法启动解决 the requested operation has failed错误，方法页面 APACHE启动出现the requested operation has failed，别复制其他地方的答案啊，情况不一样，NETSTAT和httpd.exe -w -n &quot;Apache&quot; -k start都不好使了，80端口好象也没问题，2.2版本选的也是对的，还有其他什么可能么，系统启动起来的时候就提示有一个服务启动错误应该就是APACHE的 补充： httpd.exe -w -n &quot;Apache&quot; -k start提示的是没有服务，但是服务开不了啊 4.1 解决问题：原因一：80端口占用例如IIS，另外就是迅雷。我的apache服务器就是被迅雷害得无法启用！ 原因二：软件冲突装了某些软件会使apache无法启动如Dr.com你打开网络连接-&gt;TcpIp属性-&gt;高级-&gt;WINS标签 把netbios的lmhosts对勾去掉，禁用tcp/ip的netbios. 然后再启动应该就可以了。 原因三：httpd.conf配置错误如果apache的配置文件httpd.conf搞错了，在windows里启动它，会提示the requested operation has failed，这是比较郁闷的事，因为查错要看个半天。其实可以用命令行模式启动apache，并带上参数，apache会提示你哪句有误，然后就可以针对性的解决，命令如下： httpd.exe -w -n &quot;Apache2&quot; -k start 还有一种情况：即使你这次启动了，下次你都有可能启动失败在运行里输入：netsh winsock reset （本人通过这个命令解决问题）一会儿cmd会提示你重启，不用理会，现在APACHE已经可以启动了。其实就是一个winsock的修复 5 附件链接: http://pan.baidu.com/s/1bo3HYIN 密码: 11ky]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>httpd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay+keepalive+sersync+rsync主从同步]]></title>
    <url>%2F2018%2F09%2F11%2FLiferay-keepalive-sersync-rsync%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[1 环境 服务器 系统 软件 JAVA MYSQL IP Liferay-a Centos7 Liferay+keepalive+sersync 1.7.0_80 5.5.42 172.20.20.59 Liferay-b Centos7 Liferay+keepalive+rsync 1.7.0_80 5.5.42 172.20.20.60 虚拟IP: 172.20.20.58 2 安装liferay安装liferay官方版本6.2-ce4,方法自行度娘。恢复数据参考《liferay备份还原文档》 3 安装keepalive1Yum install keepalive 3.1 编辑配置文件1Yum install keepalive Liferay-a: Liferay-b: 启动keepalive，建立虚拟IP，主服务器当机，从服务器获得Ip. 4 mysql主从同步下载mysql-5.5.42-linux2.6-x86_64.tar.gz 解压到/usr/local,重命名为mysql 4.1 编辑配置文件1Vi /etc/my.cnf Liferay-a: Liferay-b 4.2 建立mysql主从同步 查看liferay-a(主服务器)，查看mysql（主）信息，并建立同步帐号： 1mysql&gt; GRANT ALL PRIVILEGES ON bitnami_liferay.* TO 'tongbu'@'%' IDENTIFIED BY 'De123456' WITH GRANT OPTION; 进入liferay-b（从服务器），在mysql中输入命令，建立连接 1mysql&gt; CHANGE MASTER TO MASTER_HOST='172.20.20.59', MASTER_USER='tongbu', MASTER_PASSWORD='De123456', MASTER_LOG_FILE='mysql-bin.000022',MASTER_LOG_POS=151424110; 输入命令，查看备服务器信息 1mysql&gt; SHOW SLAVE STATUS\G 通过mysql命令恢复bitnami_liferay数据库备份到lifreay-a(主服务器)，自动实时同步数据到从服务器。 5 目录同步5.1 安装rsync(liferay-b)只需在liferay-b服务器上安装rsync。 Liferay-b作为rsync服务器，lifray-a作为客户端，实时同步目录数据到liferay-b 1Yum install rsync 5.1.1 编辑配置文件1vi /etc/rsyncd.conf liferay-b: 5.1.2 新建rsyncd.pass1密码文件在/etc/目录文件格式 帐号:密码 设置文件权限为600（必须） 5.2 安装sersync（liferay-a）Liferay-a安装实时同步工具sersync2.5.4_64bit_binary_stable_final.tar.gz 下载sersync2.5.4_64bit_binary_stable_final.tar.gz 解压到/usr/local 5.2.1 编辑confxml.xml1vi confxml.xml 5.2.2 在/etc/目录创建密码文件rsyncd.pass1文件格式只填写密码（注意和同步服务器的密码文件内的密码一样） 5.2.3 启动sersync，开启实时同步1/usr/local/sersync/sersync2 -d -r -o /usr/local/sersync/confxml.xml]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>keepalived</tag>
        <tag>rsync</tag>
        <tag>liferay</tag>
        <tag>同步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openkm部署]]></title>
    <url>%2F2018%2F09%2F10%2FOpenkm%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1 环境： 系统：Centos7.1 软件：openkm-6.3.1-community-linux-x64-installer.run 辅助插件：Apache_OpenOffice_4.1.2_Linux_x86-64_install-rpm_zh-CN.tar.gz 数据库：mariadb-5.5.47-1.el7_2.x86_64 2 安装步骤2.1 数据库安装1[root@bogon openkm]# yum install mysql -y //yum安装数据库 //登录数据库 1MariaDB [(none)]&gt; CREATE DATABASE okmdb DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_bin; //创建okmdb数据库,支持utf8 12345MariaDB [(none)]&gt; grant all on okmdb.* to openkm@localhost identified by 'openkm';//创建本地用户并赋予数据库okmdb所有权限MariaDB [(none)]&gt; flush privileges; ///刷新权限 2.2 openKM安装12345[root@bogon openkm]# chmod 755 openkm-6.3.1-community-linux-x64-installer.run//赋予安装包权限[root@bogon openkm]# ./openkm-6.3.1-community-linux-x64-installer.run //安装软件 2.2.1 配置数据库迁移（数据库由HSQL迁移到mysql）1[root@bogon openkm]# vi /opt/openkm-6.3.1-community/tomcat/OpenKM.cfg //修改HSQLDialect（代表HSQL数据库）为MySQL5Dialect（代表mysql数据库）, none改为create(初始化数据库)。 1[root@bogon openkm]# vi /opt/openkm-6.3.1-community/tomcat/conf/server.xml //修改配置文件 //修改mysql相关 2.2.2 修改web浏览端口号1[root@bogon openkm]# vi /opt/openkm-6.3.1-community/tomcat/conf/server.xml 3 办公插件安装3.1 Openoffice安装软件下载地址：http://www.openoffice.org/download/index.html 123456789101112[root@bogon openkm]# wget http://freefr.dl.sourceforge.net/project/openofficeorg.mirror/4.1.2/binaries/zh-CN/Apache_OpenOffice_4.1.2_Linux_x86-64_install-rpm_zh-CN.tar.gz//下载安装包[root@bogon openkm]# tar -zxvf Apache_OpenOffice_4.1.2_Linux_x86-64_install-rpm_zh-CN.tar.gz//解压缩安装包[root@bogon openkm]# cd zh-CN/RPMS/ // 进入解压后的目录[root@bogon RPMS]# rpm -Uvih *rpm //安装新版本，默认将会安装/升级Apache OpenOffice到/opt目录 修改配置，指向openoffice安装目录，实现office文档预览。 3.2 swftools安装1234567891011121314151617[root@bogon openkm]# yum install gcc* automake zlib-devel libjpeg-devel giflib-devel freetype-devel//安装所需的库和组件[root@bogon openkm]# wget http://www.swftools.org/swftools-0.9.2.tar.gz//下载安装包[root@bogon openkm]# tar vxzf swftools-0.9.2.tar.gz //解压安装包[root@bogon openkm]# cd swftools-0.9.2/ //进入解压目录[root@bogon swftools-0.9.2]# ./configure --prefix=/usr/local/swftools//编辑文件，并指定安装位置[root@bogon swftools-0.9.2]# make install //安装 报错： 解决办法： 修改swftools-0.9.2/swfs下的 Makefile和Makefile.in文件； install:$(mkinstalldirs) $(pkgdatadir)$(mkinstalldirs) $(pkgdatadir)/swfs$(INSTALL_DATA) ./simple_viewer.swf $(pkgdatadir)/swfs/simple_viewer.swf$(INSTALL_DATA) ./keyboard_viewer.swf $(pkgdatadir)/swfs/keyboard_viewer.swf$(INSTALL_DATA) ./PreLoaderTemplate.swf $(pkgdatadir)/swfs/PreLoaderTemplate.swf$(INSTALL_DATA) ./tessel_loader.swf $(pkgdatadir)/swfs/tessel_loader.swf$(INSTALL_DATA) ./swft_loader.swf $(pkgdatadir)/swfs/swft_loader.swfrm -f $(pkgdatadir)/swfs/default_viewer.swf -o -L $(pkgdatadir)/swfs/default_viewer.swf$(LN_S) $(pkgdatadir)/swfs/simple_viewer.swf $(pkgdatadir)/swfs/default_viewer.swfrm -f $(pkgdatadir)/swfs/default_loader.swf -o -L $(pkgdatadir)/swfs/default_loader.swf$(LN_S) $(pkgdatadir)/swfs/tessel_loader.swf $(pkgdatadir)/swfs/default_loader.swf 将两个文件中的标记红色的-o -L 去掉； 然后删除安装目录/usr/local/swftools,重新编译安装即可； 123456[root@bogon swftools-0.9.2]# vi /etc/profile//编辑文件，添加如下内容，设置swftools环境变量，使pdf2swf成为一个可执行命令export PATH=$PATH:/usr/local/swftools/bin/ 安装xpdf语言包。下载xpdf-chinese-simplified.tar.gz文件，解压到/usr/local下，编辑add-to-xpdfrc文件，如下： 1[root@bogon ~]# vi /usr/local/xpdf-chinese-simplified/add-to-xpdfrc //编辑文件 12fontDir /usr/share/fonts/win displayCIDFontTT Adobe-GB1 /usr/share/fonts/win/simhei.ttf //管理-配置-编辑路径 3.3 ImageMagick安装1[root@bogon ~]# yum install ImageMagick –y //yum安装插件 //管理-配置-添加路径 4 资料linux-安装openkm6.3.doc xpdf-chinese-simplified.tar.gz swftools-0.9.2.tar.gz OpenKM_6_zh-CN.rar 链接: 所有相关文件 密码: iryc]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>openkm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OTRS部署及优化过程]]></title>
    <url>%2F2018%2F09%2F10%2FOTRS%E9%83%A8%E7%BD%B2%E5%8F%8A%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1 环境 系统：centos 7.2 1511 Otrs：otrs-5.0.14-02.noarch.rpm Mysql：5.5.50-MariaDB Apache: Apache/2.4.6 2 安装步骤2.1 禁用SELinux在文件/etc/selinux/config中配置SELINUX=disabled 1[root@drbd2 home]# vi /etc/selinux/config 重启系统。重启后确认命令getenforce返回为Disabled 2.2 安装数据库1[root@drbd2 home] yum -y install mariadb-server 这会在你的系统上使用默认选项安装MySQL，你需要修改默认设置以适用于OTRS。使用文本编辑器来 创建一个新文件/etc/my.cnf.d/zotrs.cnf，包含如下内容： 现在执行systemctl start mariadb来重启数据库服务器并激活刚才的修改内容。然后运行命令/usr/bin/mysql_secure_installation，并按照屏幕上的指令来设置数据库的root密码、移除匿名访问及删除test数据库。 2.3 安装otrs使用yum通过命令行来安装OTRS，它还会拉入一些依赖包如Apache WEB服务器和一些Perl模块。 备注确保你已经将OTRS RPM文件复制到了当前目录。(必须在otrsrpm安装包的目录运行安装命令)1[root@drbd2 home]# yum install --nogpgcheck otrs-5.0.14-02.noarch.rpm现在使用命令systemctl restart httpd.service重启Apache以载入为OTRS修改的配置。## 2.4 安装额外的perl模块除了通过RPM包安装的Perl模块外，OTRS还需要一些其它的Perl模块，你可以手动安装。通过执行位于目录/opt/otrs下的文件bin/otrs.CheckModules.pl来检查缺失的模块。一些模块只是可选的功能才需要，比如与IMAP服务器通讯或生成PDF。具体可参考《OTRS-4.0.5系统安装手册 otrs_5.0_管理员说明书》。## 2.5 使用Web安装器在安装完OTRS软件后，你可以使用OTRS的WEB安装器来设置和配置OTRS数据库。WEB安装器是一个能通过浏览器访问的WEB页面。WEB安装器的地址是：http://localhost/otrs/installer.pl 。启动WEB安装器后，请跟随下面的步骤来设置你的系统：1、 检验OTRS办公室信息并点击‘下一步’以继续（见下图）。2、 阅读GNU Affero通用公共许可证（见下图）并页面底部的相应按钮接受许可。3、 选择你要在OTRS中使用的数据库。如果你选择MySQL或PostgreSQL，你还能在这里选择是通过WEB安装器新建一个数据库还是使用你的数据库管理员已经创建好的空数据库。4、 根据你选择的数据库的不同，以及在上一步中是用WEB安装器新建数据库还是使用已有数据库，这个窗口可用有一点点差异。在这个窗口输入数据库认证信息。5、 创建一个新的数据库用户，选择一个数据库名称，并点击‘下一步’（见下图）。OTRS会为你生成一个强密码，当然如果你愿意也可以输入你自己的密码。这个密码会写入到配置文件Kernel/Config.pm，所以无需记住这个密码。6、 如果需要会创建数据库，并填充相应数据，如图所示。点击‘下一步’进入下一个窗口。7、 提供所有必填的系统设置，并点击‘下一步’（如下图）。8、 若需要，你可以提供需要的数据来配置收发邮件，或者点击窗口底部右边的按钮跳过这一步（如下图）。暂时先跳过这一步，后期在进行配置。9、 记录管理员密码，安装完成。## 2.6 登陆地址服务人员登录地址：http://172.20.22.108/otrs/index.pl客户登陆地址：http://172.20.22.108/otrs/customer.pl# 3 备份与恢复## 3.1 备份有两种类型的数据需要备份：应用程序文件（如/opt/otrs目录下的文件）和存储在数据库中的数据。为了简化备份，在每个OTRS安装中已经包括了脚本scripts/backup.pl。运行它可以备份所有重要的数据。123456789[root@drbd2 scripts]# ./backup.pl -d /home/Backup /home//2016-12-13_13-18/Config.tar.gz ... doneBackup /home//2016-12-13_13-18/Application.tar.gz ... doneDump MySQL rdbms ... doneCompress SQL-file... done所有数据都保存在目录/home/2016-12-13_13-18/下。另外，这些数据被保存到一个.tar.gz文件。## 3.2 恢复要恢复一个备份，保存的应用程序数据必须被写回到安装目录，如/opt/otrs。还必须要恢复数据库。每个OTRS安装都自带了一个脚本文件scripts/restore.pl（见下面的脚本），它简化了恢复过程，支持MySQL和PostgreSQL。恢复要求数据库为空 12345678910111213[root@drbd2 scripts]# ./restore.pl -b /home/2016-12-13_13-18 -d /opt/otrsRestore /home/2016-12-13_13-18//Config.tar.gz ...Restore /home/2016-12-13_13-18//Application.tar.gz ...create MySQLdecompresses SQL-file ...cat SQL-file into MySQL databasecompress SQL-file... 4 优化4.1 开启工单关注系统配置》搜索配置参数Ticket::watcher》激活功能。 4.2 PDF打印中文乱码解决方案 此方法适合于Linux，Win系列可以直接跳过拷贝文件设定配置即可解决，也可安装其它字体。 安装中文字体 从windows 7/vista/2008 系统中拷贝微软雅黑字体msyh.ttf文件 将字体文件上传至/usr/share/fonts/chinese/TrueType/msyh.ttf 打开OTRS配置页面 在otrs配置 core::PDF中添加字体文件的绝对路径/usr/share/fonts/chinese/TrueType/msyh.ttf Win下可直接设定字体名称，otrs系统会自动至Fonts目录查找指定的字体用于处理中文。 进入 OTRS to Admin &gt; SysConfig &gt; Framework &gt; Core::PDF并更新字体位置，为PDF打印选择指定的新字体 4.3 FAQ显示更多属性首先，在系统配置的FAQ中选择配置项Frontend::Agent::FAQ::ViewExplorer。 然后在打开的页面中找到配置项FAQ::Frontend::AgentFAQExplorer###ShowColumns。按照下图进行配置。 最后，我们返回到FAQ浏览器页面，就可以看到FAQ文章的更多属性。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>otrs</tag>
        <tag>工单</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix在php7下安装always-populate-raw-post-data=-1问题]]></title>
    <url>%2F2018%2F09%2F10%2Fzabbix%E5%9C%A8php7%E4%B8%8B%E5%AE%89%E8%A3%85always-populate-raw-post-data-1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[LNMP 平台 php7 ，zabbix 安装可能会出现的问题always-populate-raw-post-data = -1，解决方案： 1vim /目录/zabbix/include/classes/setup/CFrontendSetup.php 找到下面代码、关于always-populate-raw-post-data;添加 12345678910111213$current = -1; public function checkPhpAlwaysPopulateRawPostData() &#123; $current = ini_get('always_populate_raw_post_data'); $current = -1; return array( 'name' =&gt; _('PHP always_populate_raw_post_data'), 'current' =&gt; ($current != -1) ? _('on') : _('off'), 'required' =&gt; _('off'), 'result' =&gt; ($current != -1) ? self::CHECK_FATAL : self::CHECK_OK, 'error' =&gt; _('PHP always_populate_raw_post_data must be set to -1.') ); &#125; 重新刷新 zabbix 安装页面即可；]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7系统安装配置kvm软件步骤]]></title>
    <url>%2F2018%2F09%2F10%2Fcentos7%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEkvm%E8%BD%AF%E4%BB%B6%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[1 kvm相关安装包及其作用 qemu-kvm 主要的KVM程序包 python-virtinst 创建虚拟机所需要的命令行工具和程序库 virt-manager GUI虚拟机管理工具 virt-top 虚拟机统计命令 virt-viewer GUI连接程序，连接到已配置好的虚拟机 libvirt C语言工具包，提供libvirt服务 libvirt-client 为虚拟客户机提供的C语言工具包 virt-install 基于libvirt服务的虚拟机创建命令 bridge-utils 创建和管理桥接设备的工具 2 安装kvm 软件大致列下步骤： 1[root@361way ~]# yum -y install qemu-kvm libvirt virt-install bridge-utils virt-manager 2.1 查看是否加载kvm模块12345678910[root@kvm ~]# lsmod|grep kvmkvm_intel 138567 0kvm 441119 1 kvm_intel#如果没有这两条，可以用"modprobe kvm"加载；#相关命令"insmod;rmmod;modinfo" 2.2 启动libvirtd1234567[root@localhost ~]# systemctl start libvirtd # 启动[root@localhost ~]# systemctl enable libvirtd # 开机启动[root@localhost ~]# systemctl list-unit-files|grep libvirtd # 检测是否开机启动libvirtd.service enabled 3 置网卡桥接3.1 修改网卡文件eno167777361234 [root@kvm ~]# cd /etc/sysconfig/network-scripts/[root@kvm network-scripts]# echo "BRIDGE=br0" &gt;&gt; ifcfg-eno16777736 # 在ifcfg-e**原网卡文件中增加"BRIDGE=br0" 3.2 新建网桥文件ifcfg-br0(网桥名称),增加内容如下1234567891011121314151617[root@kvm network-scripts]# vi ifcfg-br0*************************************************DEVICE=br0 TYPE="Bridge" #大小写敏感，所以必须是BridgeBOOTPROTO="dhcp" ONBOOT="yes"DELAY="0"STP="yes" #这一行是来启动STP,和brctl命令行出来的结果有关************************************************* 3.3 重启NetworkManager及network服务：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@kvm network-scripts]# systemctl restart NetworkManager# 当你手动修改了网卡文件后，需要重启NetworkManager服务来重新接管网络配置# 网卡配置文件和NetworkManager配置冲突时，解决方案:｛1、重启NetworkManager;2、关闭NetworkManager｝ [root@kvm network-scripts]# systemctl restart network[root@kvm network-scripts]# ip a #ip命令用来查看和管理ip信息1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eno16777736: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master br0 state UP qlen 1000 link/ether 00:0c:29:61:5c:1d brd ff:ff:ff:ff:ff:ff3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN link/ether 52:54:00:b6:45:5b brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN qlen 500 link/ether 52:54:00:b6:45:5b brd ff:ff:ff:ff:ff:ff6: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP link/ether 00:0c:29:61:5c:1d brd ff:ff:ff:ff:ff:ff inet 192.168.0.32/24 brd 192.168.0.255 scope global dynamic br0 valid_lft 11979sec preferred_lft 11979sec inet6 fe80::20c:29ff:fe61:5c1d/64 scope link valid_lft forever preferred_lft forever7: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master br0 state UNKNOWN qlen 500 link/ether fe:54:00:cb:63:b2 brd ff:ff:ff:ff:ff:ff inet6 fe80::fc54:ff:fecb:63b2/64 scope link valid_lft forever preferred_lft forever 3.4 查看网桥连接123456789[root@kvm network-scripts]# brctl show #brctl是一个网桥连接管理命令bridge name bridge id STP enabled interfaces #如果上面不设置STP=yes，这里就会显示nobr0 8000.000c29615c1d yes eno16777736 vnet0virbr0 8000.525400b6455b yes virbr0-nic 4 selinux防火墙关闭12# setenforce 0 # sed -i 's/=enforcing/=disabled/g' /etc/selinux/config 5 打开图形界面 1[root@bogon ~]# virt-manager]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7创建分区及扩展分区]]></title>
    <url>%2F2018%2F09%2F10%2FCentos7%E5%88%9B%E5%BB%BA%E5%88%86%E5%8C%BA%E5%8F%8A%E6%89%A9%E5%B1%95%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[1 添加硬盘并挂载 1.1 查询硬盘信息（如果使用的是新加硬盘，可以在最下方看到硬盘信息，例子使用本地空闲空间） 例如： 1 [root@bogon /]# fdisk -l 1[root@bogon /]# df –h 1.2 Fdisk创建分区1[root@bogon /]# fdisk /dev/sda 1.3 查看硬盘信息1[root@bogon /]# fdisk –l 1.4 Partprobe刷新分区表（注：最好每做一步遇到找不到问题都刷新一下）1[root@bogon /]# partprobe 1.5 创建物理卷（PV）使用pvcreate 命令创建物理卷，pvdisplay 查看物理卷信息： 12345[root@bogon /]# pvcreate /dev/sda2 Physical volume "/dev/sda2" successfully created [root@bogon /]# pvdisplay 1.6 将PV加入卷组（VG）使用vgdisplay 查看卷组信息，下图显示卷组名为centos，空闲大小为0： 使用 vgextend 命令把/dev/vdb1加入到centos： 1[root@bogon /]# vgextend centos /dev/sda2 1.7 创建逻辑卷（LV）使用lvcreate 命令从卷组里划分一个新的逻辑卷，这里创建了名称为newlv，大小9GB的逻辑卷分区；使用lvdisplay 查看逻辑卷信息： 12345678[root@bogon /]# lvcreate -L 9G -n newlv centosWARNING: xfs signature detected on /dev/centos/newlv at offset 0. Wipe it? [y/n]: yWiping xfs signature on /dev/centos/newlv.Logical volume "newlv" created. 1[root@bogon /]# lvdisplay 1[root@bogon /]# vgdisplay 1.8 格式化逻辑卷并挂载新逻辑卷经过格式化就可以挂载到系统里存储数据了。使用mkfs.xfs 格式化为CentOS7的xfs文件系统： 1[root@bogon /]# mkfs.xfs /dev/centos/newlv 创建drbd目录，并挂载到/drbd目录，挂载后查询容量为9个G. 设置开机自动挂载，编辑 /etc/fstab文件，加入最后一行：]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows2003域控迁移到Windows2008域控]]></title>
    <url>%2F2018%2F09%2F10%2FWindows2003%E5%9F%9F%E6%8E%A7%E8%BF%81%E7%A7%BB%E5%88%B0Windows2008%E5%9F%9F%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[1 前期背景： 现有DNS、DHCP服务器域控为Windows 2003 系统，需要将Windows 2003 域控升级为2008. 2 环境：主域控 ：Server 2003 X86 迁移域控服务器：Server 2008 R2 X64 3 操作步骤3.1 配置Windows server 2008域控兼容环境 首先将Windows 2008系统加入 Windows 2003 域控的域中。这里为测试的，暂为 shdc.com。加入以后重启2008系统。 重启后用shdc\administrator登录2008系统。在测试进行之前，我们来查看一下， 现有的FSMO角色情况。（运行CMD，输入 etdom query fsmo），显示结果所有的角色都在2003的域控shdc-1中。 在升级域控到 Windows Server 2008之前，必须进行相关的扩展，这一点，与从Windows Server 2000域升级到Windows Server 2003域一样。在这里我们必须在原 Windows Server 2003 域控制 器上运行Windows Server 2008的 ADPREP工具，该工具位于 Windows Server 2008光盘中的 Source\adprep目录下，请复制 adprep目录到Windows Server 2003域控制上的任意磁盘分区中（计算机名为:shdc-1），本案例将此文件夹复制 到SHDC-1的磁盘分区C。特别说明，敬请留意：原 Windows Server 2000域升级到 Windows Server 2003域，只需对 Forest和 Domian进行扩展，但在 Windows Server 2003域升级到 Windows Server 2008域中，还必须对 RODC进行扩展，以便 Windows Server 2008能在基于 Windows Server 2003的域中担任域控制器类型角色 下面操作在shdc-1（域控制器）上进行操作。 开始 －运行 －CMD，进入C分区的ADPREP目录输入 adprep /forestprep根据提示，选择”C “，并按下Enter键继续。 完成 Forest扩展。接下来的是RODC请输入：adprep /rodcprep（从显示结果 来看，似乎这些扩展已经更新了，但之前尝试过跳过 RODC扩展时，不能继续。） 完成 RODC的扩展之后，接下来进行的是Domain的扩展。 输入：adprep /domainprep /gpprep 结果出现如下错误。 网络搜索发现，没有提升域模式，在Active Directory用户和计算机中点击右键，提升域功能级别。 选择 Windows 2000纯模式。点提升。 再次运行adprep /domainprep /gpprep ，如下图。 OK，我们已经做好了将 Windows Server2008（计算机名 Server2008）提升为域控 制器的准备工作。 3.2 提升Windows 2008 为域控制器 现在，以shdc\administrator身份登录成员服务器 Server 2008。 开始 －运行 － dcpromo（和Windows 2003中的类似） 出现”Active Directory 域服务安装向导“，在该向导中，我们选中”`使用高级模式 安装“，点击下一步`： 在” Active Directory 域服务安装向导“中，我们选中” 现有林“中的”`向现有域添加 域控制器”，并点击“下一步` ” 点击“ 下一步 ”继续 选择域，点击“ 下一步 ”继续 选择站点，点击“ 下一步 ”继续 “ 其他域控制器选项”，以这个选项中，默认的 角色为“ DNS服务器“和” 全局编录 “ ，而” 只读域控制器“，则为不可选，点击“ 下一步 ”继续 选择”是”继续。 默认选项，从现有的域控制器上接收更新数据。点击“下一步 ”继续 点击“ 下一步 ”继续 点击“ 下一步 ”继续 设置目录服务还原模式密码。 点击“ 下一步 ”开始提升 Server 2008为域控制器。 其过程如图： 因为选中了“ 完成后重新启动 “，故 DCPROMO完成之后，系统自动启动。让我们来看 Server 2008提升为域控制器之后，ADUC的对比情况。 3.3 角色转移 接下来的工作，就是传说中的 FSMO角色的转移，首先我们要进行的是对架构主机角色的转移，在这之前，我们必须使用 regsvr32 schmmgmt.dll来注册Active Directory 架构 。以便利用 MMC工具来添加 架构 管理控制台。这个地方要注意，要以管理员身份运行CMD，否则会出错。2008直接在运行里输CMD似乎权限不够 开始 －运行 －MMC，打开微软管理控制台。这里我们要添加 Activer Directory架构 管理控制台。如下图。 右键单击“ Active Directory 架构 “根部，并且选择 “ 更改Active Directory 域控制器 “ 选取要连接的域控制器 shdc-2.shdc.com。 再次右键单击“ Active Directory 架构 “根部，并且选择 “ 操作主机 “在对话框中，点击“ 更改“，系统会提示您” 你确实要更改操作主机？“点击” 是 ”继续 系统提示架构主机角色的已成功转移到 shdc-2.shdc.com 。 接下来的工作，就是分别将RID、PDC、基础结构主机角色转移到 shdc-2.com 。请在运行中输入dsa.msc打开 Active Directory 用户和计算机。 注意：在以下主机角色转移之前，请右键 shdc.com根部，选择“更改域控制器“， 并且选中 2008域控。 请右键 shdc.com根部，选择“ 操作主机“，依次将RID、PDC、Infrastructure主机角色转移到 shdc-2.com 最后我们需要做的是将域命名（Domain Name）主机，转移到 shdc-2.com上，打开 Active Directory域和信任关系，完成域命名主机的转移。 最后需要做的是取消 shdc-1.com的 GC（全局编录）角色，打开 Active Directory 站点和服务，依次选中 Site – Default First Site Name – Servers – shdc-1 ，右键单击 NTDS Setting，选择属性，然后将 全局编录 前面的勾去掉，只何留shdc-2.com（Windows 2008）为GC即可。 最后确定一下各种主机角色的状态，在运行里输入 cmd进行命令提示行，输入 1netdom query fsmo 至此 Windows Server 2003迁移到 Windows Server 2008完毕。 4 错误4.1 错误1：输入adprep /rodcprep报错 症状如下： 12345678Adprep could not contact a replica for partition DC=DomainDnsZones,DC=Contoso,DC=com Adprep failed the operation on partition DC=DomainDnsZones,DC=Contoso,DC=com Skipping to next partition. Adprep could not contact a replica for partition DC=ForestDnsZones,DC=Contoso,DC=com Adprep encountered an LDAP error. Error code: 0x0. Server extended error code: 0x0, Server error message: (null). Adprep failed the operation on partition DC=ForestDnsZones,DC=Contoso,DC=com Skipping to next partition. Adprep completed with errors. Not all partitions are updated. 原因是： 分区或在错误消息中不再存在引用分区 为引用的分区或分区结构主机已经强行降级或离线。 解决办法： 参考资料： https://support.microsoft.com/zh-cn/kb/949257 http://bbs.51cto.com/thread-1067016-1-1.html http://www.68idc.cn/help/jiabenmake/qita/2014040986015.html 备注：微软给出的fixfsmo.vbs脚本无需更改内容（当时纠结了一天，挺着心肝尝试了下，惨确定无须修改。)Fixfsmo.vbs脚本内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101'-------fixfsmo.vbs------------------const ADS_NAME_INITTYPE_GC = 3const ADS_NAME_TYPE_1779 = 1const ADS_NAME_TYPE_CANONICAL = 2 set inArgs = WScript.Arguments if (inArgs.Count = 1) then ' Assume the command line argument is the NDNC (in DN form) to use. NdncDN = inArgs(0)Else Wscript.StdOut.Write "usage: cscript fixfsmo.vbs NdncDN"End if if (NdncDN &lt;&gt; "") then ' Convert the DN form of the NDNC into DNS dotted form. Set objTranslator = CreateObject("NameTranslate") objTranslator.Init ADS_NAME_INITTYPE_GC, "" objTranslator.Set ADS_NAME_TYPE_1779, NdncDN strDomainDNS = objTranslator.Get(ADS_NAME_TYPE_CANONICAL) strDomainDNS = Left(strDomainDNS, len(strDomainDNS)-1) Wscript.Echo "DNS name: " &amp; strDomainDNS ' Find a domain controller that hosts this NDNC and that is online. set objRootDSE = GetObject("LDAP://" &amp; strDomainDNS &amp; "/RootDSE") strDnsHostName = objRootDSE.Get("dnsHostName") strDsServiceName = objRootDSE.Get("dsServiceName") Wscript.Echo "Using DC " &amp; strDnsHostName ' Get the current infrastructure fsmo. strInfraDN = "CN=Infrastructure," &amp; NdncDN set objInfra = GetObject("LDAP://" &amp; strInfraDN) Wscript.Echo "infra fsmo is " &amp; objInfra.fsmoroleowner ' If the current fsmo holder is deleted, set the fsmo holder to this domain controller. if (InStr(objInfra.fsmoroleowner, "\0ADEL:") &gt; 0) then ' Set the fsmo holder to this domain controller. objInfra.Put "fSMORoleOwner", strDsServiceName objInfra.SetInfo ' Read the fsmo holder back. set objInfra = GetObject("LDAP://" &amp; strInfraDN) Wscript.Echo "infra fsmo changed to:" &amp; objInfra.fsmoroleowner End if End if 分别运行运行脚本： 123cscript fixfsmo.vbs DC = ForestDnsZones，DC = dealeasy，DC = localcscript fixfsmo.vbs DC = DomainDnsZones，DC = dealeasy，DC = local 问题解决 5 附件：链接: http://pan.baidu.com/s/1kVBPuuV 密码: aevv]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>工作域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openvpn部署之部署基于AD域认证访问内网]]></title>
    <url>%2F2018%2F09%2F10%2Fopenvpn%E9%83%A8%E7%BD%B2%E4%B9%8B%E9%83%A8%E7%BD%B2%E5%9F%BA%E4%BA%8EAD%E5%9F%9F%E8%AE%A4%E8%AF%81%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%2F</url>
    <content type="text"><![CDATA[1 安装环境 Centos6.5 openvpn2.3.11 2 步骤2.1 添加fedora的yum源1rpm -ivh http://mirrors.ustc.edu.cn/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm 2.2 安装openvpn1234567yum install openvpn -yyum -y install openssl openssl-devel -y yum -y install lzo lzo-devel -y yum install -y libgcrypt libgpg-error libgcrypt-devel 2.3 安装openvpn认证插件1yum install openvpn-auth-ldap -y 2.4 安装easy-rsa由于openvpn2.3之后，在openvpn里面剔除了easy-rsa文件，所以需要单独安装 123yum install easy-rsa cp -rf /usr/share/easy-rsa/2.0 /etc/opevpn/easy-rsa 2.5 生成openvpn的key及证书修改/opt/openvpn/etc/easy-rsa/2.0/vars参数 123456789101112$ vi varsexport KEY_COUNTRY="CN" 国家export KEY_PROVINCE="ZJ" 省份export KEY_CITY="NingBo" 城市export KEY_ORG="TEST-VPN" 组织exportKEY_EMAIL="81367070@qq.com" 邮件export KEY_OU="baidu" 单位 保存退出 2.5.1 初始化source vars 或者 . ./vars(两个点之间有空格) # 初始化命令./clean-all #初始化，删除原证书文件./build-ca #制作ca证书./build-dh #./build-key-server server #制作服务器证书./build-key client #制作客户端证书 2.6 编辑openvpn服务端配置文件：1$ cat /etc/openvpn/server.conf 12345678910111213141516171819202122232425port 1194proto tcpdev tunca keys/ca.crtcert keys/server.crtkey keys/server.key # This file should be kept secretdh keys/dh2048.pemserver 10.8.0.0 255.255.255.0 //客户端分配的ip地址push "route 172.20.17.0 255.255.255.0" //推送客户端的路由push "route 172.20.18.0 255.255.255.0"push "route 172.20.19.0 255.255.255.0"push "route 172.20.20.0 255.255.255.0"push "route 172.20.22.0 255.255.255.0"push "redirect-gateway" //修改客户端的网关，使其直接走vpn流量ifconfig-pool-persist ipp.txtkeepalive 10 120comp-lzopersist-keypersist-tunstatus openvpn-status.logverb 3plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so "/etc/openvpn/auth/ldap.conf"client-cert-not-requiredusername-as-common-name log /var/log/openvpn.log 2.7 修改openvpn-ldap-auth的配置文件：1vi /etc/openvpn/auth/ldap.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;LDAP&gt; # LDAP server URL #更改为AD服务器的ip URL ldap://172.20.20.10:389 # Bind DN (If your LDAP server doesn't support anonymous binds) # BindDN uid=Manager,ou=People,dc=example,dc=com #更改为域管理的dn,可以通过ldapsearch进行查询,-h的ip替换为服务器ip，-d换为管理员的dn，-b为基础的查询dn，*为所有 #ldapsearch -LLL -x -h 172.16.76.238 -D "administrator@xx.com" -W -b "dc=xx,dc=com" "*" BindDN " cn=administrator,cn=users,dc=dealeasy,dc=local" # Bind Password # Password SecretPassword #域管理员的密码 Password passwd # Network timeout (in seconds) Timeout 15 # Enable Start TLS TLSEnable no # Follow LDAP Referrals (anonymously) FollowReferrals no # TLS CA Certificate File #TLSCACertFile /usr/local/etc/ssl/ca.pem # TLS CA Certificate Directory #TLSCACertDir /etc/ssl/certs # Client Certificate and key # If TLS client authentication is required #TLSCertFile /usr/local/etc/ssl/client-cert.pem #TLSKeyFile /usr/local/etc/ssl/client-key.pem # Cipher Suite # The defaults are usually fine here # TLSCipherSuite ALL:!ADH:@STRENGTH&lt;/LDAP&gt; &lt;Authorization&gt; # Base DN #查询认证的基础dn BaseDN " ou=de,dc=dealeasy,dc=local" # User Search Filter #SearchFilter "(&amp;(uid=%u)(accountStatus=active))" #其中sAMAccountName=%u的意思是把sAMAccountName的字段取值为用户名，后面“memberof=CN=myvpn,DC=xx,DC=com”指向要认证的vpn用户组，这样任何用户使用vpn，只要加入这个组就好了 SearchFilter "( (&amp;(sAMAccountName=%u)(memberof=cn=myvpn,ou=vpn,ou=de,DC=dealeasy,DC=local" # Require Group Membership RequireGroup false # Add non-group members to a PF table (disabled) #PFTable ips_vpn_users &lt;Group&gt; #BaseDN "ou=Groups,dc=example,dc=com" #SearchFilter "(|(cn=developers)(cn=artists))" #MemberAttribute uniqueMember # Add group members to a PF table (disabled) #PFTable ips_vpn_eng BaseDN " ou=vpn,ou=de,dc=dealeasy,dc=local" SearchFilter " (cn=myvpn)" MemberAttribute "member" &lt;/Group&gt;&lt;/Authorization&gt; 2.8 拷贝/etc/openvpn/key目录下的ca.crt证书，以备客户端使用。注：客户端使用ca.crt和客户端配置文件即可正常使用openvpn了 2.8.1 配置客户端配置文件1$ vi client.ovpn 123456789101112clientdev tunproto tcp //注意协议，跟服务器保持一致remote 172.20.20.25 1194 //xx.xx.com替换为你的服务器ipresolv-retry infinitenobindpersist-keypersist-tunca ca.crtauth-user-pass //客户端使用账户密码登陆的选项，用于客户端弹出认证用户的窗口comp-lzoverb 3 2.9 开启路由转发1vi /etc/sysctl.conf 修改参数 net.ipv4.ip_forward = 1（默认为0，修改成1 表示开启路由转发，如果默认是空内容，请自行加上-腾讯云貌似就是空的） 重启sysctl生效路由转发： 1sysctl -p 2.9.1 配置防火墙及路由转发策略：1234567iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE #做NAT转换iptables -A INPUT -p TCP --dport 1194 -j ACCEPT #OpenVPN服务端口，可自定义，不可冲突service iptables saveservice iptables restart 如下为其他配置案例。此处为策略转发示例2：配置内核路由转发和iptables转发： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113 # sed -i '/net.ipv4.ip_forward/s/0/1/' /etc/sysctl.conf # sysctl -p //可以先去熟悉如何定义iptables策略 # vi /etc/sysconfig/iptables（红色部分表示重要的策略） # Generated by iptables-save v1.4.7 on Mon Nov 2 19:19:12 2015 *nat :PREROUTING ACCEPT [0:0] :POSTROUTING ACCEPT [0:0]:OUTPUT ACCEPT [0:0]#不允许访问10.0.9/8/7/6.*网段，这是因为内网网络是跟另外一个网络建立了vpn连接，所以不想用Openvpn直接访问另外一个网络 -A PREROUTING -d 10.0.9.0/24 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1-A PREROUTING -d 10.0.8.0/27 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1-A PREROUTING -d 10.0.8.128/25 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1-A PREROUTING -d 10.0.8.32/27 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1-A PREROUTING -d 10.0.8.64/27 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1 -A PREROUTING -d 10.0.7.0/25 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1 -A PREROUTING -d 10.0.7.128/26 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1-A PREROUTING -d 10.0.7.192/26 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1 -A PREROUTING -d 10.0.6.0/26 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1-A PREROUTING -d 10.0.6.64/26 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1-A PREROUTING -d 10.0.6.128/26 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1 -A PREROUTING -d 10.0.6.192/26 -p tcp -m tcp --dport 22 -j DNAT --to-destination 127.0.0.1 #伪装10.10.10.0/24的数据-A POSTROUTING -s 10.10.10.0/24 -o eth0 -j MASQUERADE #地址转换-A POSTROUTING -s 10.10.10.0/24 -d 172.16.112.0/24 -j SNAT --to-source 172.16.112.171COMMIT # Completed on Mon Nov 2 19:19:12 2015# Generated by iptables-save v1.4.7 on Mon Nov 2 19:19:12 2015*filter:INPUT ACCEPT [603:48381]:FORWARD ACCEPT [594:717393] :OUTPUT ACCEPT [1777:901584]-A INPUT -i lo -j ACCEPT-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT -A INPUT -p tcp -m tcp --dport 389 -j ACCEPT -A INPUT -p tcp -m tcp --dport 943 -j ACCEPT -A INPUT -p tcp -m tcp --dport 8080 -j ACCEPT-A INPUT -p tcp -m tcp --dport 8088 -j ACCEPT-A INPUT -p tcp -m tcp --dport 3306 -j ACCEPT-A INPUT -p tcp -m tcp --dport 3389 -j ACCEPT -A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT44. -A INPUT -m state --state ESTABLISHED -j ACCEPT-A INPUT -s 172.16.112.0/24 -p tcp -j ACCEPT -A INPUT -s 172.16.113.0/24 -p tcp -j ACCEPT-A INPUT -s 172.16.114.0/24 -p tcp -j ACCEPT-A INPUT -s 192.168.21.0/24 -p tcp -j ACCEPT -A INPUT -s 10.10.10.0/24 -d 172.16.112.0/24 -i eth0 -p tcp -m tcp --dport 1194 -j ACCEPT-A INPUT -i tun0 -j ACCEPT -A FORWARD -i tun0 -j ACCEPT COMMIT# Completed on Mon Nov 2 19:19:12 2015 # service openvpn start # chkconfig openvpn on # chkconfig iptables on # service iptables restart 2.10 开启 HTTP代理连接openvpn服务器通过此方法可以解决跨运营商连接中断及缓慢的问题，首先需要有一台三网HTTP代理服务器。公司使用的是景安的云服务器做HTTP代理。 参考资料：http://www.365mini.com/page/18.htm 1、 在景安云服务器部署代理软件CCProxy,并开启HTTP代理，端口443（可自定义）。 2、 在客户端配置文件添加如下语句。 1http-proxy 122.114.100.229 443 或者在客户端手动配置（如图） 配置完成。可以正常连接使用。 3 用到的文件下载：openvpn安装说明.docx openvpn-auth-ldap-2.0.3-1.1.x86_64.rpm openvpn-auth-ldap-2.0.3-9.fc17.i686.rpm easy-rsa-master.zip lzo-2.09.tar.gz openvpn-2.3.11.tar.gz openvpn2.3.exe]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>openvpn</tag>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7部署vpn服务器_PPTP]]></title>
    <url>%2F2018%2F09%2F10%2FCentos7%E9%83%A8%E7%BD%B2vpn%E6%9C%8D%E5%8A%A1%E5%99%A8-PPTP%2F</url>
    <content type="text"><![CDATA[1 环境 系统：centos 7.2 1511 2 详细步骤2.1 验证内核是否加载了MPPE模块：内核的MPPE模块用于支持Microsoft Point-to-Point Encryption。Windows自带的VPN客户端就是使用这种加密方式，主流的Linux Desktop也都有MPPE支持。其实到了我们这个内核版本，默认就已经加载了MPPE，只需要使用下面命令验证一下，显示MPPE ok即可： 1modprobe ppp-compress-18 &amp;&amp; echo MPPE is ok 2.2 安装所需的软件包：1yum install –y ppp pptpd iptables 2.3 修改ppp配置文件配置ppp需要编辑它的两个配置文件，一个是option（选项）文件，一个是用户账户文件。首先编辑option文件： 12345vi /etc/ppp/options.pptpdms-dns 8.8.8.8 #去掉“＃”，设置ＤＮＳms-dns 8.8.4.4 1vi /etc/ppp/chap-secrets 这个文件非常简单，其中用明文存储VPN客户的用户名、服务名称（option.pptpd文件name pptpd vpn的服务器名字）、密码和IP地址范围，每行一个账户： 123username1 pptpd passwd1 *username2 pptpd passwd2 * 其中第一第三列分别是用户名和密码；第二列应该和上面的文件/etc/ppp/options.pptpd中name后指定的服务名称一致；最后一列限制客户端IP地址，星号表示没有限制。 2.4 修改pptpd配置文件123456789vi /etc/pptpd.confoption /etc/ppp/options.pptpdlogwtmplocalip 192.168.0.1remoteip 192.168.0.207-217 其中option选项指定使用/etc/ppp/options.pptpd中的配置；logwtmp表示使用WTMP日志。 后面两行是比较重要的两行。VPN可以这样理解，Linux客户端使用一个虚拟网络设备ppp0（Windows客户端也可以理解成VPN虚拟网卡），连接到服务器的虚拟网络设备ppp0上，这样客户端就加入了服务器端ppp0所在的网络。localip就是可以分配给服务器端ppp0的IP地址，remoteip则是将要分配给客户端ppp0（或者虚拟网卡）的。 这两项都可以是多个IP，一般localip设置一个IP就行了，remoteip则视客户端数目，分配一段IP。其中remoteip的IP段需要和localip的IP段一致。 localip和remoteip所处的IP段可以随意些指定，但其范围内不要包含实际网卡eth0的IP地址。一般情况下，使用上面配置文件中的配置就好使了，你需要做的只是把192.168.0.207-217这个IP区间修改成你喜欢的192.168.0.a-b，其中1&lt;a&lt;b&lt;255。 2.5 打开内核的IP转发功能：12345vi /etc/sysctl.conf找到其中的行：net.ipv4.ip_forward = 0 修改为： 1net.ipv4.ip_forward = 1 然后执行下面命令使上述修改生效： 1sysctl –p 2.6 配置iptables防火墙放行和转发规则：最后，还需要配置防火墙。这里配置防火墙有三个目的：一是设置默认丢弃规则，保护服务器的安全；二是放行我们允许的数据包，提供服务；三是通过配置nat表的POSTROUTING链，增加NAT使得VPN客户端可以通过服务器访问互联网。总之我们的原则就是，只放行我们需要的服务，其他统统拒绝。 首先介绍跟PPTP VPN相关的几项： 允许GRE(Generic Route Encapsulation)协议，PPTP使用GRE协议封装PPP数据包，然后封装成IP报文 放行1723端口的PPTP服务 放行状态为RELATED,ESTABLISHED的入站数据包（正常提供服务的机器上防火墙应该都已经配置了这一项） 放行VPN虚拟网络设备所在的192.168.0.0/24网段与服务器网卡eth0之间的数据包转发 为从VPN网段192.168.0.0/24转往网卡eth0的出站数据包做NAT 如果你其他的防火墙规则已经配置好无需改动，只需要增加上述相关VPN相关的规则，那么执行下面几条命令即可（第三条一般不用执行，除非你原来的防火墙连这个规则都没允许，但是多执行一遍也无妨）： 1234567891011iptables -A INPUT -p gre -j ACCEPTiptables -A INPUT -p tcp -m tcp --dport 1723 -j ACCEPTiptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPTiptables -A FORWARD -s 192.168.0.0/24 -o eth0 -j ACCEPTiptables -A FORWARD -d 192.168.0.0/24 -i eth0 -j ACCEPTiptables -t nat -A POSTROUTING -s 192.168.0.0/24 -o eth0 -j MASQUERADE 3 问题总结使用centos 6.5部署vpn pptp 遇到连接错误，总是连接不上 3.1 日志错误1：12345678910[root@localhost log]#vi /var/log/messages Jun 13 14:00:25 localhost pptpd[9248]: CTRL: Client 101.69.242.170 control connection startedJun 13 14:00:25 localhost pptpd[9248]: CTRL: Starting call (launching pppd, opening GRE)Jun 13 14:00:25 localhost pppd[9249]: Warning: can't open options file /root/.ppprc: Permission deniedJun 13 14:00:25 localhost pppd[9249]: The remote system is required to authenticate itselfJun 13 14:00:25 localhost pppd[9249]: but I couldn't find any suitable secret (password) for it to use to do so.Jun 13 14:00:25 localhost pppd[9249]: (None of the available passwords would let it use an IP address.)Jun 13 14:00:25 localhost pptpd[9248]: GRE: read(fd=6,buffer=6124a0,len=8196) from PTY failed: status = -1 error = Input/output error, usually caused by unexpected termination of pppd, check option syntax and pppd logsJun 13 14:00:25 localhost pptpd[9248]: CTRL: PTY read or GRE write failed (pty,gre)=(6,7)Jun 13 14:00:25 localhost pptpd[9248]: CTRL: Client 101.69.242.170 control connection finished 相关配置文件 123456789101112131415161718192021[root@localhost ~]# more /etc/pptpd.conf |grep -v ^#option /etc/ppp/options.pptpddebug /var/log/pptpd.loglocalip 10.10.1.20remoteip 10.10.1.30-254[root@localhost ~]# more /etc/ppp/options.pptpd|grep -v ^#name pptpdrefuse-paprefuse-chaprefuse-mschaprequire-mschap-v2require-mppe-128ms-dns 221.228.255.1ms-dns 223.6.6.6proxyarplocknobsdcomp novjnovjccompnologfd 我使用的centos 6.5 64bit ，相关安装包如下： 123456789101112131415161718[root@localhost ~]# rpm -qa |grep "ppp*"libreport-plugin-rhtsupport-2.0.9-19.el6.centos.x86_64device-mapper-event-libs-1.02.79-8.el6.x86_64ppl-0.10.2-11.el6.x86_64pptpd-1.4.0-3.el6.x86_64abrt-addon-ccpp-2.0.8-21.el6.centos.x86_64device-mapper-1.02.79-8.el6.x86_64device-mapper-event-1.02.79-8.el6.x86_64tcp_wrappers-7.6-57.el6.x86_64cloog-ppl-0.15.7-1.2.el6.x86_64snappy-1.1.0-1.el6.x86_64kernel_ppp_mppe-1.0.2-3dkms.noarchpptp-1.7.2-8.1.el6.x86_64device-mapper-libs-1.02.79-8.el6.x86_64tcp_wrappers-libs-7.6-57.el6.x86_64cpp-4.4.7-4.el6.x86_64device-mapper-persistent-data-0.2.8-2.el6.x86_64ppp-2.4.5-5.el6.x86_64 现在不知道问题出在什么地方，麻烦各位帮忙看看，谢谢！！！ 已经解决，修改配置 123options.pptpd把require-mschap-v2require-mppe-128 这两行注释掉即可。如下图： 这里是关闭加密，跳过加密。此方法修改过没有用处。 3.2 错误2：1234567891011121314151617181920212223242526272829[root@webserver ~]# sysctl -pnet.ipv4.ip_forward = 1net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 0kernel.core_uses_pid = 1net.ipv4.tcp_syncookies = 1error: "net.bridge.bridge-nf-call-ip6tables" is an unknown keyerror: "net.bridge.bridge-nf-call-iptables" is an unknown keyerror: "net.bridge.bridge-nf-call-arptables" is an unknown keykernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296You have mail in /var/spool/mail/root 版本没错但是发现转发有错 加载模块然后重启解决 1234567891011121314151617181920212223[root@webserver ~]# modprobe bridge[root@webserver ~]# lsmod | grep bridgebridge 79078 0 stp 2218 1 bridgellc 5546 2 bridge,stp[root@webserver ~]# service pptpd restartShutting down pptpd: [ OK ]Starting pptpd: [ OK ]Warning: a pptpd restart does not terminate existing connections, so new connections may be assigned the same IP address and cause unexpected results. Use restart-kill to destroy existing connections during a restart. 此处是重新加载bridge模块，重新加载了，没有作用，后来实验，发现是否重新加载，都不影响vpn使用,最终就没管他，不知道是否有影响。 最终如何解决的，其实也没有做什么修改，就是重新yum remove –y ppp* pptp*后，重新yum install –y ppp* pptp* iptables* 后，多次重启，和重设置vpn帐号，又可以用了，I don’t know why. 最终解决方案： 后来发现一个问题，当客户端开启防火墙的时候就能连接，当关闭防火墙的时候就无法连接报错。I don’t know why.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>vpn</tag>
        <tag>pptp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双机热备Heartbeat+drbd]]></title>
    <url>%2F2018%2F09%2F07%2F%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87Heartbeat-drbd%2F</url>
    <content type="text"><![CDATA[1 环境 系统：Centos7.2 1151 软件：drbd84-utils-8.9.5-1.el7.elrepo.x86_64 主服务器：172.20.20.66 备服务器：172.20.20.35 虚拟IP（virtual_ip）：172.20.20.237 修改主服务器名称：masterNode 修改备服务器名称： backupNode 主备服务器添加下方地址到/etc/hosts文件： 172.20.20.66 masterNode 172.20.20.35 backupNode 2 简单说明主备服务器分别创建一个分区，挂载到新创建的目录drbd中，通过drbd目录完成数据同步。 只有主服务器挂载目录，备服务器不用挂载。 备服务器要挂载目录，需要先把主服务器设置成secondary（备机），备服务器设置成primary（主机），然后挂载到目录。 3 添加硬盘并挂载3.1 查询硬盘信息（如果使用的是新加硬盘，可以在最下方看到硬盘信息，例子使用本地空闲空间） 例如： 1[root@bogon /]# fdisk -l 1[root@bogon /]# df –h 3.2 Fdisk创建分区1[root@bogon /]# fdisk /dev/sda 注：下图设置分区格式可以省略，后面会进行格式化。 3.3 查看硬盘信息1[root@bogon /]# fdisk –l 3.4 Partprobe刷新分区表（注：最好每做一步遇到找不到问题都刷新一下）1[root@bogon /]# partprobe 4 安装配置drbd4.1 YUM安装drbd12345[root@localhost mapper]# rpm --import http://elrepo.org/RPM-GPG-KEY-elrepo.org[root@localhost mapper]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm[root@localhost mapper]# yum -y install drbd84-utils kmod-drbd84 4.1.1 问题1二次安装存在YUM安装失败，需要源码安装，源码安装包在http://oss.linbit.com/drbd/ 下载。 Rpm包在https://pkgs.org/centos-7/elrepo-x86_64/kmod-drbd84-8.4.6-1.el7.elrepo.x86_64.rpm.html https://pkgs.org/centos-7/elrepo-x86_64/drbd84-utils-8.9.5-1.el7.elrepo.x86_64.rpm/download/ 下载。 1[root@drbd2 home]# rpm -Uvh http://elrepo.org/linux/elrepo/el7/x86_64/RPMS/drbd84-utils-8.9.5-1.el7.elrepo.x86_64.rpm [root@htest2 home]# rpm -Uvh http://elrepo.org/linux/elrepo/el7/x86_64/RPMS/kmod-drbd84-8.4.6-1.el7.elrepo.x86_64.rpm 4.2 配置global_common.conf编辑全局配置： 1vi /etc/drbd.d/global_common.conf 确保文件中包含有下内容： 123456789101112131415global &#123; usage-count yes; &#125; common &#123; net &#123; protocol C; # 使用协议C.表示收到远程主机的写入确认后,则认为写入完成. &#125; &#125; 当然，还可以有其它配置，这是最基本的。 4.3 配置r0资源：创建r0资源： 注：r0可以随便命名。 1vi /etc/drbd.d/r0.res 写入文件内容： 123456789101112131415161718192021222324252627resource r0&#123; on masterNode&#123; device /dev/drbd1; #逻辑设备的路径 disk /dev/sda3; #物理设备 address 192.168.58.128:7788; meta-disk internal; &#125; on backupNode&#123; device /dev/drbd1; disk /dev/sda3; address 192.168.58.129:7788; meta-disk internal; &#125; &#125; 需要把上面用到的防火墙7788端口打开，这个端口是自定义的，如果嫌麻烦可以直接关掉防火墙。 注：masterNode/ backupNode是主机名字，根据实际情况书写。 说明： device 是自定义的物理设备的逻辑路径 disk 是磁盘设备，或者是逻辑分区 address 是机器监听地址和端口 meta-disk 这个还没弄明白，看到的资料都是设为：internal（局域网） 4.4 建立resource123modprobe drbd //载入 drbd 模块 lsmod | grep drbd //确认 drbd 模块是否载入 12345dd if=/dev/zero of=/dev/sda2 bs=1M count=100 //把一些资料塞到 sda3 內 (否则 create-md 时会报错) drbdadm create-md r0 //建立 drbd resource drbdadm up r0 // #启用资源 4.4.1 我遇到的问题1：12345[root@backupNode /]# drbdadm up r0 1: Failure: (104) Can not open backing device. Command 'drbdsetup attach 1 /dev/sda2 /dev/sda3 internal' terminated with exit code 原因是我之前已经挂在了/dev/sda3，需要先卸载/dev/sda3设备，解决办法: 1umount /dev/sda2 问题解决. 4.4.2 我遇到的问题2：123[root@backupNode /]# drbdadm create-md r0 ‘r0‘ not defined in your config (for this host). 原因是drbd.conf配置文件计算机名和本机的计算机名不一致。 修改配置文件计算机名。 问题解决。 4.4.3 我遇到的问题3：12345[root@htest2 drbd.d]# drbdadm create-md r0drbd.d/r0.res:3: no minor given nor device name contains a minor numberdrbd.d/r0.res:9: no minor given nor device name contains a minor number r0.res中drbd资源必须含有数字，猜测与分区模块有重名。 问题解决。 4.4.4 我遇到的问题4在centos 6.4上部署时，yum安装完成后加载模块报错。 1234正确安装drbd模块后，使用modprobe进行加载# modprobe drbdFATAL: Module drbd not found.出现如上错误 原因：这是因为系统默认的内核并不支持此模块，所以需要更新内核更新内核的方法：可以用 yum install kernel* 方式来更新。如果你要节约点时间的话可以只更新一下的几个包： kernel-devel kernel kernel-headers 更新后，记得要重新启动操作系统！！！ &gt; 注：以上每一步骤，都需要在主备服务器上进行配置设置。 4.5 设置Primary Node将masterNode设为主服务器(primary node)，在masterNode上执行： 1[root@backupNode /]# drbdadm primary --force r0 查看drbd状态： 1234567891011[root@backupNode /]# cat /proc/drbd version: 8.4.1 (api:1/proto:86-100) GIT-hash: 91b4c048c1a0e06777b5f65d312b38d47abaea80 build by root@masterNode, 2012-05-27 18:34:27 1: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- ns:4 nr:9504584 dw:9504588 dr:1017 al:1 bm:576 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0 已经变成了主服务器。 4.6 创建DRBD文件系统上面已经完成了/dev/drbd1的初始化，现在来把/dev/drbd1格式化成ext4格式的文件系统,在masterNode上执行： 1[root@masterNode /]# mkfs.ext3 /dev/drbd1 输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243mke2fs 1.41.12 (17-May-2010) 文件系统标签= 操作系统:Linux 块大小=4096 (log=2) 分块大小=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 589824 inodes, 2358959 blocks 117947 blocks (5.00%) reserved for the super user 第一个数据块=0 Maximum filesystem blocks=2415919104 72 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632 正在写入inode表: 完成 Creating journal (32768 blocks): 完成 Writing superblocks and filesystem accounting information: 完成 This filesystem will be automatically checked every 21 mounts or 180 days, whichever comes first. Use tune2fs -c or -i to override. 然后，将/dev/drbd1挂载到之前创建好的/drbd目录： 1[root@masterNode /]# mount /dev/drbd1 /drbd 现在只要把数据写入/drbd目录，drbd即会立刻把数据同步到backupNode的/dev/sda2分区上了。 4.7 DRBD同步测试1、首先，在主服务器上先将设备卸载，同时将主服务器降为备用服务器： 123[root@masterNode /]# umount /dev/drbd1 [root@masterNode /]# drbdadm secondary r0 2、然后，登录备用服务器，将备用服务器升为主服务器，同时挂载drbd1设备到 /drbd目录： 1234567 [root@masterNode /]# ssh backup Last login: Sun May 27 19:57:17 2012 from masternode [root@backupNode ~]# drbdadm primary r0 [root@backupNode ~]# mount /dev/drbd1 /drbd/ 3、最后，进入/drbd目录，就可以看到之前在另外一台机器上放入的数据了，如果没有看到，说明同步失败！ 4.8 设置开机启动 注：设置DRBD开机启动，需要手动设置主备级别，找到方法后补充。 1[root@drbd2 rc.d]# systemctl enable drbd 5 安装配置keepalived(使用Heartbeat代替，只做了解，功能和heartbeat一样，Heartbeat自带drbd检测脚本) 5.1 YUM安装keepalivedCentos 7 自带 keepalived，可以直接YUM安装。 1[root@drbd1 rc3.d]# yum install -y keepalived 5.2 配置keepalived.conf1[root@drbd1 /]# vi /etc/keepalived/keepalived.conf 5.2.1 主服务器配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; //定义vrrp实例 state MASTER //主节点 ，备用节点为BACKUP interface enp2s0 //绑定的网卡 virtual_router_id 51 //ID,默认就行 .同一实例下virtual_router_id必须相同 priority 100 //优先级，备用节点设置为比100小的值 advert_int 1 //MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS //验证类型和密码 auth_pass 1111 &#125; virtual_ipaddress &#123; 172.20.20.237 //设置虚拟IP地址，可以多个 &#125;&#125; 5.2.2 备服务器配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; //定义vrrp实例 state BACKUP //备节点 ，主用节点为MASTER interface ens192 //绑定的网卡 virtual_router_id 51 //ID,默认就行 .同一实例下virtual_router_id必须相同 priority 50 //优先级，备用节点设置为比100小的值 advert_int 1 //MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS //验证类型和密码 auth_pass 1111 &#125; virtual_ipaddress &#123; 172.20.20.237 //设置虚拟IP地址，可以多个 &#125;&#125; 5.3 启动服务1[root@drbd1 /]# systemctl start keepalived 5.4 查看网卡信息 当启动keepalived后，查看网卡信息，主服务器会显示挂载虚拟IP172.20.20.237成功，备服务器没有挂载虚拟IP，如果主备服务器同时显示挂载虚拟Ip,则属于脑裂故障，需要继续调试。如果主服务器keepalived程序挂掉后，备用服务器会自动升级为主服务器，并挂载虚拟ip，主服务器恢复后恢复主从关系。 1[root@drbd1 /]# ip a 5.5 验证测试 1、在主服务器上新建一个网页，内容为 172.20.20.66 2、在备用服务器上新建一个网页，内容为 172.20.20.35 3、启动主备服务器的http服务和Keepalived服务 4、通过浏览数，输入虚拟IP地址 172.20.20.237 页面显示为 `172.20.20.66` 5、关闭主服务器的Keepalived服务，通过浏览器输入IP地址172.20.20.237 页面显示为 `172.20.20.35` 6、再次启动主服务器的Keepalived服务，通过浏览器输入IP地址172.20.20.237 页面显示为 `172.20.20.66` 5.6 设置开机启动服务1[root@drbd2 rc.d]# systemctl enable keepalived 6 安装配置heartbeat6.1 创建用户和组先创建用户和组，否则glue安装报错。 123# groupadd -g 200 haclient #创建GID为200的用户组# useradd -g haclient -u 200 -s /bin/false -M hacluster #创建用户（具体什么意思没懂） YUM安装所需组件 1# yum install -y libtool libtool-ltdl-devel glib2-devel libxml2-devel libxml2 bzip2-devel e2fsprogs-devel libuuid-devel libxslt-devel asciidoc docbook-style-xsl libnet 6.2 安装glue源码安装heartbeat之前首先得源码安装glue。 下载glue：http://www.linux-ha.org/wiki/Download 进入源码目录安装：./autogen.sh 生成配置文件：./configure 编译安装：make &amp;&amp; make install ./autogen.sh安装报错及解决办法 1234567报错信息：You must have autoconf installed to compile the cluster-glue package.Download the appropriate package for your system,or get the source tarball at: ftp://ftp.gnu.org/pub/gnu/autoconf/ 解决办法： 1yum install libtool 6.3 安装agents不安装agents的话，在安装Heartbeat时会报错，具体什么错误忘记记录了。 下载http://www.linux-ha.org/wiki/Download 12345./autogen.sh./configuremake &amp;&amp; make install 6.4 安装heartbeat： 安装glue/agents/heartbeat,编译安装路径最好默认，自定义的话有报错。 下载heartbeat：http://www.linux-ha.org/wiki/Downloads 进入源码目录生成配置文件：`./ConfigureMe configure --disable-swig --disable-snmp-subagent` 编译安装：make &amp;&amp; make install 没有按照上述操作可能遇到的错误： 1234567libtoolize: putting libltdl files in `libltdl'. libtoolize: `COPYING.LIB' not found in `/usr/share/libtool/libltdl./bootstrap exiting due to error (sorry!). 解决办法： 1yum install libtool-ltdl-devel 6.5 同步时间同步两台节点的时间 1234567# rm -rf /etc/localtime# \cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime# yum install -y ntp# ntpdate -d cn.pool.ntp.org 6.6 配置主节点heartbeat总共有三个文件需要配置:ha.cf 监控配置文件haresources 资源管理文件authkeys 心跳线连接加密文件 配置文件位置 /usr/share/doc/heartbeat/ 1# cp ha.cf haresources authkeys /etc/ha.d/ #拷贝配置文件到/etc/ha.d 6.6.1 修改ha.cf1234567891011121314151617181920212223242526272829303132333435debugfile /var/log/ha-debug #用于记录 heartbeat 的调试信息logfile /var/log/ha-log #指名heartbeat的日志存放位置。logfacility local0 #如果未定义上述的日志文件,那么日志信息将送往local0(对应的#/var/log/messages),如果这 3 个日志文件都未定义,那么 heartbeat 默认情况下 将在/var/log 下建立 ha-debug 和 ha-log 来记录 相应的日志信息。 #bcast eth1 #指明心跳使用以太网广播方式，并且是在eth1接口上进行 广播。（实际操作中没有指明。）keepalive 2 #发送心跳报文的间隔,默认单位为秒,如果你毫秒为单位, 那么需要在后面跟 ms 单位,如 1500ms 即代表 1.5s deadtime 30 #指定若备用节点在30秒内没有收到主节点的心跳信 号，则立即接管主节点的服务资源。 warntime 10 #指定心跳延迟的时间为10秒。当10秒钟内备份节点不能接收到主节点的心跳信号时，就会往日志中写入一 个警告日志，但此时不会切换服务。发出最后的心跳 警告 信息的间隔。 initdead 120 #在某些系统上，系统启动或重启之后需要经过一段时间 网络才能正常工作，该选项用于解决这种情况产生 的时 间间隔。取值至少为deadtime的两倍。 udpport 694 #设置广播/单播通信使用的端口，694为默认使用的端口号 ucast eth0 192.168.60.132 #采用网卡eth0的udp单播来组织心跳，后面跟的 IP地址应为双机对方的IP地址。 对方IPauto_failback off #用来定义当主节点恢复后，是否将服务自动切回。如果不想启用，请设置为off，默认为on。heartbeat的两台主机分别为主节点和备份节点。主节点在正常情况下占用资源并运行所有的服务，遇到故障时把资源交给备份节点并由备份节点运行服务。在该选项设为on的情况下，一旦主节点恢复运行，则自动获取资源并取代备份节点；如果该选项设置为off，那么当主节点恢复后，将变为备份节点，而原来的备份节点成为主节点。根据实际情况，不能开启，选择offnode drbd1 #主节点主机名，可以通过命令"uanme -n"查看。 node drbd2 #备用节点主机名。 #ping 192.168.60.1 #选择ping的节点，ping节点选择的越好，HA集群就 越强壮，可以选择固定的路由器作为ping节点，或者 应用服务器但是 最好不要选择集群中的成员作为ping 节点，ping节点 仅仅用来测试网络连接。如果指定了多个ping节点如ping 192.168.0.1 192.168.0.2那么只有当能ping通所有ping节点 时才认为网络是连通的，否则则认为不连通.实际中我没有开通检测。respawn hacluster /usr/libexec/heartbeat/ipfail #该选项是可选配置， 意思 是以 hacluster 这 个用户身份运行/usr/lib/heartbeat/ ipfail 这个 插件 respawn列出与heartbeat一起启动和关闭的 进 程，该进程一般是和heartbeat集成的插件，这些 进程遇到故障可以自动重新启动。最常用的进程是 ipfail，此进程用于检测和处理网络故障，需要配合 ping或者ping_group语句,其中指定的ping node 来检测网络的连通性。在v2版本中，ipfail和crm有 冲突，不能同时使用，如果启用crm的情况下，可以 使用pingd插件代替ipfailapiauth ipfail gid=haclient uid=hacluster #指定对客户端 api 的访问控制,缺省为不可 访问，这里指定了 有权限访问 ipfail用户和组。(使用前边创建的用户和组)#apiauth default gid=haclient （实际中没有开启此项设置，因为创建的用户和组就是默认用户和组的名字）当配置了默认用户组时，其他所有api授权命令失效且该用户组中的成员可以访问任何api库如果不在ha.cf文件指定api库的访问权限，则默认的访问权限如下service default apiauthipfailuid=haclusterccmgid=haclientpinggid=haclientcl_statusgid=haclientlha-snmpagentuid=rootcrmuid=hacluster 6.6.2 修改haresources1234567drbd1 IPaddr::192.168.84.132/24/eno16777736 drbddisk::r0 Filesystem::/dev/drbd1::/drbddrbd1是主节点主机名，主备配置文件都一样。解说：node1 IPaddr::192.168.60.200/24/eth0/ Filesystem::/dev/sdb5::/webdata::ext3 httpd cp.sh db2::db2inst1 其中，node1是HA集群的主节点，IPaddr为heartbeat自带的一个执行脚步，Heartbeat首先将执行/etc/ha.d/resource.d/IPaddr 192.168.60.200/24 start的操作，也就是虚拟出一个子网掩码为255.255.255.0，IP为192.168.60.200的地址。此IP为Heartbeat对外提供服务的网络地址，同时指定此IP使用的网络接口为eth0。接着，Heartbeat将执行共享磁盘分区的挂载操作，"Filesystem::/dev/sdb5::/webdata::ext3"相当于在命令行下执行mount操作，即"mount -t ext3 /dev/sdb5 /webdata"，然后启动httpd，接下列执行cp.sh这个脚本文件之后以db2inst1的身份启动db2。其中cp.sh必须放置在/etc/ha.d/resource.d/或/etc/init.d/目录中。 注意主节点和备份节点中资源文件haresources要完全一样。 6.6.3 修改authkeys 此配置文件必须设置权限为600（固定的） 1Chmod 600 /etc/ha.d/authkeys 123456789auth 1 1 crc #2 sha1 sha1_any_password #3 md5 md5_any_password authkeys文件用于设定Heartbeat的认证方式，共有3种可用的认证方式，即crc、md5和sha1。3种认证方式的安全性依次提高，但是占用的系统资源也依次增加。如果Heartbeat集群运行在安全的网络上，可以使用crc方式；如果HA每个节点的硬件配置很高，建议使用sha1，这种认证方式安全级别最高；如果是处于网络安全和系统资源之间，可以使用md5认证方式。这里我们使用crc认证方式.需要说明的一点是：无论auth后面指定的是什么数字，在下一行必须作为关键字再次出现，例如指定了"auth 6"，下面一定要有一行"6 认证类型"。 最后确保这个文件的权限是600（即-rw-------）。 6.6.4 对配置文件进行软链接在/usr/etc/ha.d/目录创建软链接，在部署中通过查看日志文件发现是通过此目录运行软件的，具体没搞明白，网上说拷贝的/etc/ha.d目录就行了，不知道为什么，为了省事，干脆创建软链接，这样就没问题了。此功能好用。 12345678910111213ln -sv /etc/ha.d/shellfuncs /usr/etc/ha.d/shellfuncsln -sv /etc/ha.d/ha.cf /usr/etc/ha.d/ha.cfln -sv /etc/ha.d/authkeys /usr/etc/ha.d/authkeysln -sv /etc/ha.d/resource.d /usr/etc/ha.d/resource.dln -sv /etc/ha.d/haresources /usr/etc/ha.d/haresourcesln -sv /etc/ha.d/harc /usr/etc/ha.d/harcln -sv /etc/ha.d/rc.d /usr/etc/ha.d/rc.d 复制ocf文件，如果没有，启动的时候会报错 1234567cp /usr/lib/ocf/lib/heartbeat/ocf-binaries /usr/lib/ocf/resource.d/heartbeat/.ocf-binariescp /usr/lib/ocf/lib/heartbeat/ocf-directories /usr/lib/ocf/resource.d/heartbeat/.ocf-directoriescp /usr/lib/ocf/lib/heartbeat/ocf-returncodes /usr/lib/ocf/resource.d/heartbeat/.ocf-returncodescp /usr/lib/ocf/lib/heartbeat/ocf-shellfuncs /usr/lib/ocf/resource.d/heartbeat/.ocf-shellfuncs 这一块没有用到，发现默认情况下目录里面就存在这些文件，此处作为参考。 备用节点配置文件和主节点配置文件基本一样，只有一个地方有区别。就是 ucast eth0 192.168.60.132 IP地址是对方的IP 7 启动程序顺序第一步：启动主备节点drbd服务 1[root@drbd1 ha.d]# drbdadm up r0 第二步：设置升级主节点为primary，并挂载到目录 123[root@drbd1 ha.d]# drbdadm primary r0[root@drbd1 ha.d]# mount /dev/drbd1 /drbd/ 第三步：启动主备节点Heartbeat服务 1[root@drbd1 ha.d]# service heartbeat start 等待主备节点连接并配置完成，可以通过ip a 命令查看虚拟ip是否生效。以下环境部署完成。在进行主节点维护中，恢复的话必须手动操作，避免出错。 8 附件链接: http://pan.baidu.com/s/1mibp7Uo 密码: y99z]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>双机</tag>
        <tag>drbd</tag>
        <tag>heartbeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay服务器迁移(32位迁移至64位)]]></title>
    <url>%2F2018%2F09%2F07%2FLiferay%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%81%E7%A7%BB-32%E4%BD%8D%E8%BF%81%E7%A7%BB%E8%87%B364%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[1 原运行环境： 系统：CentOS 6.4 32位 Liferay:32位版本 bitnami-liferay-6.2-7-linux-installer 2 新运行环境：系统：CentOS7.2 64位 Liferay:由原软件目录直接拷贝到新系统，修改java、apache-tomcat目录为64位安装版目录，迁移数据和配置文件：/opt/liferay/apache-tomcat/temp /opt/liferay-6.2-7/apache-tomcat.bak/webapps/liferay/WEB-INF/classes/ portal-ext.properties 3 新系统运行32位软件需安装64位系统运行32位软件，需要安装32位运行库等。 123yum install glibc.i686yum install xulrunner.i686 安装后访问平台流程插件需要重新安装，否则流程工作台无法正常工作及显示。 Liferay 集成公司OA流程控制台方法： 用管理员登录liferay，进入应用程序管理页面，上传流程控制台插件，然后安装。如下图 安装完后，可以在程序管理页面看到安装后的插件，如下图。 切换到站点首页，添加页面内容，然后把任务提醒拉到要显示的位置，如下图： ssh登录到服务器，将/opt/liferay-6.2-7/apache-tomcat/webapps/workflow-portlet目录备份更名，然后解压workflow-portlet20170208.tar.gz到/opt/liferay-6.2-7/apache-tomcat/webapps/workflow-portlet目录，如下图。 解压后，重启liferay。 在服务器上再次部署流程控制台web服务器tomcat。将tomcat程序复制到服务器/opt目录下，然后调整tomcat的端口，避免和liferay的tomcat端口冲突。修改/opt/tomcat7/conf/server.xml 文件。 修改位置： tomcat配置好后，找研发人员要最新版本的流程开发程序包。文件名：DeWorkFlow.war。 将此文件放在/opt/tomcat7/webapps 目录下。如下图： 备注：DeWorkFlow.war插件公司内部提供。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>liferay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay中文乱码(tomcat中文乱码)]]></title>
    <url>%2F2018%2F09%2F07%2FLiferay%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81-tomcat%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[都是修改tomcat配置文件server.xml,添加URIEncoding=”UTF-8”解决问题。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
        <tag>liferay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK部署]]></title>
    <url>%2F2018%2F09%2F07%2FELK%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1 环境 系统：centos 7.2 x86_64 -1511 JDK: 1.8.0_111 ELK: elasticsearch kibana logstash https://www.elastic.co/cn/downloads 2 Elasticsearch部署下载并解压elasticsearch-5.3.2.tar.gz 1tar -zxvf elasticsearch-5.3.2.tar.gz -C /home/elk 2.1 配置elasticsearch.yml1[root@elk-node1 src]# cat /etc/elasticsearch/elasticsearch.yml | grep -v “#” 123456789101112131415161718192021cluster.name: elk #自定义集群名，相同集群内的节点设置相同的集群名node.name: elk-node1 #自定义节点名，建议统一采用节点hostnamepath.data: /var/lib/elasticsearch #data存储路径，可不取消注释，默认即此路径path.logs: /var/log/elasticsearch #log存储路径，可不取消注释，默认即此路径network.host: 0.0.0.0 #es监听地址，采用”0.0.0.0”，允许所有设备访问http.port: 9200 #es监听端口，可不取消注释，默认即此端口discovery.zen.ping.unicast.hosts: ["elk-node1", "elk-node2"] #集群节点发现列表，也可采用ip的形式discovery.zen.minimum_master_nodes: 2 #集群可做master的最小节点数#以下两项设置es5.x版本的head插件可以访问eshttp.cors.enabled: true #开启跨域访问支持，默认为falsehttp.cors.allow-origin: "*" #跨域访问允许的域名地址，使用正则表达式 2.2 配置head插件2.2.1 下载并配置nodejsnodejs官网：https://nodejs.org/en/ 12345678910[root@elk-node1 ~]# cd /home/elk[root@elk-node1 elk]# wget https://nodejs.org/dist/v6.9.5/node-v6.9.5-linux-x64.tar.xz[root@elk-node1 elk]# xz -d node-v6.10.2-linux-x64.tar.xz[root@elk-node1 elk]# tar -xvf node-v6.10.2-linux-x64.tar -C /usr/local/[root@elk-node1 ~]# ln -s /usr/local/node-v6.10.2-linux-x64/bin/node /usr/bin/node[root@elk-node1 ~]# ln -s /usr/local/node-v6.10.2-linux-x64/bin/npm /usr/bin/npm[root@elk-node1 ~]# node -vv6.10.2[root@elk-node1 ~]# npm -v3.10.10 2.2.2 安装head插件安装grunt 12[root@elk-node1 ~]# npm install -g grunt-cli[root@elk-node1 ~]# ln -s /usr/local/node-v6.10.2-linux-x64/lib/node_modules/grunt-cli/bin/grunt /usr/bin/grunt #grunt是一个方便的构建工具，可以进行打包压缩、测试、执行等等的工作，5.x里的head插件就是通过grunt启动的； #”-g”参数代表全局安装，一般安装到nodejs的”lib/node_modules”目录下；不带参数”-g”，则是本地安装，一般安装到运行npm命令时所在的目录，这里就需要安装到head插件目录； #为grunt命令建软链接，方便全局执行，或加入环境变量；； #如果镜像速度不理想，可提前切到国内的镜像，在安装grunt-cli前执行：npm config set registry https://registry.npm.taobao.org。 2.2.3 下载并配置head12[root@elk-node1 ~]# cd /home/elk/elasticsearch[root@elk-node1 elasticsearch]# git clone git://github.com/mobz/elasticsearch-head.git #配置elasticsearch.yml文件允许head插件跨域访问es，请见上述章节。 2.2.4 安装head12[root@elk-node1 ~]# cd /var/lib/elasticsearch/elasticsearch-head/[root@elk-node1 elasticsearch]# npm install # 安装完成后可能有一些报错，不影响使用，报错原因未知，本人对nodejs不了解，github上有相近的问题，但并未解决。解决方法如下： (1) 查看报错信息”Error: Cannot find module ‘/var/lib/elasticsearch/elasticsearch-head/node_modules/phantomjs-prebuilt/install.js’”，未找到” phantomjs-prebuilt/install.js”文件； (2) 采取比较土的办法，将完整的”phantomjs-prebuilt/”目录上传到相应位置，重新执行”npm install”，无报错。 # 同时有3个警告信息，忽略即可，其中“npm WARN elasticsearch-head@0.0.0 license should be a valid SPDX license expression”警告信息可做如下处理：http://www.itdadao.com/articles/c15a1031952p0.html 即修改”./elasticsearch-head”目录下“package.json”文件第17行的””Apache2“ “为”Apache-2.0“，涉及到开源软件与其他合作类软件的使用声明。 # 如果没有全局安装grunt二进制程序，可在”elasticsearch-head”目录下执行”npm install grunt --save“或” npm install grunt-cli“。 2.2.5 配置app.js和Guuntfile.js1234[root@elk-node1 ~]# cd /home/elk/elasticsearch-5.3.2/elasticsearch-head/[root@elk-node1 elasticsearch-head]# cd _site/[root@elk-node1 _site]# cp app.js app.js.bak[root@elk-node1 _site]# vim app.js #在4328行将原”http://localhost:9200“修改为”http://172.20.20.29:9200“，否则head插件不能获取节点状态信息。 123[root@elk-node1 ~]# cd /home/elk/elasticsearch-5.3.2/elasticsearch-head/[root@elk-node1 elasticsearch-head]# cp Gruntfile.js Gruntfile.js.bak[root@elk-node1 elasticsearch-head]# vim Gruntfile.js #在93行新增”hostname: 0.0.0.0’, “，确保能被访问。 2.3 启动并验证 创建elk用户 使用elk用户启动elasticsearch 1su - elk -c "/home/elk/elasticsearch-5.3.2/bin/elasticsearch &amp;" 启动head 12cd /home/elk/elasticsearch-5.3.2/elasticsearch-head/grunt server &amp; #启动head插件，需要到head目录下 访问https://172.20.20.29:9200 访问https://172.20.20.29:9100 3 Logstash部署3.1 下载logstash下载logstash-5.3.2 1tar -zxvf logstash-5.3.2 -C /home/elk 3.2 配置logstahs3.2.1 配置文件默认配置文件： /home/elk/logstash-5.3.2/config/logstash.yml 12345[root@elk-node1 ~]# cd /etc/logstash/[root@elk-node1 logstash]# cat logstash.yml | grep -v "#"path.data: /var/lib/logstashpath.config: /etc/logstash/conf.dpath.logs: /var/log/logstash *#其中默认配置已经明确数据，日志，logstash pipeline实例文件的存储位置，保持默认即可； #其中“http.host”参数指定数据输入主机，默认为localhost；“http.port”参数指定数据输入端口，默认为9600～9700（每实例占用1个），此验证暂时不做变更* 3.2.2 pipeline文件根据默认配置，pipeline实例文件默认应放置于/home/elk/logstash-5.3.2/config/目录，此时目录下无实例文件，可根据实际情况新建实例，以处理本机log信息为例，如下： 123456789101112131415161718[root@elk-node1 config]# vim messages.confinput &#123; file &#123; path =&gt; "/var/log/messages" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["172.20.20.29:9200","172..20.20.29:9200"] index =&gt; "messages-%&#123;+YYYY.MM.dd&#125;" &#125; stdout &#123;# codec =&gt; rubydebug &#125;&#125;[root@elk-node1 conf.d]# cd ..[root@elk-node1 logstash]# chown -R logstash:logstash conf.d/[root@elk-node1 ~]# chmod 644 /var/log/messages *#pipeline #pipeline实例文件以”input”，”output”，”filter”等区域组成，前两者为必选项； #”input”与”output”利用插件进行数据输入与输出，如这里”file”即输入插件，“elasticseach”与“stdout”即输出插件； #在各插件内再具体定义行为，如”input”定义了数据源，“elasticseach”定义了输出节点与数据输出的索引与格式； #“codec =&gt; rubydebug”会产生大量的debug文件至message（也可重定向到其他位置），此处注释掉； #请注意权限，这里message常规权限是400，logstash无法读取，如果无法调用，在logstash的启动日志中会有相应的记录。* 3.3 启动并验证1[root@elk-node1 ~]# /home/elk/logstash-5.3.2/bin/logstash -f /home/elk/logstash-5.3.2/conf.d/messages.conf &amp; &amp; 是在后台启动 1[root@elk-node1 ~]# netstat -tunlp 浏览器访问http://172.20.20.29:9100 4 Kibana部署4.1 解压下载的kibana1tar -zxvf kibana-5.3.2-linux-x86_64.tar.gz -C /home/elk 4.2 配置kibana123456[root@elk-node1 ~]# cd /home/elk/kibana-5.3.2-linux-x86_64/config[root@elk-node1 kibana]# vim kibana.yml[root@elk-node1 kibana]# cat /etc/kibana/kibana.yml | grep -v "^$" | grep -v "#"server.port: 5601 #默认即5601server.host: "0.0.0.0" #允许被访问elasticsearch.url: "http://172.20.20.29:9200" #es地址与端口 4.3 启动并验证1/home/elk/kibana-5.3.2-linux-x86_64/bin/kibana &amp; &amp; 在后台启动 1netstat -tunlp | grep 5601 4.4 kibana展示浏览器访问：http://172.20.20.29:5601 需要在“Index name or pattern”处创建索引名，elassticsearch中并没有以”logstash-*”命名的索引，则不能创建，新建索引对应logstash的pipeline输出插件定义的”index”，即”messages-*”，如下： 在“Index name or pattern”处填写入正确的索引名字，”@timestamp”会自动填充，点击”create”创建，见到如下界面即索引创建完成； 在“Discover”页面，可以搜索与浏览Elasticsearch中的数据，默认搜索的是最近15分钟的数据，可以自定义选择时间。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay+drbd双机热备部署]]></title>
    <url>%2F2018%2F09%2F07%2FLiferay-drbd%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1 主节点配置 1.1 添加硬盘为主服务器添加一块硬盘50G。 1.2 创建分区1fdisk /dev/sdb 为新硬盘创建分区sdb1,大小50G.分区格式lvm 1.3 创建物理卷1pvcreate /dev/sdb1 1.4 将物理卷添加入卷组1vgextend vg_cosliferay /dev/sdb1 卷组名通过vgdisplay查看。vg_cosliferay是卷组名。 1.5 创建逻辑卷1lvcreate -L 50G -n drbdlv vg_cosliferay 创建大小为50G，名字为drbdlv的逻辑卷。 1.6 安装drbd1yum -y install drbd84-utils kmod-drbd84 1.6.1 配置global_common.conf编辑全局配置： 1vi /etc/drbd.d/global_common.conf 确保文件中包含有下内容： 123456789101112global &#123; usage-count yes; &#125; common &#123; net &#123; protocol C; # 使用协议C.表示收到远程主机的写入确认后,则认为写入完成. &#125; &#125; 当然，还可以有其它配置，这是最基本的。 1.6.2 配置r0资源：创建r0资源： 注：r0可以随便命名。 1vi /etc/drbd.d/r0.res 写入文件内容： 123456789101112131415161718192021222324252627resource r0&#123; on masterNode&#123; device /dev/drbd1; #逻辑设备的路径 disk /dev/sda3; #物理设备 address 192.168.58.128:7788; meta-disk internal; &#125; on backupNode&#123; device /dev/drbd1; disk /dev/sda3; address 192.168.58.129:7788; meta-disk internal; &#125; &#125; 需要把上面用到的防火墙7788端口打开，这个端口是自定义的，如果嫌麻烦可以直接关掉防火墙。 注：masterNode/ backupNode是主机名字，根据实际情况书写。 说明： 1234567device 是自定义的物理设备的逻辑路径disk 是磁盘设备，或者是逻辑分区address 是机器监听地址和端口meta-disk 这个还没弄明白，看到的资料都是设为：internal（局域网） 1.6.3 建立resource123modprobe drbd //载入 drbd 模块 lsmod | grep drbd //确认 drbd 模块是否载入 12345dd if=/dev/zero of=/dev/sda2 bs=1M count=100 //把一些资料塞到 sda3 內 (否则 create-md 时会报错) drbdadm create-md r0 //建立 drbd resource drbdadm up r0 // #启用资源 2 备节点配置把以上主节点的操作在备节点上操作一次，操作过程完全一样。 2.1 设置Primary Node将masterNode设为主服务器(primary node)，在masterNode上执行： 1[root@backupNode /]# drbdadm primary --force r0 查看drbd状态： 123456789[root@backupNode /]# cat /proc/drbd version: 8.4.1 (api:1/proto:86-100) GIT-hash: 91b4c048c1a0e06777b5f65d312b38d47abaea80 build by root@masterNode, 2012-05-27 18:34:27 1: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- ns:4 nr:9504584 dw:9504588 dr:1017 al:1 bm:576 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0 已经变成了主服务器。 2.2 创建DRBD文件系统上面已经完成了/dev/drbd1的初始化，现在来把/dev/drbd1格式化成ext4格式的文件系统,在masterNode上执行： 1 [root@masterNode /]# mkfs.ext3 /dev/drbd1 3 安装配置keepalived3.1 YUM安装keepalived1[root@drbd1 rc3.d]# yum install -y keepalived 3.2 配置keepalived.conf1[root@drbd1 /]# vi /etc/keepalived/keepalived.conf 3.2.1 主服务器配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; //定义vrrp实例 state MASTER //主节点 ，备用节点为BACKUP interface enp2s0 //绑定的网卡 virtual_router_id 51 //ID,默认就行 .同一实例下virtual_router_id必须相同 priority 100 //优先级，备用节点设置为比100小的值 advert_int 1 //MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS //验证类型和密码 auth_pass 1111 &#125; virtual_ipaddress &#123; 172.20.20.237 //设置虚拟IP地址，可以多个 &#125;&#125; 3.2.2 备服务器配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125; vrrp_instance VI_1 &#123; //定义vrrp实例 state BACKUP //备节点 ，主用节点为MASTER interface ens192 //绑定的网卡 virtual_router_id 51 //ID,默认就行 .同一实例下virtual_router_id必须相同 priority 50 //优先级，备用节点设置为比100小的值 advert_int 1 //MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 authentication &#123; auth_type PASS //验证类型和密码 auth_pass 1111 &#125; virtual_ipaddress &#123; 172.20.20.237 //设置虚拟IP地址，可以多个 &#125;&#125; 3.3 启动服务1[root@drbd1 /]# systemctl start keepalived 4 问题Liferay配置drbd双击热备中，在备机启动程序时，遇到以下问题。问题根本原因是原主机是32位系统安装32位程序，在备机64位系统运行32位程序没有运行库而报错。 4.1 问题1：123456789101112131415[09:55:50][root@liferay-backup opt]# /opt/liferay-6.2-7/ctlscript.sh start[09:55:50]/opt/liferay-6.2-7/mysql/bin/my_print_defaults: /opt/liferay-6.2-7/mysql/bin/my_print_defaults.bin: /lib/ld-linux.so.2: bad ELF interpreter: 没有那个文件或目录[09:55:50]/opt/liferay-6.2-7/mysql/bin/my_print_defaults:行12: /opt/liferay-6.2-7/mysql/bin/my_print_defaults.bin: 成功[09:55:50]/opt/liferay-6.2-7/mysql/bin/my_print_defaults: /opt/liferay-6.2-7/mysql/bin/my_print_defaults.bin: /lib/ld-linux.so.2: bad ELF interpreter: 没有那个文件或目录[09:55:50]/opt/liferay-6.2-7/mysql/bin/my_print_defaults:行12: /opt/liferay-6.2-7/mysql/bin/my_print_defaults.bin: 成功[09:55:50]161221 09:55:53 mysqld_safe Logging to '/opt/liferay-6.2-7/mysql/data/mysqld.log'.[09:55:50]161221 09:55:53 mysqld_safe Starting mysqld daemon with databases from /opt/liferay-6.2-7/mysql/data[09:55:50]161221 09:55:53 mysqld_safe mysqld from pid file /opt/liferay-6.2-7/mysql/data/mysqld.pid ended 解决办法： 1[root@liferay-backup opt]# yum install glibc.i686 安装后又报错： 123456789101112131415[root@liferay-backup data]# /opt/liferay-6.2-7/ctlscript.sh start[10:30:47]libgcc_s.so.1 must be installed for pthread_cancel to work[10:30:47]libgcc_s.so.1 must be installed for pthread_cancel to work[10:30:47]161221 10:30:50 mysqld_safe Logging to '/opt/liferay-6.2-7/mysql/data/mysqld.log'.[10:30:47]161221 10:30:50 mysqld_safe Starting mysqld daemon with databases from /opt/liferay-6.2-7/mysql/data[10:30:48]libgcc_s.so.1 must be installed for pthread_cancel to work[10:30:48]/opt/liferay-6.2-7/mysql/bin/mysqld_safe: 行 165: 19283 已放弃 (吐核)LD_LIBRARY_PATH=/opt/liferay-6.2-7/mysql/lib\:/opt/liferay-6.2-7/mysql/lib\:/opt/liferay-6.2-7/sqlite/lib\:/opt/liferay-6.2-7/apache2/lib\:/opt/liferay-6.2-7/common/lib\: nohup /opt/liferay-6.2-7/mysql/bin/mysqld --defaults-file=/opt/liferay-6.2-7/mysql/my.cnf --basedir=/opt/liferay-6.2-7/mysql --datadir=/opt/liferay-6.2-7/mysql/data --plugin-dir=/opt/liferay-6.2-7/mysql/lib/plugin --user=mysql --lower-case-table-names=1 --log-error=/opt/liferay-6.2-7/mysql/data/mysqld.log --pid-file=/opt/liferay-6.2-7/mysql/data/mysqld.pid --socket=/opt/liferay-6.2-7/mysql/tmp/mysql.sock --port=3306 &lt; /dev/null &gt;&gt; /opt/liferay-6.2-7/mysql/data/mysqld.log 2&gt;&amp;1[10:30:48]161221 10:30:51 mysqld_safe mysqld from pid file /opt/liferay-6.2-7/mysql/data/mysqld.pid ended 没有安装32位运行库 直接安装yum install glibc.i686无法成功，系统已安装64位运行库，无法在安装。需要 123456789Linux的有些软件需要32位运行库才能运行，如Dr.com客户端等yum在线安装： sudo yum install xulrunner.i686或者： sudo yum install ia32-libs.i686ubuntu下： sudo apt-get install ia32-libs 最终没有采用此方法，使用的方法为本博客内的Rsync+sersync实时双向同步 参考资料1：http://blog.csdn.net/tmy257/article/details/41013985 参考资料2：http://blog.csdn.net/attagain/article/details/17026433]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>双机</tag>
        <tag>liferay</tag>
        <tag>drbd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bitnami删除页面右下角Manage图标]]></title>
    <url>%2F2018%2F09%2F07%2Fbitnami%E5%88%A0%E9%99%A4%E9%A1%B5%E9%9D%A2%E5%8F%B3%E4%B8%8B%E8%A7%92Manage%E5%9B%BE%E6%A0%87%2F</url>
    <content type="text"><![CDATA[Bitnami info page How to remove the banner(如何删除旗帜) 如果你想删除的旗帜，你只需要使用工具bnconfig,执行下面命令。 Linux and OS X Systems 1sudo /opt/bitnami/apps/软件目录/bnconfig --disable_banner 1 然后重启APache: 1$ sudo /opt/bitnami/ctlscript.sh restart apache Windows Systems 进入算计路径：: cd apps\软件目录 1bnconfig.exe --disable_banner 1]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>bitnami</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bitnami软件修改URL方法]]></title>
    <url>%2F2018%2F09%2F07%2Fbitnami%E8%BD%AF%E4%BB%B6%E4%BF%AE%E6%94%B9URL%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1 更改首页页面 1install/apache2/conf/bitnami/bitnami.conf 修改bitnami.con文件，更改首页页面。 1234567&lt;VirtualHost _default_:80&gt; DocumentRoot "/opt/mediawiki-1.26.2-2/apache2/htdocs" &lt;Directory "/opt/mediawiki-1.26.2-2/apache2/htdocs"&gt; 改为&lt;VirtualHost _default_:80&gt; DocumentRoot "/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs" &lt;Directory "/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs"&gt; 2 更改应用访问地址后缀例如：http://172.20.20.37/moodle 改为http://172.20.20.37 1Install/apps/moodle/conf/httpd-prefix.conf 修改httpd-prefix.conf文件，更改应用页面更目录 1234567891011Alias /moodle/ "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login/"Alias /moodle "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login/" 改为 DocumentRoot "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login" #Alias /moodle/ "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login/"#Alias /moodle "C:\Bitnami\moodle-2.8.5-1/apps/moodle/htdocs/login/" 3 更改应用根目录。一般情况下，只需修改bitnami.con文件就可以达到URL后缀显示的问题，个别情况下需要修改apps下的httpd-prefix.conf文件。 还可以修改后缀名称。 例如：http://172.20.20.21/moodle 改为 http://172.20.20.21/mmkk 也是修改httpd-prefix.conf文件。 4 修改端口号：参考上面80位置1install/apache2/conf/bitnami/bitnami.conf 如果还不行，同时修改apache配置文件端口号。 1install/apache2/conf/httpd.conf]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>bitnami</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK部署过程中问题总结]]></title>
    <url>%2F2018%2F09%2F07%2FELK%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1 错误1： 启动elasticsearch-5.3.0报错 不能使用root用户启动。新建用户elk，用户组elk. 2 错误2：使用elk启动报错 改变elasticsearch文件夹所有者到当前用户 3 错误3：修改network-name : 0.0.0.0,后报错 注意： es启动要求提高一些系统参数配置，否则会报错 12345678910111213141516a. 增大vm.max_map_count到至少262144$ sudo vim /etc/sysctl.conf添加 vm.max_map_count=262144$ sudo sysctl -pb. 增大文件句柄数至少 65536 ulimit -a查看$ sudo vim /etc/security/limits.conf* soft nofile 65536* hard nofile 65536 4 错误4：启动elasticsearch报错 elasticsearch-head · 不能放在elasticsearch的plugins、modules 目录下 · 不能使用 elasticsearch-plugin install 移动elasticsearch-head,到别的目录，启动成功。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsyslog+loganalyzer+evtsys搭建日志服务器]]></title>
    <url>%2F2018%2F09%2F07%2Frsyslog-loganalyzer-evtsys%E6%90%AD%E5%BB%BA%E6%97%A5%E5%BF%97%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1 环境要求 将所有的服务器的日志都集中保存在一台rsyslog日志服务器中，mysql作为数据库，loganalyzer将服务器的日志数据进行分析展现，evtsys是windows服务器提交日志的客户端软件。 实验环境如下： 节点 OS IP 网络 rsyslog Server Centos6.4 192.168.200.106 单网卡 Linux Client Centos6.4 192.168.200.106 单网卡 Windows Client Windows2008x64 192.168.200.31 单网卡 2 rsyslog+loganalyzer日志服务器的部署2.1 配置rsyslog日志服务器因为rsyslog要把日志存到mysql中，所以要有mysql服务器，还要有rsyslog配置文件加载。 2.1.1 连接mysql的模块1[iyunv@rsyslog ~]# yum -y install rsyslog mysql-server rsyslog-mysql 2.1.2 配置数据库12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[iyunv@rsyslog ~]# rpm -ql rsyslog-mysql #首先查看rsyslog-mysql安装生成了那些文件/lib64/rsyslog/ommysql.so/usr/share/doc/rsyslog-mysql-5.8.10/usr/share/doc/rsyslog-mysql-5.8.10/createDB.sql #此sql文件就是需要导入到数据库中的数据文件#[iyunv@rsyslog ~]# service mysqld start #启动mysqld服务[iyunv@rsyslog ~]# mysql #连接mysqlWelcome to the MySQL monitor. Commands end with ; or g.Your MySQL connection id is 2Server version: 5.1.73 Source distributionCopyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or 'h' for help. Type 'c' to clear the current input statement.mysql&gt;mysql&gt;mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || test |+--------------------+3 rows in set (0.00 sec) #此时，只有3个库#mysql&gt; source /usr/share/doc/rsyslog-mysql-5.8.10/createDB.sql; #导入rsyslog的数据文件mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || Syslog || mysql || test |+--------------------+4 rows in set (0.01 sec)mysql&gt; use Syslog; #Syslog即是记录日志文件的数据库Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+------------------------+| Tables_in_Syslog |+------------------------+| SystemEvents || SystemEventsProperties |+------------------------+2 rows in set (0.00 sec)##接下来，即是为rsyslog服务器授权。此处一定是rsyslog服务器的IP#如果写成各服务器的IP，那就错了mysql&gt; grant all on Syslog.* to 'syslogroot'@'127.0.0.1' identified by 'syslogpass';Query OK, 0 rows affected (0.00 sec)mysql&gt; grant all on Syslog.* to 'syslogroot'@'192.168.200.106' identified by 'syslogpass';Query OK, 0 rows affected (0.04 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; qBye 2.1.3 修改rsyslog日志服务器配置文件1234567891011121314[iyunv@rsyslog ~]# grep -v "^$" /etc/rsyslog.conf | grep -v "^#"$ModLoad imuxsock$ModLoad imklog$ModLoad imudp #加载udp的模块$UDPServerRun 514 #允许接收udp 514的端口传来的日志$ModLoad imtcp #加载tcp的模块$InputTCPServerRun 514 #允许接收tcp 514的端口传来的日志$ModLoad ommysql #加载mysql的模块$ActionFileDefaultTemplateRSYSLOG_TraditionalFileFormat$IncludeConfig /etc/rsyslog.d/*.conf*.* :ommysql:192.168.200.106,Syslog,syslogroot,syslogpass #添加此行，所有设施的所有日志都记录到此数据库服务器的Syslog数据库中，以syslogroot用户，syslogpass密码访问数据库local7.* /var/log/boot.log$template SpiceTmpl,"%TIMESTAMP%.%TIMESTAMP:::date-subseconds% %syslogtag% %syslogseverity-text%:%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%":programname, startswith, "spice-vdagent" /var/log/spice-vdagent.log;SpiceTmpl 2.1.4 修改完成后，重启rsyslog服务123[iyunv@rsyslog ~]# service rsyslog restartShutting down system logger: [ OK ]Starting system logger: [ OK ] 2.2 配置rsyslog客户端2.2.1 修改配置文件123456789[iyunv@mariadb ~]# grep -v "^$" /etc/rsyslog.conf | grep -v "^#"$ModLoad imuxsock # provides support for local system logging (e.g. via logger command)$ModLoad imklog # provides kernel logging support (previously done by rklogd)$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat$IncludeConfig /etc/rsyslog.d/*.conf*.* @192.168.200.106*.* :ommysql:192.168.200.106,Syslog,syslogroot,syslogpass$template SpiceTmpl,"%TIMESTAMP%.%TIMESTAMP:::date-subseconds% %syslogtag% %syslogseverity-text%:%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%":programname, startswith, "spice-vdagent" /var/log/spice-vdagent.log;SpiceTmpl 2.2.2 修改完成后，重启rsyslog服务123[iyunv@rsyslog ~]# service rsyslog restartShutting down system logger: [ OK ]Starting system logger: [ OK ] 2.3 验证客户端日志文件的存放2.3.1 使用logger生成一条日志信息1[iyunv@mariadb ~]# logger -p info "I'm mariadb" 2.3.2 在rsyslog服务器上验证12345678910111213141516171819202122232425262728293031[iyunv@rsyslog ~]# mysqlmysql&gt; use Syslog;mysql&gt; select * from SystemEventsG*************************** 279. row ***************************ID: 279CustomerID: NULLReceivedAt: 2014-08-13 20:07:39DeviceReportedTime: 2014-08-13 20:07:40Facility: 1Priority: 6FromHost: mariadbMessage: I'm mariadb #我在做的时候，就是因为第二部的1的（2）中mysql授权时，写的是客户端IP，导致这里获取不到数据。NTSeverity: NULL #因此，在数据库授权的时候，要授权的是rsyslog日志服务器的IPImportance: NULLEventSource: NULLEventUser: NULLEventCategory: NULLEventID: NULLEventBinaryData: NULLMaxAvailable: NULLCurrUsage: NULLMinUsage: NULLMaxUsage: NULLInfoUnitID: 1SysLogTag: root:EventLogType: NULLGenericFileName: NULLSystemID: NULLprocessid:checksum: 0279 rows in set (0.00 sec) 到这里，rsyslog日志服务器就部署完成了，但此时日志处于rsyslog日志服务器的mysql数据库中，并不方便查看与管理，所以我们再部署一个loganalyzer日志分析器，来减小日志管理的复杂度。 2.4 部署loganalyzer日志分析器2.4.1 安装LAMP环境123[iyunv@rsyslog ~]# yum -y install httpd php php-mysql php-gd[iyunv@rsyslog ~]# mkdir /var/www/html/loganalyzer/mkdir: created directory `/var/www/html/loganalyzer/' 2.4.2 解压loganalyzer源码包123456789101112[iyunv@rsyslog ~]# tar xf loganalyzer-3.6.5.tar.gz[iyunv@rsyslog ~]# cd loganalyzer-3.6.5[iyunv@rsyslog loganalyzer-3.6.5]#[iyunv@rsyslog loganalyzer-3.6.5]# lsChangeLog contrib COPYING doc INSTALL src[iyunv@rsyslog loganalyzer-3.6.5]# mv src/* /var/www/html/loganalyzer/ #src下是php的网页文件[iyunv@rsyslog loganalyzer-3.6.5]# ls contrib/configure.sh secure.sh[iyunv@rsyslog loganalyzer-3.6.5]# mv contrib/* /var/www/html/loganalyzer/ #contrib目录下的两个脚本，可以打开看看#[iyunv@rsyslog loganalyzer-3.6.5]# cd /var/www/html/loganalyzer/[iyunv@rsyslog loganalyzer]# sh configure.sh #执行脚本 2.4.3 配置httpd修改DocumentRoot网页根目录 1234[iyunv@rsyslog ~]# vim /etc/httpd/conf/httpd.confDocumentRoot "/var/www/html/loganalyzer"#[iyunv@rsyslog ~]# service httpd start 2.4.4 配置httpd和mysql开机启动123[iyunv@rsyslog ~]# chkconfig mysqld on[iyunv@rsyslog ~]# chkconfig httpd on 2.4.5 创建loganalyzer数据库，并授权12345678[iyunv@rsyslog ~]# mysqlEnter password:mysql&gt; create database loganalyzer;Query OK, 1 row affected (0.04 sec)mysql&gt; grant all on loganalyzer.* to dianyi@'192.168.200.106' identified by 'dianyi123';Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 2.4.6 安装loganalyzer2.4.6.1 安装界面 安装完成，enjoy it 2.5 Windows客户端的安装配置2.5.1 安装Syslog日志客户端123456789下载Evtsys软件，分32位和64位安装方法一样。解压后将evtsys.dll、evtsys.exe复制到c:\windows\system32\下。 cd c:\windows\system32evtsys.exe -i -h 192.168.200.31 -p 514net start evtsys 2.2.2 开启系统相关审计打开windows组策略编辑器 (开始-&gt;运行 输入 gpedit.msc) 在windows 设置－&gt; 安全设置 －&gt; 本地策略－&gt;审核策略中，打开你需要记录的windows日志。evtsys会实时的判断是否有新的windows日志产生，然后把新产生的日志转换成syslogd可识别的格式,通过UDP端口发送给syslogd服务器。 3 Mysql数据库的优化的一些想法随着mysql数据库中的记录越来越多，查询的速度会越来越慢。我们可以定期到数据表中删除较早以前的记录来减少数据量从而起到优化的作用。可以定期执行如下SQL语句： 1delete from SystemEvents(数据表名) where ReceivedAt(日期字段名)&lt;curdate() - interval 3 month; //表示删除” SystemEvents”数据表” ReceivedAt”日期字段大于3个月的记录 也可以考虑使用其他数据库，比如具有循环功能的sqllite数据库，不过这样的话就要进行二次开发了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>rsyslog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mediawiki安装及LDAP认证问题]]></title>
    <url>%2F2018%2F09%2F07%2FMediawiki%E5%AE%89%E8%A3%85%E5%8F%8ALDAP%E8%AE%A4%E8%AF%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1 环境 安装环境：CentOS-7-x86_64-DVD-1511 Mediawiki: bitnami-mediawiki-1.26.2-2-linux-x64-installer.run LDAP扩展插件：LdapAuthentication-REL1_26-70ab129.tar.gz 2 安装mediawiki首先安装bitnami-mediawiki-1.26.2-2-linux-x64-installer.run 123#chmod 755 bitnami-mediawiki-1.26.2-2-linux-x64-installer.run //赋予读写权限#./bitnami-mediawiki-1.26.2-2-linux-x64-installer.run //执行安装 安装到最后出现mysql数据库错误： 1234567891011121314151617181920Error: Error running /opt/mediawiki-1.26.2-2/mysql/scripts/myscript.sh/opt/mediawiki-1.26.2-2/mysql ****: FATAL ERROR: please install the followingPerl modules before executing scripts/mysql_install_db:Data::DumperERROR 2002 (HY000): Can't connect to local MySQL server through socket'/opt/mediawiki-1.26.2-2/mysql/tmp/mysql.sock' (2)ERROR 2002 (HY000): Can't connect to local MySQL server through socket'/opt/mediawiki-1.26.2-2/mysql/tmp/mysql.sock' (2)ERROR 2002 (HY000): Can't connect to local MySQL server through socket'/opt/mediawiki-1.26.2-2/mysql/tmp/mysql.sock' (2) （根据提示，发现系统缺少Dumper模块） 根据提示安装Data-Dumper.x86_64 1# yum install perl-Data-Dumper.x86_64 //安装插件模块。 …………… OK, Mediawiki安装成功。 3 开启AD域用户认证登录Ldap认证需要php支持ldap,需要安装ldap支持模块（php-ldap），及开启php的ldap功能(修改php.ini文件)。 &gt; 备注：不安装php-ldap，LdapAuthentication插件不工作 1、 安装php-ldap模块： 1[root@localhost mediawiki]# yum install php-ldap 2、 开启ldap功能即修改php.ini文件，将extension=php_ldap.dll前的分号去掉。 1[root@localhost mediawiki]# vi /opt/mediawiki-1.26.2-2/php/etc/php.ini 3、下载LdapAuthentication插件：LdapAuthentication-REL1_26-70ab129.tar.gz 解压插件到extensions目录： 1# tar -xzf LdapAuthentication-REL1_26-70ab129.tar.gz /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions/ 4、编辑LocalSettings.php配置文件 在LocalSettings.php 末尾添加如下内容： 123456789101112131415161718192021222324252627282930313233343536373839require_once("extensions/LdapAuthentication/LdapAuthentication.php");$wgAuth= new LdapAuthenticationPlugin(); ## 这两行激活插件$wgLDAPDomainNames = array( "dealeasy" ); //域名简写$wgLDAPServerNames = array( "dealeasy"=&gt;"172.20.20.10" ); //域控域名或者ip$wgLDAPSearchStrings = array( "dealeasy"=&gt;"USER-NAME@dealeasy" ); //USER-NAME 不要修改它,默认用户名替代位置，特定字符$wgLDAPBaseDNs = array( "dealeasy"=&gt;"dc=dealeasy,dc=local");$wgLDAPSearchAttributes = array( "dealeasy"=&gt;"sAMAccountName"); //加上这两句就可以把DC上的用户名都同步过来了$wgLDAPUseLocal = false; 是否使用本地用户,ture代表使用，这里写入不使用$wgLDAPUpdateLDAP = false;$wgLDAPMailPassword = false;$wgMinimalPasswordLength = 1;$wgLDAPEncryptionType = array("dealeasy"=&gt;"clear");$wgShowSQLErrors = true;$wgDebugDumpSql = true;$wgShowDBErrorBacktrace = true; //这三行网上找到，因ldap用户登录出现数据库查询错误，添加这三行后，在出现错误时可以在页面上显示错误信息 Ok, LocalSettings.php编辑完成。 下面就可以直接使用域用户登录了。 4 Database错误 报错（前面LocalSettings.php添加后三行才能看到报错详情）： Database error12345678910111213141516171819202122232425262728293031323334353637383940414243444546A database query error has occurred. This may indicate a bug in the software.（翻译：数据库错误 一个数据库查询错误发生。这可能表明软件中的缺陷。）Query:SELECT domain FROM `ldap_domains` WHERE user_id = '4' LIMIT 1Function: LdapAuthenticationPlugin::loadDomainError: 1146 Table 'bitnami_mediawiki.ldap_domains' doesn't exist (localhost:3306)Backtrace:#0 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/db/Database.php(1076): DatabaseBase-&gt;reportQueryError('Table 'bitnami_...', 1146, 'SELECT domain ...', 'LdapAuthenticat...', false)#1 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/db/Database.php(1600): DatabaseBase-&gt;query('SELECT domain ...', 'LdapAuthenticat...')#2 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/db/Database.php(1689): DatabaseBase-&gt;select('ldap_domains', Array, Array, 'LdapAuthenticat...', Array, Array)#3 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions/LdapAuthentication/LdapAuthentication.php(2041): DatabaseBase-&gt;selectRow('ldap_domains', Array, Array, 'LdapAuthenticat...')#4 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions/LdapAuthentication/LdapAuthentication.php(2060): LdapAuthenticationPlugin::loadDomain(Object(User))#5 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions/LdapAuthentication/LdapAuthentication.php(1237): LdapAuthenticationPlugin::saveDomain(Object(User), 'dealeasy')#6 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specials/SpecialUserlogin.php(830): LdapAuthenticationPlugin-&gt;updateUser(Object(User))#7 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specials/SpecialUserlogin.php(958): LoginForm-&gt;authenticateUserData()#8 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specials/SpecialUserlogin.php(341): LoginForm-&gt;processLogin()#9 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specialpage/SpecialPage.php(384): LoginForm-&gt;execute(NULL)#10 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/specialpage/SpecialPageFactory.php(553): SpecialPage-&gt;run(NULL)#11 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/MediaWiki.php(281): SpecialPageFactory::executePath(Object(Title), Object(RequestContext))#12 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/MediaWiki.php(714): MediaWiki-&gt;performRequest()#13 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/includes/MediaWiki.php(508): MediaWiki-&gt;main()#14 /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/index.php(41): MediaWiki-&gt;run()#15 &#123;main&#125; 最终，通过报错信息Error: 1146 Table &#39;bitnami_mediawiki.ldap_domains&#39; doesn&#39;t exist (localhost:3306)，发现不存在ldap_domains表，即bitnami_mediawiki不存在这个表，后来尝试在phpmyAdmin上为bitnami_mediawiki创建ldap_domains表，并在表内创建user_id和domain列，问题解决。 备注：根据多次实验和页面错误提示，才确认建立user_id和domain列 5 备份备份数据库、配置文件、附件目录images、插件目录extensions 5.1 备份脚本123456789101112131415161718192021Now=$(date +"%Y%m%d%H")File=bitnami_mediawiki-$Now.sqlecho **********start$Now************ &gt;&gt; /mnt/mediawiki-bak/log/$Now.log/opt/mediawiki-1.26.2-2/mysql/bin/mysqldump -uroot -pDe123456 bitnami_mediawiki &gt; /mnt/mediawiki-bak/$Fileecho **********cp LocalSettings.php********** &gt;&gt; /mnt/mediawiki-bak/log/$Now.logrsync -avzrtopgL --progress /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/LocalSettings.php /mnt/mediawiki-bak/LocalSettings.phpecho *********cp images ******** &gt;&gt; /mnt/mediawiki-bak/log/$Now.logrsync -avzrtopgL --progress /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/images /mnt/mediawiki-bak/echo ********cp extensions ********* &gt;&gt; /mnt/mediawiki-bak/log/$Now.logrsync -avzrtopgL --progress /opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/extensions /mnt/mediawiki-bak/ &gt;&gt; /mnt/mediawiki-bak/log/$Now.logecho ********end$Now********* &gt;&gt; /mnt/mediawiki-bak/log/$Now.log 6 安装包及插件等 安装包：bitnami_mediawiki（自己下载）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>wiki</tag>
        <tag>Mediawiki</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mediawiki优化]]></title>
    <url>%2F2018%2F09%2F07%2FMediawiki%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1 修改URL后缀 修改网址显示（http://172.20.20.21/mediawiki/首页改为http://172.20.20.21/首页） 修改/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/ LocalSettings.php文件 12345$wgArticlePath = "/mediawiki/$1";改为$wgArticlePath = "/$1"; 再修改/opt/mediawiki-1.26.2-2/apache2/conf/bitnami/bitnami.conf 12345678910111213&lt;VirtualHost _default_:80&gt;DocumentRoot "/opt/mediawiki-1.26.2-2/apache2/htdocs"&lt;Directory "/opt/mediawiki-1.26.2-2/apache2/htdocs"&gt;改为&lt;VirtualHost _default_:80&gt;DocumentRoot "/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs"&lt;Directory "/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs"&gt; 2 修改网站LOGO2.1 右上角logo方法一,替换图片文件。 1/opt/mediawiki-1.26.2-2/apps/mediawiki/htdocs/resources/assets/wiki.png 方法二，修改LocalSettings.php文件，重新指定LOGO路径 1$wgLogo = "$wgScriptPath/resources/assets/huahe.png"; 2.2 左下角在配置文件LocalSettings.php中加入如下行即unset($wgFooterIcons[&#39;poweredby&#39;]); 3 修改目录悬浮加自动隐藏让目录悬浮起来，并且在不用时让它自动折叠起来，方便阅读和其他操作。自动折叠通过CSS的hover选择器实现，当鼠标移动到目录上时，目录框自动变大。 代码 先进入到下面页面（也许你需要将localhost替换成其他的）： http://localhost/mediawiki/index.php/MediaWiki:Common.css 在此页你可以设置全局的css样式，在这里加入如下： 12345678910111213141516171819202122232425262728293031323334353637383940#toc&#123; display: block; position: fixed; top: 100px; right: 0px; min-width: 100px; max-width: 350px; max-height: 20px; overflow-y: scroll; border: 1px solid #aaa; border-radius: 0 0 1px 1px; -moz-border-radius: 0 0 1px 1px; background: rgba(249,249,249,0.75); padding: 12px; box-shadow: 0 1px 8px #; -webkit-box-shadow: 0 1px 8px #; -moz-box-shadow: 0 1px 8px #;&#125; #toc:hover&#123; display: block; position: fixed; top: 100px; right: 0px; min-width: 100px; max-width: 350px; max-height: 500px; overflow-y: scroll; border: 1px solid #aaa; border-radius: 0 0 1px 1px; -moz-border-radius: 0 0 1px 1px; background: rgba(249,249,249,0.75); padding: 12px; box-shadow: 0 1px 8px #; -webkit-box-shadow: 0 1px 8px #; -moz-box-shadow: 0 1px 8px #; &#125; body &#123; overflow-x: hidden;&#125; 保存，清除浏览器缓存，看看如何！ 简直炫酷！。 关键点解释 1234567891011top: 100px; 目录框到顶部距离right: 0px; 目录框到右边框距离min-width: 100px; 目录框最小宽度max-width: 350px; 目录框最大宽度max-height: 500px; 目录框最大高度background: rgba(249,249,249,0.75); 背景色和透明度 MediaWiki版本 1.20.2 参考http://blog.klniu.com/post/mediawiki-floating-directory-and-scroll/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mediawiki</tag>
        <tag>wiki</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bitnami_Liferay系统设置去掉后缀]]></title>
    <url>%2F2018%2F09%2F07%2FBitnami-Liferay%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE%E5%8E%BB%E6%8E%89%E5%90%8E%E7%BC%80%2F</url>
    <content type="text"><![CDATA[1 修改配置文件 执行以下操作： 12345mv /opt/liferay-6.2-7/apache-tomcat/webapps/ROOT/ /opt/liferay-6.2-7/apache-tomcat/webapps/ROOT.backupmv /opt/liferay-6.2-7/apache-tomcat/webapps/liferay /opt/liferay-6.2-7/apache-tomcat/webapps/ROOTsed -i 's/portal.ctx/#portal.ctx/g' /opt/liferay-6.2-7/apache-tomcat/webapps/ROOT/WEB-INF/classes/portal-ext.properties 打开/opt/liferay-6.2-7/apps/liferay/conf/httpd-app.conf文件做如下修改： 12345678910111213141516171819202122232425262728293031323334353637将&lt;Location /liferay&gt; ProxyPass ajp://localhost:8009/liferay &lt;IfModule pagespeed_module&gt; ModPagespeedDisallow "*" &lt;/IfModule&gt;&lt;/Location&gt; 改为：&lt;Location /&gt; ProxyPass ajp://localhost:8009/ &lt;IfModule pagespeed_module&gt; ModPagespeedDisallow "*" &lt;/IfModule&gt;&lt;/Location&gt; 并且在最后加上：# App url redirectRewriteEngine OnRedirectMatch ^/$ / 2 修改数据库访问bitnami_liferay数据库，找到journalarticle表，用： 1update journalarticle set content=REPLACE (content,'/liferay/document','/document') 对表中content字段的记录进行批量替换 最后手工修改“规章制度”和“文档模板”的链接 现在访问http://172.20.20.58已经可以正常显示了 参考：https://wiki.bitnami.com/Applications/BitNami_Liferay#Manual_Approach]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>liferay</tag>
        <tag>bitnami</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liferay6.2-ce4官方原版安装简述]]></title>
    <url>%2F2018%2F09%2F07%2FLiferay6-2-ce4%E5%AE%98%E6%96%B9%E5%8E%9F%E7%89%88%E5%AE%89%E8%A3%85%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1 软件： Tomcat: apache-tomcat-7.0.61.tar.gz Java: java version “1.7.0_76” Liferay: liferay-portal-6.2-ce-ga4-20150416163831865.war 2 简单步骤 解压java、tomcat,设置环境变量 创建liferay目录，把java、tomcat放进这个目录 下载liferay依赖包，放到tomcat/lib/ext目录（重要） 新建setenc.sh 新建ROOT.xml 修改catalina.properties、server.xml]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>liferay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bitnami_Liferay自定义路径]]></title>
    <url>%2F2018%2F09%2F07%2FBitnami-Liferayx%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[1 修改文件存储路径 #配置文件portal-ext.properties 修改文件存储路径 1[root@htest2 classes]# vi /opt/liferay-6.2-7/apache-tomcat/webapps/liferay/WEB-INF/classes/portal-ext.properties 移动存储目录/opt/liferay-6.2-7/apps/liferay/data/document_library到/drbd,修改portal-ext.properties内dl.store.file.system.root.dir=参数（没有这项手动添加参数） 1dl.store.file.system.root.dir=/drbd/document_library 2 修改数据库路径移动/opt/liferay-6.2-7/mysql/data/到/home/mysql/data 2.1 修改my.cnf参数1[root@htest2 home]# vi /opt/liferay-6.2-7/mysql/my.cnf 修改my.cnf数据库路径参数 12345datadir=/opt/liferay-6.2-7/mysql/data改为datadir=/home/mysql/data 2.2 修改ctl.sh路径参数1[root@htest2 drbd]# vi /opt/liferay-6.2-7/mysql/scripts/ctl.sh 123456789101112131415161718192021MYSQL_PIDFILE=/ opt/liferay-6.2-7/mysql/data/mysqld.pidMYSQL_START="/opt/liferay-6.2-7/mysql/bin/mysqld_safe --defaults-file=/opt/liferay-6.2-7/mysql/my.cnf --port=3306 --socket=/opt/liferay-6.2-7/mysql/tmp/mysql.sock --datadir=/opt/liferay-6.2-7/mysql/data --log-error=/opt/liferay-6.2-7/mysql/data/mysqld.log --pid-file=$MYSQL_PIDFILE --lower-case-table-names=1 "改为MYSQL_PIDFILE=/home/mysql/data/mysqld.pidMYSQL_START="/opt/liferay-6.2-7/mysql/bin/mysqld_safe --defaults-file=/opt/liferay-6.2-7/mysql/my.cnf --port=3306 --socket=/opt/liferay-6.2-7/mysql/tmp/mysql.sock --datadir=/home/mysql/data --log-error=/home/mysql/data/mysqld.log --pid-file=$MYSQL_PIDFILE --lower-case-table-names=1 " 3 修改插件目录路径(暂不修改)3.1 修改catalina.bat参数移动/opt/liferay-6.2-7/apache-tomcat/temp到/drbd,修改catalina.bat参数 1[htest2@htest2 bin]$ vi /opt/liferay-6.2-7/apache-tomcat/bin/ catalina.bat 12345678910111213if not "%CATALINA_TMPDIR%" == "" goto gotTmpdirset "CATALINA_TMPDIR=%CATALINA_BASE%\temp":gotTmpdir改为if not "%CATALINA_TMPDIR%" == "" goto gotTmpdirset "CATALINA_TMPDIR=\drbd\temp":gotTmpdir 3.2 修改ctl.sh1[htest1@htest1 ~]$ vi /opt/liferay-6.2-7/apache-tomcat/scripts/ctl.sh 修改CATALINA_PID路径 12345CATALINA_PID=/opt/liferay-6.2-7/apache-tomcat/temp/catalina.pid改为CATALINA_PID=/drbd/temp/catalina.pid]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>liferay</tag>
        <tag>bitnami</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OTRS发送邮件设置]]></title>
    <url>%2F2018%2F09%2F07%2FOTRS%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[分三步: (图一)在系统配置里找到Framework -&gt; Core::Sendmail 配置你的 SMTP 信息, 如帐号, 密码, 邮件服务器地址直达地址:https://your.otrs.host/otrs/in … ework 如果发送邮件失败，按图中的位置把邮箱地址写上 (图二)在系统管理找到配置邮件发送地址管理, 增加系统邮件地址直达地址:https://your.otrs.host/otrs/in … dress (图三)在队列调用该邮件地址直达地址:https://your.otrs.host/otrs/in … D%3D1 创建工单, 测试发送邮件, 如果无法发送请抓系统日志来看. 可能遇到的问题, 通知邮件发送不出去, 请使用本网站的搜索功能, 此问题已经讨论过很多了. 如果你找很久(骂前面解决问题后失踪人)才找到, 那么请对有用的答案点赞, 这样就会影响问题的先后排序]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>otrs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JIRA6.3.6部署及破解]]></title>
    <url>%2F2018%2F09%2F05%2FJIRA6-3-6%E9%83%A8%E7%BD%B2%E5%8F%8A%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1 环境： 操作系统：CentOS7.2 1151 数据库：mysql 5.6.35 Java: java 1.8.0_65 平台软件：JIRA 6.3.6 2 安装步骤2.1 安装JDK安装JDK 1.7或1.8，本次部署直接YUM安装，版本1.8. 2.2 安装数据库安装mysql5.6.35,并创建jira数据库。 2.3 下载JIRA下载地址：https://www.atlassian.com/software/jira/download-archives 下载atlassian-jira-6.3.6.tar.gz 2.4 安装JIRA123[root@jira-24 home]# tar zxvf atlassian-jira-6.3.6.tar.gz[root@jira-24 home]# mv atlassian-jira-6.3.6 /opt/jira 2.5 修改端口修改端口号为80 1[root@jira-24 home]# vi /opt/jira/conf/server.xml 2.6 创建及配置jira_home1[root@jira-24 home]# mkdir /home/jira_home 配置路径 1[root@jira-24 home]# vi /opt/jira/atlassian-jira/WEB-INF/classes/jira-application.properties 2.7 启动JIRA1[root@jira-24 home]# /opt/jira/bin/start-jira.sh 打开http://172.20.20.24 配置完数据库之后，在新界面录入程序标题，点击“向后”按钮 输入临时授权码，进行注册： 注册完之后，填写管理员账户和密码 2.8 汉化汉化包位置：\\172.20.20.14\Software\Development\jira 中文软件包下载完毕后，我们需要登陆到jira系统找到Add-ons–Manage add-ons–upload add-on，如下： 中文软件包安装完毕后，我们现在配置jira，如下： 通过上图，我们可以很明显的看出，jira已经被中文语言了。 2.9 破解JIRA破解文件位置：\\172.20.20.14\Software\Development\jira 到了这一步，就是破解的过程了，此时操作如下： 1）先将JIRA服务关掉，不必关闭浏览器。 2）解压破解包到你的硬盘指定目录下，然后按如下指令操作。 1、用atlassian-extras-2.2.2.jar替换你的JIRA的安装目录的\atlassian-jira\WEB-INF\lib同名jar包。 2、用atlassian-universal-plugin-manager-plugin-2.10.1.jar替换你的JIRA的安装目录的\atlassian-jira\atlassian-bundled-plugins中的同名jar包。 3、根据自己的情况，按照keytpl.txt的格式填写好自己的license。 点击右上角齿轮形状的管理图标，选择“系统”，再选择“授权”，看到使用日期不到1个月，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879填写授权码，授权码参数范例如下：Description=JIRA: Commercial,CreationDate=你的安装日期，格式（yyyy-mm-dd）,jira.LicenseEdition=ENTERPRISE,Evaluation=false,jira.LicenseTypeName=COMMERCIAL,jira.active=true,licenseVersion=2,MaintenanceExpiryDate=你想设置的失效日期如：2099-12-31,Organisation=joiandjoin,SEN=你申请到的SEN注意没有前缀LID,ServerID=你申请到的ServerID,jira.NumberOfUsers=-1,LicenseID=LID你申请到的SEN，注意LID前缀不要丢掉,LicenseExpiryDate=你想设置的失效日期如：2099-12-31,PurchaseDate=你的安装日期，格式（yyyy-mm-dd） 本次安装授权码实例为：Description=JIRA: Commercial,CreationDate=2017-01-25,jira.LicenseEdition=ENTERPRISE,Evaluation=false,jira.LicenseTypeName=COMMERCIAL,jira.active=true,licenseVersion=2,MaintenanceExpiryDate=2099-12-31,Organisation=pl,SEN=SEN-L9178727,ServerID=BIBT-L882-5ZZ3-F2IZ,jira.NumberOfUsers=-1,LicenseID=AAABdA0ODAoPeNp9UU1PwkAUvO+vaOJFD9vQRS2SbCK0a1JTwNBqIvGyloeswrZ5u0X011tojaDgcd7HvJl5J2kJzq3UDjt3WKvrse75pRMkaQU8n7wggJ7nRQHoxioDbUBMlVW55mKYivHdOEoEGZbLZ8DR7N4AGk498qpQun+qdyVmc2kglBb4hp62PMouSEOcfhQwlEvgwWgwEOMg6sXfLbEuFH7s7DHK2iTItZWZFQOpFvxtPS+l/lTX3mXbzfIlSQBXgFHI+1E/pXGnw+jFZNKmNyya1AILzKdlZt0NoCaf2XeJ4FaMagXcYgn12HHfB9I5ZKLSpy1oqbMjRv5R8yfE5k7lK47CRAxpfOX5HZ/5pAJ8r/APbWIlWkA+kwsDZIQvUisjt/5C0YtFL3kkAcK28vtdi1rBQyVoM8/2YoDKKRaoTJNgCCZDVWyZb6Nxz0kaCc5p/aCzp64jVnJRbm/Vmo+94FC4u8d39344a/wFDcT8NjAsAhRCNRjYwj4n0JHDJmMZWChYSaegEgIUd1blNKrDbGRX6QLiCbEyzoyRJnA=X02i6,LicenseExpiryDate=2099-12-31,PurchaseDate=2017-01-25 将以上授权码信息填入授权码输入框，点击“增加”按钮，如下所示： 之后，看到授权信息更新了，就表示破解成功，会看到如下成功信息, 如下图： 2.10 插件破解插件破解和软件破解方法不一样，插件破解需要破解包内自带的Key. 破解Key在我百度云内。 参考资料：http://www.cnblogs.com/yangxia-test/p/4448002.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>jira</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsync和sersync实时双向同步]]></title>
    <url>%2F2018%2F09%2F04%2Frsync-sersync%E5%AE%9E%E6%97%B6%E5%8F%8C%E5%90%91%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[1 环境 操作系统 Centos7.1 511 主服务器 172.20.20.111 从服务器 172.20.22.99 测试目的：实现主服务器/rsync目录与从服务器/rsync目录实时双向同步. 参考资料：http://blog.sina.com.cn/s/blog_9f4962b10102vqua.html http://402753795.blog.51cto.com/10788998/1713179 2 以下操作主从服务器都要操作2.1 关闭selinux123456[root@server1]# vi /etc/selinux/config #编辑防火墙配置文件#SELINUX=enforcing #注释掉SELINUX=disabled #增加[root@server1]# setenforce 0 #立即生效 2.2 开启防火墙tcp 873端口（Rsync默认端口）123[root@server1 ]# firewall-cmd --zone=public --add-port=873/tcp --permanent[root@server1 ]# firewall-cmd --reload 主服务做rsync服务端，从服务器做客户端 3 安装rsync（服务端）1Yum install rsync 3.1 编辑配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253Vi /etc/rsyncd.confuid = root #/rsync目录用户属性gid = root #/rsync目录用户组属性port = 873 #rsync同步端口号，默认873address = 172.20.20.111 #本机IP地址use chroot = yes #是否禁锢用户read only = no # no客户端可上传文件,yes只读write only = no # no客户端可下载文件,yes不能下载#list = yeshosts allow = 172.20.22.99 #指定可以联系的客户端主机名或IPhosts deny = * #指定拒绝访问的客户端主机名或IPmax connections = 50 # 客户端最大连接数目（设置大些，否则同步中可能报错）motd file = /etc/rsyncd.motd pid file = /var/run/rsyncd.pid #启动后将进程PID放入此文件log file = /var/log/rsyncd.log # rsync使用syslog输出日志lock file = /var/run/rsync.lock #设置rsync锁文件transfer logging = yeslog format = %t%a%m%bsyslog facility = local3timeout = 300 #超时时间[liferay1] # 要同步的模块名path =/rsync # 要同步的目录（客户端同步文件到哪个目录）list = yes ignore errorsauth users = root # 登陆系统使用的用户名，没有默认为匿名（非主机用户，自定义）secrets file = /etc/rsyncd.pass1 ## 密码文件存放的位置comment = linuxsir liferay1 # 这个名名称无所谓，最后模块名一直 3.2 配置密码文件密码文件为配置文件中所写的文件/etc/rsyncd.secrets格式为**账户:密码** 1[root@server1 ]# vi /etc/rsyncd.pass1 输入帐号密码（自定义） 3.3 修改配置文件及密码文件权限(必须600)123 # chmod 600 /etc/rsyncd.conf # chmod 600 /etc/rsyncd.pass1 3.4 检查rsync是否启动123# lsof -i :873 或# netstat -an |grep 873 4 配置从服务器(客户端)4.1 设定密码文件配置密码文件 (注：为了安全，设定密码档案的属性为：600。rsync.pass1的密码一定要和Rsync 服务器端/etc/rsyncd.pass1的设定的密码一样) 1# vi /etc/rsyncd.pass1 密码文件可与服务端密码文件不一样，这里为了便于记忆，都设置为rsyncd.pass1 客户端密码文件只输入密码，不输入帐号。 4.2 赋予600权限1# chmod 600 /etc/rsyncd.pass1 # 必须修改权限 4.3 测试1$ rsync -avzP --password-file=/etc/rsyncd.pass1 /opt/liferay/data/ tongbu@172.20.20.111::liferay1 从客户端同步/rsync目录到服务端 4.4 安装sersync下载地址：https://code.google.com/archive/p/sersync/downloads 下载sersync2.5.4_64bit_binary_stable_final.tar 4.4.1 解压1Tar –xvf sersync2.5.4_64bit_binary_stable_final.tar 解压文件到/usr/local,重命名为serync 4.4.2 修改配置文件1Vi /usr/local/serync/confxml.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;&lt;head version="2.5"&gt; &lt;host hostip="localhost" port="8008"&gt;&lt;/host&gt; &lt;debug start="true"/&gt; &lt;fileSystem xfs="false"/&gt; &lt;filter start="false"&gt; #设置为true，开启同步过滤，这里不开启 &lt;exclude expression="(.*)\.svn"&gt;&lt;/exclude&gt; &lt;exclude expression="(.*)\.gz"&gt;&lt;/exclude&gt; &lt;exclude expression="^info/*"&gt;&lt;/exclude&gt; &lt;exclude expression="^static/*"&gt;&lt;/exclude&gt; &lt;/filter&gt; &lt;inotify&gt; &lt;delete start="true"/&gt; &lt;createFolder start="true"/&gt; &lt;createFile start="true"/&gt; &lt;closeWrite start="true"/&gt; &lt;moveFrom start="true"/&gt; &lt;moveTo start="true"/&gt; &lt;attrib start="false"/&gt; &lt;modify start="false"/&gt; &lt;/inotify&gt; &lt;sersync&gt; &lt;localpath watch="/rsync"&gt; &lt;remote ip="172.20.20.111" name="liferay1"/&gt; &lt;!--&lt;remote ip="192.168.8.39" name="tongbu"/&gt;--&gt; &lt;!--&lt;remote ip="192.168.8.40" name="tongbu"/&gt;--&gt; &lt;/localpath&gt; &lt;rsync&gt; &lt;commonParams params="-artuz"/&gt; &lt;auth start="true" users="root" passwordfile="/etc/rsyncd.pass1"/&gt; &lt;userDefinedPort start="false" port="874"/&gt;&lt;!-- port=874 --&gt; &lt;timeout start="false" time="100"/&gt;&lt;!-- timeout=100 --&gt; &lt;ssh start="false"/&gt; &lt;/rsync&gt; &lt;failLog path="/usr/local/sersync/rsync_fail_log.sh" timeToExecute="60"/&gt;&lt;!--default every 60mins execute once--&gt; &lt;crontab start="false" schedule="600"&gt;&lt;!--600mins--&gt; &lt;crontabfilter start="false"&gt; &lt;exclude expression="*.php"&gt;&lt;/exclude&gt; &lt;exclude expression="info/*"&gt;&lt;/exclude&gt; &lt;/crontabfilter&gt; &lt;/crontab&gt; &lt;plugin start="false" name="command"/&gt; &lt;/sersync&gt; &lt;plugin name="command"&gt; &lt;param prefix="/bin/sh" suffix="" ignoreError="true"/&gt; &lt;!--prefix /opt/tongbu/mmm.sh suffix--&gt; &lt;filter start="false"&gt; &lt;include expression="(.*)\.php"/&gt; &lt;include expression="(.*)\.sh"/&gt; &lt;/filter&gt; &lt;/plugin&gt; &lt;plugin name="socket"&gt; &lt;localpath watch="/opt/tongbu"&gt; 123456789101112131415161718修改的代码如下:&lt;delete start="true"/&gt;&lt;createFolder start="true"/&gt;&lt;createFile start="true"/&gt;&lt;closeWrite start="true"/ #对于大多数应用，可以尝试把createFile（监控文件事件选项）设置为false来提高性能，减少 rsync通讯。因为拷贝文件到监控目录会产生create事件与close_write事件，所以如果关闭create事件，只监控文件拷贝结束时的事件close_write，同样可以实现文件完整同步。 注意：强将createFolder保持为 true，如果将createFolder设为false，则不会对产生的目录进行监控，该目录下的子文件与子目录也不会被监控。所以除非特殊需要，请开启。默认情况下对创建文件（目录）事件与删除文件（目录）事件都进行监控，如果项目中不需要删除远程目标服务器的文件（目录），则可以将delete 参数设置为false，则不对删除事件进行监控。对于大多数应用，可以尝试把createFile（监控文件事件选项）设置为false来提高性能，减少 rsync通讯。因为拷贝文件到监控目录会产生create事件与close_write事件，所以如果关闭create事件，只监控文件拷贝结束时的事件close_write，同样可以实现文件完整同步。 注意：强将createFolder保持为 true，如果将createFolder设为false，则不会对产生的目录进行监控，该目录下的子文件与子目录也不会被监控。所以除非特殊需要，请开启。默认情况下对创建文件（目录）事件与删除文件（目录）事件都进行监控，如果项目中不需要删除远程目标服务器的文件（目录），则可以将delete 参数设置为false，则不对删除事件进行监控。&lt;localpath watch="/rsync"&gt;： #本地监听目录，源服务器同步目录&lt;remote ip="172.20.20.111" name="liferay1"/&gt;： #服务端IP地址和模块名&lt;auth start="true" users="root" passwordfile="/etc/rsyncd.pass1"/&gt;#服务端模块rsync认证用户名,服务端rsync认证用户的密码存放路径failLog path="/tmp/rsync_fail_log.sh" #脚本运行失败日志路径 4.4.3 创建日志文件1Vi /usr/local/serync/ rsync_fail_log.sh 4.4.4 启动sersync1/usr/local/sersync/sersync2 -d -r -o /usr/local/sersync/confxml.xml 此时客户端可以实施同步目录文件到服务端了。 5 实现双向同步把主从服务器角色互换，从服务器作为服务端，主服务器作为客户端重新部署一次，这样就可以双向实时同步了。 6 附件：=sersync2.5.4_64bit_binary_stable_final.tar.gz]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>rsync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux运维工具]]></title>
    <url>%2F2018%2F09%2F04%2FLinux%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[1、linux常用的监控命令 1.1. top显示所有正在运行而且处于活动状态的实时进程， 而且会定期更新显示结果；它显示了CPU使用率，内存使用率，交换内存使用大小，调整缓存使用大小，缓冲区使用大小，进程PID， 使用的命令等信息。 1.2. vmstat 123456789101112131415161718192021222324252627一般是通过两个数字参数来完成的，第一个参数是采样时间间隔，单位是秒， 第二个参数是采样的次数r: 表示运行队列，如果队列过大说明CPU很繁忙，一般会造成CPU使用率高b: 表示阻塞的进程数swap: 虚拟内存已使用的大小，如果大于0，说明机器的物理内存不够了free: 空闲的物理内存大小buff: 系统占用的缓存大小（写缓存）cache： 直接用来记忆我们打开的文件，给文件做缓冲，读缓存si: 每秒从磁盘读入虚拟内存大小，如果这个值大于0，表示物理内存不足了so: 每秒虚拟内存写入磁盘的大小，如果这个值大于0， 表示物理内存不足了us: 用户cpu时间sy: 系统CPU时间， 如果值 太高，说明系统调用，例如是IO操作频繁id: 空闲CPU时间，一般来说 id + us + sy = 100wt: 等待IO的CPU时间 1.3. lsof列出打开的文件；它常用于以列表形式显示所有打开的文件和进程，包括磁盘文件，网络套接字，管道，设备和进程。 主要情形之一就是 无法挂载磁盘和显示正在使用或者打开某个文件的错误时，查看谁正在使用。 1.4. tcpdumpapt-get install tcpdump 用于捕捉或过滤网络上指定接口上接收或者传输的TCP/IP包。 -i : 网络接口 -c ： 需要输出包数量 1.5. netstat用于监控进出网络的包和网络接口统计的命令行工具，非常有用，用来监控网络性能，解决网络相关问题。 -h : 查看帮助 -r : 显示路由表 -i : 查看网络接口 1.6. Htop一个非常高级的交互式实时linux进程监控工具，和top相似，但更友好, 还支持鼠标。 sudo apt-get install htop 1.7. iotop监控linux磁盘I/O, 用于查找大量使用磁盘读写进程的时候。python版本需要2.7以上。 1$ apt-get install iotop -h: 查看帮助 1.8. iostat查看存储设备输入和输出状态统计的工具，用来追踪存储设备的性能 问题；包括设备，磁盘，NFS远程磁盘。 sudo apt-get install sysstat 123456789101112131415161718192021%user: 在用户级别运行所使用的CPU百分比%nice: 优先进程消耗的CPU时间，占所有CPU百分比%system: 在系统级别运行所使用的CPU百分比%iowait: cpu等待硬件I/O时，所占用的CPU百分比%steal: 管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比%idle: CPU空闲时间的百分比tps: 每秒发送到I/O的请求数KB_read/s: 每秒读取的block数KB_wrtn/s: 每秒写入的block数KB_read: 启动到现在block总数KB_wrtn: 启动到现在写入的block总数 1.9. iptraf用于采集通过网络接口的IP流量信息，包括tcp标记，icmp信息，TCP，UDP信等。 123$ sudo apt-get install iptraf$ sudo iptraf 1.10. nethogs监控每个进程使用的网络带宽 123$ sudo apt-get install nethogs$ sudo nethogs 1.11. iftop监控网络接口的应用网络带宽使用情况 123$ sudo apt-get install iftop$ sudo iftop 12345678910111213=&gt; : 表示 流量方向TX： 发送的流量RX： 接收的流量TOTAL： 总流量Cumm: 运行iftop到目前总流量peak: 流量峰会rates: 分别表示 过去2秒，10秒，40秒的平均流量 1.12. system monitor监控cpu,内存，进程，硬盘的信息；分为进程监控，资源监控，文件监控; 遗憾的是需要图形界面支持。 123sudo apt-get install gnome-system-monitorgnome-system-monitor 2、nmtui配置网卡使用nmtui命令（上一篇博客里有介绍界面）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>运维工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openvpn基于LDAP认证下进行包过滤（控制访问权限）]]></title>
    <url>%2F2018%2F08%2F31%2FOpenvpn%E5%9F%BA%E4%BA%8ELDAP%E8%AE%A4%E8%AF%81%E4%B8%8B%E8%BF%9B%E8%A1%8C%E5%8C%85%E8%BF%87%E6%BB%A4-%E6%8E%A7%E5%88%B6%E8%AE%BF%E9%97%AE%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[1 说明 方案：采用minimal_pf.so模块和包过滤 此方法同样适用于基本认证。参考资料：http://backreference.org/2010/06/18/openvpns-built-in-packet-filter/ http://732233048.blog.51cto.com/9323668/1713088 2 安装openvpn及设置LDAP认证详细步骤参考本站 openvpn部署之部署基于ad域认证访问内网 3 控制访问权限3.1 创建及编辑minimal_pf.c模块123[root@openvpn ~]# cd /etc/openvpn/[root@openvpn openvpn]# vim minimal_pf.so 键入以下内容（内容固定） 123456789101112131415161718192021222324252627282930313233343536373839404142434445/* minimal_pf.c * ultra-minimal OpenVPN plugin to enable internal packet filter */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; #include "include/openvpn-plugin.h" /* dummy context, as we need no state */struct plugin_context &#123; int dummy;&#125;; /* Initialization function */OPENVPN_EXPORT openvpn_plugin_handle_t openvpn_plugin_open_v1 (unsigned int *type_mask, const char *argv[], const char *envp[]) &#123; struct plugin_context *context; /* Allocate our context */ context = (struct plugin_context *) calloc (1, sizeof (struct plugin_context)); /* Which callbacks to intercept. */ *type_mask = OPENVPN_PLUGIN_MASK (OPENVPN_PLUGIN_ENABLE_PF); return (openvpn_plugin_handle_t) context;&#125; /* Worker function */OPENVPN_EXPORT int openvpn_plugin_func_v2 (openvpn_plugin_handle_t handle, const int type, const char *argv[], const char *envp[], void *per_client_context, struct openvpn_plugin_string_list **return_list) &#123; if (type == OPENVPN_PLUGIN_ENABLE_PF) &#123; return OPENVPN_PLUGIN_FUNC_SUCCESS; &#125; else &#123; /* should not happen! */ return OPENVPN_PLUGIN_FUNC_ERROR; &#125;&#125; /* Cleanup function */OPENVPN_EXPORT void openvpn_plugin_close_v1 (openvpn_plugin_handle_t handle) &#123; struct plugin_context *context = (struct plugin_context *) handle; free (context);&#125; 3.2 构建插件下载并解压OpenVPN源码压缩包（严格来说，只需要openvpn-plugin.h） openvpn源码安装包：openvpn-2.3.11.tar.gz ，解压文件，复制include目录到/etc/openvpn` 并使用以下命令构建插件： 12345INCLUDE="-I/etc/openvpn" # CHANGE THIS!!!!CC_FLAGS="-O2 -Wall -g"NAME=minimal_pfgcc $CC_FLAGS -fPIC -c $INCLUDE $NAME.c &amp;&amp; \gcc $CC_FLAGS -fPIC -shared -Wl,-soname,$NAME.so -o $NAME.so $NAME.o -lc 3.3 创建包过滤文件123mkdir /etc/openvpn/ccdcd /etc/openvpn/ccd 创建以用户名.pf命名的文件，输入以下内容。 1234567891011vi client1.pf #客户client1，只对10.10.1.0网段有权限[CLIENTS ACCEPT][SUBNETS DROP]+10.10.1.0/24[END]vi client.pf #客户client，对所有内网服务器都有权限[CLIENTS ACCEPT][SUBNETS ACCEPT][END] 3.3.1 包过滤文件补充包过滤文件格式： 12345678910111213141516171819202122232425[CLIENTS DROP|ACCEPT]&#123;+|-&#125;common_name1&#123;+|-&#125;common_name2 . . .[SUBNETS DROP|ACCEPT]&#123;+|-&#125;subnet1&#123;+|-&#125;subnet2 . . .[END]过滤文件语法：CLIENTS部分用于定义common name；SUBNETS部分用于定义IP地址、IP网段；DROP|ACCEPT用于设置默认规则，就是没有明确指明的common name，那么他们将会使用；&#123;+|-&#125;用于设置是否允许，如果是“+”，那么表示允许，如果是“-”则表示不允许；[END]表示策略文件的结束cat client10.pf[CLIENTS ACCEPT][SUBNETS ACCEPT]-192.168.9.7+192.168.9.0/24[END] 注意事项： 创建过滤文件时，允许访问的地址写到上面，禁止访问的地址写在后面。如果先禁止访问网段，在允许访问网段IP地址，依然受限。 例如： 3.4 创建客户端连接脚本123cd /etc/openvpnvim client-connect.sh 1234567891011121314#!/bin/sh # /etc/openvpn/client-connect.sh: sample client-connect script using pf rule files # rules template filetemplate="/etc:wq/openvpn/ccd/$&#123;common_name&#125;.pf" # create the file OpenVPN wants with the rules for this clientif [ -f "$template" ] &amp;&amp; [ ! -z "$pf_file" ]; then cp -- "$template" "$pf_file"else # if anything is not as expected, fail exit 1fi 3.5 修改openvpn配置文件1vim /etc/openvpn/server.conf 12345678910111213141516171819202122232425262728293031323334353637383940ca /etc/openvpn/easy-rsa/keys/ca.crtcert /etc/openvpn/easy-rsa/keys/server.crtkey /etc/openvpn/easy-rsa/keys/server.keydh /etc/openvpn/easy-rsa/keys/dh2048.pemserver 10.8.0.0 255.255.255.0ifconfig-pool-persist ipp.txt;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100;push "redirect-gateway def1 bypass-dhcp";push "redirect-gateway";push "dhcp-option DNS 172.20.20.10";push "dhcp-option DNS 202.102.224.68";push "route 0.0.0.0 0.0.0.0"push "route 172.20.20.0 255.255.255.0"push "route 172.20.22.0 255.255.255.0"push "route 172.20.10.0 255.255.255.0"push "route 172.20.19.0 255.255.255.0"push "route 10.224.255.224 255.255.255.224";push "route 172.20.18.0 255.255.255.0";push "route 172.20.17.0 255.255.255.0";push "dhcp-option DNS 172.20.20.10";push "dhcp-option DNS 202.102.224.68"client-to-clientduplicate-cnkeepalive 10 120#tls-auth /etc/openvpn/easy-rsa/ta.key 0comp-lzomax-clients 10persist-keypersist-tunstatus openvpn-status.loglog /var/log/openvpn.loglog-append /var/log/openvpn-append.logverb 3plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so "/etc/openvpn/auth/ldap.conf"client-cert-not-requiredusername-as-common-nameclient-config-dir /etc/openvpn/ccd #添加plugin /etc/openvpn/minimal_pf.so #添加 client-connect /etc/openvpn/client-connect.sh #添加#script-security 3 3.6 重启openvpn服务器1/etc/init.d/openvpn restart 配置完成。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>openvpn</tag>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql命令示例]]></title>
    <url>%2F2018%2F08%2F31%2FMysql%E5%91%BD%E4%BB%A4%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[1 创建数据库 1Mysql&gt;create database 库名; 1.1 创建utf-8格式的数据库1Mysql&gt;CREATE DATABASE `库名` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 2 删除数据库1Mysql&gt;drop database 库名; 3 查询数据库1Mysql&gt;show databases; 4 恢复数据库123Mysql&gt;use 数据库名 //修改要恢复的数据库Mysql&gt;source 数据文件名（可以带路径）； 例如： 1Mysql&gt;source /home/123sql; 5 修改用户密码5.1 修改root密码5.1.1 方法1： 用SET PASSWORD命令首先登录MySQL。 格式：mysql&gt; set password for 用户名@localhost = password(‘新密码’); 例子： 1mysql&gt; set password for root@localhost = password('123'); 5.1.2 方法2：用mysqladmin 格式：mysqladmin -u用户名 -p旧密码 password 新密码 例子： 1mysqladmin -uroot -p123456 password 123 5.1.3 方法3：用UPDATE直接编辑user表首先登录MySQL。 123mysql&gt; use mysql;mysql&gt; update user set password=password('123') where user='root' and host='localhost';mysql&gt; flush privileges; 5.1.4 方法4：在忘记root密码的时候，可以这样以windows为例： 关闭正在运行的MySQL服务。 打开DOS窗口，转到mysql\bin目录。 输入mysqld –skip-grant-tables 回车。–skip-grant-tables 的意思是启动MySQL服务的时候跳过权限表认证。 再开一个DOS窗口（因为刚才那个DOS窗口已经不能动了），转到mysql\bin目录。 输入mysql回车，如果成功，将出现MySQL提示符 &gt;。 连接权限数据库： use mysql; 。 改密码：update user set password=password(“123”) where user=”root”;（别忘了最后加分号） 。 刷新权限（必须步骤）：flush privileges; 。 退出 quit。 注销系统，再进入，使用用户名root和刚才设置的新密码123登录。 6 Mysql远程访问首先确认防火墙开启3306端口,如果my.cnf文件配置有bind-address=127.0.0.1，用#注释掉。 6.1 改表法：可能是你的帐号不允许从远程登陆，只能在localhost。这个时候只要在localhost的那台电脑，登入mysql后，更改 “mysql” 数据库里的 “user” 表里的“host” 项，从“localhost”改称“%” 登录数据库 1234mysql&gt; use mysql;mysql&gt; update user set host = ‘%’ where user = ‘root’;mysql&gt; select host, user from user; //列出host名和user名，查看修改。mysql&gt; flush privileges; //刷新权限列表注：mysql&gt; flush privileges; //使修改生效。 6.2 授权法（推荐）例如，你想myuser使用mypassword从任何主机连接到mysql服务器的话。 1mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'myuser'@'%' IDENTIFIED BY 'mypassword' WITH GRANT OPTION; 如果你想允许用户myuser从ip为192.168.1.3的主机连接到mysql服务器，并使用mypassword作为密码 1mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'myuser'@'192.168.1.3' IDENTIFIED BY 'mypassword’ WITH GRANT OPTION; 下面的语句表示将 discuz 数据库的所有权限授权给 myuser 这个用户，允许 myuser 用户在 123.123.123.123 这个 IP 进行远程登陆，并设置 myuser 用户的密码为 mypassword 。 1mysql&gt; GRANT ALL PRIVILEGES ON discuz.* TO 'myuser'@'192.168.1.3' IDENTIFIED BY 'mypassword’ WITH GRANT OPTION; 注：代表所有数据库，discuz. 代表discuz数据库。 7 Mysqlbinlog用法 注意：用mysqlbinlog前一定要加上数据库安装路径，否则默认情况下调用的是系统默认安装的mysql中的mysqlbinlog 7.1 查看二进制日志查看二进制日志mysql-bin.000014 1/usr/local/mysql/bin/mysqlbinlog /usr/local/mysql/data/mysql-bin.000014 7.2 转换二进制日志生成普通日志转换mysql-bin.000014为f.log,转换到88624位置为止。 1/usr/local/mysql/bin/mysqlbinlog -v --stop-position=88624 /usr/local/mysql/data/mysql-bin.000014 &gt; /home/f.log]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7下Mysql5.5数据同步]]></title>
    <url>%2F2018%2F08%2F30%2FCentos7%E4%B8%8BMysql5-5%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[1 环境 服务器 系统 服务 java mysql ip Liferay-a Centos7 Liferay+keepalive+sersync 1.7.0_80 5.5.42 172.20.20.59 Liferay-b Centos7 Liferay+keepalive+rsync 1.7.0_80 5.5.42 172.20.20.60 虚拟IP:172.20.20.58 2 安装liferay安装liferay官方版本6.2-ce4,方法自行度娘。恢复数据参考《liferay备份还原文档》 3 安装keepalive1Yum install keepalive 3.1 编辑配置文件1vi /etc/keepalived/keepalived.conf Liferay-a: Liferay-b: 启动keepalive，建立虚拟IP，主服务器当机，从服务器获得Ip. 4 mysql主从同步下载mysql-5.5.42-linux2.6-x86_64.tar.gz解压到/usr/local,重命名为mysql 4.1 编辑配置文件1Vi /etc/my.cnf Liferay-a: Liferay-b 4.2 建立mysql主从同步查看liferay-a(主服务器)，查看mysql（主）信息，并建立同步帐号： 1GRANT ALL PRIVILEGES ON bitnami_liferay.* TO 'tongbu'@'%' IDENTIFIED BY 'De123456' WITH GRANT OPTION; 进入liferay-b（从服务器），在mysql中输入命令，建立连接 1CHANGE MASTER TO MASTER_HOST='172.20.20.59', MASTER_USER='tongbu', MASTER_PASSWORD='De123456', MASTER_LOG_FILE='mysql-bin.000022',MASTER_LOG_POS=151424110; 输入命令， 查看备服务器信息 1SHOW SLAVE STATUS\G 4.3 恢复数据库通过mysql命令恢复bitnami_liferay数据库备份到lifreay-a(主服务器)，自动实时同步数据到从服务器。 5 目录同步5.1 安装rsync(liferay-b)只需在liferay-b服务器上安装rsync。 Liferay-b作为rsync服务器，lifray-a作为客户端，实时同步目录数据到liferay-bYum install rsync 5.1.1 编辑配置文件1vi /etc/rsyncd.conf liferay-b: 新建rsyncd.pass1密码文件在/etc/目录 文件格式 帐号:密码 设置文件权限为600（必须） 5.2 安装sersync（liferay-a）Liferay-a安装实时同步工具sersync2.5.4_64bit_binary_stable_final.tar.gz下载sersync2.5.4_64bit_binary_stable_final.tar.gz解压到/usr/local 5.2.1 编辑confxml.xml1vi confxml.xml 5.2.2 在/etc/目录创建密码文件rsyncd.pass1文件格式只填写密码（注意和同步服务器的密码文件内的密码一样） 5.3 启动sersync，开启实时同步1/usr/local/sersync/sersync2 -d -r -o /usr/local/sersync/confxml.xml]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix通过jmx监控tomcat]]></title>
    <url>%2F2018%2F08%2F30%2FZabbix%E9%80%9A%E8%BF%87jmx%E7%9B%91%E6%8E%A7tomcat%2F</url>
    <content type="text"><![CDATA[1 环境 名称 IP zabbix server 172.20.20.22 tomcat server 172.20.20.111 2 服务端安装2.1 安装zabbix_java_Gateway这里直接安装zabbix_java_gateway到zabbix server上面 1rpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-java-gateway-3.0.5-1.el7.x86_64.rpm 2.2 修改配置文件zabbix_java-gateway.修改zabbix_java_gateway.conf 1234LISTEN_IP="0.0.0.0"LISTEN_PORT=10052PID_FILE="/tmp/zabbix_java.pid"START_POLLERS=5 2.3 修改zabbix_server.conf添加如下几行 JavaGateway=127.0.0.1 //zabbix_server与zabbix_java_gateway在一台服务器上，这里指定java_gateway服务器地址为本机；JavaGatewayPort=10052StartJavaPollers=5 2.4 重启zabbix_server1[root@zabbixserver ~]# service zabbix-server restart 3 tomcat客户端配置添加如下代码到tomcat目录/bin/catalina.sh 12345CATALINA_OPTS="-Djava.rmi.server.hostname=172.20.20.111 //tomcat客户端ip地址-Djavax.management.builder.initial=-Dcom.sun.management.jmxremote=true-Dcom.sun.management.jmxremote.ssl=false-Dcom.sun.management.jmxremote.authenticate=false" 3.1 下载catalina-jmx-remote.jar因为tomcat服务器安装的java版本是1.7，所以这里下载的jar包是1.7的版本，不同版本的tomcat对应不同版本的catalina-jmx-remote.jar； 在http://tomcat.apache.org/download-70.cgi 找到以下JMX Remote jar,把这个文件放到tomcat安装目录的lib子目录下 Extras: JMX Remote jar (pgp, md5, sha1) //下载jmx包 Web services jar (pgp, md5, sha1) JULI adapters jar (pgp, md5, sha1) JULI log4j jar (pgp, md5, sha1) 3.2 修改tomcat安装目录conf子目录下的server.xml配置文件添加如下几行 12&lt;Listener className="org.apache.catalina.mbeans.JmxRemoteLifecycleListener" rmiRegistryPortPlatform="12345" rmiServerPortPlatform="12346" /&gt; 完整显示： 12345678910111213141516&lt;Server port="8005" shutdown="SHUTDOWN"&gt; &lt;Listener className="org.apache.catalina.startup.VersionLoggerListener" /&gt; &lt;!-- Security listener. Documentation at /docs/config/listeners.html &lt;Listener className="org.apache.catalina.security.SecurityListener" /&gt; --&gt; &lt;!--APR library loader. Documentation at /docs/apr.html --&gt; &lt;Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" /&gt; &lt;!--Initialize Jasper prior to webapps are loaded. Documentation at /docs/jasper-howto.html --&gt; &lt;Listener className="org.apache.catalina.core.JasperListener" /&gt; &lt;!-- Prevent memory leaks due to use of particular java/javax APIs--&gt; &lt;Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" /&gt; &lt;Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" /&gt; &lt;Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" /&gt; &lt;Listener className="org.apache.catalina.mbeans.JmxRemoteLifecycleListener" rmiRegistryPortPlatform="12345" rmiServerPortPlatform="12346" /&gt;省略... 3.3 重启tomcat.123[root@localhost apache-tomcat-7.0.69]# /opt/tomcat/bin/shutdown.sh [root@localhost apache-tomcat-7.0.69]# /opt/tomcat/bin/startup.sh 测试是否可以获得数据 测试需要cmdline-jmxclient-0.10.3.jar包，下载包 http://dl.bintray.com/typesafe/maven-releases/cmdline-jmxclient/cmdline-jmxclient/0.10.3/cmdline-jmxclient-0.10.3.jar 12345cd /homewget http://dl.bintray.com/typesafe/maven-releases/cmdline-jmxclient/cmdline-jmxclient/0.10.3/cmdline-jmxclient-0.10.3.jarjava -jar /home/cmdline-jmxclient-0.10.3.jar - 127.0.0.1:12345 java.lang:type=Memory NonHeapMemoryUsage 如上图显示，说明可以正常获得数据。 3.4 配置防火墙开放12345/12346端口 到这里配置完成。 参考资料： http://jaychang.iteye.com/blog/2214830 4 问题：按照网上的教程，修改tomcat目录中的catalina.sh ，无法再关闭防火墙的前提下正常获取到数据，是因为即使按照配置设定了端口，但实际中并不是只使用设置的端口。 5 配置例子： （关闭防火墙开启12345端口就无法获取数据） 12345CATALINA_OPTS="-Dcom.sun.management.jmxremote-Dcom.sun.management.jmxremote.authenticate=false-Dcom.sun.management.jmxremote.ssl=false-Dcom.sun.management.jmxremote.port=12345 #定义jmx监听端口-Djava.rmi.server.hostname=客户端IP"]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
        <tag>tomcat</tag>
        <tag>jmx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[workpress编辑器插件UEditor]]></title>
    <url>%2F2018%2F08%2F30%2Fworkpress%E7%BC%96%E8%BE%91%E5%99%A8%E6%8F%92%E4%BB%B6UEditor%2F</url>
    <content type="text"><![CDATA[百度UEditor-KityFormula for wordpress 2.0.2发布 自从UEditor-KityFormula-for-wordpress 2.0.1版本发布以来，用户反应较好，对某些使用者提出的没有百度地图、google地图及iframe,2.0.2版本进行了补充。现已发布，不过谷歌地图由于某些原因(被墙)，国内用不了，需要使用vpn，建议使用百度地图。 更新说明：应网友的要求，添加了百度地图、谷歌地图及iframe,属于小版本升级。 wordpress版本要求wordpress 4.3+ 安装说明：下载 zip版本，直接使用插件-&gt;安装插件-&gt;上传-&gt;启用插件 。 下载地址：A: 插件地址：http://www.yangshengliang.com/kaiyuan-shijie/zuopin/399.html B: 插件地址：http://www.geroro.com/archives/258 解压缩包到wordpress插件文件夹。最终的路径像这样：/wp-content/plugins/UEditor-KityFormula，也可以直接在后台通过插件上传进行安装。 附件： A:全屏左菜单栏遮挡 =UEditor-KityFormula-for-wordpress.zip B:较好，不全屏使用较好。=ueditor1.4.3.3forwordpress4.6.1.zip]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>workpress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openvpn+squid搭建通过http代理访问openvpn服务器]]></title>
    <url>%2F2018%2F08%2F30%2Fopenvpn-squid%E6%90%AD%E5%BB%BA%E9%80%9A%E8%BF%87http%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AEopenvpn%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1 环境： OS : Centos 7.2 x86_64 2 Squid部署1[root@bogon ~]# yum install squid 2.1 编辑配置文件1[root@bogon ~]# vim /etc/squid/squid.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374## Recommended minimum configuration:## Example rule allowing access from your local networks.# Adapt to list your (internal) IP networks from where browsing# should be allowedacl all src all #添加允许所有IP地址访问acl localnet src 10.0.0.0/8 # RFC1918 possible internal networkacl localnet src 172.16.0.0/12 # RFC1918 possible internal networkacl localnet src 192.168.0.0/16 # RFC1918 possible internal networkacl localnet src fc00::/7 # RFC 4193 local private network rangeacl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machinesacl SSL_ports port 443acl Safe_ports port 80 # httpacl Safe_ports port 21 # ftpacl Safe_ports port 443 # httpsacl Safe_ports port 70 # gopheracl Safe_ports port 210 # waisacl Safe_ports port 1025-65535 # unregistered portsacl Safe_ports port 280 # http-mgmtacl Safe_ports port 488 # gss-httpacl Safe_ports port 591 # filemakeracl Safe_ports port 777 # multiling httpacl CONNECT method CONNECT## Recommended minimum Access Permission configuration:## Deny requests to certain unsafe portshttp_access allow !Safe_ports ## Deny CONNECT to other than secure SSL portshttp_access allow CONNECT !SSL_ports #设置允许所有SSL通过(不设置的话无法连接到http服务器)# Only allow cachemgr access from localhosthttp_access allow localhost managerhttp_access allow manageraccess_log /var/log/squid/access.log combinedcache_log /var/log/squid/cache.log# We strongly recommend the following be uncommented to protect innocent# web applications running on the proxy server who think the only# one who can access services on "localhost" is a local user#http_access deny to_localhost## INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS## Example rule allowing access from your local networks.# Adapt localnet in the ACL section to list your (internal) IP networks# from where browsing should be allowed# And finally deny all other access to this proxyhttp_access allow all #修改允许所有人使用该代理.因为这里是代理加速web服务器http_reply_access allow all# Squid normally listens to port 3128#http_port 3128http_port 4346 #修改默认监听端口，即代理端口。# Uncomment and adjust the following to add a disk cache directory.#cache_dir ufs /var/spool/squid 100 16 256# Leave coredumps in the first cache dircoredump_dir /var/spool/squid## Add any of your own refresh_pattern entries above these.#refresh_pattern ^ftp: 1440 20% 10080refresh_pattern ^gopher: 1440 0% 1440refresh_pattern -i (/cgi-bin/|\?) 0 0% 0refresh_pattern . 0 20% 4320 2.2 启动squid及设置开机启动123[root@bogon ~]# systemctl start squid.service[root@bogon ~]# systemctl enable squid.service 2.3 openvpn客户端配置proxy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131############################################### Sample client-side OpenVPN 2.0 config file ## for connecting to multi-client server. ## ## This configuration can be used by multiple ## clients, however each client should have ## its own cert and key files. ## ## On Windows, you might want to rename this ## file so it has a .ovpn extension ################################################ Specify that we are a client and that we# will be pulling certain config file directives# from the server.client# Use the same setting as you are using on# the server.# On most systems, the VPN will not function# unless you partially or fully disable# the firewall for the TUN/TAP interface.;dev tapdev tun# Windows needs the TAP-Win32 adapter name# from the Network Connections panel# if you have more than one. On XP SP2,# you may need to disable the firewall# for the TAP adapter.;dev-node MyTap# Are we connecting to a TCP or# UDP server? Use the same setting as# on the server.proto tcp;proto udp# The hostname/IP and port of the server.# You can have multiple remote entries# to load balance between the servers.;remote 172.20.20.25 1194remote openvpn服务器地址或域名 1194;remote my-server-2 1194;http-proxy vpnproxy.dealeasy.com 4346http-proxy squid服务器地址 4346 #添加http代理# Choose a random host from the remote# list for load-balancing. Otherwise# try hosts in the order specified.;remote-random# Keep trying indefinitely to resolve the# host name of the OpenVPN server. Very useful# on machines which are not permanently connected# to the internet such as laptops.resolv-retry infinite# Most clients don't need to bind to# a specific local port number.nobind# Downgrade privileges after initialization (non-Windows only);user nobody;group nobody# Try to preserve some state across restarts.persist-keypersist-tun# If you are connecting through an# HTTP proxy to reach the actual OpenVPN# server, put the proxy server/IP and# port number here. See the man page# if your proxy server requires# authentication.;http-proxy-retry # retry on connection failures;http-proxy [proxy server] [proxy port #]# Wireless networks often produce a lot# of duplicate packets. Set this flag# to silence duplicate packet warnings.;mute-replay-warnings# SSL/TLS parms.# See the server config file for more# description. It's best to use# a separate .crt/.key file pair# for each client. A single ca# file can be used for all clients.ca ca.crt#cert client.crt#key client.keyauth-user-pass# Verify server certificate by checking that the# certicate has the correct key usage set.# This is an important precaution to protect against# a potential attack discussed here:# http://openvpn.net/howto.html#mitm## To use this feature, you will need to generate# your server certificates with the keyUsage set to# digitalSignature, keyEncipherment# and the extendedKeyUsage to# serverAuth# EasyRSA can do this for you.remote-cert-tls server# If a tls-auth key is used on the server# then every client must also have the key.;tls-auth ta.key 1# Select a cryptographic cipher.# If the cipher option is used on the server# then you must also specify it here.;cipher x# Enable compression on the VPN link.# Don't enable this unless it is also# enabled in the server config file.comp-lzo# Set log file verbosity.verb 3# Silence repeating messages;mute 20 现在可以通过http代理连接VPN服务器。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>openvpn</tag>
        <tag>squid</tag>
        <tag>vpn</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows重置网卡解决无法上网问题]]></title>
    <url>%2F2018%2F08%2F30%2Fwindows%E9%87%8D%E7%BD%AE%E7%BD%91%E5%8D%A1%E8%A7%A3%E5%86%B3%E6%97%A0%E6%B3%95%E4%B8%8A%E7%BD%91%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1 故障表现： 网卡获取地址正常，无法访问域名，Ping域名无法找到主机，表现为DNS故障。 1.2 解决办法1：管理员打开命令行输入以下命令 netsh int ip reset #重置ip配置 netsh winhttp reset proxy #重置代理 ipconfig /flushdns #重置DNS 1.3 解决办法2：（一般此方法解决问题）管理员打开命令行输入以下命令重置网络连接配置 netsh winsock reset]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>网卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装redis]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E5%AE%89%E8%A3%85redis%2F</url>
    <content type="text"><![CDATA[1 下载redis下载redis-3.2.9.tar.gz 12345wget http://download.redis.io/releases/redis-3.2.9.tar.gztar -zxvf redis-3.2.9.tar.gzmv redis-3.2.9 redis 2 安装redis123[root@bogon home]# cd redis/[root@bogon redis]# make &amp;&amp; make install 2.1 初始化redis123[root@bogon redis]# cd utils/[root@bogon utils]# ./install_server.sh 通过上图，我们可以看出redis初始化后redis配置文件为/etc/redis/6379.conf，日志文件为/var/log/redis_6379.log，数据文件dump.rdb存放到/var/lib/redis/6379目录下，启动脚本为/etc/init.d/redis_6379。 现在我们要使用systemd，所以在 /etc/systems/system 下创建一个单位文件名字为redis_6379.service。 1vim /etc/systemd/system/redis_6379.service 填写以下内容： 12345678[Unit]Description=Redis on port 6379[Service]Type=forkingExecStart=/etc/init.d/redis_6379 startExecStop=/etc/init.d/redis_6379 stop[Install]WantedBy=multi-user.target 2.2 查看redis版本1redis-cli --version 3 参考资料：http://www.cnblogs.com/sandea/p/5782192.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis+Keepalived双机热备]]></title>
    <url>%2F2018%2F08%2F30%2FRedis-Keepalived%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87%2F</url>
    <content type="text"><![CDATA[1 环境 主机 IP OS 主机 IP OS master-server 172.20.22.145 centos7.2 slave-server 172.20.22.53 centos7.2 2 参考资料https://my.oschina.net/guol/blog/182491 http://blog.csdn.net/qguanri/article/details/51120178 http://blog.csdn.net/zgf19930504/article/details/52024724 3 整体思路在keepalived+redis的使用过程中有四种情况： 一种是keepalived挂了，同时redis也挂了，这样的话直接VIP飘走之后，是不需要进行redis数据同步的，因为redis挂了，你也无法去master上同步，不过会损失已经写在master上却还没同步到slave上面的这部分数据。 另一种是keepalived挂了，redis没挂，这时候VIP飘走后，redis的master/slave还是老的对应关系，如果不变化的话会把数据写入redis slave中，从而不会同步到master上去，这就要借助监控脚本反转redis的master/slave关系。这时候就要预留一点时间进行数据同步，然后反转master/slave。 还有一种是keepalived没挂，redis挂了，这时候根据监控脚本会检测到redis挂了，并且降低keepalived master的优先级，同样会导致VIP飘走，情况和第二种一样，也是需要进行数据同步，然后反转当前redis的master/slave关系的。 随后一种是keepalived没挂，redis也没挂，大吉大利啊，什么都不用操作。 本文的实验环境四种情况都适合，第一种是不需要同步数据的，脚本会默认去同步数据，但是其实是不会成功的。脚本主要是用来处理第二和第三种情况的。 4 安装redis参考本站：centos7安装redis 4.1 修改主从redis配置文件1vim /etc/redis/6379.conf 4.1.1 主redis:12#修改bind绑定IP,只有绑定的IP才能访问redis，0.0.0.0标识所有地址都能访问。bind 0.0.0.0 4.1.2 从redis:1234#修改bind绑定IP,只有绑定的IP才能访问redis，0.0.0.0标识所有地址都能访问。bind 0.0.0.0#添加slaveof&amp;nbsp;IP&amp;nbsp;Port,表示作为该IP服务器的备机slaveof 172.20.22.145 6379 4.2 启动并测试1service redis_6379 start 4.2.1 主redis1234567891011[root@bogon ~]# redis-cli -p 6379 #登录redis127.0.0.1:6379&gt; set hao 333 #添加hao值为333 OK127.0.0.1:6379&gt; get hao #查看hao的值"333"127.0.0.1:6379&gt; 4.2.2 从redis1234567[root@bogon scripts]# redis-cli -p 6379127.0.0.1:6379&gt; get hao #查看hao的值，获取到值说明同步成功。"333"127.0.0.1:6379&gt; 5 安装keepalived1yum install keepalived 5.1 修改配置文件1vim /etc/keepalived/keepalived.conf 1234567891011121314151617181920212223242526272829303132333435#MASTER配置文件：! Configuration File for keepalivedvrrp_script chk_redis &#123; script "/etc/keepalived/scripts/chk_redis.sh" #监控脚本 interval 2 #监控间隔时间 秒 timout 2 #响应超时：超过多长时间未响应认为是失败 fall 3 #检测失败几次，认为是redis服务器挂了 weight -60 #自我确定服务器挂了之后,优先级加多少, 也就是宕机之后 priority 加多少weight，这里是减60&#125;vrrp_instance VI_1 &#123; state MASTER #MASTER表示主，BACKUP表示为从 interface eth0 #eth0表示监听的网卡 virtual_router_id 51 priority 100 #优先级，从服务器推选主服务器就是根据这个来比较的，从服务器必须小于主服务器优先级。 advert_int 1 authentication &#123; auth_type PASS #认证凭证，可自定义 auth_pass 1111 &#125; track_script &#123; chk_redis #运行chk_redis模块 &#125; virtual_ipaddress &#123; 172.20.22.199 #VIP地址（虚拟IP） &#125; unicast_src_ip 172.20.22.145 #keepalived 内部通信,本机ip 地址（没加也没影响） unicast_peer &#123; 172.20.22.53 &#125; notify_master /etc/keepalived/scripts/redis-master.sh #keepalived 状态切换master时执行的脚本 notify_backup /etc/keepalived/scripts/redis-backup.sh #keepalived 状态切换为backup时执行的脚本 notify_fault /etc/keepalived/scripts/redis-fault.sh #keepalived 状态为fault时执行的脚本 notify_stop /etc/keepalived/scripts/redis-stop.sh #keepalived 服务停止时执行脚本&#125; 1234567891011121314151617181920212223242526272829303132333435363738# BACKUP配置文件backup配置文件至修改了2处，其它都一样。! Configuration File for keepalivedvrrp_script chk_redis &#123; script "/etc/keepalived/scripts/chk_redis.sh" ###监控脚本 interval 2 ###监控时间 timout 2 #响应超市：超过多长时间未响应认为是失败 fall 3 #检测失败几次，认为是redis服务器挂了 weight -60 #自我确定服务器挂了之后,优先级加多少, 也就是宕机之后 priority + weight&#125;vrrp_instance VI_1 &#123; state BACKUP interface enp2s0virtual_router_id 51 priority 50 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_redis &#125; virtual_ipaddress &#123; 172.20.22.199 &#125; unicast_src_ip 172.20.22.53 unicast_peer &#123; 172.20.22.145 &#125; notify_master /etc/keepalived/scripts/redis-master.sh notify_backup /etc/keepalived/scripts/redis-backup.sh notify_fault /etc/keepalived/scripts/redis-fault.sh notify_stop /etc/keepalived/scripts/redis-stop.sh&#125; 5.2 创建脚本创建scripts目录 /etc/keepalived,在scripts目录内创建redis-master.sh,redis-backup.sh,redis-fault.sh,redis-stop.sh,chk_redis.sh 5.2.1 chk_redis.sh脚本1234567891011121314151617#!/bin/bash #检测redis是否正常运行，根据检测结果返回不同的值 #日志文件位置 logFile=/usr/etc/redis/keepalived/logs/redis-keepalived.log #ping 本机redis服务 pingRS=`/usr/local/bin/redis-cli -a 123456 PING` #如果ping 的结果为PONG,那么返回0 ,否则返回1；PONG代表PING通redis,当返回值为1时，执行降低优先级操作。if [ $pingRS == "PONG" ]; then echo "[`date`] ping is ok !" &gt;&gt;$logFile exit 0 else echo "[`date`] ping is error !" &gt;&gt;$logFile exit 1 fi 5.2.2 redis-master.sh （master主机脚本与slave主机基本雷同，只需修改远程ip地址） 1234567891011#!/bin/bashrediscli="redis-cli"logfile="/var/log/keepalived-redis-state.log"date &gt;&gt; %logfileecho "[master]" &gt;&gt; $logfileecho " 运行作为远程server备机并同步数据命令" &gt;&gt; $logfile$rediscli SLAVEOF 172.20.22.53 6379 &gt;&gt; $logfile #远程redis IP地址，解释为以该IP为主，做数据同步。sleep 10 #等待10秒，再运行下面命令。echo "运行升级为主服务器命令" &gt;&gt; $logfile$rediscli SLAVEOF NO ONE &gt;&gt; $logfileecho "切换master完成" &gt;&gt; $logfile 5.2.3 redis-backup.sh （master主机脚本与slave主机基本雷同，只需修改远程ip地址） 12345678910#!/bin/bashrediscli="redis-cli"logfile="/var/log/keepalived-redis-state.log"date &gt;&gt; $logfileecho "[backup]" &gt;&gt; $logfileecho "等待13秒同步数据后运行下面命令" &gt;&gt; $logfilesleep 13echo "以远方IP为主机，同步数据" &gt;&gt; $logfile$rediscli slaveof 172.20.22.53 6379 &gt;&gt; $logfileecho "切换backup完成" &gt;&gt; $logfile keepalived进入backup/stop/fault时的检测脚本，由于内容都一致，所以只写出redis_backup.sh 主从主机脚本内容一致，只需修改IP地址为对方IP 1$rediscli slaveof 172.20.22.53 6379 #修改为对方IP 6 测试7 注意事项： VRRP脚本(vrrp_script)和VRRP实例(vrrp_instance)属于同一个级别 notify_master 、notify_backup、notify_fault、notify_stop参数 notify_stop keepalived停止运行前运行notify_stop指定的脚本notify_master keepalived切换到master时执行的脚本notify_backup keepalived切换到backup时执行的脚本notify_fault keepalived出现故障时执行的脚本 启动顺序，先启动redis,后启动keepalived 8 说明此方法可以实现完全的自动化主从切换，但同步数据的时间（即脚本中的sleep时间）生产环境中无法完全掌控，实际使用中建议手动切回，或在主服务器上keepalived.conf中添加不抢占主机参数nopreempt（此参数要求配置文件中keepalived状态都为backup，根据优先级选择master） 示例： 123456789101112131415161718192021vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 51i nopreempt #redis或keepalived恢复后不抢占master，依然作为backup运行。 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_redis &#125; virtual_ipaddress &#123; 172.20.22.199 &#125; notify_backup /etc/keepalived/scripts/redis_backup.sh notify_master /etc/keepalived/scripts/redis_master.sh notify_fault /etc/keepalived/scripts/redis_fault.sh&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>keepalived</tag>
        <tag>双机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7通过smtp发送邮件（解决阿里云ECS不能通过25端口发送邮件）]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E9%80%9A%E8%BF%87smtp%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6-%E8%A7%A3%E5%86%B3%E9%98%BF%E9%87%8C%E4%BA%91ECS%E4%B8%8D%E8%83%BD%E9%80%9A%E8%BF%8725%E7%AB%AF%E5%8F%A3%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1 参考资料 https://bbs.aliyun.com/read/316576.html http://blog.csdn.net/qq_25551295/article/details/51803942 2 安装mailx1yum install mailx 2.1 修改配置文件1vim /etc/mail.rc 追加如下内容 123456set smtp="smtps://smtp.mxhichina.com:465"set smtp-auth=loginset smtp-auth-user="sales@vfutai.xxx"set smtp-auth-password="Ni-De-Mi-Ma"set ssl-verify=ignoreset nss-config-dir=/etc/pki/nssdb 2.2 发送测试例子1： 1echo message3 | mail -v -r "sales@vfutai.xxx" -s "This is the subject" dongshan3@foxmail.xxx message3 正文 `dongshan3@foxmail.xxx` 收件人地址，发送多人加逗号添加邮件地址 -r “sales@vfutai.xxx” 发件人地址 -s &quot;This is the subject&quot; 邮件标题 -a /etc/*.txt 附件 列子2： 123456789#!/usr/bin/bash#发送邮件到kxhuanzi@163.comsj=`date +%Y%m%d`echo "备份mysql" &gt;&gt; /opt/mysqldata/$sj.logmysqldump -uroot -ppassword bitnami_wordpress &gt; /opt/mysqldata/$sj.sql &gt;&gt; /opt/mysqldata/$sj.logecho "发送mysql备份电子邮件到k*****@163.com" &gt;&gt; /opt/mysqldate/$sj.logecho MYSQL备份，备份文件名字:$sj.sql,备份日期:`date`.| mail -v -r "32****@qq.com" -a /opt/mysqldata/$sj.sql -s "wordpress备份/$sj" k****@163.com,xh****@gmail.comecho "发送完成" &gt;&gt; /opt/mysqldata/$sj.log~]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>email</tag>
        <tag>smtp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix监控VMware]]></title>
    <url>%2F2018%2F08%2F30%2Fzabbix%E7%9B%91%E6%8E%A7VMware%2F</url>
    <content type="text"><![CDATA[1 环境 Centos 7.2 Zabbix 3.0 VMware 5.5/6.0 2 步骤2.1 修改配置文件 修改zabbix server配置文件 1vim /etc/zabbix/zabbix_server.conf 添加如下内容 12345StartVMwareCollectors=5VMwareFrequency=60VMwareCacheSize=8M StartVMwareCollectors=5 #预启动的VMware数据采集线程数量，值范围：0~250 VMwareFrequency=60 #VMware的数据检测缓存大小，值范围：256K~2G VMwareCacheSize=8M #数据采集的频率，值范围：10~86400 重启zabbix—server服务 1service zabbix-server restart 2.2 添加VM主机监控 添加主机网上很多教程都说端口改为80,但我改成80后无法获取到数据，但使用10050端口可以获取到（默认VMware无需安装agent） 添加模版 添加宏 12345678&#123;$PASSWORD&#125; #连接VMware的用户密码&#123;$URL&#125;#固定格式：`https://IP/sdk ` 网页访问地址错误，`curl -I -k https://192.168.0.19/sdk `测试 没问题&#123;$USERNAME&#125; #连接VMware的用户，默认root或administrator 过一会就可以看到自动检测到的虚拟主机了，自动匹配监控项，**图形需要自行创建**。]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7搭建sock5代理]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E6%90%AD%E5%BB%BAsock5%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[1 环境 OS:centos 7.3 sock5:ss5-3.8.9 2 安装SS52.1 安装必要组件12yum install pam-devel openldap-devel openssl-devel yum install gcc gcc-c++ 2.2 下载ss51wget https://nchc.dl.sourceforge.net/project/ss5/ss5/3.8.9-8/ss5-3.8.9-8.tar.gz 2.2.1 解压安装1234tar -zxvf ss5-3.8.9-8.tar.gzcd ss5-3.8.9/./configuremake&amp;&amp;makeinstall 2.3 修改配置文件1vim /etc/opt/ss5/ss5.conf 修改如下配置： 12345678910111213# ///////////////////////////////////////////////////////////////////////////////////# SHost SPort Authentication##auth 0.0.0.0/0 - -#取消注释，并修改如下，意思是使用用户密码连接，默认是允许所有人连接。auth 0.0.0.0/0 - u# /////////////////////////////////////////////////////////////////////////////////////////////////# Auth SHost SPort DHost DPort Fixup Group Band ExpDate##permit - 0.0.0.0/0 - 0.0.0.0/0 - - - - - #取消注释，并修改如下，意思是使用用户密码连接，默认是允许所有人连接。permit u 0.0.0.0/0 - 0.0.0.0/0 - - - - - 2.4 修改默认端口号1vim /etc/rc.d/init.d/ss5 修改脚本如下 1234#默认端口号为1080，修改为10888daemon /usr/sbin/ss5 -t $SS5_OPTS修改为daemon /usr/sbin/ss5 -t $SS5_OPTS -b 0.0.0.0:10888 2.5 添加帐号密码1vim /etc/opt/ss5/ss5.passwd 格式: 用户+密码 2.6 启动ss5123chmod 700 /etc/rc.d/init.d/ss5 #添加执行权限service ss5 start #启动服务chkconfig ss5 on #开机自启 2.7 检测启动12[root@blog ss5-3.8.9]# netstat -tunlp|grep ss5tcp 0 0 0.0.0.0:10888 0.0.0.0:* LISTEN 22387/ss5 安装完成。 可使用sock5客户端proxifier等进行连接。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>sock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[window下安装压缩版mysql5.6.35]]></title>
    <url>%2F2018%2F08%2F30%2Fwindow%E4%B8%8B%E5%AE%89%E8%A3%85%E5%8E%8B%E7%BC%A9%E7%89%88mysql5-6-35%2F</url>
    <content type="text"><![CDATA[1 环境： OS: windows 7 x64 mysql:mysql-5.6.35-winx64.zip 2 部署2.1 下载及解压解压缩mysql到自定义位置，并重命名为mysql 2.2 添加mysql环境变量编辑path系统变量，在后面追加C:\mysql\bin; 前面用分号“；”隔开。 path完整变量显示如下： 1C:\ProgramData\Oracle\Java\javapath;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;D:\Program Files\MySQL\MySQL Server 5.5\bin;D:\Program Files\php-5.4;D:\Program Files\php-5.4\ext;C:\mysql\bin; 重启生效 2.3 安装mysql使用管理员权限打开命令窗口 12mysqld --install #安装mysql命令mysqld --remove #卸载mysql命令 123net start mysql #启动数据库mysql -uroot -p #登录mysql,默认密码为空。net stop mysql #停止数据库]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>部署</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7缩减home目录扩展根目录（xfs分区格式）]]></title>
    <url>%2F2018%2F08%2F30%2FCentos7%E7%BC%A9%E5%87%8Fhome%E7%9B%AE%E5%BD%95%E6%89%A9%E5%B1%95%E6%A0%B9%E7%9B%AE%E5%BD%95-xfs%E5%88%86%E5%8C%BA%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1 磁盘占用 修改home目录大小，增加根目录容量。 2 注意事项 xfsf分区减小分区无法做到无损数据（必须备份数据） 扩容分区可以保存数据无损 3 xfs分区扩展根目录命令过程把/home内容备份，然后将/home文件系统所在的逻辑卷删除，扩大/root文件系统，新建/home： 12345678910tar cvf /tmp/home.tar /home #备份/homeumount /home #卸载/home，如果无法卸载，先终止使用/home文件系统的进程lvremove /dev/centos/home #删除/home所在的lv（可删除分区，也可使用下行缩减分区）lvreduce -L 180G /dev/centos/home #缩小分区到180G(缩小分区后分区数据丢失，需重新格式化并挂载分区，否则重启故障)lvextend -L +50G /dev/centos/root #扩展/root所在的lv，增加50Gxfs_growfs /dev/centos/root #扩展/root文件系统lvcreate -L 56G -n home centos #重新创建home lvmkfs.xfs /dev/centos/home #创建文件系统及格式化mount /dev/centos/home /home #挂载df -h 4 收缩分区 ext与xfs格式分区收缩分区命令不同，lvextend -L+19.8G /dev/VolGroup00/LogVol00分区命令后，执行相应收缩命令后扩展的容量才能正常使用 xfs格式如果分区格式为xfs，扩展使用 要用xfs_growfs命令而不是resize2fs命令收缩分区， 1[root@rac2 ~]# xfs_growfs -p /dev/VolGroup00/LogVol00 执行以上命令后，df -h命令查看才会显示扩容成功。 ext2、ext3、ext4格式（补充）Ext2、ext3、ext4等使用resize2fs收缩分区，xfs使用xfs-growfs命令收缩分区。详情找度娘。 例如: 1[root@rac2 ~]# resize2fs -p /dev/VolGroup00/LogVol00]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux调整目录分区大小，linux调整home目录大小，linux调整root目录大小（ext格式/LVM）]]></title>
    <url>%2F2018%2F08%2F30%2Flinux%E8%B0%83%E6%95%B4%E7%9B%AE%E5%BD%95%E5%88%86%E5%8C%BA%E5%A4%A7%E5%B0%8F%EF%BC%8Clinux%E8%B0%83%E6%95%B4home%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F%EF%BC%8Clinux%E8%B0%83%E6%95%B4root%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F-ext%E6%A0%BC%E5%BC%8F-LVM%2F</url>
    <content type="text"><![CDATA[说明：ext格式分区可无损扩大或缩小分区。要先对文件系统进行缩小，然后才能缩小逻辑卷，一层层向下。和扩大正好相反。 注意vg_sql-lv_home其中的sql其实为hostname! resize2fs命令resize2fs命令被用来增大或者收缩未加载的“ext2/ext3”文件系统的大小。如果文件系统是处于mount状态下，那么它只能做到扩容，前提条件是内核支持在线resize。，linux kernel 2.6支持在mount状态下扩容但仅限于ext3文件系统。 来自: http://man.linuxde.net/resize2fs 一、首先df -h查看分区情况（这里我想调整home目录）123456[root@sql ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_sql-lv_root 50G 906M 46G 2% /tmpfs 935M 0 935M 0% /dev/shm/dev/sda1 477M 30M 422M 7% /boot/dev/mapper/vg_sql-lv_home 341G 67M 323G 1% /home 二、卸载home目录umount /home123456[root@sql ~]# umount /home[root@sql ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_sql-lv_root 50G 706M 46G 2% /tmpfs 935M 0 935M 0% /dev/shm/dev/sda1 477M 30M 422M 7% /boot 三、重新指定/home目录大小缩小文件系统 12345678910111213141516[root@sql ~]# e2fsck -f /dev/mapper/vg_sql-lv_homee2fsck 1.41.12 (17-May-2010)Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary information/dev/mapper/vg_sql-lv_home: 11/22650880 files (0.0% non-contiguous), 1471409/90597376 blocks[root@sql ~]# resize2fs -p /dev/mapper/vg_sql-lv_home 30Gresize2fs 1.41.12 (17-May-2010)Resizing the filesystem on /dev/mapper/vg_sql-lv_home to 7864320 (4k) blocks.Begin pass 2 (max = 32768)Relocating blocks XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXBegin pass 3 (max = 2765)Scanning inode table XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXThe filesystem on /dev/mapper/vg_sql-lv_home is now 7864320 blocks long. 四、挂载/home，然后查看调整后的大小1234567[root@sql ~]# mount /home[root@sql ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_sql-lv_root 50G 706M 46G 2% /tmpfs 935M 0 935M 0% /dev/shm/dev/sda1 477M 30M 422M 7% /boot/dev/mapper/vg_sql-lv_home 30G 44M 28G 1% /home 五、用lvreduce命令把目标分区(/home)减小至30G缩小逻辑卷 123456[root@sql ~]# lvreduce -L 30G /dev/mapper/vg_sql-lv_homeWARNING: Reducing active and open logical volume to 30.00 GiBTHIS MAY DESTROY YOUR DATA (filesystem etc.)Do you really want to reduce lv_home? [y/n]: y Size of logical volume vg_sql/lv_home changed from 345.60 GiB (88474 extents) to 30.00 GiB (7680 extents). Logical volume lv_home successfully resized 六、用vgdisplay命令查看多余的空间，可以看到多出约320G的空间123456789101112131415161718192021[root@sql ~]# vgdisplay --- Volume group --- VG Name vg_sql System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 5 VG Access read/write VG Status resizable MAX LV 0 Cur LV 3 Open LV 3 Max PV 0 Cur PV 1 Act PV 1 VG Size 399.51 GiB PE Size 4.00 MiB Total PE 102274 Alloc PE / Size 21480 / 83.91 GiB Free PE / Size 80794 / 315.60 GiB VG UUID L9OUKR-6alh-ms7H-yimo-ypYm-lLYa-DqkpMC 七、用lvextend命令将多余的约320G空间挂载到/目录下扩大逻辑卷 注：在设定lv_root的大小时，不要把Free PE / Size的空间全部都用上，这很可能会出现FreePE空间不足的现象，建议保留一点Free PE的空间。 另：我这里搞上完没有出错，其实没有出错，查看空闲大小，显示Free PE / Size 0 / 0 1234[root@sql ~]# lvextend -L +315.60G /dev/mapper/vg_sql-lv_root Rounding size to boundary between physical extents: 315.60 GiB Size of logical volume vg_sql/lv_root changed from 50.00 GiB (12800 extents) to 365.60 GiB (93594 extents). Logical volume lv_root successfully resized 八、激活目录大小（扩展后的/目录）扩大文件系统 注：执行这个命令后，会进入漫长的等待，这里我是机械硬盘，且调整分区约320G，耗时较长 123456[root@sql ~]# resize2fs -p /dev/mapper/vg_sql-lv_rootresize2fs 1.41.12 (17-May-2010)Filesystem at /dev/mapper/vg_sql-lv_root is mounted on /; on-line resizing requiredold desc_blocks = 4, new_desc_blocks = 23Performing an on-line resize of /dev/mapper/vg_sql-lv_root to 95840256 (4k) blocks.The filesystem on /dev/mapper/vg_sql-lv_root is now 95840256 blocks long. 九、df -h查看修改成功后的分区情况123456[root@sql ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_sql-lv_root 360G 720M 341G 1% /tmpfs 935M 0 935M 0% /dev/shm/dev/sda1 477M 30M 422M 7% /boot/dev/mapper/vg_sql-lv_home 30G 44M 28G 1% /home 转载：https://www.cplusplus.me/2316.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux调整目录分区大小，linux调整home目录大小，linux调整root目录大小(xfs格式/LVM)]]></title>
    <url>%2F2018%2F08%2F30%2Flinux%E8%B0%83%E6%95%B4%E7%9B%AE%E5%BD%95%E5%88%86%E5%8C%BA%E5%A4%A7%E5%B0%8F%EF%BC%8Clinux%E8%B0%83%E6%95%B4home%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F%EF%BC%8Clinux%E8%B0%83%E6%95%B4root%E7%9B%AE%E5%BD%95%E5%A4%A7%E5%B0%8F-xfs%E6%A0%BC%E5%BC%8F-LVM%2F</url>
    <content type="text"><![CDATA[说明：xfs格式分区无法无损缩减分区，可无损扩大分区。 一：环境概览： 二、操作步骤：12345678910111213141516171819# 1.终止占用 /home 进程 fuser -m -v -i -k /home # 2.备份/home cp -r /home/ homebak/ # 3.卸载 /home umount /home # 4.删除/home所在的lv lvremove /dev/mapper/centos-home # 5.扩展/root所在的lv，增加100G lvextend -L +100G /dev/mapper/centos-root # 6.扩展/root文件系统 xfs_growfs /dev/mapper/centos-root # 7.重新创建home lv lvcreate -L 40G -n home centos # 8.创建文件系统 mkfs.xfs /dev/centos/home # 9.挂载 mount /dev/centos/home /home # 10.还原 /home 相关文件以及对应目录权限 三、执行结果： 四、实测成功：依旧教程扩容xfs，测试成功！ 转载：https://www.cplusplus.me/2717.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7进入单用户模式修改root密码]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E8%BF%9B%E5%85%A5%E5%8D%95%E7%94%A8%E6%88%B7%E6%A8%A1%E5%BC%8F%E4%BF%AE%E6%94%B9root%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[init方法 1、centos7的grub2界面会有两个入口，正常系统入口和救援模式； 2、修改grub2引导 在正常系统入口上按下”e“，会进入edit模式，搜寻ro那一行，以linux16开头的； 把ro更改成rw；（把只读更改成可写） 把rhgb quiet删除；（quiet模式没有代码行唰唰的走，可以删除） 增加init=/bin/sh；（或init=/bin/bash,指定shell环境) 按下ctrl+x来启动系统。 3、修改root密码 #passwd #修改密码 #touch /.autorelabel #据说是selinux在重启后更新label #exec /sbin/init #正常启动init进程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7如何进入救援模式_resue]]></title>
    <url>%2F2018%2F08%2F30%2Fcentos7%E5%A6%82%E4%BD%95%E8%BF%9B%E5%85%A5%E6%95%91%E6%8F%B4%E6%A8%A1%E5%BC%8F-resue%2F</url>
    <content type="text"><![CDATA[进入系统开机引导界面，按↓键，按e键，找到linux16开头的行，在后面添加systemd.unit=rescue.target,然后按ctrl+x来进入系统，就能够进入rescue的环境了，输入帐号及密码，就可以进行相应操作。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[widowns配额管理]]></title>
    <url>%2F2018%2F08%2F30%2Fwidowns%E9%85%8D%E9%A2%9D%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[1 简介 磁盘配额就是管理员可以为用户所能使用的磁盘空间进行配额限制，每一用户只能使用最大配额范围内的磁盘空间。 2 案例公共共享文件夹限制用户上传文件占用空间； 3 操作3.1 第1步：在“我的电脑”窗口中右键单击共享文件夹所在的磁盘分区，选择“属性”快捷命令，打开磁盘属性对话框。然后切换到“配额”选项卡，保持“启用配额管理”和“拒绝将磁盘空间给超过配额限制的用户”复选框的选中状态。另外建议选中“用户超出配额限制时记录事件”和“用户超过警告等级时记录事件”两个复选框，以便将配额告警记录到日志中。接着单击“配额项”按钮。 3.2 第2步：打开“本地磁盘的配额项”窗口，依次单击“配额”→“新建配额项”菜单命令，在打开的“选择用户”对话框中查找并选中目标用户并单击“确定”按钮。 3.3 第3步:在打开的“添加新配额项”对话框中选中“将磁盘空间限制为”单选框，并设置空间大小为100MB。接着在“将警告等级设置为”编辑框中设置空间大小为95MB。最后单击“确定”按钮使设置生效。 3.4 第4步:返回“本地磁盘的配额项”窗口，重复上述步骤针对其他用户新建配额项，设置完毕关闭该窗口，返回“本地磁盘 属性”对话框后单击“确定”按钮。设置配额项的用户只能使用规定容量以内的磁盘空间。]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker基础命令]]></title>
    <url>%2F2018%2F08%2F28%2FDocker%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1 获取镜像 从 Docker Registry 获取镜像的命令是 docker pull。其命令格式为： 1docker pull [选项] [Docker Registry地址]&lt;仓库名&gt;:&lt;标签&gt; 例子： 1docker pull ubuntu:14.04 2 运行镜像docker run 1234567891011121314$ docker run -it --rm ubuntu:14.04 bashroot@e7009c6ce357:/# cat /etc/os-releaseNAME="Ubuntu"VERSION="14.04.5 LTS, Trusty Tahr"ID=ubuntuID_LIKE=debianPRETTY_NAME="Ubuntu 14.04.5 LTS"VERSION_ID="14.04"HOME_URL="http://www.ubuntu.com/"SUPPORT_URL="http://help.ubuntu.com/"BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"root@e7009c6ce357:/# exitexit$ docker run 就是运行容器的命令，具体格式我们会在后面的章节讲解，我们这里简要的说明一下上面用到的参数。 -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用--rm 可以避免浪费空间。 ubuntu:14.04：这是指用 ubuntu:14.04 镜像为基础来启动容器。 bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是bash。 进入容器后，我们可以在 Shell 下操作，执行任何所需的命令。这里，我们执行了 cat /etc/os-release，这是 Linux 常用的查看当前系统版本的命令，从返回的结果可以看到容器内是 Ubuntu 14.04.5 LTS 系统。 最后我们通过exit退出了这个容器。 3 列出镜像docker images 123456789$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MBmongo 3.2 fe9198c04d62 5 days ago 342 MB&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MBubuntu 16.04 f753707788c5 4 weeks ago 127 MBubuntu latest f753707788c5 4 weeks ago 127 MBubuntu 14.04 1e0c3dd64ccd 4 weeks ago 188 MB 列表包含了仓库名、标签、镜像 ID、创建时间以及所占用的空间。 其中仓库名、标签在之前的基础概念章节已经介绍过了。镜像 ID 则是镜像的唯一标识，一个镜像可以对应多个标签。因此，在上面的例子中，我们可以看到ubuntu:16.04 和ubuntu:latest拥有相同的 ID，因为它们对应的是同一个镜像。 4 虚悬镜像上面的镜像列表中，还可以看到一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 &lt;none&gt;。： 1&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像： 123$ docker images -f dangling=trueREPOSITORY TAG IMAGE ID CREATED SIZE&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 5 删除镜像一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。 1$ docker rmi $(docker images -q -f dangling=true) 6 中间层镜像为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像。所以在使用一段时间后，可能会看到一些依赖的中间层镜像。默认的 docker images 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加-a参数。 1$ docker images -a 这样会看到很多无标签的镜像，与之前的虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。实际上，这些镜像也没必要删除，因为之前说过，相同的层只会存一遍，而这些镜像是别的镜像的依赖，因此并不会因为它们被列出来而多存了一份，无论如何你也会需要它们。只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除。 7 列出部分镜像不加任何参数的情况下，docker images会列出所有顶级镜像，但是有时候我们只希望列出部分镜像。docker images有好几个参数可以帮助做到这个事情。 根据仓库名列出镜像 12345$ docker images ubuntuREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 16.04 f753707788c5 4 weeks ago 127 MBubuntu latest f753707788c5 4 weeks ago 127 MBubuntu 14.04 1e0c3dd64ccd 4 weeks ago 188 MB 列出特定的某个镜像，也就是说指定仓库名和标签 123$ docker images ubuntu:16.04REPOSITORY TAG IMAGE ID CREATED SIZEubuntu 16.04 f753707788c5 4 weeks ago 127 MB 除此以外，docker images还支持强大的过滤器参数 --filter，或者简写-f。之前我们已经看到了使用过滤器来列出虚悬镜像的用法，它还有更多的用法。比如，我们希望看到在 mongo:3.2之后建立的镜像，可以用下面的命令： 1234$ docker images -f since=mongo:3.2REPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MB 想查看某个位置之前的镜像也可以，只需要把 since换成 before 即可。 此外，如果镜像构建时，定义了 LABEL，还可以通过 LABEL 来过滤。 12$ docker images -f label=com.example.version=0.1... 8 以特定格式显示默认情况下，docker images 会输出一个完整的表格，但是我们并非所有时候都会需要这些内容。比如，刚才删除虚悬镜像的时候，我们需要利用 docker images 把所有的虚悬镜像的 ID 列出来，然后才可以交给docker rmi 命令作为参数来删除指定的这些镜像，这个时候就用到了-q参数。 12345678$ docker images -q5f515359c7f805a60462f8bafe9198c04d6200285df0df87f753707788c5f753707788c51e0c3dd64ccd --filter 配合-q产生出指定范围的 ID 列表，然后送给另一个 docker 命令作为参数，从而针对这组实体成批的进行某种操作的做法在 Docker命令行使用过程中非常常见，不仅仅是镜像，将来我们会在各个命令中看到这类搭配以完成很强大的功能。因此每次在文档看到过滤器后，可以多注意一下它们的用法。 另外一些时候，我们可能只是对表格的结构不满意，希望自己组织列；或者不希望有标题，这样方便其它程序解析结果等，这就用到了 Go 的模板语法。 比如，下面的命令会直接列出镜像结果，并且只包含镜像ID和仓库名： 12345678$ docker images --format "&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;"5f515359c7f8: redis05a60462f8ba: nginxfe9198c04d62: mongo00285df0df87: &lt;none&gt;f753707788c5: ubuntuf753707788c5: ubuntu1e0c3dd64ccd: ubuntu 或者打算以表格等距显示，并且有标题行，和默认一样，不过自己定义列： 123456789$ docker images --format "table &#123;&#123;.ID&#125;&#125;\t&#123;&#123;.Repository&#125;&#125;\t&#123;&#123;.Tag&#125;&#125;"IMAGE ID REPOSITORY TAG5f515359c7f8 redis latest05a60462f8ba nginx latestfe9198c04d62 mongo 3.200285df0df87 &lt;none&gt; &lt;none&gt;f753707788c5 ubuntu 16.04f753707788c5 ubuntu latest1e0c3dd64ccd ubuntu 14.04 9 启动容器1docker run --name webserver -d -p 80:80 nginx 这条命令会用nginx镜像启动一个容器，命名为 webserver，并且映射了 80 端口，这样我们可以用浏览器去访问这个 nginx 服务器。 9.1 运行容器容器创建后二次启动及关闭使用： 1234[root@mini ~]# docker start webweb[root@mini ~]# docker stop webweb 10 进入容器现在，假设我们非常不喜欢这个欢迎页面，我们希望改成欢迎 Docker 的文字，我们可以使用 docker exec命令进入容器，修改其内容。 1234$ docker exec -it webserver bashroot@3729b97e8226:/# echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.htmlroot@3729b97e8226:/# exitexit 我们以交互式终端方式进入 webserver 容器，并执行了 bash 命令，也就是获得一个可操作的 Shell。 然后，我们用&lt;h1&gt;Hello, Docker!&lt;/h1&gt; 覆盖了/usr/share/nginx/html/index.html 的内容。 现在我们再刷新浏览器的话，会发现内容被改变了。 我们修改了容器的文件，也就是改动了容器的存储层。我们可以通过 docker diff 命令看到具体的改动。 1234567891011121314151617$ docker diff webserverC /rootA /root/.bash_historyC /runC /usrC /usr/shareC /usr/share/nginxC /usr/share/nginx/htmlC /usr/share/nginx/html/index.htmlC /varC /var/cacheC /var/cache/nginxA /var/cache/nginx/client_tempA /var/cache/nginx/fastcgi_tempA /var/cache/nginx/proxy_tempA /var/cache/nginx/scgi_tempA /var/cache/nginx/uwsgi_temp 11 利用commit定制镜像现在我们定制好了变化，我们希望能将其保存下来形成镜像。 要知道，当我们运行一个容器的时候（如果不使用卷的话），我们做的任何文件修改都会被记录于容器存储层里。而 Docker 提供了一个docker commit 命令，可以将容器的存储层保存下来成为镜像。换句话说，就是在原有镜像的基础上，再叠加上容器的存储层，并构成新的镜像。以后我们运行这个新镜像的时候，就会拥有原有容器最后的文件变化。 docker commit的语法格式为： 1docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]] 我们可以用下面的命令将容器保存为镜像： 123456$ docker commit \ --author "Tao Wang &lt;twang2218@gmail.com&gt;" \ --message "修改了默认网页" \ webserver \ nginx:v2sha256:07e33465974800ce65751acc279adc6ed2dc5ed4e0838f8b86f0c87aa1795214 其中--author 是指定修改的作者，而 --message则是记录本次修改的内容。这点和git 版本控制相似，不过这里这些信息可以省略留空。 我们可以在docker images中看到这个新定制的镜像： 12345$ docker images nginxREPOSITORY TAG IMAGE ID CREATED SIZEnginx v2 07e334659748 9 seconds ago 181.5 MBnginx 1.11 05a60462f8ba 12 days ago 181.5 MBnginx latest e43d811ce2f4 4 weeks ago 181.5 MB 我们还可以用 docker history具体查看镜像内的历史记录，如果比较nginx:latest 的历史记录，我们会发现新增了我们刚刚提交的这一层。 1234567891011$ docker history nginx:v2IMAGE CREATED CREATED BY SIZE COMMENT07e334659748 54 seconds ago nginx -g daemon off; 95 B 修改了默认网页e43d811ce2f4 4 weeks ago /bin/sh -c #(nop) CMD ["nginx" "-g" "daemon 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) EXPOSE 443/tcp 80/tcp 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c ln -sf /dev/stdout /var/log/nginx/ 22 B&lt;missing&gt; 4 weeks ago /bin/sh -c apt-key adv --keyserver hkp://pgp. 58.46 MB&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) ENV NGINX_VERSION=1.11.5-1 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) MAINTAINER NGINX Docker Ma 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) CMD ["/bin/bash"] 0 B&lt;missing&gt; 4 weeks ago /bin/sh -c #(nop) ADD file:23aa4f893e3288698c 123 MB 新的镜像定制好后，我们可以来运行这个镜像。 1docker run --name web2 -d -p 81:80 nginx:v2 这里我们命名为新的服务为 web2，并且映射到 81 端口。如果是Docker for Mac/Windows 或Linux 桌面的话，我们就可以直接访问http://localhost:81 看到结果，其内容应该和之前修改后的 webserver 一样。 至此，我们第一次完成了定制镜像，使用的是 docker commit 命令，手动操作给旧的镜像添加了新的一层，形成新的镜像，对镜像多层存储应该有了更直观的感觉。 11.1 慎用 docker commit使用docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念，但是实际环境中并不会这样使用。 首先，如果仔细观察之前的 docker diff webserver 的结果，你会发现除了真正想要修改的/usr/share/nginx/html/index.html 文件外，由于命令的执行，还有很多文件被改动或添加了。这还仅仅是最简单的操作，如果是安装软件包、编译构建，那会有大量的无关内容被添加进来，如果不小心清理，将会导致镜像极为臃肿。 此外，使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。而且，即使是这个制作镜像的人，过一段时间后也无法记清具体在操作的。虽然 docker diff 或许可以告诉得到一些线索，但是远远不到可以确保生成一致镜像的地步。这种黑箱镜像的维护工作是非常痛苦的。 而且，回顾之前提及的镜像所使用的分层存储的概念，除当前层外，之前的每一层都是不会发生改变的，换句话说，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。如果使用 docker commit制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到™。这会让镜像更加臃肿。 docker commit 命令除了学习之外，还有一些特殊的应用场合，比如被入侵后保存现场等。但是，不要使用 docker commit 定制镜像，定制行为应该使用 Dockerfile 来完成。下面的章节我们就来讲述一下如何使用 Dockerfile 定制镜像。 12 使用 Dockerfile 定制镜像从刚才的 docker commit 的学习中，我们可以了解到，镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 还以之前定制 nginx镜像为例，这次我们使用 Dockerfile 来定制。 在一个空白目录中，建立一个文本文件，并命名为 Dockerfile： 123$ mkdir mynginx$ cd mynginx$ touch Dockerfile 其内容为： 12FROM nginxRUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html 这个Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN。 12.1 FROM 指定基础镜像所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定基础镜像，因此一个Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 在 Docker Hub1 上有非常多的高质量的官方镜像， 有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat等； 也有一些方便开发、构建、运行各种语言应用的镜像，如node、openjdk、python、ruby、golang 等。 可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。 如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine等，这些操作系统的软件库为我们提供了更广阔的扩展空间。 除了选择现有镜像为基础镜像外，Docker还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。 12FROM scratch... 如果你以scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。 不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见，比如 warm、coreos/etcd。对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch会让镜像体积更加小巧。使用 Go 语言 开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 Go 是特别适合容器微服务架构的语言的原因之一。 13 RUN 执行命令RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式：RUN &lt;命令&gt;，就像直接在命令行中输入的命令一样。刚才写的Dockrfile中的 RUN 指令就是这种格式。 1RUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html exec 格式：RUN [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;]，这更像是函数调用中的格式。 123456789FROM debian:jessieRUN apt-get updateRUN apt-get install -y gcc libc6-dev makeRUN wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz"RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 之前说过，Dockerfile 中每一个指令都会建立一层，RUN也不例外。每一个 RUN的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit这一层的修改，构成新的镜像。 而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。 Union FS是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 上面的 Dockerfile 正确的写法应该是这样： 1234567891011121314FROM debian:jessieRUN buildDeps='gcc libc6-dev make' \ &amp;&amp; apt-get update \ &amp;&amp; apt-get install -y $buildDeps \ &amp;&amp; wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz" \ &amp;&amp; mkdir -p /usr/src/redis \ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \ &amp;&amp; make -C /usr/src/redis \ &amp;&amp; make -C /usr/src/redis install \ &amp;&amp; rm -rf /var/lib/apt/lists/* \ &amp;&amp; rm redis.tar.gz \ &amp;&amp; rm -r /usr/src/redis \ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个RUN 对一一对应不同的命令，而是仅仅使用一个 RUN指令，并使用 &amp;&amp; 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 并且，这里为了格式化还进行了换行。Dockerfile 支持Shell 类的行尾添加\的命令换行方式，以及行首#进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。 此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 很多人初学Docker制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 14 构建镜像好了，让我们再回到之前定制的 nginx 镜像的 Dockerfile 来。现在我们明白了这个 Dockerfile 的内容，那么让我们来构建这个镜像吧。 在Dockerfile文件所在目录执行： 123456789$ docker build -t nginx:v3 .Sending build context to Docker daemon 2.048 kBStep 1 : FROM nginx ---&gt; e43d811ce2f4Step 2 : RUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html ---&gt; Running in 9cdc27646c7b ---&gt; 44aa4490ce2cRemoving intermediate container 9cdc27646c7bSuccessfully built 44aa4490ce2c 从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2 中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。 这里我们使用了 docker build命令进行镜像构建。其格式为： docker build [选项] &lt;上下文路径/URL/-&gt;在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，我们可以像之前运行 nginx:v2 那样来运行这个镜像，其结果会和 nginx:v2 一样。 15 镜像构建上下文（Context）如果注意，会看到 docker build 命令最后有一个.。.表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定Dockerfile所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定上下文路径。那么什么是上下文呢？ 首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种C/S设计，让我们操作远程服务器的 Docker引擎变得轻而易举。 当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过COPY指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是Docker 引擎中构建的。那么在这种客户端/服务端的架构中，如何才能让服务端获得本地文件呢？ 这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。 如果在Dockerfile中这么写： 1COPY ./package.json /app/ 这并不是要复制执行docker build 命令所在的目录下的 package.json，也不是复制Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的package.json。 因此，COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 COPY ../package.json /app 或者COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。 现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 Docker引擎以帮助构建镜像。 如果观察 docker build 输出，我们其实已经看到了这个发送上下文的过程： 123$ docker build -t nginx:v3 .Sending build context to Docker daemon 2.048 kB... 理解构建上下文对于镜像构建是很重要的，避免犯一些不应该的错误。比如有些初学者在发现 COPY /opt/xxxx /app 不工作后，于是干脆将 Dockerfile 放到了硬盘根目录去构建，结果发现 docker build执行后，在发送一个几十 GB 的东西，极为缓慢而且很容易构建失败。那是因为这种做法是在让 docker build 打包整个硬盘，这显然是使用错误。 一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 Docker引擎的。 那么为什么会有人误以为 . 是指定Dockerfile所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为Dockerfile。 这只是默认行为，实际上Dockerfile 的文件名并不要求必须为 Dockerfile，而且并不要求必须位于上下文目录中，比如可以用 -f ../Dockerfile.php 参数指定某个文件作为Dockerfile。 当然，一般大家习惯性的会使用默认的文件名 Dockerfile，以及会将其置于镜像构建上下文目录中。 16 其它 docker build 的用法16.1 直接用 Git repo 进行构建或许你已经注意到了，docker build 还支持从 URL 构建，比如可以直接从 Git repo 中构建： 12345678$ docker build https://github.com/twang2218/gitlab-ce-zh.git#:8.14docker build https://github.com/twang2218/gitlab-ce-zh.git\#:8.14Sending build context to Docker daemon 2.048 kBStep 1 : FROM gitlab/gitlab-ce:8.14.0-ce.08.14.0-ce.0: Pulling from gitlab/gitlab-ceaed15891ba52: Already exists773ae8583d14: Already exists... 这行命令指定了构建所需的Git repo，并且指定默认的 master 分支，构建目录为 /8.14/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。 16.2 用给定的 tar 压缩包构建1$ docker build http://server/context.tar.gz 如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。 16.3 从标准输入中读取 Dockerfile 进行构建1docker build - &lt; Dockerfile 或 1cat Dockerfile | docker build - 如果标准输入传入的是文本文件，则将其视为Dockerfile，并开始构建。这种形式由于直接从标准输入中读取 Dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件COPY 进镜像之类的事情。 16.4 从标准输入中读取上下文压缩包进行构建1$ docker build - &lt; context.tar.gz 如果发现标准输入的文件格式是 gzip、bzip2 以及xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos搭建私有仓库]]></title>
    <url>%2F2018%2F08%2F28%2FCentos%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1 本地YUM源 1.1 安装createrepo1yum -y install createrepo 1.2 创建目录1mkdir -p /yum/yum-custom/packages 1.3 下载rpm包及依赖到packages rpm包放在/yum-custom/ “``”可以是自定义文件夹，不一定要是packages 1yum install httpd --downloadonly --downloaddir=/yum/yum-custom/packages 1.4 创建repo12345678910111213$ createrepo /yum/yum-custom/Spawning worker 0 with 1 pkgsSpawning worker 1 with 0 pkgsWorkers FinishedSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs complete$ ll /yum/yum-custom/total 4drwxr-xr-x. 2 root root 41 Dec 20 07:03 packagesdrwxr-xr-x. 2 root root 4096 Dec 20 07:08 repodata 1.5 配置自定义repo1.5.1 删除备份repo文件123456[root@localhost /]# cd /etc/yum.repos.d/[root@localhost yum.repos.d]# lsCentOS-Base.repo CentOS-CR.repo CentOS-fasttrack.repo CentOS-Media.repo.bak CentOS-QEMU-EV.repo CentOS-Vault.repoCentOS-Ceph-Hammer.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-OpenStack-mitaka.repo CentOS-Sources.repo repo.bak.tar[root@localhost yum.repos.d]# tar zcvf *.repo[root@localhost yum.repos.d]# rm -f *.repo 1.5.2 创建CentOS-Media.repo123456789# vi /etc/yum.repos.d/CentOS-Media.repo# 填入如下内容[c7-media]name=CentOS-$releasever - Mediabaseurl=file:///yum/yum-custom/gpgcheck=0 enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 1.6 重建YUM缓存12345678910111213[root@linuxidc.com ~]# yum clean all #删除缓存Loaded plugins: fastestmirrorCleaning repos: c7-mediaCleaning up everythingCleaning up list of fastest mirrors[root@linuxidc.com ~]# yum makecache #建立缓存Loaded plugins: fastestmirrorc7-media | 3.0 kB 00:00:00 (1/3): c7-media/filelists_db | 880 B 00:00:00 (2/3): c7-media/primary_db | 1.8 kB 00:00:00 (3/3): c7-media/other_db | 1.3 kB 00:00:00 Determining fastest mirrorsMetadata Cache Created 1.7 使用本地yum1yum install httpd 2 局域网YUM源（vsftpd）2.1 安装createrepo1yum -y install createrepo 2.2 安装vsftpd1yum -y install vsftpd 2.2.1 配置vsftpd.conf1234# vi /etc/vsftpd/vsftpd.conf#并增加匿名用户root目录（默认已经启用匿名访问）anon_root=/yum/ 2.2.2 启动vsftpd12systemctl start vsftpd #启动服务systemctl enable vsftpd #开机启动 2.3 创建目录1mkdir -p /yum/yum-custom/packages 2.4 下载rpm包及依赖到packages rpm包放在/yum-custom/ “”可以是自定义文件夹，不一定要是packages 1yum install httpd --downloadonly --downloaddir=/yum/yum-custom/packages 2.5 创建repo12345678910111213# createrepo /yum/yum-custom/Spawning worker 0 with 1 pkgsSpawning worker 1 with 0 pkgsWorkers FinishedSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs complete# ll /yum/yum-custom/total 4drwxr-xr-x. 2 root root 41 Dec 20 07:03 packagesdrwxr-xr-x. 2 root root 4096 Dec 20 07:08 repodata 以下步骤局域网机器操作！ 2.5.1 删除备份repo文件123456[root@localhost /]# cd /etc/yum.repos.d/[root@localhost yum.repos.d]# lsCentOS-Base.repo CentOS-CR.repo CentOS-fasttrack.repo CentOS-Media.repo.bak CentOS-QEMU-EV.repo CentOS-Vault.repoCentOS-Ceph-Hammer.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-OpenStack-mitaka.repo CentOS-Sources.repo repo.bak.tar[root@localhost yum.repos.d]# tar zcvf *.repo[root@localhost yum.repos.d]# rm -f *.repo 2.5.2 修改自定义repo如下 修改局域网服务器的repo，哪个服务器使用，修改哪个服务器文件。 12345678910[root@localhost /]# vim /etc/yum.repos.d/CentOS-Media.repo# 内容如下[c7-media]name=CentOS-$releasever - Mediabaseurl=ftp://192.168.118.133/yum-custom #注意是ftp，不是filegpgcheck=0enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7# 其中192.168.118.133为上面vsftp服务器地址 2.6 重建YUM缓存12345678910111213[root@linuxidc.com ~]# yum clean all #删除缓存Loaded plugins: fastestmirrorCleaning repos: c7-mediaCleaning up everythingCleaning up list of fastest mirrors[root@linuxidc.com ~]# yum makecache #建立缓存Loaded plugins: fastestmirrorc7-media | 3.0 kB 00:00:00 (1/3): c7-media/filelists_db | 880 B 00:00:00 (2/3): c7-media/primary_db | 1.8 kB 00:00:00 (3/3): c7-media/other_db | 1.3 kB 00:00:00 Determining fastest mirrorsMetadata Cache Created 2.7 使用局域网yum1yum install httpd 3 局域网YUM源（httpd）3.1 安装createrepo1yum -y install createrepo 3.2 安装httpd1yum -y install httpd 3.2.1 配置httpd.conf1234567891011121314151617181920212223242526272829303132333435363738394041$ vim /etc/httpd/conf/httpd.conf# 修改http根目录DocumentRoot "/yum" #修改## Relax access to content within /var/www.#&lt;Directory "/var/www/html"&gt; AllowOverride None # Allow open access: Require all granted&lt;/Directory&gt;# Further relax access to the default document root:&lt;Directory "/yum"&gt; #修改 # # Possible values for the Options directive are "None", "All", # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that "MultiViews" must be named *explicitly* --- "Options All" # doesn't give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be "All", "None", or any combination of the keywords: # Options FileInfo AuthConfig Limit # AllowOverride None # # Controls who can get stuff from this server. # Require all granted&lt;/Directory&gt; 3.2.2 启动httpd12systemctl start httpd #启动服务systemctl enable httpd #开机启动 3.3 创建目录1mkdir -p /yum/yum-custom/packages 3.4 下载rpm包及依赖到packages rpm包放在/yum-custom/ “”可以是自定义文件夹，不一定要是packages 1yum install httpd --downloadonly --downloaddir=/yum/yum-custom/packages 3.5 创建repo12345678910111213#createrepo /yum/yum-custom/ Spawning worker 0 with 1 pkgsSpawning worker 1 with 0 pkgsWorkers FinishedSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs complete#ll /yum/yum-custom/total 4drwxr-xr-x. 2 root root 41 Dec 20 07:03 packagesdrwxr-xr-x. 2 root root 4096 Dec 20 07:08 repodata 以下步骤局域网机器操作！ 3.5.1 删除备份repo文件123456[root@localhost /]# cd /etc/yum.repos.d/[root@localhost yum.repos.d]# lsCentOS-Base.repo CentOS-CR.repo CentOS-fasttrack.repo CentOS-Media.repo.bak CentOS-QEMU-EV.repo CentOS-Vault.repoCentOS-Ceph-Hammer.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-OpenStack-mitaka.repo CentOS-Sources.repo repo.bak.tar[root@localhost yum.repos.d]# tar zcvf *.repo[root@localhost yum.repos.d]# rm -f *.repo 3.5.2 修改自定义repo如下 修改局域网服务器的repo，哪个服务器使用，修改哪个服务器文件。 12345678910[root@localhost /]# vim /etc/yum.repos.d/CentOS-Media.repo#内容如下[c7-media]name=CentOS-$releasever - Mediabaseurl=http://192.168.118.133/yum-custom #注意是http，不是file、ftpgpgcheck=0enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#其中192.168.118.133为上面vsftp服务器地址 3.6重建YUM缓存12345678910111213[root@linuxidc.com ~]# yum clean all #删除缓存Loaded plugins: fastestmirrorCleaning repos: c7-mediaCleaning up everythingCleaning up list of fastest mirrors[root@linuxidc.com ~]# yum makecache #建立缓存Loaded plugins: fastestmirrorc7-media | 3.0 kB 00:00:00 (1/3): c7-media/filelists_db | 880 B 00:00:00 (2/3): c7-media/primary_db | 1.8 kB 00:00:00 (3/3): c7-media/other_db | 1.3 kB 00:00:00 Determining fastest mirrorsMetadata Cache Created 3.7 使用局域网yum1yum install httpd 4 注意事项：1、YUM源服务器每次添加rpm包到yum-custom目录，都需要重新运行#createrepo /yum/yum-custom/命令（YUM源服务器运行），同时局域网电脑重新建立缓存#yum clean all 和#yum makecache，否则无法发现新加安装包。 2、YUM源在/yum/yum-custom下可创建自定义目录，可根据rpm包进行分类，便于本地YUM源管理，同上，有任何改变，必须重新运行事项1命令。 3、局域网主机安装rpm的时候出现报错 原因：使用 1createrepo -u -d /yum/yum-custom/ 创建repo， 解决办法：使用命令 1createrepo /yum/yum-custom/ 不添加任何参数。 4、selinux必须关闭，否则局域网机器无法从仓库建立缓存。 资料http://www.linuxidc.com/Linux/2017-03/141391.htm]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-mitaka调整实例大小报错]]></title>
    <url>%2F2018%2F08%2F28%2Fopenstack-mitaka%E8%B0%83%E6%95%B4%E5%AE%9E%E4%BE%8B%E5%A4%A7%E5%B0%8F%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[1 问题1 在openstack图形界面对实例进行迁移操作时，出现如下错误 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748492017-09-18 11:00:21.828 1386 INFO nova.compute.resource_tracker [req-4fe2d206-e9bc-4a61-9389-a01007335c61 - - - - -] Total usable vcpus: 12, total allocated vcpus: 52017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return self._do_dispatch(endpoint, method, ctxt, args)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py", line 127, in _do_dispatch2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher result = func(ctxt, **new_args)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/exception.py", line 114, in wrapped2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher payload)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.force_reraise()2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/exception.py", line 89, in wrapped2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return f(self, context, *args, **kw)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 359, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher LOG.warning(msg, e, instance=instance)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.force_reraise()2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 328, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return function(self, context, *args, **kwargs)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 409, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return function(self, context, *args, **kwargs)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 316, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher migration.instance_uuid, exc_info=True)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.force_reraise()2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 293, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return function(self, context, *args, **kwargs)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 387, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher kwargs['instance'], e, sys.exc_info())2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.force_reraise()2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher six.reraise(self.type_, self.value, self.tb)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 375, in decorated_function2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher return function(self, context, *args, **kwargs)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 3933, in resize_instance2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.instance_events.clear_events_for_instance(instance)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib64/python2.7/contextlib.py", line 35, in __exit__2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher self.gen.throw(type, value, traceback)2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 6643, in _error_out_instance_on_exception2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher raise error.inner_exception2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher ResizeError: Resize error: not able to execute ssh command: Unexpected error while running command.2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher Command: ssh -o BatchMode=yes 192.168.1.116 mkdir -p /var/lib/nova/instances/724a9fc0-3786-4ba4-8a43-744ec19a08812017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher Exit code: 2552017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher Stdout: u''2017-09-18 10:55:08.695 1386 ERROR oslo_messaging.rpc.dispatcher Stderr: u'Host key verification failed.\r\n 建立各计算节点、控制节点之间的SSH免密登录，建立私钥 解决方案 开启nova用户登录权限 1usermod -s /bin/bash nova 切换到nova用户 1su nova 生成密钥（各个计算节点执行，控制节点也执行） 12$ ssh-keygen -t rsa#默认路径，直接回车 所有计算节点均配置 依然在nova用户下操作 123456789$cat &lt;&lt; EOF &gt; ~/.ssh/config&gt; Host *&gt; StrictHostKeyChecking no&gt; UserKnownHostsFile=/dev/null&gt; EOF 发送公钥到控制节点 compute1 1scp id_rsa.pub 10.20.0.2:/var/lib/nova/.ssh/id_rsa.pub2 compute2 1scp id_rsa.pub 10.20.0.2:/var/lib/nova/.ssh/id_rsa.pub3 contrloller(10.20.0.2) 123# cat id_dsa.pub id_dsa.pub2 id_rsa.pub id_rsa.pub2 id_rsa.pub3 id_dsa.pub3 &gt; authorized_keys# scp authorized_keys computer1:/var/lib/nova/.ssh# scp authorized_keys computer2:/var/lib/nova/.ssh 修改权限 1# chown nova:nova /var/lib/nova/.ssh/id_rsa /var/lib/nova/.ssh/authorized_keys 登录测试 1ssh nova@computer 在迁移实例，不再报错，并成功。 实例迁移根据使用存储不同，有不同差异，详细介绍参考openstack官方文档]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack调整实例大小_冷迁移]]></title>
    <url>%2F2018%2F08%2F28%2FOpenstack%E8%B0%83%E6%95%B4%E5%AE%9E%E4%BE%8B%E5%A4%A7%E5%B0%8F-%E5%86%B7%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[有时虚拟机创建后发现虚拟机规格太小，满足不了业务需求。于是需要在线拉伸虚拟机的规格。1、用admin用户登录dashboard，创建满足需求的虚拟机规格 2、输入适当的参数 3、修改controller和各个computer节点的nova.cnf文件，打开下面两个参数 12allow_resize_to_same_host=True scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter 4、重启控制节点nova服务 1# systemctl restart openstack-nova-api.service openstack-nova-conductor.service openstack-nova-scheduler.service openstack-nova-cert.service openstack-nova-consoleauth.service openstack-nova-compute.service openstack-nova-novncproxy.service 5、重启计算节点nova服务 1# service openstack-nova-compute restart 参考资料：http://www.cnblogs.com/goodcook/p/6509808.html]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack的临时(Ephemeral)存储和块(Block)存储]]></title>
    <url>%2F2018%2F08%2F28%2FOpenstack%E7%9A%84%E4%B8%B4%E6%97%B6(Ephemeral)%E5%AD%98%E5%82%A8%E5%92%8C%E5%9D%97(Block)%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[转载：http://www.aboutyun.com/thread-8114-1-1.html 1 Openstack的临时(Ephemeral)存储和块(Block)存储1.2 问题导读：1、OpenStack中什么是临时存储，有什么好处？2、什么是块存储，和临时的有什么不同？ 1.3 背景Openstack不管是Ephemeral Storage还是Block Storage, 其实从接口上看，其实都是块服务。那么为什么要搞两个不同的类型呢，本文从这两种不同类型块存储的实现上来分析下其中的原因。 1.4 临时存储Openstack临时存储是由Nova提供的，主要是利用主机的本地存储给虚拟机提供卷服务。如果虚拟机被删除了，挂在这个虚拟机上的任何临时存储自动释放。这样的实现方式决定了：使用Ephemeral Storage的虚拟机不能支持迁移，以及和虚拟机迁移相关的特性，包括 1) HA 2) 动态调度 等等。存放在Ephemeral Storage上的数据是高度不可靠的，任何虚拟机和主机的故障都可能会导致数据丢失。 1.5 块存储目前Openstack的块存储由Cinder提供，其后端支持很多类型的存储设备，比如多个厂商不同型号的阵列设备，或者是Ceph, Glusterfs, Sheepdog之类的分布式存储系统。基于块存储，可以为用户提供：高可靠的存储（基于阵列的RAID, 或者是分布式存储的多副本机制；甚至还可以充分利用设备的备份，远程复制能力） 共享存储 （意味着可以支持HA, 虚拟机迁移等等） 1.6 临时存储的妙用这么看来，临时存储岂不是几乎没什么作用了，那为什么还需要提供这个服务呢？其实原因非常简单： 这个服务便宜，而且便宜到令人发指的地步，比如AWS的Ephemeral Storage, 就是免费的。用户可以用它来做不少有意思的事情，比如：无状态虚拟机，为系统提供Cache服务为虚拟机操作系统提供交换分区，或者用来存放其它类型的临时文件改进EBS的性能，比如买4个EBS盘，再配置2个免费的Ephermal盘，组建一个RAID 10系统 1.7 总结对于云服务提供商，不管采用什么样的后端技术，为用户提供7个9甚至更高可靠性的EBS服务，成本是巨大的，如果使用阵列，其价格本来就昂贵；如果使用分布式存储，起码要3个副本，再考虑到定期备份，快照，跨地域容灾，成本一样很高。现在的SATA, SAS盘便宜而且量又足，很容易造成在本地主机上空闲，所以干脆直接送给用户，由他们去玩，而且对于玩的好的用户，还真能对业务有不少帮助。最后再附上Openstack官方文档对几种存储的对比：]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack热迁移和冷迁移]]></title>
    <url>%2F2018%2F08%2F28%2FOpenstack%E7%83%AD%E8%BF%81%E7%A7%BB%E5%92%8C%E5%86%B7%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[1 迁移类型： *非在线迁移 (有时也称之为‘迁移’)。也就是在迁移到另外的计算节点时的这段时间虚拟机实例是处于宕机状态的。在此情况下，实例需要重启才能工作。 *在线迁移 (或 ‘真正的在线迁移‘)。实例几乎没有宕机时间。用于当实例需要在迁移时保持运行。在线迁移有下面几种类型： 基于共享存储的在线迁移。所有的Hypervisor都可以访问共享存储。 块在线迁移。无须共享存储。但诸如CD-ROM之类的只读设备是无法实现的。 基于卷的在线迁移。实例都是基于卷的而不是临时的磁盘，无须共享存储，也支持迁移(目前仅支持基于libvirt的hypervisor)。 1.1 什么是热迁移热迁移（Live Migration，又叫动态迁移、实时迁移），即虚拟机保存/恢复(Save/Restore)：将整个虚拟机的运行状态完整保存下来，同时可以快速的恢复到原有硬件平台甚至是不同硬件平台上。恢复以后，虚拟机仍旧平滑运行，用户不会察觉到任何差异。 1.2 Openstack热迁移OpenStack有两种在线迁移类型：live migration和block migration。Livemigration需要实例保存在NFS共享存储中，这种迁移主要是实例的内存状态的迁移，速度应该会很快。Block migration除了实例内存状态要迁移外，还得迁移磁盘文件，速度会慢些，但是它不要求实例存储在共享文件系统中。NFS允许一个系统在网络上与他人共享目录和文件。通过使用NFS，用户和程序可以像访问本地文件一样访问远端系统上的文件。 1.2.1 迁移步骤 迁移前的条件检查动态迁移要成功执行，一些条件必须满足，所以在执行迁移前必须做一些条件检查。 权限检查，执行迁移的用户是否有足够的权限执行动态迁移。 参数检查，传递给 API 的参数是否足够和正确，如是否指定了 block-migrate 参数。 检查目标物理主机是否存在。 检查被迁移的虚拟机是否是 running 状态。 检查源和目的物理主机上的 nova-compute service 是否正常运行。 检查目的物理主机和源物理主机是否是同一台机器。 检查目的物理主机是否有足够的内存(memory)。 检查目的和源物理主机器 hypervisor 和 hypervisor 的版本是否相同。 迁移前的预处理在真正执行迁移前，必须做一下热身，做一些准备工作。 在目的物理主机上获得和准备虚拟机挂载的块设备(volume)。 在目的物理主机上设置虚拟机的网络(networks)。 目的物理主机上设置虚拟机的防火墙(fireware)。 迁移条件满足并且做完了预处理工作后，就可以执行动态迁移了。主要步骤如下： 调用 libvirt python 接口 migrateToURI，来把源主机迁移到目的主机。dom.migrateToURI(CONF.live_migration_uri % dest,logical_sum,None,CONF.live_migration_bandwidth)live_migration_uri：这个 URI 就是在 3.2.2 里介绍的 libvirtd 进程定义的。live_migration_bandwidth：这个参数定义了迁移过程中所使用的最大的带宽。 以一定的时间间隔（0.5）循环调用 wait_for_live_migration 方法，来检测虚拟机迁移 的状态，一直到虚拟机成功迁移为止。 迁移后的处理当虚拟机迁移完成后，要做一些善后工作。 在源物理主机上 detach volume。 在源物理主机上释放 security group ingress rule。 在目的物理主机上更新数据库里虚拟机的状态。 在源物理主机上删除虚拟机。上面四步正常完成后，虚拟机就成功的从源物理主机成功地迁移到了目的物理主机了 1.2.2 Live Migration 的实现热迁移条件： 计算节点之间可以通过主机名互相访问 计算节点和控制节点的nova uid和gid保持一致 vncserver_proxyclient_address和vncserver_listen 监听的是本地IP 必须有共享存储，实例存放在共享存储中，且每个计算节点都可以访问共享存储。否则只能使用块迁移 1.2.3 配置 添加live_migration_flag修改nova的配置文件，在[libvirt] 段下 添加如下字段 1live_migration_flag="VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED" 配置配置versh免密码连接，修改/etc/libvirt/libvirtd.conf添加如下配置 123456789listen_tls = 0listen_tcp = 1tcp_port = "16509"listen_addr = "172.16.201.8" #根据自己的计算节点IP改写auth_tcp = "none" 修改/etc/sysconfig/libvirtd 添加如下参数 123LIBVIRTD_CONFIG=/etc/libvirt/libvirtd.confLIBVIRTD_ARGS="--listen" 重启libvirt 1systemctl restart libvirtd.service 查看监听端口： 123[root@compute1 ~]# netstat -lnpt | grep libvirtdtcp 0 0 172.16.206.6:16509 0.0.0.0:* LISTEN 9852/libvirtd 测试： 1234567在compute1节点上：virsh -c qemu+tcp://compute2/system在compute2节点上virsh -c qemu+tcp://compute1/system 如果能无密码连接上去，表示配置没问题 1.2.4动态迁移 查看所有实例nova list 查看需要迁移虚拟机实例nova show f3d749ba-98e1-4624-9782-6da729ad164c 查看可用的计算节点nova-manage service list 查看目标节点资源nova-manage service describe_resource computer1 开始迁移，正常无任何回显nova live-migration 8da00f69-05f6-4425-9a8a-df56b79a474f computer1 也可以通过dashboard 节点迁移用节点迁移需要使用admin管理员用户执 1.3 冷迁移配置 冷迁移需要启动nova账户，并配置ssh 免密码认证 123456789usermod -s /bin/bash novasu - novassh-keygen -t rsa# 生成密钥cp -fa id_rsa.pub authorized_keys 将密钥复制到所有计算节点的/var/lib/nova/.ssh下，并设置权限为nova用户 编辑/etc/nova/nova.conf的配置文件，修改下面参数 123allow_resize_to_same_host=True scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter 在计算节点重启nova服务 1systemctl restart openstack-nova-compute 在controller节点重启nova 相关服务 1systemctl restart openstack-nova-api.service openstack-nova-scheduler.service]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dokcer常用命令]]></title>
    <url>%2F2018%2F08%2F28%2FDokcer%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1 Docker常用命令 1.1 镜像管理12345678910# 列出本地所有镜像docker images# 查找imagedocker search &lt;IMAGE_ID/NAME&gt;# 下载imagedocker pull &lt;IMAGE_ID&gt;# 上传imagedocker push &lt;IMAGE_ID&gt;# 删除imagedocker rmi &lt;IMAGE_ID&gt; 1.2 容器管理123456789101112131415161718192021222324252627282930313233343536373839404142434445docker run -i -t &lt;IMAGE_ID&gt; /bin/bash： -i：标准输入给容器 -t：分配一个虚拟终端 /bin/bash：执行bash脚本-d：以守护进程方式运行（后台）-P：默认匹配docker容器的5000端口号到宿主机的49153 to 65535端口-p &lt;HOT_PORT&gt;:&lt;CONTAINER_PORT&gt;：指定端口号--name： 指定容器的名称--rm：退出时删除容器# 停止containerdocker stop &lt;CONTAINER_ID&gt;#重新启动containerdocker start &lt;CONTAINER_ID&gt;#显示运行的容器docker ps-l：显示最后启动的容器-a：同时显示停止的容器，默认只显示启动状态# 连接到启动的容器docker attach &lt;CONTAINER_ID&gt;#输出容器日志docker logs &lt;CONTAINER_ID&gt;-f：实时输出# 复制容器内的文件到宿主机目录上docker cp &lt;CONTAINER_ID&gt;:路径 宿主机路径# 删除containerdocker rm &lt;CONTAINER_ID&gt;# 删除所有容器docker rm `docker ps -a -q`docker kill `docker ps -q`docker rmi `docker images -q -a`docker wait &lt;CONTAINER_ID&gt;：阻塞对容器的其他调用方法，直到容器停止后退出# 查看容器中运行的进程docker top &lt;CONTAINER_ID&gt;# 查看容器中的变化docker diff &lt;CONTAINER_ID&gt;# 查看容器详细信息（输出为Json）docker inspect &lt;CONTAINER_ID&gt;-f：查找特定信息，如docker inspect -f '&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;'docker commit -m "comment" -a "author" &lt;CONTAINER_ID&gt; ouruser/imagename:tagdocker extc -it &lt;CONTAINER&gt; &lt;COMMAND&gt;：在容器里执行命令，并输出结果 1.3 网络管理1234567891011121314# 随机分配端口号docker run -P# 绑定特定端口号（主机的所有网络接口的5000端口均绑定容器的5000端口）docker run -p 5000:5000# 绑定主机的特定接口的端口号docker run -p 127.0.0.1:5000:5000# 绑定udp端口号docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py# 查看容器的5000端口对应本地机器的IP和端口号docker port &lt;CONTAINER_ID&gt; 5000# 使用Docker Linking连接容器：# Docker为源容器和接收容器创建一个安全的通道，容器之间不需要暴露端口，接收的容器可以访问源容器的数据docker run -d -P --name &lt;CONTAINER_NAME&gt; --link &lt;CONTAINER_NAME_TO_LINK&gt;:&lt;ALIAS&gt; 1.4 数据管理1234567891011121314151617181920212223# Data Volumes：volume是在一个或多个容器里指定的特殊目录# 数据卷可以在容器间共享和重复使用# 可以直接修改容器卷的数据# 容器卷里的数据不会被包含到镜像中# 容器卷保持到没有容器再使用它# 可以在容器启动的时候添加-v参数指定容器卷，也可以在Dockerfile里用VOLUMN命令添加docker run -d -P --name web -v /webapp training/webapp python app.py# 也可以将容器卷挂载到宿主机目录或宿主机的文件上，&lt;容器目录或文件&gt;的内容会被替换为&lt;宿主机目录或文件&gt;的内容，默认容器对这个目录有可读写权限docker run -d -P --name web -v &lt;宿主机目录&gt;:&lt;容器目录&gt; training/webapp python app.py# 可以通过指定ro，将权限改为只读docker run -d -P --name web -v &lt;宿主机目录&gt;:&lt;容器目录&gt;:ro training/webapp python app.py# 在一个容器创建容器卷后，其他容器便可以通过--volumes-from共享这个容器卷数据，如下：docker run -d -v /dbdata --name db1 training/postgres echo Data-only container for postgres# 首先启动了一个容器，并为这个容器增加一个数据卷/dbdata，然后启动另一个容器，共享这个数据卷docker run -d --volumes-from db1 --name db2 training/postgres# 此时db2使用了db1的容器卷，当容器db1被删除时，容器卷也不会被删除，只有所有容器不再使用此容器卷时，才会被删除docker rm -v：删除容器卷# 除了共享数据外，容器卷另一个作用是用来备份、恢复和迁移数据docker run --volumes-from db1 -v /home/backup:/backup ubuntu tar cvf /backup/backup.tar /dbdata# 启动一个容器数据卷使用db1容器的数据卷，同时新建立一个数据卷指向宿主机目录/home/backup，将/dbdata目录的数据压缩为/backup/backup.tardocker run -v /dbdata --name dbdata2 ubuntu /bin/bashdocker run --volumes-from dbdata2 -v /home/backup:/backup busybox tar xvf /backup/backup.tar# 启动一个容器，同时把backup.tar的内容解压到容器的backup 1.5 仓库管理docker login：登录 1.6 镜像打包及恢复1.6.1 保存镜像为文件1234# centos为镜像，centos为打包名，centos后可以指定标签docker save -o centos.tar centosordocker save -o centos.tar centos:7.2 1.6.2 从文件载入镜像1234# home/centos.tar为镜像文件路径docker load --input /home/centos.tarordocker load &lt; /home/centos.tar]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu部署本地源仓库]]></title>
    <url>%2F2018%2F08%2F27%2FUbuntu%E9%83%A8%E7%BD%B2%E6%9C%AC%E5%9C%B0%E6%BA%90%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1 本地源的制作 1.1 安装所需软件包1sudo apt-get install dpkg-dev 1.2 打包deb软件包12#加上-d参数，只下载安装包，不安装及解压。sudo apt-get install -d nginx 将/var/cache/apt/archives/下的所有deb文件拷到/home/packages/下的Natty目录中：/home/packages/Natty，拷贝前建议执行一下: 12#autoclean - 删除已下载的旧包文件sudo apt-get autoclean 1.3 进入指定目录上一级目录拷完后在终端中进入刚才新建的目录Natty所在的上一级目录，也就是： 1cd /home/packages 生成软件包依赖信息文件 1sudo dpkg-scanpackages Natty/ | gzip &gt;Natty/Packages.gz 至此本地源的软件包已经准备完毕；下面接着介绍如何使用。 2 本地源的使用2.1 本机源服务器的搭建将地址加入更新源列表文件 123$ sudo vim /etc/apt/sources.list# 添加以下路径，其它deb信息使用#号注释掉deb file:///home/packages/ Natty/ #注意Natty后面有一个斜杠,前面还要有空格(这是书写方式) 2.2 更新源信息12#更新信息，生成数据缓存$ sudo apt-get update 之后即可正常安装所需软件。 3 局域网源服务器3.1 安装apache21sudo apt-get install apache2 启动服务 注意：配置apache2的时候注意端口，不要配置成可能被其他网络应用使用的端口就可以。 3.2 配置服务器上的Ubuntu源12#在apache2发布目录/var/www/html位置创建到源目录的软链接sudo ln -s /home/packages/Natty/ /var/www/html/ubuntu-local 3.3 配置局域网客户机sources.list1234$ sudo vim /etc/apt/sources.list#添加如下信息,其它deb信息使用#号注释掉deb http://192.168.1.224 ubuntu-local/ #注意书写方式，ip地址后空格，目录地址/ 3.4 客户机更新源信息12#更新信息，生成数据缓存sudo apt-get update 之后即可正常安装所需软件。 4 参考资料https://www.iyunv.com/thread-384273-1-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu同步源仓库到本地]]></title>
    <url>%2F2018%2F08%2F27%2FUbuntu%E5%90%8C%E6%AD%A5%E6%BA%90%E4%BB%93%E5%BA%93%E5%88%B0%E6%9C%AC%E5%9C%B0%2F</url>
    <content type="text"><![CDATA[1 步骤 1.1 安装apt-mirror1sudo apt-get install apt-mirro 1.2 创建下载目录12345#路径自定义.../ubuntu.../ubuntu/mirror.../ubuntu/skel.../ubuntu/var 1.3 修改配置文件1sudo vi /etc/apt/mirror.list 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263############# config ###################set base_path /home/packages/ubuntu#set mirror_path $base_path/mirrorset skel_path $base_path/skelset var_path $base_path/varset cleanscript $var_path/clean.shset nthreads 20set _tilde 0############## end config ###############deb http://archive.ubuntu.com/ubuntu trusty main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-security main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-updates main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-proposed main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-backports main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-security main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-updates main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-proposed main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-backports main restricted universe multiverse#clean http://archive.ubuntu.com/ubuntu#自定义#参考以下配置文件：#清空原有的配置文件，直接使用以下配置文件即可############# config ################### 以下注释的内容都是默认配置，如果需要自定义，取消注释修改即可#set base_path /var/spool/apt-mirror## 镜像文件下载地址# set mirror_path $base_path/mirror# 临时索引下载文件目录，也就是存放软件仓库的dists目录下的文件（默认即可）# set skel_path $base_path/skel# 配置日志（默认即可）# set var_path $base_path/var# clean脚本位置# set cleanscript $var_path/clean.sh# 架构配置，i386/amd64，默认的话会下载跟本机相同的架构的源#set defaultarch amd64# set postmirror_script $var_path/postmirror.sh# set run_postmirror 0# 下载线程数#set nthreads 20#set _tilde 0############## end config ############### Ali yun（这里没有添加deb-src的源）#我们把常用的软件同步过来就够用了deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse#当某些软件包在服务器端进行了升级，或者服务器端不再需要这些软件包时，我们使用了 apt-mirror与服务器同步后，会在本地的$var_path/下生成一个clean.sh的脚本，列出了遗留在本地的旧版本和无用的软件包，你可 以手动运行这个脚本来删除遗留在本地的且不需要用的软件包clean http://mirrors.aliyun.com/ubuntu 如果用amd64位架构下的包，可以加上deb-amd64的标记如果什么都不加，直接使用deb http…..这种格式，则在同步时，只同步当前系统所使用的架构下的软件包。比如一个64位系统，直接debhttp….只同步64位的软件 包。如果还嫌麻烦，直接去改set defaultarch 这个参数就好，比如改成set defaultarch i386，这样你使用debhttp…..这种格式，则在同步时，只同步i386的软件包了。 如果你还想要源码，可以把源码也加到mirror.list里面同步过来，比如加上deb-src这样的标记。想要其他的东西也可以追加相应的标记来完成。 1.4 同步源配置好后我们就可以和指定的镜像进行同步了 1sudo apt-mirror 1.5 建立本地源参考：Ubuntu部署本地源仓库 2 参考资料http://www.linuxidc.com/Linux/2014-08/105415.htm https://www.iyunv.com/thread-384273-1-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群-Docker_Swarm]]></title>
    <url>%2F2018%2F08%2F27%2FDocker%E9%9B%86%E7%BE%A4-Docker-Swarm%2F</url>
    <content type="text"><![CDATA[1 安装 Swarm 1.1 下载镜像1$ docker pull swarm 可以使用下面的命令来查看 Swarm 版本，验证是否成功下载 Swarm 镜像。 12$ docker run --rm swarm -vswarm version 1.2.2 (34e3da3) 1.2 配置节点Docker 主机在加入 Swarm 集群前，需要进行一些简单配置，添加 Docker daemon 的网络监听。 例如，在启动 Docker daemon 的时候通过 -H 参数： 1$ sudo docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 注：Docker 1.8.0 版本之前不支持 daemon 命令，可以用 -d 代替。 如果是通过服务方式启动，则需要修改服务的配置文件。 以 Ubuntu 14.04 为例，配置文件为 /etc/default/docker（其他版本的 Linux 上略有不同）。 在文件的最后添加： 1DOCKER_OPTS="$DOCKER_OPTS -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock" 1.3 启动集群Docker 集群管理需要使用服务发现（Service Discover）功能，Swarm 支持以下的几种方式：DockerHub、本地文件、etcd、consul、zookeeper 和手动指定节点 IP 地址信息等。 除了手动指定外，这些方法原理上都是通过维护一套数据库机制，来管理集群中注册节点的 Docker daemon 的访问信息。 本地配置集群推荐使用 consul 作为服务发现后端。利用社区提供的 Docker 镜像，整个过程只需要三步即可完成。 1.3.1 启动 Consul 服务后端启动 consul 服务容器，映射到主机的 8500 端口。 1$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap 获取到本地主机的地址作为 consul 的服务地址：&lt;consul_ip&gt;:8500。 1.3.2 启动管理节点首先，启动一个主管理节点，映射到主机的 4000 端口，并获取所在主机地址为 &lt;manager0_ip&gt;。其中 4000 端口是 Swarm 管理器的默认监听端口，用户也可以指定映射为其它端口。 1$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &lt;manager0_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500 为了提高高可用性，用户也可以启动从管理节点。假定获取所在主机地址为 &lt;manager1_ip&gt;。 1$ docker run -d swarm manage -H :4000 --replication --advertise &lt;manager1_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500 1.3.3 启动工作节点需要在每个工作节点上启动 agent 服务。 获取节点的主机地址为 &lt;node_ip&gt;，并指定前面获取到的 consul 服务地址。 1$ docker run -d swarm join --advertise=&lt;node_ip&gt;:2375 consul://&lt;consul_ip&gt;:8500 节点启动后，用户可以指定 Docker 服务地址为 &lt;manager0_ip&gt;:4000&gt; 来测试各种 Docker 命令，可以看到整个 Swarm 集群就像一个虚拟的 Docker 主机一样正常工作。 由于 Swarm 实际上是通过 agent 调用了本地的 Docker daemon 来运行容器，当 Swarm 集群服务出现故障时，无法接受新的请求，但已经运行起来的容器将不会受到影响。 2 使用 Swarm前面演示了基于 consul 服务发现后端来配置一个本地 Swarm 集群。其中，consul 也可以被替换为 etcd、zookeeper 等。 另外一个更方便的方式是直接使用 DockerHub 提供的免费服务发现后端。 下面使用这种方式来演示 Swarm 的主要操作，包括： create：创建一个集群； list：列出集群中的节点； manage：管理一个集群； join：让节点加入到某个集群。 注意，使用 DockerHub 的服务发现后端，需要各个节点能通过公网访问到 DockerHub 的服务接口。 2.1 创建集群 id在任意一台安装了 Swarm 的机器上执行 swarm create 命令来在 DockerHub 服务上进行注册。 Swarm 会通过服务发现后端（此处为 DockerHub 提供）来获取一个唯一的由数字和字母组成的 token，用来标识要管理的集群。 12$ docker run --rm swarm create946d65606f7c2f49766e4dddac5b4365 注意返回的字符串，这是集群的唯一 id，加入集群的各个节点将需要这个信息。 2.2 配置集群节点在所有要加入集群的普通节点上面执行 swarm join 命令，表示把这台机器加入指定集群当中。 例如某台机器 IP 地址为 192.168.0.2，将其加入我们刚创建的 946d65606f7c2f49766e4dddac5b4365 集群，则可以通过： 12$ docker run --rm swarm join --addr=192.168.0.2:2375 token://946d65606f7c2f49766e4dddac5b4365time="2015-12-09T08:59:43Z" level=info msg="Registering on the discovery service every 20s..." addr="192.168.0.2:2375" discovery="token://946d65606f7c2f49766e4dddac5b4365" 注：其中 –addr 指定的 IP 地址信息将被发送给服务发现后端，用以区分集群不同的节点。manager服务必须要通过这个地址可以访问到该节点。 通过控制台可以看到，上述命令执行后，默认每隔 20 秒（可以通过 –heartbeat 选项指定），会输出一条心跳信息。对于发现服务后端来说，默认如果超过 60 秒（可以通过 –ttl 选项指定）没有收到心跳信息，则将节点从列表中删除。 如果不希望看到输出日志信息，则可以用 -d 选项替换 –rm 选项，让服务后台执行。 执行 swarm join 命令实际上是通过 agent 把自己的信息注册到发现服务上，因此，此时对于后端的发现服务来说，已经可以看到有若干节点注册上来了。那么，如何管理和使用这些节点呢，这就得需要 Swarm 的 manager 服务了。 2.3 配置管理节点配置管理节点需要通过 swarm manage 命令，该命令将启动 manager 服务，默认监听到 2375 端口，所有对集群的管理可以通过该服务接口进行。 读者可能注意到，manager 服务默认监听的端口跟 Docker 服务监听端口是一样的，这是为了兼容其它基于 Docker 的服务，可以无缝地切换到 Swarm 平台上来。 仍然在节点 192.168.0.2 进行操作。由于我们是采用 Docker 容器形式启动 manager 服务，本地的 2375端口已经被 Docker Daemon 占用。我们将 manager 服务监听端口映射到本地一个空闲的 12375 端口。 12$ docker run -d -p 12375:2375 swarm manage token://946d65606f7c2f49766e4dddac5b43651e1ca8c4117b6b7271efc693f9685b4e907d8dc95324350392b21e94b3cffd18 可以通过 docker ps 命令来查看启动的 swarm manager 服务容器。 123$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1e1ca8c4117b swarm "/swarm manage token:" 11 seconds ago Up 10 seconds 0.0.0.0:12375-&gt;2375/tcp jovial_rosalind 命令如果执行成功会返回刚启动的 Swarm 容器的 ID，此时一个简单的 Swarm 集群就已经搭建起来了，包括一个普通节点和一个管理节点。 2.4 查看集群节点列表集群启动成功以后，用户可以在任何一台节点上使用 swarm list 命令查看集群中的节点列表。例如 12$ docker run --rm swarm list token://946d65606f7c2f49766e4dddac5b4365192.168.0.2:2375 显示正是之前用 swarm join 命令加入集群的节点的地址。 我们在另外一台节点 192.168.0.3 上同样使用 swarm join 命令新加入一个节点： 123$ docker run --rm swarm join --addr=192.168.0.3:2375 token://946d65606f7c2f49766e4dddac5b4365time="2015-12-10T02:05:34Z" level=info msg="Registering on the discovery service every 20s..." addr="192.168.0.3:2375" discovery="token://946d65606f7c2f49766e4dddac5b4365"... 再次使用 swarm list 命令查看集群中的节点列表信息，可以看到新加入的节点： 123$ docker run --rm swarm list token://946d65606f7c2f49766e4dddac5b4365192.168.0.3:2375192.168.0.2:2375 2.5 使用集群服务那么，怎么使用 Swarm 提供的服务呢？ 实际上，所有 Docker 客户端可以继续使用，只要指定使用 Swarm manager 服务的监听地址即可。 例如，manager 服务监听的地址为 192.168.0.2:12375，则可以通过指定 -H 192.168.0.2:12375 选项来继续使用 Docker 客户端，执行任意 Docker 命令，例如 ps、info、run 等等。 在任意节点上使用 docker run 来启动若干容器，例如 12$docker -H 192.168.0.2:12375:12375 run -d ubuntu ping 127.0.0.14c9bccbf86fb6e2243da58c1b15e9378fac362783a663426bbe7058eea84de46 使用 ps 命令查看集群中正在运行的容器。 12345$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4c9bccbf86fb ubuntu "ping 127.0.0.1" About a minute ago Up About a minute clever_wright730061a3801a registry:latest "docker-registry" 2 minutes ago Up 2 minutes 192.168.0.2:5000-&gt;5000/tcp Host-1/registry_registry_172d99f24a06f redis:3.0 "/entrypoint.sh redis" 2 minutes ago Up 2 minutes 6379/tcp Host-1/registry_redis_1,Host-1/registry_registry_1/redis,Host-1/registry_registry_1/redis_1,Host-1/registry_registry_1/registry_redis_1 输出结果中显示目前集群中正在运行的容器（注意不包括 Swarm manager 服务容器），可以在不同节点上使用 docker ps 查看本地容器，发现这些容器实际上可能运行在集群中多个节点上（被 Swarm 调度策略进行分配）。 使用 info 查看所有节点的信息。 1234567891011121314151617181920$ docker -H 192.168.0.2:12375 infoContainers: 18Images: 36Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 2 Host-1: 192.168.0.2:2375 └ Containers: 15 └ Reserved CPUs: 0 / 4 └ Reserved Memory: 1 GiB / 4.053 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-43-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs Host-2: 192.168.0.3:2375 └ Containers: 3 └ Reserved CPUs: 0 / 8 └ Reserved Memory: 0 B / 16.46 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufsCPUs: 12Total Memory: 20.51 GiBName: 1e1ca8c4117b 结果输出显示这个集群目前只有两个节点，地址分别是 192.168.0.2 和 192.168.0.3。 类似的，也可以通过 Compose 模板来启动多个服务。不过请注意，要想让服务分布到多个 Swarm 节点上，需要采用版本 2 的写法。 2.6 使用网络Swarm 为了支持跨主机的网络，默认采用了 overlay 网络类型，实现上通过 vxlan 来构建联通整个 Swarm 集群的网络。 首先，在集群中所有节点上，添加配置 Docker daemon 选项： 1--cluster-store=&lt;DISCOVERY_HOST:PORT&gt; --cluster-advertise=&lt;DOCKER_DAEMON_HOST:PORT&gt; 以 consul 服务为例，可能类似： 1--cluster-store=consul://&lt;consul 服务地址&gt;:8500 --cluster-advertise=192.168.0.3:2375 之后重启 Docker 服务。 首先，创建一个网络。 1$ docker -H 192.168.0.2:12375 network create swarm_network 查看网络，将看到一个 overlay 类型的网络。 123$ docker -H 192.168.0.2:12375 network lsNETWORK ID NAME DRIVER6edf2d16ec97 swarm_network overlay 此时，所有添加到这个网络上的容器将自动被分配到集群中的节点上，并且彼此联通。 3 使用其它服务发现后端Swarm 目前可以支持多种服务发现后端，这些后端功能上都是一致的，即维护属于某个集群的节点的信息。不同方案并无优劣之分，在实际使用时候，可以结合自身需求和环境限制进行选择，甚至自己定制其它方案。 使用中可以通过不同的路径来选择特定的服务发现后端机制。 -token://&lt;token&gt;：使用 DockerHub 提供的服务，适用于可以访问公网情况； file://path/to/file：使用本地文件，需要手动管理； consul://&lt;ip&gt;/&lt;path&gt;：使用 consul服务，私有环境推荐； etcd://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;：使用 etcd 服务，私有环境推荐； zk://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;：使用 zookeeper 服务，私有环境推荐； [nodes://]&lt;ip1&gt;,&lt;ip2&gt;：手动指定集群中节点的地址，方便进行服务测试。 3.1 使用文件使用本地文件的方式十分简单，就是讲所有属于某个集群的节点的 Docker daemon 信息写入一个文件中，然后让 manager 从这个文件中直接读取相关信息。 首先，在 Swarm 管理节点（192.168.0.2）上新建一个文件，把要加入集群的机器的 Docker daemon 信息写入文件： 1234$ tee /tmp/cluster_info &lt;&lt;-'EOF'192.168.0.2:2375192.168.0.3:2375EOF 然后，本地执行 swarm manage 命令，并指定服务发现机制为本地文件，注意因为是容器方式运行 manager，需要将本地文件挂载到容器内。 1$ docker run -d -p 12375:2375 -v /tmp/cluster_info:/tmp/cluster_info swarm manage file:///tmp/cluster_info 接下来就可以通过使用 Swarm 服务来进行管理了，例如使用 info 查看所有节点的信息。 1234567891011121314151617181920$ docker -H 192.168.0.2:12375 infoContainers: 18Images: 36Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 2 Host-1: 192.168.0.2:2375 └ Containers: 15 └ Reserved CPUs: 0 / 4 └ Reserved Memory: 1 GiB / 4.053 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-43-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs Host-2: 192.168.0.3:2375 └ Containers: 3 └ Reserved CPUs: 0 / 8 └ Reserved Memory: 0 B / 16.46 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufsCPUs: 12Total Memory: 20.51 GiBName: e71eb5f1d48b 3.2 其它发现服务后端其它服务发现后端的使用方法，也是大同小异，不同之处在于使用 Swarm 命令时指定的路径格式不同。 例如，对于前面介绍的 consul 服务后端来说。 快速部署一个 consul 服务的命令为： 1$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap 之后创建 Swarm 的管理服务，指定使用 consul 服务，管理端口监听在本地的 4000 端口。 1$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &lt;manager_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500 Swarm 节点注册时候命令格式类似于： 1$ swarm join --advertise=&lt;node_ip:2375&gt; consul://&lt;consul_addr&gt;/&lt;optional path prefix&gt; 对于 etcd 服务后端来说，节点注册时候命令格式类似于： 1$ swarm join --addr=&lt;node_addr:2375&gt; etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt; 启动管理服务时候，格式类似于： 1$ swarm manage -H tcp://&lt;manager_ip&gt;:4000 etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt; 3.3 地址和端口的范围匹配对于基于文件，以及手动指定节点信息两种服务发现后端机制来说，其中地址和端口域可以支持指定一个范围，以一次性指定多个地址。 例如： 192.168.0.[2:10]:2375 代表 192.168.0.2:2375 – 192.168.0.10:2375 一共 9 个地址； 192.168.0.2:[2:9]375 代表 192.168.0.2:2375 – 192.168.0.2:9375 一共 8 个地址。 4 Swarm 中的调度器调度是集群十分重要的功能，Swarm 目前支持三种调度策略：spread、binpack 和 random。 在执行swarm manage命令启动管理服务的时候，可以通过--strategy 参数指定调度策略，默认的是 spread。 简单来说，这三种调度策略的优化目标如下： spread：如果节点配置相同，选择一个正在运行的容器数量最少的那个节点，即尽量平摊容器到各个节点； binpack：跟 spread 相反，尽可能的把所有的容器放在一台节点上面运行，即尽量少用节点，避免容器碎片化。 random：直接随机分配，不考虑集群中节点的状态，方便进行测试使用。 4.1 spread 调度策略仍然以之前创建好的集群为例，来演示下 spread 策略的行为。 在 192.168.0.2 节点启动管理服务，管理 token://946d65606f7c2f49766e4dddac5b4365 的集群。 12$ docker run -d -p 12375:2375 swarm manage --strategy "spread" token://946d65606f7c2f49766e4dddac5b4365c6f25e6e6abbe45c8bcf75ac674f2b64d5f31a5c6070d64ba954a0309b197930 列出集群中节点。 123$ docker run --rm swarm list token://946d65606f7c2f49766e4dddac5b4365192.168.0.3:2375192.168.0.2:2375 此时，两个节点上除了 swarm 外都没有运行其它容器。 启动一个 ubuntu 容器。 12$ docker -H 192.168.0.2:12375 run -d ubuntu:14.04 ping 127.0.0.1bac3dfda5306181140fc959969d738549d607bc598390f57bdd432d86f16f069 查看发现它实际上被调度到了 192.168.0.3 节点（当节点配置相同时候，初始节点随机选择）。 再次启动一个 ubuntu 容器。 12$ docker -H 192.168.0.2:12375 run -d ubuntu:14.04 ping 127.0.0.18247067ba3a31e0cb692a8373405f95920a10389ce3c2a07091408281695281c 查看它的位置，发现被调度到了另外一个节点：192.168.0.2 节点。 1234$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8247067ba3a3 ubuntu:14.04 "ping 127.0.0.1" 1 minutes ago Up 1 minutes Host-2/sick_galileobac3dfda5306 ubuntu:14.04 "ping 127.0.0.1" 2 minutes ago Up 2 minutes Host-3/compassionate_ritchie 当节点配置不同的时候，spread会更愿意分配到配置较高的节点上。 4.2 binpack 调度策略现在来看看 binpack 策略下的情况。 直接启动若干 ubuntu 容器，并查看它们的位置。 1234567$ docker -H 192.168.0.2:12375 run -d ubuntu:14.04 ping 127.0.0.1$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4c4f45eba866 ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/hopeful_brown5e650541233c ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/pensive_wright99c5a092530a ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/naughty_engelbart4ab392c26eb2 ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/thirsty_mclean 可以看到，所有的容器都是分布在同一个节点（192.168.0.3）上运行的。 5 Swarm 中的过滤器Swarm 的调度器可以按照指定调度策略自动分配容器到节点。但有些时候希望能对这些分配加以干预。比如说，让 IO 敏感的容器分配到安装了 SSD 的节点上；让计算敏感的容器分配到 CPU 核数多的机器上；让网络敏感的容器分配到高带宽的机房；让某些容器尽量放同一个节点……。 这可以通过过滤器（filter）来实现，目前支持 Constraint、Affinity、Port、Dependency、Health等五种过滤器。 5.1 Constraint 过滤器Constraint 过滤器是绑定到节点的键值对，相当于给节点添加标签。 可在启动 Docker 服务的时候指定，例如指定某个节点颜色为 red。 1$ sudo docker daemon --label color=red -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 同样的，可以写在 Docker 服务的配置文件里面（以 Ubuntu 14.04 为例，是 /etc/default/docker）。 1DOCKER_OPTS="--label color=red -H 0.0.0.0:2375 -H unix:///var/run/docker.sock" 使用 Swarm 启动容器的时候，采用 -e constarint:key=value 的形式，可以过滤选择出匹配条件的节点。 例如，我们将 192.168.0.2 节点打上红色标签，192.168.0.3 节点打上绿色标签。 然后，分别启动两个容器，指定使用过滤器分别为红色和绿色。 1234$ docker -H 192.168.0.2:12375 run -d -e constraint:color==red ubuntu:14.04 ping 127.0.0.1252ffb48e64e9858c72241f5eedf6a3e4571b1ad926faf091db3e26672370f64$ docker -H 192.168.0.2:12375 run -d -e constraint:color==green ubuntu:14.04 ping 127.0.0.13d6f8d7af8583416b17061d038545240c9e5c3be7067935d3ef2fbddce4b8136 注：指定标签中间是两个等号 查看它们将被分配到指定节点上。 1234$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES252ffb48e64e ubuntu:14.04 "ping 127.0.0.1" 1 minutes ago Up 1 minutes Host-2/sick_galileo3d6f8d7af858 ubuntu:14.04 "ping 127.0.0.1" 2 minutes ago Up 2 minutes Host-3/compassionate_ritchie 另外，Docker 内置了一些常见的过滤器，包括 node、storagedriver、executiondriver、kernelversion、operatingsystem 等。这些值可以通过 docker info 命令查看。 例如，目前集群中各个节点的信息为： 1234567891011121314151617181920$ docker -H 192.168.0.2:12375 infoContainers: 5Images: 39Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 2 Host-2: 192.168.0.2:2375 └ Containers: 4 └ Reserved CPUs: 0 / 4 └ Reserved Memory: 1 GiB / 4.053 GiB └ Labels: color=red, executiondriver=native-0.2, kernelversion=3.16.0-43-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs Host-3: 192.168.0.3:2375 └ Containers: 1 └ Reserved CPUs: 0 / 8 └ Reserved Memory: 0 B / 16.46 GiB └ Labels: color=green, executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufsCPUs: 12Total Memory: 20.51 GiBName: 946d65606f7c 5.2 Affinity 过滤器Affinity 过滤器允许用户在启动一个容器的时候，让它分配到某个已有容器的节点上。 例如，下面我们将启动一个 nginx 容器，让它分配到已经运行某个 ubuntu 容器的节点上。 在 Constraint 过滤器的示例中，我们分别启动了两个 ubuntu 容器 sick_galileo 和 compassionate_ritchie，分别在 Host-2 和 Host-3 上。 现在启动一个 nginx 容器，让它跟容器 sick_galileo 放在一起，都放到 Host-2 节点上。可以通过 -e affinity:container==&lt;name or id&gt; 参数来实现。 1$ docker -H 192.168.0.2:12375 run -d -e affinity:container==sick_galileo nginx 然后启动一个 redis 容器，让它跟容器 compassionate_ritchie 放在一起，都放到 Host-3 节点上。 1$ docker -H 192.168.0.2:12375 run -d -e affinity:container==compassionate_ritchie redis 查看所有容器运行情况。 123456$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0a32f15aa8ee redis "/entrypoint.sh redis" 2 seconds ago Up 1 seconds 6379/tcp Host-3/awesome_darwind2b9a53e67d5 nginx "nginx -g 'daemon off" 29 seconds ago Up 28 seconds 80/tcp, 443/tcp Host-2/fervent_wilson252ffb48e64e ubuntu:14.04 "ping 127.0.0.1" 2 minutes ago Up 2 minutes Host-2/sick_galileo3d6f8d7af858 ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/compassionate_ritchie 5.3 其它过滤器其它过滤器的使用方法也是大同小异，例如通过 affinity:image==&lt;name or id&gt; 来选择拥有指定镜像的节点；通过 -e affinity:label_name==value 来选择拥有指定标签的容器所允许的节点。 此外，当容器端口需要映射到宿主机指定端口号的时候，Swarm 也会自动分配容器到指定宿主机端口可用的节点。 当不同容器之间存在数据卷或链接依赖的时候，Swarm 会分配这些容器到同一个节点上。 6 本章小结本章笔者介绍了 Docker Swarm 的安装、使用和主要功能。 通过使用 Swarm，用户可以将若干 Docker 主机节点组成的集群当作一个大的虚拟 Docker 主机使用。并且，原先基于单机的 Docker 应用，可以无缝的迁移到 Swarm 上来。 实现这些功能的前提是服务自动发现能力。在现代分布式系统中，服务的自动发现、注册、更新等能力将成为系统的基本保障和重要基础。 在生产环境中，Swarm 的管理节点和发现服务后端要采用高可用性上的保护，可以采用集群模式。 值得一提的是，Swarm V2 功能已经被无缝嵌入到了 Docker 1.12+ 版本中，用户今后可以直接使用 Docker 命令来完成相关功能的配置，这将使得集群功能的管理更加简便。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>swarm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack云镜像制作-Centos7篇]]></title>
    <url>%2F2018%2F08%2F27%2FOpenstack%E4%BA%91%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C-Centos7%E7%AF%87%2F</url>
    <content type="text"><![CDATA[一、制作步骤 1、安装kvm参考centos 7系统安装配置kvm软件步骤 1、创建虚拟硬盘大小10G 名称：centos7-dis.qcow2 2、安装系统 注意一：分区，分区的时候只给”/“ 根目录分一个区即可，其他都不要。格式ext4注意二：网络设置方面，确保你的网卡eth0是DHCP状态的，而且请务必勾上”auto connect”的对勾 2、进入虚拟机系统操作关于CentOS镜像制作需要注意以下几点： (1) 修改网络信息 /etc/sysconfig/network-scripts/ifcfg-eth0 （删掉mac信息)，如下： 12345TYPE=Ethernet DEVICE=eth0 ONBOOT=yes BOOTPROTO=dhcp NM_CONTROLLED=no (2) 删除已生成的网络设备规则，否则制作的镜像不能上网1$ rm -rf /etc/udev/rules.d/70-persistent-net.rules (3)增加一行到/etc/sysconfig/network 1NOZERCONF=yes (4)安装cloud-init（可选），cloud-init可以在开机时进行密钥注入以及修改hostname等，关于cloud-init，陈沙克的一篇博文有介绍：http://www.chenshake.com/about-openstack-centos-mirror/ 1$ yum install -y cloud-utils cloud-init parted 修改配置文件/etc/cloud/cloud.cfg ，在cloud_init_modules 下面增加: 1- resolv-conf (5)设置系统能自动获取openstack指定的hostname和ssh-key（可选）编辑/etc/rc.local文件，该文件在开机后会执行，加入以下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950if [ ! -d /root/.ssh ]; thenmkdir -p /root/.sshchmod 700 /root/.sshfi# Fetch public key using HTTPATTEMPTS=30FAILED=0 while [ ! -f /root/.ssh/authorized_keys ]; docurl -f http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key &gt; /tmp/metadata-key 2&gt;/dev/nullif [ $? -eq 0 ]; thencat /tmp/metadata-key &gt;&gt; /root/.ssh/authorized_keyschmod 0600 /root/.ssh/authorized_keysrestorecon /root/.ssh/authorized_keysrm -f /tmp/metadata-keyecho “Successfully retrieved public key from instance metadata”echo “*****************”echo “AUTHORIZED KEYS”echo “*****************”cat /root/.ssh/authorized_keysecho “*****************”curl -f http://169.254.169.254/latest/meta-data/hostname &gt; /tmp/metadata-hostname 2&gt;/dev/nullif [ $? -eq 0 ]; thenTEMP_HOST=`cat /tmp/metadata-hostname`sed -i “s/^HOSTNAME=.*$/HOSTNAME=$TEMP_HOST/g” /etc/sysconfig/network/bin/hostname $TEMP_HOSTecho “Successfully retrieved hostname from instance metadata”echo “*****************”echo “HOSTNAME CONFIG”echo “*****************”cat /etc/sysconfig/networkecho “*****************”elseecho “Failed to retrieve hostname from instance metadata. This is a soft error so we’ll continue”firm -f /tmp/metadata-hostnameelseFAILED=$(($FAILED + 1))if [ $FAILED -ge $ATTEMPTS ]; thenecho “Failed to retrieve public key from instance metadata after $FAILED attempts, quitting”breakfiecho “Could not retrieve public key from instance metadata (attempt #$FAILED/$ATTEMPTS), retrying in 5 seconds…”sleep 5fidone 或者 1234567891011121314151617181920212223# set a random pass on first bootif [ -f /root/firstrun ]; then dd if=/dev/urandom count=50|md5sum|passwd --stdin root passwd -l root rm /root/firstrunfiif [ ! -d /root/.ssh ]; then mkdir -m 0700 -p /root/.ssh restorecon /root/.sshfi# Get the root ssh key setup# Get the root ssh key setupReTry=0while [ ! -f /root/.ssh/authorized_keys ] &amp;&amp; [ $ReTry -lt 10 ]; do sleep 2 curl -f http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key &gt; /root/.ssh/pubkey if [ 0 -eq 0 ]; then mv /root/.ssh/pubkey /root/.ssh/authorized_keys fi ReTry=$[Retry+1]donechmod 600 /root/.ssh/authorized_keys &amp;&amp; restorecon /root/.ssh/authorized_keys 主要目的就是获取hostname和公钥 (6)其他 route命令查看一下路由表 查看/etc/ssh/sshd_conf中PermitRootLogin是不是为yes 清除操作记录 123456789101112131415161718192021222324252627282930清除登陆系统成功的记录[root@localhost root]# echo &gt; /var/log/wtmp //此文件默认打开时乱码，可查到ip等信息[root@localhost root]# last //此时即查不到用户登录信息清除登陆系统失败的记录[root@localhost root]# echo &gt; /var/log/btmp //此文件默认打开时乱码，可查到登陆失败信息[root@localhost root]# lastb //查不到登陆失败信息 清除历史执行命令[root@localhost root]# history -c //清空历史执行命令[root@localhost root]# echo &gt; ./.bash_history //或清空用户目录下的这个文件即可 导入空历史记录[root@localhost root]# vi /root/history //新建记录文件[root@localhost root]# history -c //清除记录 [root@localhost root]# history -r /root/history.txt //导入记录 [root@localhost root]# history //查询导入结果example [root@localhost root]# vi /root/history[root@localhost root]# history -c [root@localhost root]# history -r /root/history.txt [root@localhost root]# history [root@localhost root]# echo &gt; /var/log/wtmp [root@localhost root]# last[root@localhost root]# echo &gt; /var/log/btmp[root@localhost root]# lastb [root@localhost root]# history -c [root@localhost root]# echo &gt; ./.bash_history[root@localhost root]# history 关闭虚拟机 3、宿主机操作资料：KVM镜像管理利器-guestfish使用详解 1）安装guestfish套件安装 1$ yum install libguestfs-tools 2）压缩镜像文件 1$ virt-sparsify --compress centos7-dis.qcow2 centos7-dis-cloud.qcow2 镜像制作完成 上传openstack 二、参考文档：penStack镜像制作-CentOS openstack镜像制作思路、指导及问题总结 openstack制作centos6.5镜像 制作OpenStack上使用的CentOS系统镜像 KVM镜像管理利器-guestfish使用详解 CentOS清除用户登录记录和命令历史方法]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware共享文件夹设置方法]]></title>
    <url>%2F2018%2F08%2F27%2FVMware%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%E5%A4%B9%E8%AE%BE%E7%BD%AE%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、安装包依赖： 12$ yum -y install kernel-devel-$(uname -r) $ yum -y install net-tools perl gcc gcc-c++ 二、安装vmtool123456$ mount /dev/cdrom /home/tmp$ cp /home/tmp/VMwareTools-9.6.0-1294478.tar.gz /tmp$ cd /tmp$ tar -zxvf VMwareTools-9.6.0-1294478.tar.gz$ cd vmware-tools-distrib$ ./vmware-install.pl 按提示操作即可。 三、问题有/mnt/hgfs但没有共享文件的解决方法： 12$ mount -t vmhgfs .host:/ /mnt/hgfsError: cannot mount filesystem: No such device 这时不能用mount工具挂载，而是得用vmhgfs-fuse，需要安装工具包 123$ yum install open-vm-tools-devel -y有的源的名字并不一定为open-vm-tools-devel(centos) ，而是open-vm-dkms(unbuntu)执行：vmhgfs-fuse .host:/ /mnt/hgfs 此时进入/mnt/hgfs就能看到你设置的共享文件夹了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSL证书制作]]></title>
    <url>%2F2018%2F08%2F27%2FSSL%E8%AF%81%E4%B9%A6%E5%88%B6%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一、环境 OS:centos 7.3 二、步骤1、安装openssl1$ yum install opensll 2、制作CA证书12$ openssl genrsa -des3 -out my-ca.key 2048$ openssl req -new -x509 -days 3650 -key my-ca.key -out my-ca.crt 3、生成服务器证书123$ openssl genrsa -des3 -out mars-server.key 1024$ openssl req -new -key mars-server.key -out mars-server.csr$ openssl x509 -req -in mars-server.csr -out mars-server.crt -sha1 -CA my-ca.crt -CAkey my-ca.key -CAcreateserial -days 3650 4、生成无密码密钥1$ openssl rsa -in mars-server.key -out mars-server.key.insecure]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ssl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[proxy_pass反向代理配置中url后面加不加/的说明]]></title>
    <url>%2F2018%2F08%2F23%2Fproxy-pass%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE%E4%B8%ADurl%E5%90%8E%E9%9D%A2%E5%8A%A0%E4%B8%8D%E5%8A%A0-%E7%9A%84%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[1 环境 OS:centos7 nginx _proxy服务器：192.168.1.23 web服务器：192.168.1.5 2 情况说明2.1 path路径后面加”/”2.1.1 情况一NGINX配置 12345678910111213[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090/; &#125;&#125; 这样，访问http://192.168.1.23/proxy/就会被代理到http://192.168.1.5:8090/。匹配的proxy目录不需要存在根目录/var/www/html里面 注意，终端里如果访问http://192.168.1.23/proxy（即后面不带”/”），则会访问失败！因为proxy_pass配置的url后面加了”/” 访问结果如下 12345678910[root@localhost conf.d]# curl http://192.168.1.23/proxy/this is 192.168.1.5[root@localhost conf.d]# curl http://192.168.1.23/proxy&lt;html&gt;&lt;head&gt;&lt;title&gt;301 Moved Permanently&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor="white"&gt;&lt;center&gt;&lt;h1&gt;301 Moved Permanently&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.10.3&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 页面访问http://103.110.186.23/proxy的时候，会自动加上”/”（同理是由于proxy_pass配置的url后面加了”/”），并反代到http://103.110.186.5:8090的结果。 2.1.2 情况二123456789101112131415161718[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service## 那么访问http://192.168.1.23/proxy或http://192.168.1.23/proxy/，都会失败！## 这样配置后，访问http://192.168.1.23/proxy/就会被反向代理到http://192.168.1.5:8090/proxy/ 2.1.3 情况三1234567891011121314151617[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090/haha/;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service[root@localhost conf.d]# curl http://192.168.1.23/proxy/192.168.1.5 haha-index.html 这样配置的话，访问http://103.110.186.23/proxy代理到http://192.168.1.5:8090/haha/ 2.1.4 情况四相对于第三种配置的url不加”/” 1234567891011121314151617181920212223[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090/haha;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service[root@localhost conf.d]# curl http://192.168.1.23/proxy/index.html192.168.1.5 hahaindex.html##################################### 上面配置后，访问http://192.168.1.23/proxy/index.html就会被代理到http://192.168.1.5:8090/hahaindex.html同理，访问http://192.168.1.23/proxy/test.html就会被代理到http://192.168.1.5:8090/hahatest.html[root@localhost conf.d]# curl http://192.168.1.23/proxy/index.html192.168.1.5 hahaindex.html 注意，这种情况下，不能直接访问http://192.168.1.23/proxy/，后面就算是默认的index.html文件也要跟上，否则访问失败！ ———————————————————————————————————————————上面四种方式都是匹配的path路径后面加”/”，下面说下path路径后面不带”/”的情况： 2.2 path路径后面不加”/”2.2.1 情况一proxy_pass后面url带”/”： 123456789101112131415[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090/;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service 2.2.2 情况二，proxy_pass后面url不带”/” 12345678910111213141516[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service[root@localhost conf.d]# 这样配置的话，访问http://103.110.186.23/proxy会自动加上”/”（即变成http://103.110.186.23/proxy/），代理到192.168.1.5:8090/proxy/ 2.2.3 情况三123456789101112131415[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090/haha/;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service 这样配置的话，访问http://103.110.186.23/proxy会自动加上”/”（即变成http://103.110.186.23/proxy/），代理到http://192.168.1.5:8090/haha/ 2.2.4 情况四相对于第三种配置的url不加”/” 123456789101112131415[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090/haha;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service 这样配置的话，访问http://103.110.186.23/proxy，和第三种结果一样，同样被代理到http://192.168.1.5:8090/haha/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xmanager远程连接CentOS7]]></title>
    <url>%2F2018%2F08%2F23%2FXmanager%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5CentOS7%2F</url>
    <content type="text"><![CDATA[1 安装epel源 1yum install -y epel-release 2 安装lightdm和xfce12yum install -y lightdm yum groupinstall -y xfce 2.1 修改配置文件1vim /etc/lightdm/lightdm.conf 内容如下 123[XDMCPServer]enabled=trueport=177 2.2 将Display Manager切换为lightdm1systemctl disable gdm &amp;&amp; systemctl enable lightdm 2.3 启动lightdm1systemctl start lightdm 2.4 关闭防火墙1systemctl stop firewalld.service 3 登录打开Xmanger客户端，选择XDMCP并输入服务器的ip，回车运行即可。输入账号密码然后就出现下图：（如果正常跳过这步）或者出现黑屏提示无法建立连接这是因为刚开始安装的是Gnome，所以系统默认使用它，现在要改成Xfce，最简单的方法就是把xfce.desktopz之外的文件都干掉。 1234cd /usr/share/xsessions/mkdir bakmv gnome* baksystemctl restart lightdm 重新连接一切正常操作之后就成功连接了。然后就可以快速便捷的工作了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>运维工具</tag>
        <tag>xmanager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编辑rc.local启动命令执行不成功处理]]></title>
    <url>%2F2018%2F08%2F23%2F%E7%BC%96%E8%BE%91rc-local%E5%90%AF%E5%8A%A8%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E4%B8%8D%E6%88%90%E5%8A%9F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[1 问题举例： rc.local内容如下 123456789101112131415#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure# that this script will be executed during boot.touch /var/lock/subsys/localmount -t cifs -o username=backup,password=xxxxxxx //192.168.1.170/192.168.1.11/ /mnt/backup_data//usr/local/tomcat/bin/startup.sh 挂载命令执行了，但tomcat没有启动 2 问题原因及处理方法2.1 原因因java使用的是解压缩版，在/etc/profile内添加java的环境变量，系统启动时先执行的是rc.local,因此tomcat启动失败 2.2 解决办法2.2.1 方法1在rc.local内添加java的环境变量命令（必须放在tomcat启动命令前） 12export JAVA_HOME=/usr/local/java/jdk1.6.0_18export JRE_HOME=/usr/local/java/jdk1.6.0_18/jre 2.2.2 方法2在tomcat的启动脚本内添加java路径分别在tomcat_home/bin目录内的catalina.sh ，setclasspath.sh脚本前面指定JAVA_HOME路径 12export JAVA_HOME=/usr/local/java/jdk1.6.0_18export JRE_HOME=/usr/local/java/jdk1.6.0_18/jre]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>运维开发</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker私有仓库搭建]]></title>
    <url>%2F2018%2F08%2F23%2Fdocker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1 搭建仓库 安装docker-ce,过程省略…….；运行以下命令启动仓库 1$ docker run --name registry --restart always -d -p 5000:5000 -v /.registry:/var/lib/registry registry 2 修改配置修改所有需要使用私有仓库的docker服务器（包括仓库服务器）的docker配置，在/etc/docker目录创建daemon.json文件内容如下：123456# 下面这句表示表示开启5000端口的非安全模式，也就是http模式，否则在push或pull时会报https错误&#123; "insecure-registries":["192.168.1.11:5000"] &#125;# 下面这句是使用阿里云镜像加速，提高外网官方仓库的下载速度，这里一起列出了，不是必须要添加的，和私有仓库没有关系。&#123; "registry-mirrors": ["https://cz0az3lb.mirror.aliyuncs.com"]&#125; 可以同时设置，写法如下 1234&#123;"insecure-registries":["192.168.1.118:5000"],"registry-mirrors": ["http://192.168.1.118:5001"]&#125; 重启docker服务，配置生效 3 查询仓库镜像列表12345678910[root@zabbix-11 docker]# curl -XGET http://192.168.1.11:5000/v2/_catalog&#123;"repositories":["nginx"]&#125;# 显示镜像nginx# 查询镜像版本``` elixir[root@zabbix-11 docker]# curl -XGET http://192.168.1.11:5000/v2/nginx/tags/list&#123;"name":"nginx","tags":["1","1.13.7"]&#125;#查询nginx镜像的版本号，有1/1.13.7版本 以上查询也可以在web页面查询，复制命令后方http链接地址就行。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>私有仓库</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix3.4.7监控日志]]></title>
    <url>%2F2018%2F08%2F23%2Fzabbix3-4-7%E7%9B%91%E6%8E%A7%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[一、日志item介绍 下面介绍zabbix另一个“重量级”的功能——日志文件监控，它最主要的是监控日志文件中有没有某个字符串的表达式，对应日志轮转与否，zabbix都支持。 在配置Item的时候，Type选择Zabbix agent (active)，这里主要需要配置的是Key。下面是监控日志的两种key——log和logtr。 log[/path/to/some/file,,,,,] logtr[/path/to/some/filename_format,,,,,] ◆ regexp：要匹配内容的正则表达式，或者直接写你要检索的内容也可以，例如我想检索带ERROR关键词的记录 ◆ encoding：编码相关，留空即可 ◆maxlines：一次性最多提交多少行，这个参数覆盖配置文件zabbxi_agentd.conf中的’MaxLinesPerSecond’，我们也可以留空 ◆ mode：默认是all，也可以是skip，skip会跳过老数据 ◆ output：输出给zabbixserver的数据。可以是\1、\2一直\9，\1表示第一个正则表达式匹配出得内容，\2表示第二个正则表达式匹配错的内容。 如果仔细看可以发现，第一个参数不一样，logrt的第一个参数可以使用正则表达式。针对日志回滚用得，例如我们每天都切割nginx日志，日志名位www.a.com_2015-01-01.log、www.a.com_2015-01-02.log等等，使用log肯定不合适，如果文件名使用正则，那么新增的日志文件会立即加入监控。 备注：不管新日志、老日志，只要他们有变更，zabbix都会监控。 只要配置了，Zabbix会根据的正则表达式来匹配日志中的内容。注意，一定要保证Zabbix用户对日志文件有可读权限，否则这个Item的状态会变成“unsupported”。 二、监控原理及注意事项1、Zabbix Server和Zabbix Agent会追踪日志文件的大小和最后修改时间，并且分别记录在字节计数器和最新的时间计数器中。 2、Agent会从上次读取日志的地方开始读取日志。 3、字节计数器和最新时间计数器的数据会被记录在Zabbix数据库，并且发送给Agent，这样能够保证Agent从上次停止的地方开始读取日志。 4、当日志文件大小小于字节计数器中的数字时，字节计数器会变为0，从头开始读取文件。 5、所有符合配置的文件，都会被监控。 6、一个目录下的多个文件如果修改时间相同，会按照字母顺序来读取。 7、到每个Update interval的时间时，Agent会检查一次目录下的文件。 8、Zabbix Agent每秒发送日志量，有一个日志行数上限，防止网络和CPU负载过高，这个数字在zabbix_agentd.conf中的MaxLinePerSecond。 9、在logtr中，正则表达式只对文件名有效，对文件目录无效。 三、日志监控配置请确保Agent有如下两项配置 1、Hostname设定为Server创建主机是填写的Host name，必须一致 2、ServerActive设定为Server的IP Host&gt;&gt;目标主机&gt;&gt;监控项&gt;&gt;创建监控项，如下： 1、log方式 2、logrt方式 说明： type必须选择zabbix agent（active），因为数据是zabbix被监控的主动提交给server key：log[/var/log/message,error]，我们这里是监控的系统日志，打印出带有error的行，大家也可以去监控其他的日志，mysql、nginx等等都是可以的。 key: logrt[“/mnt/backup/log/[0-9]+_[0-9]+.log”,END],监控备份日志，例如20180305_113523.log,正则表达式[0-9]+_[0-9]+.log，具体可以百度学习正则表达式写法；“”双引号在使用表达式时 候最好用上，否则在保存时可能报语法错误； log time format：MMpddphh:mm:ss，对应日志的行头Sep 14 07:32:38，y表示年、M表示月、d表示日、p和:一个占位符，h表示小时，m表示分钟，s表示秒。 四、结果查看切换到最新数据里面，找到相应数据，如下是我的监控截图 五、触发器设置创建触发器，在周期24h内，写入日志则备份正常，否则告警；这里如下简单设置]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins邮件通知]]></title>
    <url>%2F2018%2F08%2F23%2Fjenkins%E9%82%AE%E4%BB%B6%E9%80%9A%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[1 简介 jenkins集成mail通知服务不再说明，功能简单，这里简单说下常用mail插件Email Extension Plugin 2 安装Email Extension Plugin 2.1 配置Email系统管理&gt;系统设置&gt;Extended E-mail Notification 下面是我的配置，改动不大，邮件通知帐号在系统集成email处配置，这里主要提下插件通知模版 2.2 配置默认模版Default Content Type配置为html 2.2.1 标题1构建通知：$PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS! 2.2.2 内容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;$&#123;ENV, var="JOB_NAME"&#125;-第$&#123;BUILD_NUMBER&#125;次构建日志&lt;/title&gt;&lt;/head&gt;&lt;body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0"&gt; &lt;table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif"&gt; &lt;tr&gt; &lt;td&gt; &lt;h2&gt; &lt;font&gt;来自Mr.Jenkins的邮件通知&lt;/font&gt; &lt;/h2&gt; &lt;font&gt;(本邮件是程序自动下发的，请勿回复！)&lt;/font&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;br /&gt; &lt;b&gt;&lt;font color="#0B610B"&gt;构建信息&lt;/font&gt;&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;ul&gt; &lt;li&gt;项目名称：$&#123;PROJECT_NAME&#125;&lt;/li&gt; &lt;li&gt;构建编号：$&#123;BUILD_NUMBER&#125;&lt;/li&gt; &lt;li&gt;构建状态：$&#123;BUILD_STATUS&#125;&lt;/li&gt; &lt;li&gt;触发原因：$&#123;CAUSE&#125;&lt;/li&gt; &lt;li&gt;构建地址：$&#123;BUILD_URL&#125;&lt;/li&gt; &lt;li&gt;构建日志：&lt;a href="$&#123;BUILD_URL&#125;console"&gt;$&#123;BUILD_URL&#125;console&lt;/a&gt;&lt;/li&gt; &lt;li&gt;单元测试报告：&lt;a href="$&#123;BUILD_URL&#125;testReport/"&gt;$&#123;BUILD_URL&#125;testReport/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;工作目录：&lt;a href="$&#123;PROJECT_URL&#125;ws"&gt;$&#123;PROJECT_URL&#125;ws&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;&lt;font color="#0B610B"&gt;构建日志:&lt;/font&gt;&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;textarea cols="80" rows="30" readonly="readonly" style="font-family: Courier New"&gt;$&#123;BUILD_LOG&#125;&lt;/textarea&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 3 添加项目构建 4 通知展示]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群WEB管理工具Shipyard]]></title>
    <url>%2F2018%2F08%2F22%2FDocker%E9%9B%86%E7%BE%A4WEB%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7Shipyard%2F</url>
    <content type="text"><![CDATA[一、 说明 Shipyard部署总体较为简单，有一键部署脚本，只需执行相应命令就可以实现部署，但在部署中还是出现很多问题，主要是shipyard官方网站无法访问，无法使用官方一键脚本部署，使用网上找到的修改版，里面有部分内容有所缺失，现已修改。 二、环境centos:7.4 master: 192.168.1.44 node1: 192.168.1.12 node2: 192.168.1.18 docker version: docker-ce-18.03.0-ce 三、安装步骤安装之前需要部署好docker环境 1、master节点执行一件部署命令12345678910111213141516171819202122232425# 正常直接从官方拉取脚本执行就行了，命令如下$ curl -s https://shipyard-project.com/deploy | bash -s[root@master ~]# curl -s https://shipyard-project.com/deploy | bash -sDeploying Shipyard -&gt; Starting DatabaseUnable to find image 'rethinkdb:latest' locallyTrying to pull repository xxx.mirror.aliyuncs.com/rethinkdb ...Pulling repository xxx.mirror.aliyuncs.com/rethinkdbTrying to pull repository docker.io/library/rethinkdb ...latest: Pulling from docker.io/library/rethinkdbDigest: sha256:29640c7d5015832c40305ad5dcc5d0996ce79b87f7e32d2fd99c9d65ad9414d4 -&gt; Starting Discovery -&gt; Starting Cert Volume -&gt; Starting Proxy -&gt; Starting Swarm Manager -&gt; Starting Swarm Agent -&gt; Starting ControllerWaiting for Shipyard on 192.168.1.44:8080 Shipyard available at http://192.168.1.44:8080Username: admin Password: shipyard# 或者下载脚本后本地执行，命令如下$ sh deploy 执行脚本的时候会自动下载相关镜像，也可以事先下载镜像文件 123456[root@master ~]# docker pull alpine[root@master ~]# docker pull library/rethinkdb[root@master ~]# docker pull microbox/etcd[root@master ~]# docker pull shipyard/docker-proxy[root@master ~]# docker pull swarm[root@master ~]# docker pull shipyard/shipyard 通过访问http://IP：8080登录shipyard,默认帐号密码：admin shipyard 2、分别在node节点执行如下命令，添加节点到shipyard1234# 部署机就是master节点$ curl -sSL https://shipyard-project.com/deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s# 或者通过本地脚本执行命令$ cat deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s 四、问题总结注意项目1：—————————————————————————————————————上面安装shipyard的脚本是英文版的，其实还有中文版的脚本，下面两种都可以使用：（两个地址都失效） 1）安装shipyard 12$ curl -sSL http://dockerclub.net/public/script/deploy | bash -s ==&gt; 中文版$ curl -sSL https://shipyard-project.com/deploy | bash -s ==&gt; 英文版 2）添加node节点 12$ curl -sSL http://dockerclub.net/public/script/deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s ==&gt; 中文版$ curl -sSL https://shipyard-project.com/deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s ==&gt; 英文版 3）删除shipyard（在节点机上执行，就会将节点从shipyard管理里踢出） 12$ curl http://dockerclub.net/public/script/deploy | ACTION=remove bash -s ==&gt; 中文版$ curl -sSL https://shipyard-project.com/deploy | ACTION=remove bash -s ==&gt; 英文版 ————————————————————————————————————— 问题项目2：————————————————————————————————————— 1）安装shipyard前不需要部署swarm集群，一键包已包含集群建设 2）部署后无法发现节点，容器页面报错，意思是到节点IP:3375端口无法建立连接，500错误；根据排查，发现所有节点swarm_manager容器都没有开启3375端口，于是在脚本里面修改 容器启动命令，添加开启3375端口，具体看下方脚本； 3）在shipyard web页面，master自身发现较慢，不知道什么原因 4）中文版shipard创建容器的时候，设置端口映射但不生效，使用英文版没有问题，未发现问题原因 5）shipyard只适合较小规模docker集群，功能上已跟不上现阶段的docker集群需求。 ————————————————————————————————————— deploy脚本这个是中文版的脚本，和官方的区别就是修改了shipyard镜像文件，下方标注部分说明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375#!/bin/bashif [ "$1" != "" ] &amp;&amp; [ "$1" = "-h" ]; then echo "Shipyard Deploy uses the following environment variables:" echo " ACTION: this is the action to use (deploy, upgrade, node, remove)" echo " DISCOVERY: discovery system used by Swarm (only if using 'node' action)" echo " IMAGE: this overrides the default Shipyard image" echo " PREFIX: prefix for container names" echo " SHIPYARD_ARGS: these are passed to the Shipyard controller container as controller args" echo " TLS_CERT_PATH: path to certs to enable TLS for Shipyard" echo " PORT: specify the listen port for the controller (default: 8080)" echo " IP: specify the address at which the controller or node will be available (default: eth0 ip)" echo " PROXY_PORT: port to run docker proxy (default: 2375)" exit 1fiif [ -z "`which docker`" ]; then echo "You must have the Docker CLI installed on your \$PATH" echo " See http://docs.docker.com for details" exit 1fiACTION=$&#123;ACTION:-deploy&#125;#官方IMAGE=$&#123;IMAGE:-shipyard/shipyard:latest&#125;#备注官方shipyard:latest标签镜像似乎有问题，实际使用shipyard:master标签镜像IMAGE=$&#123;IMAGE:-dockerclub/shipyard:latest&#125;PREFIX=$&#123;PREFIX:-shipyard&#125;SHIPYARD_ARGS=$&#123;SHIPYARD_ARGS:-""&#125;TLS_CERT_PATH=$&#123;TLS_CERT_PATH:-&#125;CERT_PATH="/etc/shipyard"PROXY_PORT=$&#123;PROXY_PORT:-2375&#125;SWARM_PORT=3375SHIPYARD_PROTOCOL=httpSHIPYARD_PORT=$&#123;PORT:-8080&#125;SHIPYARD_IP=$&#123;IP&#125;DISCOVERY_BACKEND=etcdDISCOVERY_PORT=4001DISCOVERY_PEER_PORT=7001ENABLE_TLS=0CERT_FINGERPRINT=""LOCAL_CA_CERT=""LOCAL_SSL_CERT=""LOCAL_SSL_KEY=""LOCAL_SSL_CLIENT_CERT=""LOCAL_SSL_CLIENT_KEY=""SSL_CA_CERT=""SSL_CERT=""SSL_KEY=""SSL_CLIENT_CERT=""SSL_CLIENT_KEY=""show_cert_help() &#123; echo "To use TLS in Shipyard, you must have existing certificates." echo "The certs must be named ca.pem, server.pem, server-key.pem, cert.pem and key.pem" echo "If you need to generate certificates, see https://github.com/ehazlett/certm for examples."&#125;check_certs() &#123; if [ -z "$TLS_CERT_PATH" ]; then return fi if [ ! -e $TLS_CERT_PATH ]; then echo "Error: unable to find certificates in $TLS_CERT_PATH" show_cert_help exit 1 fi if [ "$PROXY_PORT" = "2375" ]; then PROXY_PORT=2376 fi SWARM_PORT=3376 SHIPYARD_PROTOCOL=https LOCAL_SSL_CA_CERT="$TLS_CERT_PATH/ca.pem" LOCAL_SSL_CERT="$TLS_CERT_PATH/server.pem" LOCAL_SSL_KEY="$TLS_CERT_PATH/server-key.pem" LOCAL_SSL_CLIENT_CERT="$TLS_CERT_PATH/cert.pem" LOCAL_SSL_CLIENT_KEY="$TLS_CERT_PATH/key.pem" SSL_CA_CERT="$CERT_PATH/ca.pem" SSL_CERT="$CERT_PATH/server.pem" SSL_KEY="$CERT_PATH/server-key.pem" SSL_CLIENT_CERT="$CERT_PATH/cert.pem" SSL_CLIENT_KEY="$CERT_PATH/key.pem" CERT_FINGERPRINT=$(openssl x509 -noout -in $LOCAL_SSL_CERT -fingerprint -sha256 | awk -F= '&#123;print $2;&#125;') if [ ! -e $LOCAL_SSL_CA_CERT ] || [ ! -e $LOCAL_SSL_CERT ] || [ ! -e $LOCAL_SSL_KEY ] || [ ! -e $LOCAL_SSL_CLIENT_CERT ] || [ ! -e $LOCAL_SSL_CLIENT_KEY ]; then echo "Error: unable to find certificates" show_cert_help exit 1 fi ENABLE_TLS=1&#125;# container functionsstart_certs() &#123; ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-certs \ -v $CERT_PATH \ alpine \ sh) if [ $ENABLE_TLS = 1 ]; then docker cp $LOCAL_SSL_CA_CERT $PREFIX-certs:$SSL_CA_CERT docker cp $LOCAL_SSL_CERT $PREFIX-certs:$SSL_CERT docker cp $LOCAL_SSL_KEY $PREFIX-certs:$SSL_KEY docker cp $LOCAL_SSL_CLIENT_CERT $PREFIX-certs:$SSL_CLIENT_CERT docker cp $LOCAL_SSL_CLIENT_KEY $PREFIX-certs:$SSL_CLIENT_KEY fi&#125;remove_certs() &#123; docker rm -fv $PREFIX-certs &gt; /dev/null 2&gt;&amp;1&#125;get_ip() &#123; if [ -z "$SHIPYARD_IP" ]; then SHIPYARD_IP=`docker run --rm --net=host alpine ip route get 8.8.8.8 | awk '&#123; print $7; &#125;'` fi&#125;start_discovery() &#123; get_ip ID=$(docker run \ -ti \ -d \ -p 4001:4001 \ -p 7001:7001 \ --restart=always \ --name $PREFIX-discovery \ microbox/etcd:latest -addr $SHIPYARD_IP:$DISCOVERY_PORT -peer-addr $SHIPYARD_IP:$DISCOVERY_PEER_PORT)&#125;remove_discovery() &#123; docker rm -fv $PREFIX-discovery &gt; /dev/null 2&gt;&amp;1&#125;start_rethinkdb() &#123; ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-rethinkdb \ rethinkdb)&#125;remove_rethinkdb() &#123; docker rm -fv $PREFIX-rethinkdb &gt; /dev/null 2&gt;&amp;1&#125;start_proxy() &#123; TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="-e SSL_CA=$SSL_CA_CERT -e SSL_CERT=$SSL_CERT -e SSL_KEY=$SSL_KEY -e SSL_SKIP_VERIFY=1" fi # Note: we add SSL_SKIP_VERIFY=1 to skip verification of the client # certificate in the proxy image. this will pass it to swarm that # does verify. this helps with performance and avoids certificate issues # when running through the proxy. ultimately if the cert is invalid # swarm will fail to return. ID=$(docker run \ -ti \ -d \ -p $PROXY_PORT:$PROXY_PORT \ --hostname=$HOSTNAME \ --restart=always \ --name $PREFIX-proxy \ -v /var/run/docker.sock:/var/run/docker.sock \ -e PORT=$PROXY_PORT \ --volumes-from=$PREFIX-certs $TLS_OPTS\ shipyard/docker-proxy:latest)&#125;remove_proxy() &#123; docker rm -fv $PREFIX-proxy &gt; /dev/null 2&gt;&amp;1&#125;start_swarm_manager() &#123; get_ip TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="--tlsverify --tlscacert=$SSL_CA_CERT --tlscert=$SSL_CERT --tlskey=$SSL_KEY" fi EXTRA_RUN_OPTS="" if [ -z "$DISCOVERY" ]; then DISCOVERY="$DISCOVERY_BACKEND://discovery:$DISCOVERY_PORT" EXTRA_RUN_OPTS="--link $PREFIX-discovery:discovery" fi ID=$(docker run \ -ti \ -d \#下面3375端口是我出现无法连接节点后自己添加的，问题是启动容器没有开放3375端口 -p 3375:3375 \ --restart=always \ --name $PREFIX-swarm-manager \ --volumes-from=$PREFIX-certs $EXTRA_RUN_OPTS \ swarm:latest \ m --replication --addr $SHIPYARD_IP:$SWARM_PORT --host tcp://0.0.0.0:$SWARM_PORT $TLS_OPTS $DISCOVERY)&#125;remove_swarm_manager() &#123; docker rm -fv $PREFIX-swarm-manager &gt; /dev/null 2&gt;&amp;1&#125;start_swarm_agent() &#123; get_ip if [ -z "$DISCOVERY" ]; then DISCOVERY="$DISCOVERY_BACKEND://discovery:$DISCOVERY_PORT" EXTRA_RUN_OPTS="--link $PREFIX-discovery:discovery" fi ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-swarm-agent $EXTRA_RUN_OPTS \ swarm:latest \ j --addr $SHIPYARD_IP:$PROXY_PORT $DISCOVERY)&#125;remove_swarm_agent() &#123; docker rm -fv $PREFIX-swarm-agent &gt; /dev/null 2&gt;&amp;1&#125;start_controller() &#123; #-v $CERT_PATH:/etc/docker:ro \ TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="--tls-ca-cert $SSL_CA_CERT --tls-cert=$SSL_CERT --tls-key=$SSL_KEY --shipyard-tls-ca-cert=$SSL_CA_CERT --shipyard-tls-cert=$SSL_CERT --shipyard-tls-key=$SSL_KEY" fi ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-controller \ --link $PREFIX-rethinkdb:rethinkdb \ --link $PREFIX-swarm-manager:swarm \ -p $SHIPYARD_PORT:$SHIPYARD_PORT \ --volumes-from=$PREFIX-certs \ $IMAGE \ --debug \ server \ --listen :$SHIPYARD_PORT \ -d tcp://swarm:$SWARM_PORT $TLS_OPTS $SHIPYARD_ARGS)&#125;wait_for_available() &#123; set +e IP=$1 PORT=$2 echo Waiting for Shipyard on $IP:$PORT docker pull ehazlett/curl &gt; /dev/null 2&gt;&amp;1 TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="-k" fi until $(docker run --rm ehazlett/curl --output /dev/null --connect-timeout 1 --silent --head --fail $TLS_OPTS $SHIPYARD_PROTOCOL://$IP:$PORT/ &gt; /dev/null 2&gt;&amp;1); do printf '.' sleep 1 done printf '\n'&#125;remove_controller() &#123; docker rm -fv $PREFIX-controller &gt; /dev/null 2&gt;&amp;1&#125;if [ "$ACTION" = "deploy" ]; then set -e check_certs get_ip echo "Deploying Shipyard" echo " -&gt; Starting Database" start_rethinkdb echo " -&gt; Starting Discovery" start_discovery echo " -&gt; Starting Cert Volume" start_certs echo " -&gt; Starting Proxy" start_proxy echo " -&gt; Starting Swarm Manager" start_swarm_manager echo " -&gt; Starting Swarm Agent" start_swarm_agent echo " -&gt; Starting Controller" start_controller wait_for_available $SHIPYARD_IP $SHIPYARD_PORT echo "Shipyard available at $SHIPYARD_PROTOCOL://$SHIPYARD_IP:$SHIPYARD_PORT" if [ $ENABLE_TLS = 1 ] &amp;&amp; [ ! -z "$CERT_FINGERPRINT" ]; then echo "SSL SHA-256 Fingerprint: $CERT_FINGERPRINT" fi echo "Username: admin Password: shipyard"elif [ "$ACTION" = "node" ]; then set -e if [ -z "$DISCOVERY" ]; then echo "You must set the DISCOVERY environment variable" echo "with the discovery system used with Swarm" exit 1 fi check_certs echo "Adding Node" echo " -&gt; Starting Cert Volume" start_certs echo " -&gt; Starting Proxy" start_proxy echo " -&gt; Starting Swarm Manager" start_swarm_manager $DISCOVERY echo " -&gt; Starting Swarm Agent" start_swarm_agent echo "Node added to Swarm: $SHIPYARD_IP" elif [ "$ACTION" = "upgrade" ]; then set -e check_certs get_ip echo "Upgrading Shipyard" echo " -&gt; Pulling $IMAGE" docker pull $IMAGE echo " -&gt; Upgrading Controller" remove_controller start_controller wait_for_available $SHIPYARD_IP $SHIPYARD_PORT echo "Shipyard controller updated"elif [ "$ACTION" = "remove" ]; then # ignore errors set +e echo "Removing Shipyard" echo " -&gt; Removing Database" remove_rethinkdb echo " -&gt; Removing Discovery" remove_discovery echo " -&gt; Removing Cert Volume" remove_certs echo " -&gt; Removing Proxy" remove_proxy echo " -&gt; Removing Swarm Agent" remove_swarm_agent echo " -&gt; Removing Swarm Manager" remove_swarm_manager echo " -&gt; Removing Controller" remove_controller echo "Done"else echo "Unknown action $ACTION" exit 1fi 参考资料https://www.cnblogs.com/kevingrace/p/6867820.html https://www.fcwys.cc/index.php/archives/145.html]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>shipyard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle-11g-R2安装教程]]></title>
    <url>%2F2018%2F08%2F20%2FOracle-11g-R2%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、安装Oracle前准备 1.创建运行oracle数据库的系统用户和用户组1234567891011121314[sonny@localhost ~]$ su root #切换到rootPassword: [root@localhost sonny]# groupadd oinstall #创建用户组oinstall[root@localhost sonny]# groupadd dba #创建用户组dba[root@localhost sonny]# useradd -g oinstall -g dba -m oracle #创建oracle用户，并加入到oinstall和dba用户组[root@localhost sonny]# passwd oracle #设置用户oracle的登陆密码，不设置密码，在CentOS的图形登陆界面没法登陆Changing password for user oracle.New password: # 密码BAD PASSWORD: The password is shorter than 8 charactersRetype new password: # 确认密码passwd: all authentication tokens updated successfully.[root@localhost sonny]# id oracle # 查看新建的oracle用户uid=1001(oracle) gid=1002(dba) groups=1002(dba)[root@localhost sonny]# 2.创建oracle数据库安装目录123456789101112[sonny@localhost ~]$ su rootPassword: [root@localhost sonny]# mkdir -p /data/oracle #oracle数据库安装目录[root@localhost sonny]# mkdir -p /data/oraInventory #oracle数据库配置文件目录[root@localhost sonny]# mkdir -p /data/database #oracle数据库软件包解压目录[root@localhost sonny]# cd /data[root@localhost data]# ls #创建完毕检查一下（强迫症）database oracle oraInventory[root@localhost data]# chown -R oracle:oinstall /data/oracle #设置目录所有者为oinstall用户组的oracle用户[root@localhost data]# chown -R oracle:oinstall /data/oraInventory[root@localhost data]# chown -R oracle:oinstall /data/database[root@localhost data]# 3.修改OS系统标识oracle默认不支持CentOS系统安装，Oracle Database 11g Release 2 的 OS要求参考： https://docs.oracle.com/cd/E11882_01/install.112/e47689/pre_install.htm#LADBI1106 我安装是64位数据库，On Linux x86-64：Red Hat Enterprise Linux 7 （RHEL 7） 另外，CentOS7.0.1511 基于 RHEL7.2 参考：http://www.linuxidc.com/Linux/2015-12/126283.htm 修改文件 /etc/RedHat-release 12345678910[sonny@localhost data]$ su rootPassword: [root@localhost data]# cat /proc/version Linux version 3.10.0-327.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) #1 SMP Thu Nov 19 22:10:57 UTC 2015[root@localhost data]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@localhost data]# vi /etc/redhat-release[root@localhost data]# cat /etc/redhat-release redhat-7 [root@localhost data]# 4.安装oracle数据库所需要的软件包重复一遍，我安装时Oracle Database 11g Release 2 64位数据库。 Oracle Database Package Requirements for Linux x86-64 如下：（参考：https://docs.oracle.com/cd/E11882_01/install.112/e47689/pre_install.htm#BABCFJFG） 1234操作系统:Oracle Linux 7 and Red Hat Enterprise Linux 7The following packages (or later versions) must be installed:$ yum install binutils compat-libstdc++ compat-libstdc++-33 elfutils-libelf-devel gcc gcc-c++ glibc-devel glibc-headers ksh libaio-devel libstdc++-devel make sysstat unixODBC-devel binutils-* compat-libstdc++* elfutils-libelf* glibc* gcc-* libaio* libgcc* libstdc++* make* sysstat* unixODBC* wget unzip 5.关闭防火墙 CentOS 7.2默认使用的是firewall作为防火墙12$ systemctl disable firewalld$ systemctl stop firewalld 6.关闭selinux（需重启生效）1234567891011121314[root@localhost /]# vi /etc/selinux/config[root@localhost /]# cat /etc/selinux/config# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled #此处修改为disabled# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection.SELINUXTYPE=targeted 7.修改内核参数1234567891011121314151617181920212223242526[sonny@localhost /]$ su rootPassword: [root@localhost /]# vi /etc/sysctl.conf [root@localhost /]# cat /etc/sysct.confcat: /etc/sysct.conf: No such file or directory[root@localhost /]# cat /etc/sysctl.conf # System default settings live in /usr/lib/sysctl.d/00-system.conf.# To override those settings, enter new settings here, or in an /etc/sysctl.d/&lt;name&gt;.conf file## For more information, see sysctl.conf(5) and sysctl.d(5).net.ipv4.icmp_echo_ignore_broadcasts = 1net.ipv4.conf.all.rp_filter = 1fs.file-max = 6815744 #设置最大打开文件数fs.aio-max-nr = 1048576kernel.shmall = 2097152 #共享内存的总量，8G内存设置：2097152*4k/1024/1024kernel.shmmax = 2147483648 #最大共享内存的段大小kernel.shmmni = 4096 #整个系统共享内存端的最大数kernel.sem = 250 32000 100 128net.ipv4.ip_local_port_range = 9000 65500 #可使用的IPv4端口范围net.core.rmem_default = 262144net.core.rmem_max= 4194304net.core.wmem_default= 262144net.core.wmem_max= 1048576[root@localhost /]# 1234567891011121314151617181920212223使配置参数生效[root@localhost /]# sysctl -pnet.ipv4.icmp_echo_ignore_broadcasts = 1net.ipv4.conf.all.rp_filter = 1sysctl: setting key "fs.file-max": Invalid argumentfs.file-max = 6815744 #设置最大打开文件数fs.aio-max-nr = 1048576sysctl: setting key "kernel.shmall": Invalid argumentkernel.shmall = 2097152 #共享内存的总量，8G内存设置：2097152*4k/1024/1024sysctl: setting key "kernel.shmmax": Invalid argumentkernel.shmmax = 2147483648 #最大共享内存的段大小sysctl: setting key "kernel.shmmni": Invalid argumentkernel.shmmni = 4096 #整个系统共享内存端的最大数kernel.sem = 250 32000 100 128sysctl: setting key "net.ipv4.ip_local_port_range": Invalid argumentnet.ipv4.ip_local_port_range = 9000 65500 #可使用的IPv4端口范围net.core.rmem_default = 262144net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048576[root@localhost /]# 8.解压安装包1234567891011[oracle@localhost /]$ cd /usr/local/src #进入/usr/local/src目录[oracle@localhost src]$ lslinux.x64_11gR2_database_1of2.zip linux.x64_11gR2_database_2of2.zip[oracle@localhost src]$ unzip linux.x64_11gR2_database_1of2.zip -d /data/database/ #解压(省略...)[oracle@localhost src]$ unzip linux.x64_11gR2_database_2of2.zip -d /data/database/ #解压(省略...)[oracle@localhost src]$ su rootPassword: [root@localhost src]# chown -R oracle:oinstall /data/database/database/[root@localhost src]# 二、oracle安装1.图形界面登陆oracle用户： 备注：root用户切换到oracle用户无法安装报错，必须使用oracle直接登录，否则报如下错误 ==Exception in thread “main” java.lang.NoClassDefFoundError== 2.启动oralce安装，到/data/database/database/目录下，执行runInstaller 3.去掉勾，懒得填，个人使用环境不需要自动接收Oracle的安全更新。 4.下一步，只安装数据库软件，个人用不要那些玩意~~5.选择单例安装，前面的所有配置均为单例安装。 6.添加语言7.默认安装版本企业版-Enterprise Edition8.确定数据软件的安装路径，自动读取前面Oracle环境变量中配置的值。9.理论上要创建Database Operation（OSOPER）Group:oper ,个人用，懒得建，就使用dba用户组10.安装检查，按照提示信息一个一个解决。11.一个一个检查package，在准备阶段中漏掉的，此处再安装，有些系统报错是因为现有的包的版本比检测要高，最后忽略即可。（点击Check_Again 多检查几次）12.准备完毕，fuck “Finish”开始安装。14.提示安装成功。安装日志懒得看，再说。 三、配置监听listener1.执行netca 报错 12345678910111213141516171819[@localhost ~]$ netcaOracle Net Services Configuration:## An unexpected error has been detected by HotSpot Virtual Machine:## SIGSEGV (0xb) at pc=0x00007f69a69fcb9d, pid=8033, tid=140092892297024## Java VM: Java HotSpot(TM) 64-Bit Server VM (1.5.0_17-b03 mixed mode)# Problematic frame:# C [libclntsh.so.11.1+0x62ab9d] snlinGetAddrInfo+0x1b1## An error report file with more information is saved as hs_err_pid8033.log## If you would like to submit a bug report, please visit:# http://java.sun.com/webapps/bugreport/crash.jsp#/data/oracle/product/11.2.0/db_1/bin/netca: line 178: 8033 Aborted (core dumped) $JRE $JRE_OPTIONS -classpath $CLASSPATH oracle.net.ca.NetCA $*[oracle@localhost ~]$ 错误原因：安装操作系统是默认主机名localhost造成错误 解决办法： 12345678910111213141516[racle]# cat /etc/sysconfig/network# Created by anaconda[root@localhost oracle]# vi /etc/sysconfig/network #增加HOSTNAME[root@localhost oracle]# cat /etc/sysconfig/network# Created by anacondaHOSTNAME=odb-sonny[root@localhost oracle]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6[root@localhost oracle]# vi /etc/hosts #增加HOSTNAME&lt;/strong&gt;[root@localhost oracle]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 odb-sonny::1 localhost localhost.localdomain localhost6 localhost6.localdomain6[root@localhost oracle]# hostname odb-sonny #执行[root@localhost oracle]# 最后注销当前oracle用户，重新登陆即可！！这次发现打开配置界面正常，安装windows下面配置即可。 四、创建Oracle数据实例Orcl执行dbca命令，启动oracle实例安装界面，剩下的与Windows上安装一样，不废话了： 注意：必须先创建监听，并且监听是启动中，否则报错。 问题总结问题一：root用户切换到oracle用户无法安装报错，必须使用oracle直接登录，否则报如下错误 Exception in thread “main” java.lang.NoClassDefFoundError 问题二：Oracle 安装报错 [INS-06101] IP address of localhost could not be determined 解决方法 出现这种错误是因为主机名和/etc/hosts 文件不一致，只需要把主机名和其IP 写入/etc/hosts 文件，就ok了。 问题三：在ORACLE11G R2 安装ORACLE时出现以下错误： [INS-08109] Unexpected error occurred while validating inputs at state ‘getOCMDetails’. 经GOOGLE看到 http://www.linkedin.com/groups/I-try-clone-oracle-grid-77941.S.38808726 说是使用了 LD_LIBRARY_PATH 环境参数，经查看.bash_profile ，有如下设置： LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib; export LD_LIBRARY_PATH 注释掉后，问题解决。 问题四：“#Error in invoking target ‘install’ of makefile ‘/data/oracle/product/11.2.0/db_1/ctx/lib/ins_ctx.mk’”解决方法：打开一个新的终端，使用root身份登入， #vi ORACLE_HOME/ctx/lib/ins_ctx.mk找到ctxhx: $(CTXHXOBJ)$(LINK_CTXHX) $(CTXHXOBJ) $(INSO_LINK)修改为(添加红色部分): ctxhx: $(CTXHXOBJ)-static $(LINK_CTXHX) $(CTXHXOBJ) $(INSO_LINK) /usr/lib64/libc.a 问题五：“#Error in invoking target ‘agent nmhs’ of makefile ‘/home/oracle_11/app/oracle/product/11.2.0/db_1/sysman/lib/ins_emagent.mk’”解决方法：打开一个新的终端，使用root身份登入， #vi $ORACLE_HOME/sysman/lib/ins_emagent.mk找到$(MK_EMAGENT_NMECTL)修改为(添加红色部分)：$(MK_EMAGENT_NMECTL) ==-lnnz11==完成后在错误提示框上retry既可]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm私库部署-cnpmjs]]></title>
    <url>%2F2018%2F08%2F16%2Fnpm%E7%A7%81%E5%BA%93%E9%83%A8%E7%BD%B2-cnpmjs%2F</url>
    <content type="text"><![CDATA[一、部署cnpm 1、获取代码1git clone git://github.com/fengmk2/cnpmjs.org.git 2、创建mysql库Default 123create database cnpmjs;use cnpmjs;source docs/db.sql【db.sql位于cnpmjs.org/docs/db.sql】 3、安装依赖安装依赖其实就是一个 npm install，不过 CNPM 把该指令已经写到 Makefile 里面了，所以直接执行下面的命令就好了。12$ cd cnpmjs.org $ npm install 当然万一你是 Windows 用户或者不会 make，那么还是要用 npm install。1$ npm install --build-from-source --registry=https://registry.npm.taobao.org --disturl=https://npm.taobao.org/mirrors/node 4、修改配置文件修改配置 1vim /cnpmjs.org/config/index.js cnpm提供两个端口：7001和7002，其中7001用于NPM的注册服务，7002用于Web访问。 bindingHost为安装cnpm的服务器ip地址，也就是在浏览器中只能通过访问http://192.168.48.57来访问cnpm以及获取npm的注册服务。 按照之前创建的数据库来进行配置 这里将会列举一些常用的配置项，其余的一些配置项请自行参考 config/index.js 文件。 配置字段参考https://segmentfault.com/a/1190000005946580 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748enableCluster：是否启用 cluster-worker 模式启动服务，默认 false，生产环节推荐为 true;registryPort：API 专用的 registry 服务端口，默认 7001；webPort：Web 服务端口，默认 7002；bindingHost：监听绑定的 Host，默认为 127.0.0.1，如果外面架了一层本地的 Nginx 反向代理或者 Apache 反向代理的话推荐不用改；sessionSecret：session 用的盐；logdir：日志目录；uploadDir：临时上传文件目录；viewCache：视图模板缓存是否开启，默认为 false；enableCompress：是否开启 gzip 压缩，默认为 false；admins：管理员们，这是一个 JSON Object，对应各键名为各管理员的用户名，键值为其邮箱，默认为 &#123; fengmk2: 'fengmk2@gmail.com', admin: 'admin@cnpmjs.org', dead_horse: 'dead_horse@qq.com' &#125;；logoURL：Logo 地址，不过对于我这个已经把 CNPM 前端改得面目全非的人来说已经忽略了这个配置了；adBanner：广告 Banner 的地址；customReadmeFile：实际上我们看到的 cnpmjs.org 首页中间一大堆冗长的介绍是一个 Markdown 文件转化而成的，你可以设置该项来自行替换这个文件；customFooter：自定义页脚模板；npmClientName：默认为 cnpm，如果你有自己开发或者 fork 的 npm 客户端的话请改成自己的 CLI 命令，这个应该会在一些页面的说明处替换成你所写的；backupFilePrefix：备份目录；database：数据库相关配置，为一个对象，默认如果不配置将会是一个 ~/.cnpmjs.org/data.sqlite 的 SQLite；db：数据的库名；username：数据库用户名；password：数据库密码；dialect：数据库适配器，可选 "mysql"、"sqlite"、"postgres"、"mariadb"，默认为 "sqlite"；hsot：数据库地址；port：数据库端口；pool：数据库连接池相关配置，为一个对象；maxConnections：最大连接数，默认为 10；minConnections：最小连接数，默认为 0；maxIdleTime：单条链接最大空闲时间，默认为 30000 毫秒；storege：仅对 SQLite 配置有效，数据库地址，默认为 ~/.cnpmjs/data.sqlite；nfs：包文件系统处理对象，为一个 Node.js 对象，默认是 [fs-cnpm]() 这个包，并且配置在 ~/.cnpmjs/nfs 目录下，也就是说默认所有同步的包都会被放在这个目录下；开发者可以使用别的一些文件系统插件（如上传到又拍云等）,又或者自己去按接口开发一个逻辑层，这些都是后话了；registryHost：暂时还未试过，我猜是用于 Web 页面显示用的，默认为 r.cnpmjs.org；enablePrivate：是否开启私有模式，默认为 false；如果是私有模式则只有管理员能发布包，其它人只能从源站同步包；如果是非私有模式则所有登录用户都能发布包；scopes：非管理员发布包的时候只能用以 scopes 里面列举的命名空间为前缀来发布，如果没设置则无法发布，也就是说这是一个必填项，默认为 [ '@cnpm', '@cnpmtest', '@cnpm-test' ]，据苏千大大解释是为了便于管理以及让公司的员工自觉按需发布；更多关于 NPM scope 的说明请参见 npm-scope；privatePackages：就如该配置项的注释所述，出于历史包袱的原因，有些已经存在的私有包（可能之前是用 Git 的方式安装的）并没有以命名空间的形式来命名，而这种包本来是无法上传到 CNPM 的，这个配置项数组就是用来加这些例外白名单的，默认为一个空数组；sourceNpmRegistry：更新源 NPM 的 registry 地址，默认为 https://registry.npm.taobao.org；sourceNpmRegistryIsCNpm：源 registry 是否为 CNPM，默认为 true，如果你使用的源是官方 NPM 源，请将其设为 false；syncByInstall：如果安装包的时候发现包不存在，则尝试从更新源同步，默认为 true；syncModel：更新模式（不过我觉得是个 typo），有下面几种模式可以选择，默认为 "none";"none"：永不同步，只管理私有用户上传的包，其它源包会直接从源站获取；"exist"：定时同步已经存在于数据库的包；"all"：定时同步所有源站的包；syncInterval：同步间隔，默认为 "10m" 即十分钟；syncDevDependencies：是否同步每个包里面的 devDependencies 包们，默认为 false；badgeSubject：包的 badge 显示的名字，默认为 cnpm；userService：用户验证接口，默认为 null，即无用户相关功能也就是无法有用户去上传包，该部分需要自己实现接口功能并配置，如与公司的 Gitlab 相对接，这也是后话了；alwaysAuth：是否始终需要用户验证，即便是 $ cnpm install 等命令；httpProxy：代理地址设置，用于你在墙内源站在墙外的情况。 下面是index.js配置实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275'use strict';var mkdirp = require('mkdirp');var copy = require('copy-to');var path = require('path');var fs = require('fs');var os = require('os');var version = require('../package.json').version;var root = path.dirname(__dirname);var dataDir = path.join(process.env.HOME || root, '.cnpmjs.org');var config = &#123; version: version, dataDir: dataDir, /** * Cluster mode */ enableCluster: false, numCPUs: os.cpus().length, /* * server configure */ //cnpm提供两个端口：7001和7002，其中7001用于NPM的注册服务，7002用于Web访问。 registryPort: 7001, webPort: 7002, //bindingHost为安装cnpm的服务器ip地址，也就是在浏览器中只能通过访问http://192.168.48.57来访问cnpm以及获取npm的注册服务。 bindingHost: '192.168.1.12', // only binding on 127.0.0.1 for local access // debug mode // if in debug mode, some middleware like limit wont load // logger module will print to stdout debug: process.env.NODE_ENV === 'development', // page mode, enable on development env pagemock: process.env.NODE_ENV === 'development', // session secret sessionSecret: 'cnpmjs.org test session secret', // max request json body size jsonLimit: '10mb', // log dir name logdir: path.join(dataDir, 'logs'), // update file template dir uploadDir: path.join(dataDir, 'downloads'), // web page viewCache viewCache: false, // config for koa-limit middleware // for limit download rates limit: &#123; enable: false, token: 'koa-limit:download', limit: 1000, interval: 1000 * 60 * 60 * 24, whiteList: [], blackList: [], message: 'request frequency limited, any question, please contact fengmk2@gmail.com', &#125;, enableCompress: false, // enable gzip response or not // default system admins admins: &#123; // name: email fengmk2: 'fengmk2@gmail.com', admin: 'admin@cnpmjs.org', dead_horse: 'dead_horse@qq.com', &#125;, // email notification for errors // check https://github.com/andris9/Nodemailer for more informations mail: &#123; enable: false, appname: 'cnpmjs.org', from: 'cnpmjs.org mail sender &lt;adderss@gmail.com&gt;', service: 'gmail', auth: &#123; user: 'address@gmail.com', pass: 'your password' &#125; &#125;, logoURL: 'https://os.alipayobjects.com/rmsportal/oygxuIUkkrRccUz.jpg', // cnpm logo image url adBanner: '', customReadmeFile: '', // you can use your custom readme file instead the cnpm one customFooter: '', // you can add copyright and site total script html here npmClientName: 'cnpm', // use `$&#123;name&#125; install package` packagePageContributorSearch: true, // package page contributor link to search, default is true // max handle number of package.json `dependencies` property maxDependencies: 200, // backup filepath prefix backupFilePrefix: '/cnpm/backup/', /** * database config */ //配置数据库连接信息 database: &#123; db: 'cnpm', username: 'root', password: '123456', // the sql dialect of the database // - currently supported: 'mysql', 'sqlite', 'postgres', 'mariadb' dialect: 'mysql', // custom host; default: 127.0.0.1 host: '192.168.1.12', // custom port; default: 3306 port: 3306, // use pooling in order to reduce db connection overload and to increase speed // currently only for mysql and postgresql (since v1.5.0) pool: &#123; maxConnections: 10, minConnections: 0, maxIdleTime: 30000 &#125;, // the storage engine for 'sqlite' // default store into ~/.cnpmjs.org/data.sqlite storage: path.join(dataDir, 'data.sqlite'), logging: !!process.env.SQL_DEBUG, &#125;, // package tarball store in local filesystem by default nfs: require('fs-cnpm')(&#123; dir: path.join(dataDir, 'nfs') &#125;), // if set true, will 302 redirect to `nfs.url(dist.key)` downloadRedirectToNFS: false, // registry url name registryHost: 'r.cnpmjs.org', /** * registry mode config */ // enable private mode or not // private mode: only admins can publish, other users just can sync package from source npm // public mode: all users can publish enablePrivate: false, // registry scopes, if don't set, means do not support scopes scopes: [ '@cnpm', '@cnpmtest', '@cnpm-test' ], // some registry already have some private packages in global scope // but we want to treat them as scoped private packages, // so you can use this white list. privatePackages: [], /** * sync configs */ // the official npm registry // cnpm wont directly sync from this one // but sometimes will request it for some package infomations // please don't change it if not necessary officialNpmRegistry: 'https://registry.npmjs.com', officialNpmReplicate: 'https://replicate.npmjs.com', // sync source, upstream registry // If you want to directly sync from official npm's registry // please drop them an email first sourceNpmRegistry: 'https://registry.npm.taobao.org', // upstream registry is base on cnpm/cnpmjs.org or not // if your upstream is official npm registry, please turn it off sourceNpmRegistryIsCNpm: true, // if install return 404, try to sync from source registry syncByInstall: true, // sync mode select // none: do not sync any module, proxy all public modules from sourceNpmRegistry // exist: only sync exist modules // all: sync all modules //配置从仓库同步到本地（重要） syncModel: 'exist', // 'none', 'all', 'exist' syncConcurrency: 1, // sync interval, default is 10 minutes syncInterval: '10m', // sync polular modules, default to false // because cnpm can't auto sync tag change for now // so we want to sync popular modules to ensure their tags syncPopular: false, syncPopularInterval: '1h', // top 100 topPopular: 100, // sync devDependencies or not, default is false syncDevDependencies: false, // try to remove all deleted versions from original registry syncDeletedVersions: true, // changes streaming sync syncChangesStream: false, handleSyncRegistry: 'http://127.0.0.1:7001', // badge subject on http://shields.io/ badgePrefixURL: 'https://img.shields.io/badge', badgeSubject: 'cnpm', // custom user service, @see https://github.com/cnpm/cnpmjs.org/wiki/Use-Your-Own-User-Authorization // when you not intend to ingegrate with your company's user system, then use null, it would // use the default cnpm user system userService: null, // always-auth https://docs.npmjs.com/misc/config#always-auth // Force npm to always require authentication when accessing the registry, even for GET requests. alwaysAuth: false, // if you're behind firewall, need to request through http proxy, please set this // e.g.: `httpProxy: 'http://proxy.mycompany.com:8080'` httpProxy: null, // snyk.io root url snykUrl: 'https://snyk.io', // https://github.com/cnpm/cnpmjs.org/issues/1149 // if enable this option, must create module_abbreviated and package_readme table in database //enableAbbreviatedMetadata: false, //配置enableAbbreviatedMetadata为true（默认false）,解决不能同步。重要！ enableAbbreviatedMetadata: true, // global hook function: function* (envelope) &#123;&#125; // envelope format please see https://github.com/npm/registry/blob/master/docs/hooks/hooks-payload.md#payload globalHook: null, opensearch: &#123; host: '', &#125;,&#125;;if (process.env.NODE_ENV === 'test') &#123; config.enableAbbreviatedMetadata = true;&#125;if (process.env.NODE_ENV !== 'test') &#123; var customConfig; if (process.env.NODE_ENV === 'development') &#123; customConfig = path.join(root, 'config', 'config.js'); &#125; else &#123; // 1. try to load `$dataDir/config.json` first, not exists then goto 2. // 2. load config/config.js, everything in config.js will cover the same key in index.js customConfig = path.join(dataDir, 'config.json'); if (!fs.existsSync(customConfig)) &#123; customConfig = path.join(root, 'config', 'config.js'); &#125; &#125; if (fs.existsSync(customConfig)) &#123; copy(require(customConfig)).override(config); &#125;&#125;mkdirp.sync(config.logdir);mkdirp.sync(config.uploadDir);module.exports = config;config.loadConfig = function (customConfig) &#123; if (!customConfig) &#123; return; &#125; copy(customConfig).override(config);&#125;; 5、启动服务搞好配置之后就可以直接启动服务了。1$ node dispatch.js 官方脚本启动官方的其它一些指令，比如你可以用 NPM 的 script 来运行。 1$ npm run start 在 CNPM 里面，npm script 还有下面几种指令 12345npm run dev：调试模式启动；npm run test：跑测试；npm run start：启动 CNPM；npm run status：查看 CNPM 启动状态；npm run stop：停止 CNPM。 6、访问cnpmhttp://192.168.1.12:7002/ 二、问题总结问题1：安装cnmp依赖问题npm 默认使用官方源（国外），速度慢,已报错；可以安装cnpm，使用cnpm install安装（淘宝源） Default 1234//安装cnpm $ npm install -g cnpm --registry=https://registry.npm.taobao.org//使用cnpm执行安装依赖命令 $ cnpm install 问题2：设置同步后报错设置同步后，syncModel设置为exist或all,通过仓库下载模块报如下错误 1[c#0] [error] [connect] sync error: TypeError: Cannot read property ‘findAll’ of null 解决办法： 在index.js文件内设置enableAbbreviatedMetadata: true，问题解决。 参考：https://github.com/cnpm/cnpmjs.org/issues/1236]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>cnpmjs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos同步yum源到本地]]></title>
    <url>%2F2018%2F08%2F16%2Fcentos%E5%90%8C%E6%AD%A5yum%E6%BA%90%E5%88%B0%E6%9C%AC%E5%9C%B0%2F</url>
    <content type="text"><![CDATA[一、环境 os:centos 7.3 1611 应用：yum-utils 互联网源：阿里云 二、步骤删除/etc/yum.repos.d下所有源文件 1、下载源repo到本地1$ wget -O /etc/yum.repos.d/aliyun.repo https://mirrors.aliyun.com/repo/Centos-7.repo 2、安装yum-utils提供reporsync服务1$ yum install yum-utils -y 3、查看yum源仓库标识1234567891011[root@localhost yum.repos.d]# yum repolist已加载插件：fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.com源标识 源名称 状态base/7/x86_64 CentOS-7 - Base - mirrors.aliyun.com 9,591extras/7/x86_64 CentOS-7 - Extras - mirrors.aliyun.com 196updates/7/x86_64 CentOS-7 - Updates - mirrors.aliyun.com 657repolist: 10,444 4、根据源标识同步源到本地目录1[root@localhost ~]# reposync -r base -p /var/www/html/ #这里同步base目录到本地 注意： 部分互联网yum源不支持同步 参考资料http://www.cnblogs.com/chengd/articles/6912938.html https://www.2cto.com/net/201512/455901.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7安装独立显卡驱动]]></title>
    <url>%2F2018%2F08%2F16%2FCentos7%E5%AE%89%E8%A3%85%E7%8B%AC%E7%AB%8B%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[1 Centos7 安装独立显卡驱动 1.1 参考：https://blog.csdn.net/u013378306/article/details/69229919 1.2 安装基础依赖环境1$ Yum install gcc kernel-delve -y 注意事项，保证内核版本和源码版本一样，否则，安装报错误6： 查看内核版本：1$ ls /boot | grep vmlinu 查看源码包版本 1rpm -aq | grep kernel-devel 从上面的输出中可以看出内核版本号和内核源码版本。为了解决这个错误，需要从FC官方网站上下载与内核版本对应的源码包进行安装。可以在以下网站下载并安装：http://rpmfind.net/linux/rpm2html/search.php?query=kernel-devel 1.3 源码安装1.3.1 在英伟达官网下载相应驱动搜索出相应的驱动后，不要直接点，而是右健，Save Link as… 否则，会出现下载半天没动静的情况。 存放的路径上最好不要有中文。 我存放的路径是 ~/Downloads/NVIDIA-Linux-x86_64-346.47.run 1.3.2 屏蔽默认带有的nouveau使用su命令切换到root用户下: su root 打开/lib/modprobe.d/dist-blacklist.conf 12345# 将nvidiafb注释掉。# blacklist nvidiafb 然后添加以下语句：blacklist nouveauoptions nouveau modeset=0 1.3.3 重建initramfs image步骤1234567891011121314$ mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak$ dracut /boot/initramfs-$(uname -r).img $(uname -r)``` ### 1.3.4 修改运行级别为文本模式``` bash$ systemctl set-default multi-user.target``` ### 1.3.5 重新启动, 使用root用户登陆``` bash$ reboot 1.3.6 查看nouveau是否已经禁用1ls mod | grep nouveau 如果没有显示相关的内容，说明已禁用。 1.3.7 进入下载的驱动所在目录123$ chmod +x NVIDIA-Linux-x86_64-346.47.run$ ./NVIDIA-Linux-x86_64-346.47.run 安装过程中，选择accept 如果提示要修改xorg.conf，选择yes 1.3.8 修改运行级别回图形模式1systemctl set-default graphical.target 1.3.9 重新启动，OK在Applications–Other可以看见NVIDIA X Server Settings菜单。 1.4 问题：错误1： 1ERROR: The Nouveau kernel driver is currently in use by your system. This driver is incompatible with the NVIDIA driver, and must be disabled before proceeding. Please consult the NVIDIA driver README and your Linux distribution's documentation for details on how to correctly disable the Nouveau kernel driver. 解释：如果没有执行屏蔽nouveau操作，报以上错误。 错误2： 1unable to find the development too 'cc' in you path; please make sure that you have the package 'gcc 解决办法： 1$ yum install gcc 错误3： 123ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the 'kernel-source' or 'kernel-devel' RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the '--kernel-source-path' command line option. 解决办法：1$ yum install kernel-delve 错误5： 1ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the 'kernel-source' or 'kernel-devel' RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the '--kernel-source-path' command line option. 解决方法： 1$ ./NVIDIA-Linux-x86_64-390.67.run --kernel-source-path=/usr/src/kernels/3.10.0-862.3.2.el7.x86_64/ 错误6： 1ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most frequently when this kernel module was built against the wrong or improperly configured kernel sources, with a version of gcc that differs from the one used to build the target kernel, or if another driver, such as nouveau, is present and prevents the NVIDIA kernel module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA GPU installed in this system is supported by this NVIDIA Linux graphics driver release. Please see the log entries &apos;Kernel module load error&apos; and &apos;Kernel messages&apos; at the end of the file &apos;/var/log/nvidia-installer.log&apos; for more information. 解决办法： 可以通过以下方式查看内核版本和源码包版本：ls /boot | grep vmlinuz如果上面的命令输出中有多个内核，则按grub.conf中指定的文件为准。rpm -aq | grep kernel-develkernel-devel-2.6.35.13-92.fc14.i686从上面的输出中可以看出内核版本号和内核源码版本。为了解决这个错误，需要从FC官方网站上下载与内核版本对应的源码包进行安装。 可以在以下网站下载并安装： http://rpmfind.net/linux/rpm2html/search.php?query=kernel-devel 备注：执行更新内核操作好需要重新执行屏蔽nouveau，及重建initramfs image步骤。 警告： 123456**WARNING: nvidia-installer was forced to guess the X library path '/usr/lib64' and X module path '/usr/lib64/xorg/modules'; these paths were not queryable from the system. If X fails to find the NVIDIA X driver module, please install the `pkg-config` utility and the X.Org SDK/development package for your distribution and reinstall the driver. 字符模式安装警告信息，可忽略。 2 安装cuda参考：https://blog.csdn.net/claroja/article/details/81034147 错误： 123Installing the CUDA Toolkit in /usr/local/cuda-6.5 ...Missing recommended library: libGLU.soMissing recommended library: libXmu.so 解决：安装第三方软件 123$ yum install freeglut-devel libX11-devel libXi-devel libXmu-devel \$ make mesa-libGLU-devel 2.1 测试CUDA1234567[root@fengyun6 ~]# find / -name deviceQuery/root/NVIDIA_CUDA-9.0_Samples/1_Utilities/deviceQuery/usr/local/cuda-9.0/extras/demo_suite/deviceQuery/usr/local/cuda-9.0/samples/1_Utilities/deviceQuery 若出现以下信息，则表示安装成功 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[root@fengyun6 ~]# /usr/local/cuda-9.0/extras/demo_suite/deviceQuery/usr/local/cuda-9.0/extras/demo_suite/deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: "GeForce GTX 1080 Ti" CUDA Driver Version / Runtime Version 9.1 / 9.0 CUDA Capability Major/Minor version number: 6.1 Total amount of global memory: 11178 MBytes (11721113600 bytes) (28) Multiprocessors, (128) CUDA Cores/MP: 3584 CUDA Cores GPU Max Clock rate: 1645 MHz (1.64 GHz) Memory Clock rate: 5505 Mhz Memory Bus Width: 352-bit L2 Cache Size: 2883584 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 2 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 9.0, NumDevs = 1, Device0 = GeForce GTX 1080 TiResult = PASS 安装cudnn参考：https://www.cnblogs.com/mar-q/p/7482720.html 下载：https://developer.nvidia.com/rdp/cudnn-archive 3 安装cudnn1$ tar -xvf cudnn-8.0-linux-x64-v6.0.tgz -C /usr/local/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>显卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins各种触发方式介绍]]></title>
    <url>%2F2018%2F08%2F16%2Fjenkins%E5%90%84%E7%A7%8D%E8%A7%A6%E5%8F%91%E6%96%B9%E5%BC%8F%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[触发远程构建使用svn存储库hooks的post-commit,调用jenkins的api触发job。（存储库更新即触发构建，不能针对某个分支目录更新触发）Build after other projects are built某个projects触发构建后执行构建 Build periodicallyBuild periodically：周期进行项目构建（它不care源码是否发生变化），我的配置如下： 0 2 * （每天2:00 必须build一次源码） Poll SCMPoll SCM：定时检查源码变更（根据SCM软件的版本号），如果有更新就checkout最新code下来，然后执行构建动作。我的配置如下： /5 * （每5分钟检查一次源码变化）可针对某个分支目录更新触发构建]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx反向代理报504超时错误]]></title>
    <url>%2F2018%2F08%2F16%2FNginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E6%8A%A5504%E8%B6%85%E6%97%B6%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[一、nginx+tomcat 后端为tomcat,nginx代理报504超时错误。 问题描述： #错误 121.198.17.123 - - [06/Jul/2018:01:48:57 +0000] "POST /mapbj3/getticket HTTP/1.1" 504 537 "https://XXXXXXXXXX.com/walkcode3/index.html?openId=oB6UW0cF3Z_dnYXnz4tG4OFt7Rt0" "Mozilla/5.0 (Linux; Android 8.1; PACM00 Build/O11019; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.143 Crosswalk/24.53.595.0 XWEB/155 MMWEBSDK/19 Mobile Safari/537.36 MicroMessenger/6.6.6.1300(0x26060638) NetType/WIFI Language/zh_CN MicroMessenger/6.6.6.1300(0x26060638) NetType/WIFI Language/zh_CN miniProgram" "-"2018/07/06 01:48:57 [error] 6#6: *2573 upstream timed out (110: Connection timed out) while connecting to upstream, client: 1.198.17.123, server: , request: "POST /mapbj3/getticket HTTP/1.1", upstream: "http://123.149.236.180:8022/mapbj3//getticket", host: "XXXXXXXX.com", referrer: "https://XXXXXXX.com/walkcode3/index.html?openId=oB6UW0cF3Z_dnYXnz4tG4OFt7Rt0" 1、项目本地访问没问题，通过nginx访问报504错误； 2、重启nginx后正常，反复发生，其它项目代理没有问题； 3、搜索了一大推”NGINX 504 Gateway Time-out tomcat”,都是与php有关的,而默认优化的就是php配置的; 问题处理：修改/etc/nginx/nginx.conf,添加如下信息 1234567891011121314151617181920212223242526272829303132333435363738394041# cat /etc/nginx/nginx.confuser nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; #用于tomcat反向代理,解决nginx 504错误 proxy_connect_timeout 300; proxy_send_timeout 300; proxy_read_timeout 300; proxy_buffer_size 16k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; proxy_temp_file_write_size 128k; # ps:以timeout结尾配置项时间要配置大点&#125; 二、nginx+php(未验证)问题如上，问题处理添加如下内容 1234567891011121314151617181920212223242526272829303132333435363738394041user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; #用于php反向代理,解决nginx 504错误 #以fastcgi_*配置项是php用的 fastcgi_connect_timeout 1000; fastcgi_send_timeout 1000; fastcgi_read_timeout 1000; fastcgi_buffer_size 64k; fastcgi_buffers 8 128k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; fastcgi_intercept_errors on;&#125; 参考文档https://blog.csdn.net/lcj_star/article/details/76672748https://www.iyunv.com/thread-319236-1-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yapi部署]]></title>
    <url>%2F2018%2F08%2F16%2Fyapi%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、在线安装 1、安装nodejs下载压缩包，设置环境变量，这里不祥述。 2、安装mongodb12345678910111213141516171819202122232425262728293031323334353637383940# 添加yum源$ vim /etc/yum.repos.d/mongodb-3.4.repo #添加下面的内容，wq保存。 [mongodb-org-3.4]name=MongoDB Repositorybaseurl=https://repo.mongodb.org/yum/RedHat/$releasever/mongodb-org/3.4/x86_64/gpgcheck= 0enabled=1# 安装mongodbyum install -y mongodb-org# 配置mongod#启动:$ service mongod start[root@CENTSVR247 vendors]# mongo&amp;gt; use admin #切换到admin数据库switched to db admin#创建dba用户&amp;gt; db.createUser(... ... &#123;... ... user: "dba",... ... pwd: "dba",... ... roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ]... ... &#125;... ... )# 创建yapi数据库use yapiswitched to db yapi给yapi数据库添加test1用户,权限为读写db.createUser(... ... &#123;... ... user: "test1",... ... pwd: "test1",... ... roles: [... ... &#123; role: "readWrite", db: "yapi" &#125; ... ... ]... ... &#125;... ... ) 3、安装Yapi官方说明：https://yapi.ymfe.org/devops/index.html 方式一：可视化部署12$ npm install -g yapi-cli --registry https://registry.npm.taobao.org$ yapi server 根据提示，浏览器访问 http://部署YApi服务器的IP:9090。 填写完信息后，点击“开始部署”。（大概等待1分钟）退出当前状态 CTRL + C 修改配置 这里我们不急着根据提示进行启动，有些参数我们可以通过修改配置达到。 123456789101112131415161718192021222324#修改config.json$ vim /root/my-yapi/config.json 修改下面的内容（邮箱可以不用163的），wq保存。&#123; "port": "80", "adminAccount": "yizitadmin@yizit.cn", "db": &#123; "servername": "127.0.0.1", "DATABASE": "yapi", "port": "27017" &#125;, "mail": &#123; "enable": true, "host": "smtp.163.com", "port": 465, "from": "可用于发送邮件的163邮箱", "auth": &#123; "user": "163邮箱", "pass": "163邮箱对应的密码或授权码" &#125; &#125;&#125; 启动 切换到部署目录下 cd /root/my-yapi启动服务 1$ node vendors/server/app.js 由于修改了配置，所以直接访问 http://部署YApi服务器的IP/login。 访问http://部署YApi服务器的IP:3000/login 默认用户密码：admin@admin.com ymfe.org 方式二：命令行部署安装yapi 123456$ mkdir yapi$ cd yapi$ git clone https://github.com/YMFE/yapi.git vendors //或者下载 zip 包解压到 vendors 目录$ cp vendors/config_example.json ./config.json //复制完成后请修改相关配置$ cd vendors$ npm install --production --registry https://registry.npm.taobao.org 安装pm2 12$ cd vendors$ npm install -S pm2 初始化及启动yapi 12$ npm run install-server //安装程序会初始化数据库索引和管理员账号，管理员账号名可在 config.json 配置$ node server/app.js //启动服务器后，请访问 127.0.0.1:&#123;config.json配置的端口&#125;，初次运行会有个编译的过程，请耐心等候 使用pm2启动方式 1234# 启动$ npx pm2 start ./server/app.js# 停止$ npx pm2 stop all 二、离线安装 离线安装只能采用命令行部署方式 node安装不再详述。 内网安装mongodb解压mongodb-linux-x86_64-3.6.4.tgz并放入mongodb文件夹中 12$ tar -zxvf mongodb-linux-x86_64-3.6.4.tgz$ mv mongodb-linux-x86_64-3.6.4 mongodb 把mongodb放入环境变量中, 修改~/.bashrc, 加入以下内容 1export PATH=&amp;lt;mongodb文件夹的路径&amp;gt;/bin:$PATH 验证安装 12$ source ~/.bashrc$ mongo --version 创建dbdata/db文件夹和dblog文件夹(请自行确保这些文件夹的读写权限) 12$ mkdir -p dbdata/db$ mkdir dblog 启动mongodb服务 1$ sudo ./mongodb/bin/mongod --fork --dbpath ./dbdata --logpath ./dblog/log 配置 参考上文mongodb配置。 离线安装yapi在一台连接互联网的pc上安装node环境 在外网机器获取yapi源码并安装依赖使用git获取yapi源码, 如果没有git命令请按照对应平台的安装方法安装git. 创建一个新文件夹yapi, 使用clone将yapi源码放入vendors中: 123456$ mkdir yapi$ cd yapi$ git clone https://github.com/YMFE/yapi.git vendors$ cp vendors/config_example.json ./config.json$ cd vendors$ npm install --production 我这里还安装了pm2 1$ npm n install -S pm2 将创建的yapi文件夹打成压缩包得到yapi.tar.gz(其目录下有config.json和vendors) 1$ tar -czf yapi.tar.gz yapi 至此, 所有需要外部网络的操作已经完成, 可以进行内网部署. 启动yapi解压yapi.tar.gz 1$ tar -zxvf yapi.tar.gz 按需要修改yapi/config.json中的相关配置(例如管理员账号等) 初始化数据库: 12$ cd ./yapi/vendors$ npm run install-server 使用pm2启动 1$ npx n pm2 start ./server/app.js 启动完成后即可尝试访问yapi看是否成功, 具体地址要根据内网机器的ip和在config.json中配置的端口号 如果要关闭yapi服务, 可以使用 1$ npx n pm2 stop all 问题总结：两种方式安装yapi,按照正常方式安装都是无法安装的，有如下错误 方式1图形界面，yapi server 启动9090服务后，页面无法打开，会报错误，原因是无网络。方式2命令行安装，npm install –production 回报git错误，因需要联网git操作，原因无网络，npm使用私库代理也不行。 参考资料：https://yapi.ymfe.org/devops/index.html http://stlighter.github.io/2018/04/19/yapi%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2/ https://www.linuxidc.com/Linux/2018-01/150513.htm https://blog.csdn.net/luwei42768/article/details/78919073]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>yapi</tag>
        <tag>部署</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vsftp部署]]></title>
    <url>%2F2018%2F08%2F16%2Fvsftp%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、安装前的准备1、关闭防火墙或者开端口权限。一般是firewalld或者iptables。 12$ systemctl stop firewalld$ systemctl disable firewalld 防火墙配置 1234567891011121314151617181920212223242526272829303132333435363738如果开启防火墙，请开放21端口，被动模式下设置最大和最小端口范围，并在防火墙开放端口范围。修改vsftpd.conf文件，添加pasv_min_port=8022 #最小端口pasv_max_port=8030 #最大端口pasv_promiscuous=YES #开启pasv（被动模式）放开21端口：firewall-cmd –zone=public –add-port=21/tcp –permanent放开8022-8030端口：firewall-cmd –zone=public –add-port=8022-8030/tcp –permanent重新加载防火墙：firewall-cmd –reload可能用到的命令：永久开放 ftp 服務：firewall-cmd –add-service=ftp –permanent (关闭ftp服务：firewall-cmd –remove-service=ftp –permanent) （验证不起作用）systemctl start firewalld 启动防火墙服务firewall-cmd –add-service=ftp 暂时开放ftp服务firewall-cmd –add-service=ftp –permanent永久开放ftp服務firewall-cmd –remove-service=ftp –permanent永久关闭ftp服務systemctl restart firewalld 重启firewalld服务firewall-cmd –reload 重载配置文件firewall-cmd –query-service ftp查看服务的启动状态firewall-cmd –list-all 显示防火墙应用列表firewall-cmd –add-port=8001/tcp 添加自定义的开放端口iptables -L -n | grep 21 查看设定是否生效firewall-cmd –state 检测防火墙状态 2、关闭sellinux12345678910# 临时关闭$ setenforce 0# 永久关闭$ vi /etc/selinux/config修改SELINUX=disabled# 查看是否关闭$ getenforce 二、安装vsftpd12345$ yum install -y vsftpd# 启动$ systemctl start vsftpd# 自启$ systemctl enable vsftpd 三、配置vsftpd创建vsftpd使用的系统用户，主目录为/home/vsftpd，禁止ssh登录。创建之后所有虚拟用户使用这个系统用户访问文件。useradd vsftpd -d /home/vsftpd -s /bin/false 方式一、虚拟用户配置1、创建虚拟用户主目录，比如虚拟用户叫ftp1，执行下面的命令。1$ mkdir -p /home/vsftpd/ftp1 2、创建这个虚拟用户123456$ vi /etc/vsftpd/loginusers.conf增加ftp1123456# 这样就创建了ftp1这个虚拟用户，密码为123456 3、根据这个文件创建数据库文件12$ db_load -T -t hash -f /etc/vsftpd/loginusers.conf /etc/vsftpd/loginusers.db$ chmod 600 /etc/vsftpd/loginusers.db 4、启用这个数据库文件12345$ vi /etc/pam.d/vsftpd注释掉所有内容后，增加下面的内容auth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/loginusersaccount sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/loginusers 5、创建虚拟用户配置文件123456789101112$ mkdir /etc/vsftpd/userconf这里的文件名称必须与虚拟用户名一致$ vi /etc/vsftpd/userconf/ftp1增加下面的内容local_root=/home/vsftpd/ftp1/ # 设定主目录为/home/vsftpd/ftp1；虚拟用户的根目录(根据实际修改)write_enable=YES # 开启写权限anon_umask=022 # 设定上传后文件的权限掩码。anon_world_readable_only=NO anon_upload_enable=YES anon_mkdir_write_enable=YESanon_other_write_enable=YES 6、最后修改主配置文件1234567891011121314151617$ vi /etc/vsftpd/vsftpd.conf# 更改anonymous_enable=NO #设定不允许匿名访问local_enable=YES #设定本地用户可以访问。注：如使用虚拟宿主用户，在该项目设定为NO的情况下所有虚拟用户将无法访问。# 去掉注释chroot_local_user=YES # 禁止本地用户访问除主目录以外的目录chroot_list_enable=YES #使用户不能离开主目录ascii_upload_enable=YES #允许使用ASCII模式上传ascii_download_enable=YES #设定支持ASCII模式的上传和下载功能。xferlog_file=/var/log/vsftpd.log #设定vsftpd的服务日志保存路径。注意，该文件默认不存在。必须要手动touch出来# 增加guest_enable=YES # 设定启用虚拟用户功能。guest_username=vsftpd # 指定虚拟用户的宿主用户。centos 里面已经有内置的ftp用户了（注：此用户在chroot_list_file=/etc/vsftpd/chroot_list文件里所指定的用户）-RHEL/CentOS中已经有内置的ftp用户了user_config_dir=/etc/vsftpd/userconf # 设定虚拟用户个人vsftp的RHEL/CentOS FTP服务文件存放路径。存放虚拟用户个性的CentOS FTP服务文件(配置文件名=虚拟用户名)allow_writeable_chroot=YES # 最新版的vsftpd为了安全必须使用用户主目录（也就是/home/vsftpd/ftp1）没有写权限，才能登录，或者使用allow_writeable_chroot=YES 配置介绍： 12345678910anonymous_enable=NO 禁止匿名用户登录chroot_local_user=YES # 禁止用户访问除主目录以外的目录ascii_upload_enable=YES ascii_download_enable=YES 设定支持ASCII模式的上传和下载功能guest_enable=YES 启动虚拟用户guest_username=vsftpd 虚拟用户使用的系统用户名user_config_dir=/etc/vsftpd/userconf 虚拟用户使用的配置文件目录allow_writeable_chroot=YES 最新版的vsftpd为了安全必须用户主目录（也就是/home/vsftpd/ftp1）没有写权限，才能登录，或者使用allow_writeable_chroot=YES最后重启服务使配置生效systemctl restart vsftpd备注：设置ftp1目录权限为vsftpd, chown -R vsftpd:vsftpd /home/vsftpd/ftp1,否则没有权限创建目录等写的权限，设置777权限也不行。 方式二、本地用户配置配置 FTP 权限 1、了解 VSFTP 配置vsftpd 的配置目录为 /etc/vsftpd，包含下列的配置文件： vsftpd.conf 为主要配置文件 ftpusers 配置禁止访问 FTP 服务器的用户列表 user_list 配置用户访问控制——这里的用户默认情况（即在/etc/vsftpd/vsftpd.conf中设置了userlist_deny=YES）下也不能访问FTP服务器 2、阻止匿名访问和切换根目录匿名访问和切换根目录都会给服务器带来安全风险，我们把这两个功能关闭。编辑 /etc/vsftpd/vsftpd.conf，找到下面两处配置并修改： 12345# 禁用匿名用户访问 YES 改为NO anonymous_enable=NO# 禁止本地用户登出自己的FTP主目录。chroot_local_user=YES 3、修改默认根目录修改ftp的根目录只要修改/etc/vsftpd/vsftpd.conf文件即可： 加入如下几行： 123local_root=/var/www/htmlchroot_local_user=YESanon_root=/var/www/html 注：local_root 针对系统用户；anon_root 针对匿名用户。 编辑完成后保存配置，重新启动 FTP 服务 service vsftpd restart 其它配置项说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051anonymous_enable=YES #允许匿名登陆local_enable=YES # 开启本地用户访问write_enable=YES # ftp写的权限local_umask=022 # 设定上传后文件的权限掩码。dirmessage_enable=YES #连接打印的消息connect_from_port_20=YES # 连接端口20端口xferlog_std_format=YESidle_session_timeout=600data_connection_timeout=300accept_timeout=60connect_timeout=60ascii_upload_enable=YES #上传ascii_download_enable=YES #下载chroot_local_user=NO #是否限制用户在主目录活动chroot_list_enable=YES #启动限制用户的列表chroot_list_file=/etc/vsftpd/chroot_list #每行一个用户名allow_writeable_chroot=YES #允许写listen=NOlisten_ipv6=YESpasv_min_port=50000 允许ftp工具访问的端口起止端口pasv_max_port=60000pam_service_name=vsftpd #配置虚拟用户需要的userlist_enable=NO #配置yes之后，user_list的用户不能访问ftptcp_wrappers=YESchroot_list 文件需要自己建,内容一行一个用户名字anon_root=/data/ftp/public #修改匿名用户的访问路径 4、 创建 FTP 用户新建一个不能登录系统用户. 只用来登录ftp服务 ,这里如果没设置用户目录。默认是在home下： 1$ useradd ftpuser -d /home/vsftpd -s /bin/false 为ftpuser用户设置密码：passwd ftpuser 可能用到： 1234567设置用户的主目录：usermod -d /data/ftp ftpuser彻底删除用户：#userdel -rf Fuser //强制删除用户及相关目录文件变更用户属性：#usermod -s /sbin/nologinftpuser (/bin/bash：可以登录shell，/bin/false：禁止登录shell )查看当前服务：#netstat -lntp 备注：需要设置根目录权限为777 ，否则会出现无法写入的问题，chmod 777 /var/www/html 四、访问FTP通过 FTP 客户端工具访问 FTP 客户端工具众多，下面推荐两个常用的： WinSCP– Windows 下的 FTP 和 SFTP 连接客户端 FileZilla – 跨平台的 FTP 客户端，支持 Windows 和 Mac 本人测试时使用的是Xftp 打开Xftp软件，新建一个会话，输入对应的信息，点击确定(查看ip地址：ip addr) 选中我们新建的会话，点击连接 连接成功后就可以使用Xftp上传文件了 五、要使用Xshell连接，则需要安装openssh-service查看是否安装ssh安装包，CentOS是被访问者，所以需要安装ssh-server安装包（如果没任何输出显示表示没有安装 openssh-server，可以通过输入 yum install openssh-serve进行安装），查看命令为：rpm -qa | grep ssh，如下图所示，已经安装： 找到/etc/ssh目录下的sshd_config文件，修改一些参数。去掉端口和监听地址的注释；然后允许远程登录；再开启使用用户名密码作为连接验证开启sshd服务，service sshd start检查sshd是否开启，ps -e|grep sshd或者查看22端口是否被监听，netstat -an | grep 22使用Xshell进行连接，打开Xshell软件，新建一个会话，输入对应的信息，点击确定(查看ip地址：ip addr)选中我们新建的会话，点击连接连接成功后就可以使用Xshell执行命令了 参考：https://blog.csdn.net/will0532/article/details/79175478 https://blog.csdn.net/qq_32786873/article/details/78730303 https://www.cnblogs.com/huangye-dream/p/3454595.html https://blog.csdn.net/yifansj/article/details/72855484]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes创建资源对象yaml文件例子--pod]]></title>
    <url>%2F2018%2F08%2F16%2Fkubernetes%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1yaml%E6%96%87%E4%BB%B6%E4%BE%8B%E5%AD%90-pod%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172apiVersion: v1 #指定api版本，此值必须在kubectl apiversion中,通过kubectl api-versions命令查询 kind: Pod #指定创建资源的角色/类型 metadata: #资源的元数据/属性 name: web04-pod #资源的名字，在同一个namespace中必须唯一 labels: #设定资源的标签，详情请见http://blog.csdn.net/liyingke112/article/details/77482384 k8s-app: apache version: v1 kubernetes.io/cluster-service: "true" annotations: #自定义注解列表 - name: String #自定义注解名字 spec:#specification of the resource content 指定该资源的内容 restartPolicy: Always #表明该容器一直运行，默认k8s的策略，在此容器退出后，会立即创建一个相同的容器 nodeSelector: #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1 zone: node1 containers: - name: web04-pod #容器的名字 image: web:apache #容器使用的镜像地址 imagePullPolicy: Never #三个选择Always、Never、IfNotPresent，每次启动时检查和更新（从registery）images的策略， # Always，每次都检查 # Never，每次都不检查（不管本地是否有） # IfNotPresent，如果本地有就不检查，如果没有就拉取 command: ['sh'] #启动容器的运行命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT args: ["$(str)"] #启动容器的命令参数，对应Dockerfile中CMD参数 env: #指定容器中的环境变量 - name: str #变量的名字 value: "/etc/run.sh" #变量的值 resources: #资源管理，请求请见http://blog.csdn.net/liyingke112/article/details/77452630 requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行 cpu: 0.1 #CPU资源（核数），两种方式，浮点数或者是整数+m，0.1=100m，最少值为0.001核（1m） memory: 32Mi #内存使用量 limits: #资源限制 cpu: 0.5 memory: 32Mi ports: - containerPort: 80 #容器开发对外的端口 name: httpd #名称 protocol: TCP livenessProbe: #pod内容器健康检查的设置，详情请见http://blog.csdn.net/liyingke112/article/details/77531584 httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常 path: / #URI地址 port: 80 #host: 127.0.0.1 #主机地址 scheme: HTTP initialDelaySeconds: 180 #表明第一次检测在容器启动后多长时间后开始 timeoutSeconds: 5 #检测的超时时间 periodSeconds: 15 #检查间隔时间 #也可以用这种方法 #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常 # command: # - cat # - /tmp/health #也可以用这种方法 #tcpSocket: //通过tcpSocket检查健康 # port: number lifecycle: #生命周期管理 postStart: #容器运行之前运行的任务 exec: command: - 'sh' - 'yum upgrade -y' preStop:#容器关闭之前运行的任务 exec: command: ['service httpd stop'] volumeMounts: #详情请见http://blog.csdn.net/liyingke112/article/details/76577520 - name: volume #挂载设备的名字，与volumes[*].name 需要对应 mountPath: /data #挂载到容器的某个路径下 readOnly: True volumes: #定义一组挂载设备 - name: volume #定义一个挂载设备的名字 #meptyDir: &#123;&#125; hostPath: path: /opt #挂载设备类型为hostPath，路径为宿主机下的/opt,这里设备类型支持很多种 转载：https://blog.csdn.net/liyingke112/article/details/76155428]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>yaml</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes创建资源对象yaml文件例子--rc]]></title>
    <url>%2F2018%2F08%2F16%2Fkubernetes%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1yaml%E6%96%87%E4%BB%B6%E4%BE%8B%E5%AD%90-rc%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990apiVersion: v1 #指定api版本，此值必须在kubectl apiversion中kind: ReplicationController #指定创建资源的角色/类型metadata: #资源的元数据/属性 name: test-rc #资源的名字，在同一个namespace中必须唯一 labels: #设定资源的标签，详情请见http://blog.csdn.net/liyingke112/article/details/77482384 k8s-app: apache software: apache project: test app: test-rc version: v1 annotations: #自定义注解列表 - name: String #自定义注解名字spec: replicas: 2 #副本数量2 selector: #RC通过spec.selector来筛选要控制的Pod software: apache project: test app: test-rc version: v1 name: test-rc template: #这里Pod的定义 metadata: labels: #Pod的label，可以看到这个label与spec.selector相同 software: apache project: test app: test-rc version: v1 name: test-rc spec:#specification of the resource content 指定该资源的内容 restartPolicy: Always #表明该容器一直运行，默认k8s的策略，在此容器退出后，会立即创建一个相同的容器 nodeSelector: #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1 zone: node1 containers: - name: web04-pod #容器的名字 image: web:apache #容器使用的镜像地址 imagePullPolicy: Never #三个选择Always、Never、IfNotPresent，每次启动时检查和更新（从registery）images的策略， # Always，每次都检查 # Never，每次都不检查（不管本地是否有） # IfNotPresent，如果本地有就不检查，如果没有就拉取 command: ['sh'] #启动容器的运行命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT args: ["$(str)"] #启动容器的命令参数，对应Dockerfile中CMD参数 env: #指定容器中的环境变量 - name: str #变量的名字 value: "/etc/run.sh" #变量的值 resources: #资源管理，请求请见http://blog.csdn.net/liyingke112/article/details/77452630 requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行 cpu: 0.1 #CPU资源（核数），两种方式，浮点数或者是整数+m，0.1=100m，最少值为0.001核（1m） memory: 32Mi #内存使用量 limits: #资源限制 cpu: 0.5 memory: 32Mi ports: - containerPort: 80 #容器开发对外的端口 name: httpd #名称 protocol: TCP livenessProbe: #pod内容器健康检查的设置，详情请见http://blog.csdn.net/liyingke112/article/details/77531584 httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常 path: / #URI地址 port: 80 #host: 127.0.0.1 #主机地址 scheme: HTTP initialDelaySeconds: 180 #表明第一次检测在容器启动后多长时间后开始 timeoutSeconds: 5 #检测的超时时间 periodSeconds: 15 #检查间隔时间 #也可以用这种方法 #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常 # command: # - cat # - /tmp/health #也可以用这种方法 #tcpSocket: //通过tcpSocket检查健康 # port: number lifecycle: #生命周期管理 postStart: #容器运行之前运行的任务 exec: command: - 'sh' - 'yum upgrade -y' preStop:#容器关闭之前运行的任务 exec: command: ['service httpd stop'] volumeMounts: #详情请见http://blog.csdn.net/liyingke112/article/details/76577520 - name: volume #挂载设备的名字，与volumes[*].name 需要对应 mountPath: /data #挂载到容器的某个路径下 readOnly: True volumes: #定义一组挂载设备 - name: volume #定义一个挂载设备的名字 #meptyDir: &#123;&#125; hostPath: path: /opt #挂载设备类型为hostPath，路径为宿主机下的/opt,这里设备类型支持很多种]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>yaml</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批处理文件小技巧]]></title>
    <url>%2F2018%2F08%2F16%2F%E6%89%B9%E5%A4%84%E7%90%86%E6%96%87%E4%BB%B6%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1 批处理|多服务多窗口 123@echo off start cmd /k "d:&amp;&amp;cd d:\123&amp;&amp;echo 这是一个窗口&amp;&amp;pause&amp;&amp;ping 192.168.1.3&amp;&amp;ping 172.168.1.10" start cmd /k "d:&amp;&amp;cd d:\321&amp;&amp;echo 这是另一个窗口&amp;&amp;pause&amp;&amp;ping 192.168.1.20" #参数说明1、start 用来启动一个应用2、cmd /k 表示cmd后面的命令执行完后不关闭窗口。如果要在执行完成后关闭窗口可以用/c 。详细请使用cmd/?查看3、”命令1&amp;&amp;命令2&amp;&amp;..” 将要执行的多条命令使用引号全部包起来，并且在命令间用&amp;&amp;分隔。如果只有一条命令则不用引号也可以。 2 批处理|choice的使用示例示例1：12345678910111213141516171819@echo off ::设置CMD窗口字体颜色为0a 在CMD中输入命令 color /? 可查看颜色列表color 0a::设置CMD窗口显示模式为100列宽 20行高MODE con: COLS=100 LINES=20echo -------------------echo choice 命令示例echo -------------------echo.echo.:: /c按键列表 /m提示内容 /d默认选择 /t等待秒数 /d 必须和 /t同时出现choice /c abcde /m "请输入" /d e /t 5 ::用户选择的结果会按项目序号数字（从1开始）返回在errorlevel变量中if %errorlevel%==1 echo 你选择了aif %errorlevel%==2 echo 你选择了bif %errorlevel%==3 echo 你选择了cif %errorlevel%==4 echo 你选择了dif %errorlevel%==5 echo 你选择了e 示例2： 12345678910111213141516171819202122232425262728293031323334353637383940@echo offecho ***安装并启动mysql请输入1echo ***启动Tomcat请输入2echo *******************************************echo 备注：echo 1.通过关闭tomcat运行窗口关闭Tomcat###echo 2.以下为mysql启动及关闭操作###echo *******************************************echo ***启动mysql请输入3echo ***关闭mysql请输入4choice /C 1234 /m ""if %errorlevel%==1 goto installmysqlif %errorlevel%==2 goto starttomcatif %errorlevel%==3 goto startmysqlif %errorlevel%==4 goto stopmysql:installmysqlecho "设置Mysql环境变量"setx PATH "%PATH%;D:\iwhereEarth\mysql-5.6.39-winx64\bin" /mecho "设置Mysql环境变量成功"echo "安装MYSQL"pushd D:\iwhereEarth\mysql-5.6.39-winx64\binmysqld installnet start mysqlecho "Mysql安装并启动成功"goto end:starttomcatecho "启动Tomcat"start cmd /k "d:&amp;&amp;cd D:\iwhereEarth\apache-tomcat-7.0.63\bin&amp;&amp;echo Tomcat运行窗口&amp;&amp;catalina.bat run"goto end:startmysqlpushd D:\iwhereEarth\mysql-5.6.39-winx64\binnet start mysqlgoto end:stopmysqlpushd D:\iwhereEarth\mysql-5.6.39-winx64\binnet stop mysqlgoto end:endecho.&amp;pause 3 批处理|脚本设置环境变量12345678910111213::set system environment variable::set ant environment variablesetx ANT_HOME E:\tools\apache-ant-1.9.0 /msetx PATH "%PATH%;%ANT_HOME%\BIN" /m::set android environment variableSETX ANDROID_HOME E:\android\android-sdk-windows /mSETX PATH "%PATH;%ANDROID_HOME%\platform-tools" /mecho "设置成功"pauseexit 4 批处理|局域网备份4.1 环境windows server 2000 (理论上可以用于所有windows) 4.2 问题说明创建以下批处理bat文件，拷贝文件及移动文件到指定位置，Z盘为网络映射盘符。 添加计划任务，定时执行脚本。任务执行时，显示执行完成，但bat文件中脚本命令并没有执行。根据网上方法另存为ANSI编码文件；添加执行用户及密码，都不行。最后在一篇文章中找到方法。 12345echo ****#####start备份#####**** &gt;&gt;F:\shell\day1.logxcopy d:\usr\sap\*.* z:\sap\D /E /H /R /Y /I /d &gt;&gt;F:\shell\day1.logecho %date%."success backup d sap" &gt;&gt;F:\shell\day1.logmove /Y H:\backup\*.* z:\sap\database &gt;&gt;F:\shell\day1.logecho %date%."success move databackup files" &gt;&gt;F:\shell\day1.log 4.3 问题解决4.3.1 参考资料http://blog.csdn.net/tzysf/article/details/51302039 https://social.microsoft.com/Forums/zh-CN/cc080642-9368-467a-b781-d108f1d6c214/windows-server-2003-scheduled-taskbat?forum=windowsxpzhchs 4.3.2 处理方法在脚本开头添加如下命令 1NET USE Z: \\XXX.XXX.XXX.XXX\D$\XXXX "Password" /User:"Administrator" 例子： NET USE Z: \\172.0.0.22\backup &quot;Password&quot; /User:&quot;Administrator&quot; Z: #网络映射启动器盘符 172.0.0.22 #网络映射远程主机的ip地址 Password 和Administrator #连接远程网络驱动器的用户名、密码（远程主机的授权账户密码） 123456NET USE Z: \\172.0.0.22\backup "Password" /User:"Administrator"echo ****#####start备份#####**** &gt;&gt;F:\shell\day1.logxcopy d:\usr\sap\*.* z:\sap\D /E /H /R /Y /I /d &gt;&gt;F:\shell\day1.logecho %date%."success backup d sap" &gt;&gt;F:\shell\day1.logmove /Y H:\backup\*.* z:\sap\database &gt;&gt;F:\shell\day1.logecho %date%."success move databackup files" &gt;&gt;F:\shell\day1.log 就是在脚本开始，添加连接到驱动器的命令，脚本执行时不知道为什么没有默认确定连接账户密码。 5 在cmd/bat脚本中获取当前脚本文件所在目录在xp、2000、2003等系统中都可以正常双击运行。在win7/Win10系统中双击运行时，会以普通用户身份运行，此时所获取的文件路径的确是当前路径，而不是C:\Windows\System32。但是运行到安装netpay_Service.exe -install 的系统服务时，普通用户显然权限是不够的。 于是在InstllSvc_En.cmd右键选择“以管理员身份运行”，此时又会出问题，win7/win10可能出于安全问题考虑，此时获得的目录是C:\Windows\System32，于是提示netpay_Service.exe命令无效或程序文件不存在，执行出错。 此时在脚本开始尝试加入命令cd %cd%，来获取当前路径，实验得知，这行语句在xp等系统中有效，但是在win7/win10中依然无效。得到的目录依然是C:\Windows\System32。 百度一下才知道要使用cd /d %~dp0命令来获取脚本所在的目录。在脚本最开始添加cd /d %~dp0即可，如下： 123cd /d %~dp0netpay_Service.exe -installnetpay_Monitor.exe -install 在Windows XP~Windows 10系统上运行此脚本，确认都没有问题。]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>小知识</tag>
        <tag>基础运维</tag>
        <tag>bat</tag>
        <tag>运维开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux小知识]]></title>
    <url>%2F2018%2F08%2F16%2FLinux%E5%B0%8F%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1、centos -Tab键命令补全 12345$ yum install -y bash-completion#执行脚本，使其生效或重新登录生效$ source /usr/share/bash-completion/bash_completion 2、自定义命令补全1234567# 自定义生成kubectl命令补全source &lt;(kubectl completion bash)# 将命令添加入bashrc文件，每次登录当前用户执行，使命令补全生效，也可添加入其它开机执行的脚本echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc 3、ubuntu-Tab键命令补全编辑/etc/bash.bashrc 里面有这几行语句，去掉#注释 123456789101112131415#enable bash completion in interactive shellsif ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi 4、Linux设置环境变量在 linux 里设置环境变量的方法 （ export PATH ） 一般来说，配置交叉编译工具链的时候需要指定编译工具的路径，此时就需要设置环境变量。例如我的mips-linux-gcc编译器在“/opt/au1200_rm /build_tools/bin”目录下，build_tools就是我的编译工具，则有如下三种方法来设置环境变量： 4.1、直接用export命令：立即生效，重启丢失。 1export PATH=$PATH:/opt/au1200_rm/build_tools/bin 查看是否已经设好，可用命令export查看： 12345678#exportdeclare -x BASH_ENV=&quot;/root/.bashrc&quot;declare -x G_BROKEN_FILENAMES=&quot;1&quot;declare -x HISTSIZE=&quot;1000&quot;PATH里面已经有了我要加的编译器的路径。 4.2、修改profile文件：1234567$ vi /etc/profile在里面加入:export PATH=”$PATH:/opt/au1200_rm/build_tools/bin”$ . /etc/profile #执行命令使配置生效 4.3. 修改.bashrc文件：1234$ vi /root/.bashrc在里面加入：export PATH=”$PATH:/opt/au1200_rm/build_tools/bin” 后两种方法一般需要重新注销系统才能生效，最后可以通过echo命令测试一下： 12$ echo $PATH #输出变量看看输出里面是不是已经有了 /my_new_path这个路径了。 5、实时查看日志tail -f /var/log/messages 6、客户端(Xshell、SecureCRT)拖拉文件到服务器yum install lrzsz 7、YUM下载rpm包及依赖包#只下载bash-completion包到home目录，不进行安装yum install –downloadonly –downloaddir=/home bash-completion 8、删除多少前天备份#删除目录/mnt/backup_data下30天前后缀为.sql的文件 find /mnt/backup_data/ -name “*.sql” -type f -mtime +30 -exec rm -f {} \; 9、nmon显示系统性能显示工具123yum install nmonnmon 参考https://linux.cn/article-6467-1.html 10、Htop进程浏览器yum install htop 11、查看磁盘i/o工具12345678910111213$ yum install sysstat#每2秒更新一次，-m 以MB显示，-k以kb显示iostat -d -m 2#oriostat -d -k 2#oriostat -d -m /dev/sda1 12345yum install iotop#c查看哪个进程占用i/oiotop 使用详解参考：http://man.linuxde.net/iotop 12、禁止用户登录系统1234567#禁止usermod -s /bin/false ftpuser#开启usermod -s /bin/base ftpuser]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>小知识</tag>
        <tag>基础运维</tag>
        <tag>运维工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker小技巧]]></title>
    <url>%2F2018%2F08%2F15%2Fdocker%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1 dockers开启privileged解决centos7容器无法使用systemctl命令的问题1docker run -d -e "container=docker" --privileged=true -v /sys/fs/cgroup:/sys/fs/cgroup --name centos7 centos:7.5 /usr/sbin/init]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>小知识</tag>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
